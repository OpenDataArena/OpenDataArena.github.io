{
  "llama": [
    {
      "id": 1,
      "name": "meta-llama/Llama-3.1-8B-Instruct",
      "domain": "instruct",
      "general_avg": 56.54,
      "math_avg": 40.91,
      "code_avg": 39.84,
      "reasoning_avg": 36.24,
      "overall_avg": 43.38,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        80.9,
        60.325,
        40.98,
        43.96214
      ],
      "math_task_scores": [
        85.67,
        49.72,
        48.2,
        12.62,
        8.335
      ],
      "code_task_scores": [
        67.68,
        71.21,
        9.68,
        5.01,
        25.11,
        60.37
      ],
      "reasoning_task_scores": [
        79.32,
        62.85,
        24.24,
        0.42163,
        14.39
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.9
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 60.325
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96214
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.335
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.21
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.01
              },
              {
                "metric": "lcb_test_output",
                "score": 25.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42163
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.39
              }
            ]
          }
        ]
      }
    },
    {
      "id": 0,
      "name": "meta-llama/Llama-3.1-8B",
      "domain": "base",
      "general_avg": 39.05,
      "math_avg": 19.93,
      "code_avg": 19.47,
      "reasoning_avg": 34.6,
      "overall_avg": 28.26,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        64.37,
        20.3375,
        36.65,
        34.8271429
      ],
      "math_task_scores": [
        56.41,
        19.7,
        19.2,
        4.34,
        0.0
      ],
      "code_task_scores": [
        27.44,
        54.47,
        3.58,
        0.21,
        0.0,
        31.1
      ],
      "reasoning_task_scores": [
        80.34,
        62.56,
        29.8,
        0.312,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.3375
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.8271429
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 31.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.312
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 34.23,
      "math_avg": 17.2,
      "code_avg": 26.81,
      "reasoning_avg": 28.77,
      "overall_avg": 26.75,
      "overall_efficiency": -0.001867,
      "general_efficiency": -0.005959,
      "math_efficiency": -0.003371,
      "code_efficiency": 0.009075,
      "reasoning_efficiency": -0.007213,
      "general_scores": [
        44.46,
        30.76,
        31.37,
        27.4807692,
        46.54,
        34.1625,
        31.7,
        26.2907692,
        47.65,
        31.745,
        31.29,
        27.2678571
      ],
      "math_scores": [
        55.42,
        10.02,
        11.4,
        7.66,
        3.33,
        54.81,
        10.46,
        10.6,
        7.7,
        0.0,
        57.77,
        10.92,
        10.6,
        7.36,
        0.0
      ],
      "code_scores": [
        35.98,
        45.53,
        4.3,
        27.56,
        9.73,
        24.39,
        37.2,
        50.58,
        1.79,
        29.23,
        17.65,
        27.44,
        39.02,
        49.03,
        3.58,
        29.85,
        18.55,
        31.1
      ],
      "reasoning_scores": [
        68.81,
        49.22,
        23.23,
        0.28815217,
        1.48,
        68.14,
        46.13,
        25.25,
        0.29815217,
        3.12,
        67.12,
        48.34,
        27.27,
        0.31217391,
        2.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.4
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.88
              },
              {
                "metric": "lcb_test_output",
                "score": 15.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 27.64
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.02
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.82,
          "math_avg": -2.73,
          "code_avg": 7.34,
          "reasoning_avg": -5.83,
          "overall_avg": -1.51,
          "overall_efficiency": -0.001867,
          "general_efficiency": -0.005959,
          "math_efficiency": -0.003371,
          "code_efficiency": 0.009075,
          "reasoning_efficiency": -0.007213,
          "general_task_scores": [
            -18.15,
            11.88,
            -5.2,
            -7.82
          ],
          "math_task_scores": [
            -0.41,
            -9.23,
            -8.33,
            3.23,
            1.11
          ],
          "code_task_scores": [
            9.96,
            -6.09,
            -0.36,
            28.67,
            15.31,
            -3.46
          ],
          "reasoning_task_scores": [
            -12.32,
            -14.66,
            -4.55,
            -0.01,
            2.37
          ]
        },
        "vs_instruct": {
          "general_avg": -22.32,
          "math_avg": -23.71,
          "code_avg": -13.04,
          "reasoning_avg": -7.48,
          "overall_avg": -16.63,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.68,
            -28.11,
            -9.53,
            -16.95
          ],
          "math_task_scores": [
            -29.67,
            -39.25,
            -37.33,
            -5.05,
            -7.22
          ],
          "code_task_scores": [
            -30.28,
            -22.83,
            -6.46,
            23.87,
            -9.8,
            -32.73
          ],
          "reasoning_task_scores": [
            -11.3,
            -14.95,
            1.01,
            -0.12,
            -12.02
          ]
        }
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 40.05,
      "math_avg": 10.36,
      "code_avg": 23.8,
      "reasoning_avg": 27.95,
      "overall_avg": 25.54,
      "overall_efficiency": -0.052308,
      "general_efficiency": 0.019319,
      "math_efficiency": -0.184069,
      "code_efficiency": 0.083415,
      "reasoning_efficiency": -0.127898,
      "general_scores": [
        53.33,
        42.8125,
        34.36,
        28.7385714,
        56.23,
        44.035,
        35.52,
        28.7321429,
        54.87,
        36.945,
        36.2,
        28.8364286
      ],
      "math_scores": [
        26.84,
        9.46,
        8.4,
        6.05,
        0.0,
        24.87,
        9.16,
        8.8,
        6.01,
        3.33,
        25.25,
        9.72,
        7.8,
        6.35,
        3.33
      ],
      "code_scores": [
        32.93,
        45.91,
        3.23,
        24.84,
        5.43,
        24.39,
        25.0,
        50.19,
        2.51,
        27.56,
        18.1,
        23.7,
        28.05,
        48.64,
        3.94,
        28.18,
        9.05,
        26.83
      ],
      "reasoning_scores": [
        70.51,
        37.81,
        25.76,
        0.3026087,
        2.82,
        71.19,
        43.84,
        25.76,
        0.29945652,
        2.82,
        70.85,
        38.37,
        26.26,
        0.30956522,
        2.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.86
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 24.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.85
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.01
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.0,
          "math_avg": -9.57,
          "code_avg": 4.34,
          "reasoning_avg": -6.65,
          "overall_avg": -2.72,
          "overall_efficiency": -0.052308,
          "general_efficiency": 0.019319,
          "math_efficiency": -0.184069,
          "code_efficiency": 0.083415,
          "reasoning_efficiency": -0.127898,
          "general_task_scores": [
            -9.56,
            20.92,
            -1.29,
            -6.06
          ],
          "math_task_scores": [
            -30.76,
            -10.25,
            -10.87,
            1.8,
            2.22
          ],
          "code_task_scores": [
            1.22,
            -6.22,
            -0.35,
            26.65,
            10.86,
            -6.13
          ],
          "reasoning_task_scores": [
            -9.49,
            -22.55,
            -3.87,
            -0.01,
            2.67
          ]
        },
        "vs_instruct": {
          "general_avg": -16.49,
          "math_avg": -30.55,
          "code_avg": -16.04,
          "reasoning_avg": -8.29,
          "overall_avg": -17.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -26.09,
            -19.07,
            -5.62,
            -15.19
          ],
          "math_task_scores": [
            -60.02,
            -40.27,
            -39.87,
            -6.48,
            -6.12
          ],
          "code_task_scores": [
            -39.02,
            -22.96,
            -6.45,
            21.85,
            -14.25,
            -35.4
          ],
          "reasoning_task_scores": [
            -8.47,
            -22.84,
            1.69,
            -0.12,
            -11.72
          ]
        }
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 40.01,
      "math_avg": 17.35,
      "code_avg": 26.92,
      "reasoning_avg": 26.17,
      "overall_avg": 27.61,
      "overall_efficiency": -0.012477,
      "general_efficiency": 0.018483,
      "math_efficiency": -0.049703,
      "code_efficiency": 0.143413,
      "reasoning_efficiency": -0.162101,
      "general_scores": [
        54.48,
        41.765,
        33.5,
        28.625,
        57.61,
        41.5825,
        33.71,
        25.4623077,
        57.71,
        42.3275,
        35.82,
        27.4953846
      ],
      "math_scores": [
        48.67,
        13.38,
        12.8,
        9.35,
        0.0,
        50.57,
        13.64,
        13.2,
        9.98,
        0.0,
        54.13,
        14.0,
        12.4,
        8.06,
        0.0
      ],
      "code_scores": [
        24.39,
        52.14,
        2.51,
        23.8,
        20.36,
        27.44,
        34.15,
        53.7,
        3.58,
        21.92,
        20.36,
        28.66,
        34.76,
        56.81,
        6.09,
        21.29,
        20.36,
        32.32
      ],
      "reasoning_scores": [
        68.14,
        37.05,
        23.23,
        0.32521739,
        2.08,
        71.53,
        29.86,
        21.21,
        0.3526087,
        2.23,
        71.53,
        39.27,
        23.23,
        0.32434783,
        2.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.34
              },
              {
                "metric": "lcb_test_output",
                "score": 20.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 29.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.4
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.39
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.96,
          "math_avg": -2.58,
          "code_avg": 7.46,
          "reasoning_avg": -8.43,
          "overall_avg": -0.65,
          "overall_efficiency": -0.012477,
          "general_efficiency": 0.018483,
          "math_efficiency": -0.049703,
          "code_efficiency": 0.143413,
          "reasoning_efficiency": -0.162101,
          "general_task_scores": [
            -7.77,
            21.55,
            -2.31,
            -7.64
          ],
          "math_task_scores": [
            -5.29,
            -6.03,
            -6.4,
            4.79,
            0.0
          ],
          "code_task_scores": [
            3.66,
            -0.25,
            0.48,
            22.13,
            20.36,
            -1.63
          ],
          "reasoning_task_scores": [
            -9.94,
            -27.17,
            -7.24,
            0.02,
            2.18
          ]
        },
        "vs_instruct": {
          "general_avg": -16.53,
          "math_avg": -23.56,
          "code_avg": -12.92,
          "reasoning_avg": -10.07,
          "overall_avg": -15.77,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.3,
            -18.44,
            -6.64,
            -16.77
          ],
          "math_task_scores": [
            -34.55,
            -36.05,
            -35.4,
            -3.49,
            -8.34
          ],
          "code_task_scores": [
            -36.58,
            -16.99,
            -5.62,
            17.33,
            -4.75,
            -30.9
          ],
          "reasoning_task_scores": [
            -8.92,
            -27.46,
            -1.68,
            -0.09,
            -12.21
          ]
        }
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 38.82,
      "math_avg": 16.11,
      "code_avg": 16.28,
      "reasoning_avg": 27.9,
      "overall_avg": 24.78,
      "overall_efficiency": -0.232003,
      "general_efficiency": -0.015068,
      "math_efficiency": -0.254436,
      "code_efficiency": -0.212215,
      "reasoning_efficiency": -0.446297,
      "general_scores": [
        65.61,
        38.7275,
        30.98,
        23.6092857,
        62.11,
        37.7775,
        25.67,
        23.5507143,
        61.78,
        42.2075,
        30.94,
        22.8771429
      ],
      "math_scores": [
        43.75,
        13.84,
        11.6,
        8.69,
        0.0,
        45.26,
        13.92,
        13.8,
        7.9,
        0.0,
        48.52,
        13.76,
        12.2,
        8.42,
        0.0
      ],
      "code_scores": [
        0.61,
        54.86,
        2.87,
        6.47,
        4.75,
        23.78,
        10.98,
        55.64,
        2.87,
        17.95,
        2.26,
        3.66,
        9.15,
        56.03,
        1.43,
        14.61,
        1.36,
        23.78
      ],
      "reasoning_scores": [
        64.41,
        52.1,
        24.24,
        0.33684783,
        3.12,
        61.36,
        49.95,
        19.7,
        0.3623913,
        3.26,
        68.81,
        43.9,
        23.23,
        0.35630435,
        3.41
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.2
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.35
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.91
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.01
              },
              {
                "metric": "lcb_test_output",
                "score": 2.79
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 17.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.86
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.26
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.23,
          "math_avg": -3.82,
          "code_avg": -3.19,
          "reasoning_avg": -6.7,
          "overall_avg": -3.48,
          "overall_efficiency": -0.232003,
          "general_efficiency": -0.015068,
          "math_efficiency": -0.254436,
          "code_efficiency": -0.212215,
          "reasoning_efficiency": -0.446297,
          "general_task_scores": [
            -1.2,
            19.23,
            -7.45,
            -11.48
          ],
          "math_task_scores": [
            -10.57,
            -5.86,
            -6.67,
            4.0,
            0.0
          ],
          "code_task_scores": [
            -20.53,
            1.04,
            -1.19,
            12.8,
            2.79,
            -14.03
          ],
          "reasoning_task_scores": [
            -15.48,
            -13.91,
            -7.41,
            0.04,
            3.26
          ]
        },
        "vs_instruct": {
          "general_avg": -17.72,
          "math_avg": -24.8,
          "code_avg": -23.56,
          "reasoning_avg": -8.34,
          "overall_avg": -18.61,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -17.73,
            -20.76,
            -11.78,
            -20.61
          ],
          "math_task_scores": [
            -39.83,
            -35.88,
            -35.67,
            -4.28,
            -8.34
          ],
          "code_task_scores": [
            -60.77,
            -15.7,
            -7.29,
            8.0,
            -22.32,
            -43.3
          ],
          "reasoning_task_scores": [
            -14.46,
            -14.2,
            -1.85,
            -0.07,
            -11.13
          ]
        }
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 44.27,
      "math_avg": 18.78,
      "code_avg": 32.36,
      "reasoning_avg": 32.05,
      "overall_avg": 31.86,
      "overall_efficiency": 0.051462,
      "general_efficiency": 0.074574,
      "math_efficiency": -0.016438,
      "code_efficiency": 0.184167,
      "reasoning_efficiency": -0.036453,
      "general_scores": [
        61.71,
        49.03,
        34.23,
        33.6464286,
        56.77,
        49.6325,
        35.49,
        33.1392857,
        59.98,
        48.0125,
        35.8,
        33.755
      ],
      "math_scores": [
        52.24,
        16.48,
        15.0,
        8.69,
        0.0,
        55.04,
        16.22,
        15.2,
        7.79,
        0.0,
        55.34,
        16.2,
        16.2,
        7.29,
        0.0
      ],
      "code_scores": [
        43.9,
        57.98,
        7.89,
        30.06,
        21.27,
        39.63,
        44.51,
        53.7,
        6.45,
        31.52,
        12.67,
        39.63,
        39.63,
        55.64,
        6.09,
        28.81,
        24.66,
        38.41
      ],
      "reasoning_scores": [
        73.56,
        59.9,
        25.76,
        0.33619565,
        2.37,
        72.54,
        57.8,
        25.25,
        0.3301087,
        1.93,
        72.2,
        58.85,
        26.77,
        0.34380435,
        2.82
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.51
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.92
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.13
              },
              {
                "metric": "lcb_test_output",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 39.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.22,
          "math_avg": -1.15,
          "code_avg": 12.89,
          "reasoning_avg": -2.55,
          "overall_avg": 3.6,
          "overall_efficiency": 0.051462,
          "general_efficiency": 0.074574,
          "math_efficiency": -0.016438,
          "code_efficiency": 0.184167,
          "reasoning_efficiency": -0.036453,
          "general_task_scores": [
            -4.88,
            28.55,
            -1.48,
            -1.32
          ],
          "math_task_scores": [
            -2.2,
            -3.4,
            -3.73,
            3.58,
            0.0
          ],
          "code_task_scores": [
            15.24,
            1.3,
            3.23,
            29.92,
            19.53,
            8.12
          ],
          "reasoning_task_scores": [
            -7.57,
            -3.71,
            -3.87,
            0.03,
            2.37
          ]
        },
        "vs_instruct": {
          "general_avg": -12.28,
          "math_avg": -22.13,
          "code_avg": -7.49,
          "reasoning_avg": -4.19,
          "overall_avg": -11.52,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.41,
            -11.44,
            -5.81,
            -10.45
          ],
          "math_task_scores": [
            -31.46,
            -33.42,
            -32.73,
            -4.7,
            -8.34
          ],
          "code_task_scores": [
            -25.0,
            -15.44,
            -2.87,
            25.12,
            -5.58,
            -21.15
          ],
          "reasoning_task_scores": [
            -6.55,
            -4.0,
            1.69,
            -0.08,
            -12.02
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 40.68,
      "math_avg": 19.07,
      "code_avg": 25.74,
      "reasoning_avg": 31.97,
      "overall_avg": 29.37,
      "overall_efficiency": 0.007734,
      "general_efficiency": 0.011457,
      "math_efficiency": -0.005982,
      "code_efficiency": 0.043853,
      "reasoning_efficiency": -0.018389,
      "general_scores": [
        45.63,
        47.9275,
        35.91,
        32.5321429,
        44.83,
        49.1075,
        34.16,
        31.6115385,
        50.2,
        49.3525,
        34.78,
        32.1721429
      ],
      "math_scores": [
        55.88,
        15.2,
        15.4,
        8.11,
        3.33,
        54.74,
        15.66,
        14.4,
        7.72,
        0.0,
        56.18,
        15.6,
        16.2,
        7.7,
        0.0
      ],
      "code_scores": [
        34.15,
        55.64,
        5.02,
        12.32,
        18.33,
        18.9,
        38.41,
        52.53,
        3.23,
        18.16,
        17.42,
        23.78,
        31.1,
        57.59,
        5.73,
        22.34,
        14.48,
        34.15
      ],
      "reasoning_scores": [
        75.25,
        54.41,
        27.78,
        0.31336957,
        2.82,
        72.88,
        55.51,
        28.79,
        0.35206522,
        2.37,
        75.25,
        56.47,
        24.24,
        0.33565217,
        2.82
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.55
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.61
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 25.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.46
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.64,
          "math_avg": -0.86,
          "code_avg": 6.27,
          "reasoning_avg": -2.63,
          "overall_avg": 1.11,
          "overall_efficiency": 0.007734,
          "general_efficiency": 0.011457,
          "math_efficiency": -0.005982,
          "code_efficiency": 0.043853,
          "reasoning_efficiency": -0.018389,
          "general_task_scores": [
            -17.48,
            28.46,
            -1.7,
            -2.72
          ],
          "math_task_scores": [
            -0.81,
            -4.21,
            -3.87,
            3.5,
            1.11
          ],
          "code_task_scores": [
            7.11,
            0.78,
            1.08,
            17.4,
            16.74,
            -5.49
          ],
          "reasoning_task_scores": [
            -5.88,
            -7.1,
            -2.86,
            0.02,
            2.67
          ]
        },
        "vs_instruct": {
          "general_avg": -15.86,
          "math_avg": -21.83,
          "code_avg": -14.11,
          "reasoning_avg": -4.27,
          "overall_avg": -14.02,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.01,
            -11.53,
            -6.03,
            -11.85
          ],
          "math_task_scores": [
            -30.07,
            -34.23,
            -32.87,
            -4.78,
            -7.22
          ],
          "code_task_scores": [
            -33.13,
            -15.96,
            -5.02,
            12.6,
            -8.37,
            -34.76
          ],
          "reasoning_task_scores": [
            -4.86,
            -7.39,
            2.7,
            -0.09,
            -11.72
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 40.22,
      "math_avg": 23.55,
      "code_avg": 23.82,
      "reasoning_avg": 33.37,
      "overall_avg": 30.24,
      "overall_efficiency": 1.82394,
      "general_efficiency": 1.080517,
      "math_efficiency": 3.337638,
      "code_efficiency": 4.013427,
      "reasoning_efficiency": -1.135824,
      "general_scores": [
        62.58,
        29.2525,
        32.64,
        36.4978571,
        62.87,
        30.36,
        31.92,
        35.9578571,
        62.6,
        29.9575,
        31.62,
        36.3535714
      ],
      "math_scores": [
        67.85,
        20.3,
        19.0,
        10.52,
        0.0,
        66.34,
        19.82,
        19.6,
        10.46,
        0.0,
        67.55,
        20.5,
        20.8,
        10.48,
        0.0
      ],
      "code_scores": [
        35.37,
        59.14,
        0.36,
        17.95,
        0.45,
        28.05,
        33.54,
        58.75,
        0.36,
        18.16,
        0.23,
        32.32,
        35.37,
        58.37,
        0.36,
        19.21,
        0.23,
        30.49
      ],
      "reasoning_scores": [
        75.25,
        61.43,
        26.26,
        0.27391304,
        4.9,
        72.2,
        61.26,
        27.27,
        0.27641304,
        4.75,
        73.9,
        61.3,
        26.77,
        0.27717391,
        4.45
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.25
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.44
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.78
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.7
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.17,
          "math_avg": 3.62,
          "code_avg": 4.35,
          "reasoning_avg": -1.23,
          "overall_avg": 1.98,
          "overall_efficiency": 1.82394,
          "general_efficiency": 1.080517,
          "math_efficiency": 3.337638,
          "code_efficiency": 4.013427,
          "reasoning_efficiency": -1.135824,
          "general_task_scores": [
            -1.69,
            9.52,
            -4.59,
            1.44
          ],
          "math_task_scores": [
            10.84,
            0.51,
            0.6,
            6.15,
            0.0
          ],
          "code_task_scores": [
            7.32,
            4.28,
            -3.22,
            18.23,
            0.3,
            -0.81
          ],
          "reasoning_task_scores": [
            -6.56,
            -1.23,
            -3.03,
            -0.03,
            4.7
          ]
        },
        "vs_instruct": {
          "general_avg": -16.32,
          "math_avg": -17.36,
          "code_avg": -16.03,
          "reasoning_avg": -2.87,
          "overall_avg": -13.15,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -18.22,
            -30.47,
            -8.92,
            -7.69
          ],
          "math_task_scores": [
            -18.42,
            -29.51,
            -28.4,
            -2.13,
            -8.34
          ],
          "code_task_scores": [
            -32.92,
            -12.46,
            -9.32,
            13.43,
            -24.81,
            -30.08
          ],
          "reasoning_task_scores": [
            -5.54,
            -1.52,
            2.53,
            -0.14,
            -9.69
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 8,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 48.66,
      "math_avg": 34.5,
      "code_avg": 30.83,
      "reasoning_avg": 34.15,
      "overall_avg": 37.03,
      "overall_efficiency": 0.008825,
      "general_efficiency": 0.009668,
      "math_efficiency": 0.014654,
      "code_efficiency": 0.011431,
      "reasoning_efficiency": -0.000451,
      "general_scores": [
        67.99,
        54.355,
        38.47,
        34.4692857,
        65.39,
        55.04,
        40.69,
        35.2471429,
        65.32,
        54.7825,
        38.87,
        33.2485714
      ],
      "math_scores": [
        78.32,
        39.04,
        41.2,
        13.37,
        0.0,
        80.89,
        40.18,
        38.6,
        14.23,
        0.0,
        79.38,
        39.68,
        39.8,
        12.76,
        0.0
      ],
      "code_scores": [
        47.56,
        48.64,
        7.53,
        18.37,
        29.64,
        31.71,
        45.12,
        50.58,
        6.45,
        13.15,
        26.7,
        34.76,
        47.56,
        49.42,
        6.81,
        29.44,
        24.89,
        36.59
      ],
      "reasoning_scores": [
        77.97,
        62.69,
        17.68,
        0.34293478,
        10.09,
        79.66,
        60.88,
        17.17,
        0.36043478,
        13.06,
        79.32,
        62.07,
        17.17,
        0.3451087,
        13.5
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.75
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.32
              },
              {
                "metric": "lcb_test_output",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 34.35
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.98
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.88
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.22
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.61,
          "math_avg": 14.57,
          "code_avg": 11.36,
          "reasoning_avg": -0.45,
          "overall_avg": 8.77,
          "overall_efficiency": 0.008825,
          "general_efficiency": 0.009668,
          "math_efficiency": 0.014654,
          "code_efficiency": 0.011431,
          "reasoning_efficiency": -0.000451,
          "general_task_scores": [
            1.86,
            34.39,
            2.69,
            -0.51
          ],
          "math_task_scores": [
            23.12,
            19.93,
            20.67,
            9.11,
            0.0
          ],
          "code_task_scores": [
            19.31,
            -4.92,
            3.35,
            20.11,
            27.08,
            3.25
          ],
          "reasoning_task_scores": [
            -1.36,
            -0.68,
            -12.46,
            0.04,
            12.22
          ]
        },
        "vs_instruct": {
          "general_avg": -7.89,
          "math_avg": -6.41,
          "code_avg": -9.01,
          "reasoning_avg": -2.09,
          "overall_avg": -6.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -14.67,
            -5.6,
            -1.64,
            -9.64
          ],
          "math_task_scores": [
            -6.14,
            -10.09,
            -8.33,
            0.83,
            -8.34
          ],
          "code_task_scores": [
            -20.93,
            -21.66,
            -2.75,
            15.31,
            1.97,
            -26.02
          ],
          "reasoning_task_scores": [
            -0.34,
            -0.97,
            -6.9,
            -0.07,
            -2.17
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 43.58,
      "math_avg": 29.13,
      "code_avg": 31.83,
      "reasoning_avg": 33.06,
      "overall_avg": 34.4,
      "overall_efficiency": 0.00613,
      "general_efficiency": 0.004525,
      "math_efficiency": 0.009185,
      "code_efficiency": 0.012344,
      "reasoning_efficiency": -0.001535,
      "general_scores": [
        50.63,
        50.9275,
        40.8,
        32.3721429,
        52.1,
        52.215,
        40.62,
        32.1278571,
        48.24,
        50.3825,
        40.58,
        31.9478571
      ],
      "math_scores": [
        74.53,
        30.68,
        32.0,
        11.72,
        0.0,
        73.62,
        29.66,
        27.8,
        10.28,
        0.0,
        74.53,
        31.88,
        30.6,
        9.64,
        0.0
      ],
      "code_scores": [
        46.34,
        52.53,
        6.09,
        29.65,
        15.16,
        41.46,
        47.56,
        52.53,
        3.58,
        27.56,
        16.29,
        40.24,
        51.83,
        56.42,
        4.66,
        27.56,
        16.29,
        37.2
      ],
      "reasoning_scores": [
        76.61,
        54.92,
        30.3,
        0.38608696,
        4.6,
        79.32,
        54.08,
        26.77,
        0.3676087,
        4.45,
        78.98,
        54.66,
        26.77,
        0.34847826,
        3.41
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.23
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.26
              },
              {
                "metric": "lcb_test_output",
                "score": 15.91
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 39.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.95
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.53,
          "math_avg": 9.2,
          "code_avg": 12.36,
          "reasoning_avg": -1.54,
          "overall_avg": 6.14,
          "overall_efficiency": 0.00613,
          "general_efficiency": 0.004525,
          "math_efficiency": 0.009185,
          "code_efficiency": 0.012344,
          "reasoning_efficiency": -0.001535,
          "general_task_scores": [
            -14.05,
            30.84,
            4.02,
            -2.68
          ],
          "math_task_scores": [
            17.82,
            11.04,
            10.93,
            6.21,
            0.0
          ],
          "code_task_scores": [
            21.14,
            -0.64,
            1.2,
            28.05,
            15.91,
            8.53
          ],
          "reasoning_task_scores": [
            -2.04,
            -8.01,
            -1.85,
            0.06,
            4.15
          ]
        },
        "vs_instruct": {
          "general_avg": -12.96,
          "math_avg": -11.78,
          "code_avg": -8.01,
          "reasoning_avg": -3.18,
          "overall_avg": -8.98,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -30.58,
            -9.15,
            -0.31,
            -11.81
          ],
          "math_task_scores": [
            -11.44,
            -18.98,
            -18.07,
            -2.07,
            -8.34
          ],
          "code_task_scores": [
            -19.1,
            -17.38,
            -4.9,
            23.25,
            -9.2,
            -20.74
          ],
          "reasoning_task_scores": [
            -1.02,
            -8.3,
            3.71,
            -0.05,
            -10.24
          ]
        }
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 24.51,
      "math_avg": 13.38,
      "code_avg": 5.14,
      "reasoning_avg": 16.68,
      "overall_avg": 14.93,
      "overall_efficiency": -0.161737,
      "general_efficiency": -0.176341,
      "math_efficiency": -0.079477,
      "code_efficiency": -0.173778,
      "reasoning_efficiency": -0.217349,
      "general_scores": [
        38.05,
        26.06,
        22.41,
        14.0907143,
        35.35,
        26.63,
        20.8,
        15.2178571,
        35.04,
        25.58,
        19.59,
        15.2864286
      ],
      "math_scores": [
        59.21,
        3.64,
        2.2,
        4.83,
        0.0,
        50.49,
        4.9,
        3.6,
        4.41,
        0.0,
        53.6,
        4.92,
        4.4,
        4.47,
        0.0
      ],
      "code_scores": [
        7.32,
        14.79,
        0.0,
        3.76,
        0.0,
        6.71,
        8.53,
        19.46,
        0.0,
        0.21,
        0.0,
        7.31,
        3.66,
        17.12,
        0.0,
        0.0,
        0.0,
        3.66
      ],
      "reasoning_scores": [
        48.81,
        22.01,
        14.65,
        0.25108696,
        1.93,
        43.73,
        22.93,
        21.72,
        0.25021739,
        2.23,
        42.37,
        16.11,
        11.11,
        0.23391304,
        1.93
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.43
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.5
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.32
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 5.89
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -14.54,
          "math_avg": -6.55,
          "code_avg": -14.33,
          "reasoning_avg": -17.92,
          "overall_avg": -13.33,
          "overall_efficiency": -0.161737,
          "general_efficiency": -0.176341,
          "math_efficiency": -0.079477,
          "code_efficiency": -0.173778,
          "reasoning_efficiency": -0.217349,
          "general_task_scores": [
            -28.22,
            5.75,
            -15.72,
            -19.97
          ],
          "math_task_scores": [
            -1.98,
            -15.21,
            -15.8,
            0.23,
            0.0
          ],
          "code_task_scores": [
            -20.94,
            -37.35,
            -3.58,
            1.11,
            0.0,
            -25.21
          ],
          "reasoning_task_scores": [
            -35.37,
            -42.21,
            -13.97,
            -0.06,
            2.03
          ]
        },
        "vs_instruct": {
          "general_avg": -32.03,
          "math_avg": -27.53,
          "code_avg": -34.7,
          "reasoning_avg": -19.56,
          "overall_avg": -28.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -44.75,
            -34.24,
            -20.05,
            -29.1
          ],
          "math_task_scores": [
            -31.24,
            -45.23,
            -44.8,
            -8.05,
            -8.34
          ],
          "code_task_scores": [
            -61.18,
            -54.09,
            -9.68,
            -3.69,
            -25.11,
            -54.48
          ],
          "reasoning_task_scores": [
            -34.35,
            -42.5,
            -8.41,
            -0.17,
            -12.36
          ]
        }
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 48.93,
      "math_avg": 29.89,
      "code_avg": 38.69,
      "reasoning_avg": 33.4,
      "overall_avg": 37.73,
      "overall_efficiency": 0.010078,
      "general_efficiency": 0.010519,
      "math_efficiency": 0.010608,
      "code_efficiency": 0.020466,
      "reasoning_efficiency": -0.001283,
      "general_scores": [
        58.35,
        72.9025,
        38.39,
        31.1471429,
        54.94,
        71.025,
        39.44,
        30.5671429,
        55.58,
        69.4225,
        36.33,
        29.0335714
      ],
      "math_scores": [
        74.45,
        31.6,
        31.6,
        13.98,
        0.0,
        76.04,
        30.36,
        30.0,
        13.89,
        0.0,
        75.97,
        28.92,
        28.4,
        13.21,
        0.0
      ],
      "code_scores": [
        65.24,
        57.59,
        5.02,
        36.53,
        18.33,
        52.44,
        63.41,
        57.98,
        7.53,
        29.02,
        18.1,
        55.49,
        62.2,
        53.31,
        3.23,
        32.78,
        23.98,
        54.27
      ],
      "reasoning_scores": [
        73.22,
        56.83,
        23.74,
        0.39130435,
        17.21,
        76.61,
        56.42,
        21.72,
        0.36663043,
        13.8,
        69.15,
        57.98,
        20.71,
        0.35043478,
        12.46
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 71.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.49
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.29
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.62
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 20.14
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 54.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.88,
          "math_avg": 9.96,
          "code_avg": 19.22,
          "reasoning_avg": -1.21,
          "overall_avg": 9.47,
          "overall_efficiency": 0.010078,
          "general_efficiency": 0.010519,
          "math_efficiency": 0.010608,
          "code_efficiency": 0.020466,
          "reasoning_efficiency": -0.001283,
          "general_task_scores": [
            -8.08,
            50.78,
            1.4,
            -4.58
          ],
          "math_task_scores": [
            19.08,
            10.59,
            10.8,
            9.35,
            0.0
          ],
          "code_task_scores": [
            36.18,
            1.82,
            1.68,
            32.57,
            20.14,
            22.97
          ],
          "reasoning_task_scores": [
            -7.35,
            -5.48,
            -7.74,
            0.06,
            14.49
          ]
        },
        "vs_instruct": {
          "general_avg": -7.61,
          "math_avg": -11.01,
          "code_avg": -1.15,
          "reasoning_avg": -2.85,
          "overall_avg": -5.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.61,
            10.8,
            -2.93,
            -13.71
          ],
          "math_task_scores": [
            -10.18,
            -19.43,
            -18.2,
            1.07,
            -8.34
          ],
          "code_task_scores": [
            -4.06,
            -14.92,
            -4.42,
            27.77,
            -4.97,
            -6.3
          ],
          "reasoning_task_scores": [
            -6.33,
            -5.77,
            -2.18,
            -0.05,
            0.1
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 12,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 39.64,
      "math_avg": 20.55,
      "code_avg": 22.25,
      "reasoning_avg": 30.72,
      "overall_avg": 28.29,
      "overall_efficiency": 3.3e-05,
      "general_efficiency": 0.000662,
      "math_efficiency": 0.000693,
      "code_efficiency": 0.003125,
      "reasoning_efficiency": -0.00435,
      "general_scores": [
        61.91,
        36.08,
        35.59,
        27.6864286,
        58.16,
        34.5625,
        36.75,
        25.6484615,
        59.0,
        36.8025,
        36.22,
        27.2292857
      ],
      "math_scores": [
        68.16,
        13.22,
        14.2,
        8.36,
        0.0,
        70.28,
        13.04,
        12.6,
        7.68,
        0.0,
        68.08,
        13.2,
        11.8,
        7.61,
        0.0
      ],
      "code_scores": [
        25.0,
        41.63,
        2.51,
        26.93,
        19.23,
        20.12,
        20.73,
        43.58,
        2.15,
        29.02,
        22.85,
        16.46,
        24.39,
        41.25,
        2.15,
        26.51,
        14.71,
        21.34
      ],
      "reasoning_scores": [
        75.25,
        50.26,
        23.23,
        0.33782609,
        1.93,
        72.88,
        50.79,
        23.74,
        0.36076087,
        3.71,
        76.95,
        51.53,
        27.27,
        0.36576087,
        2.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.19
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.88
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 23.37
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.27
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.49
              },
              {
                "metric": "lcb_test_output",
                "score": 18.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 19.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.62
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.59,
          "math_avg": 0.62,
          "code_avg": 2.79,
          "reasoning_avg": -3.88,
          "overall_avg": 0.03,
          "overall_efficiency": 3.3e-05,
          "general_efficiency": 0.000662,
          "math_efficiency": 0.000693,
          "code_efficiency": 0.003125,
          "reasoning_efficiency": -0.00435,
          "general_task_scores": [
            -4.68,
            15.48,
            -0.46,
            -7.98
          ],
          "math_task_scores": [
            12.43,
            -6.55,
            -6.33,
            3.54,
            0.0
          ],
          "code_task_scores": [
            -4.07,
            -12.32,
            -1.31,
            27.28,
            18.93,
            -11.79
          ],
          "reasoning_task_scores": [
            -5.31,
            -11.7,
            -5.05,
            0.04,
            2.62
          ]
        },
        "vs_instruct": {
          "general_avg": -16.91,
          "math_avg": -20.36,
          "code_avg": -17.59,
          "reasoning_avg": -5.52,
          "overall_avg": -15.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.21,
            -24.51,
            -4.79,
            -17.11
          ],
          "math_task_scores": [
            -16.83,
            -36.57,
            -35.33,
            -4.74,
            -8.34
          ],
          "code_task_scores": [
            -44.31,
            -29.06,
            -7.41,
            22.48,
            -6.18,
            -41.06
          ],
          "reasoning_task_scores": [
            -4.29,
            -11.99,
            0.51,
            -0.07,
            -11.77
          ]
        }
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 13,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 43.74,
      "math_avg": 25.31,
      "code_avg": 31.77,
      "reasoning_avg": 33.52,
      "overall_avg": 33.58,
      "overall_efficiency": 0.532271,
      "general_efficiency": 0.469274,
      "math_efficiency": 0.538333,
      "code_efficiency": 1.230111,
      "reasoning_efficiency": -0.108631,
      "general_scores": [
        63.4,
        34.355,
        41.24,
        38.8053846,
        55.65,
        35.99,
        41.57,
        39.8642857,
        57.83,
        35.845,
        41.24,
        39.0771429
      ],
      "math_scores": [
        63.84,
        24.68,
        22.4,
        9.55,
        3.33,
        65.05,
        24.52,
        26.4,
        9.71,
        0.0,
        66.72,
        26.12,
        27.4,
        9.98,
        0.0
      ],
      "code_scores": [
        43.29,
        57.59,
        6.81,
        19.21,
        26.24,
        43.29,
        49.39,
        57.59,
        8.96,
        11.06,
        27.6,
        40.24,
        46.01,
        57.59,
        7.89,
        2.51,
        25.34,
        41.21
      ],
      "reasoning_scores": [
        75.25,
        59.83,
        27.78,
        0.26684783,
        5.19,
        75.93,
        58.97,
        27.78,
        0.27586957,
        6.38,
        75.93,
        60.5,
        24.24,
        0.26858696,
        4.15
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.23
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.59
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 10.93
              },
              {
                "metric": "lcb_test_output",
                "score": 26.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 41.58
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.77
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.6
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.69,
          "math_avg": 5.38,
          "code_avg": 12.3,
          "reasoning_avg": -1.09,
          "overall_avg": 5.32,
          "overall_efficiency": 0.532271,
          "general_efficiency": 0.469274,
          "math_efficiency": 0.538333,
          "code_efficiency": 1.230111,
          "reasoning_efficiency": -0.108631,
          "general_task_scores": [
            -5.41,
            15.06,
            4.7,
            4.42
          ],
          "math_task_scores": [
            8.79,
            5.41,
            6.2,
            5.41,
            1.11
          ],
          "code_task_scores": [
            18.79,
            3.12,
            4.31,
            10.72,
            26.39,
            10.48
          ],
          "reasoning_task_scores": [
            -4.64,
            -2.79,
            -3.2,
            -0.04,
            5.24
          ]
        },
        "vs_instruct": {
          "general_avg": -12.8,
          "math_avg": -15.6,
          "code_avg": -8.08,
          "reasoning_avg": -2.73,
          "overall_avg": -9.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.94,
            -24.93,
            0.37,
            -4.71
          ],
          "math_task_scores": [
            -20.47,
            -24.61,
            -22.8,
            -2.87,
            -7.22
          ],
          "code_task_scores": [
            -21.45,
            -13.62,
            -1.79,
            5.92,
            1.28,
            -18.79
          ],
          "reasoning_task_scores": [
            -3.62,
            -3.08,
            2.36,
            -0.15,
            -9.15
          ]
        }
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 32.66,
      "math_avg": 11.5,
      "code_avg": 21.64,
      "reasoning_avg": 23.97,
      "overall_avg": 22.44,
      "overall_efficiency": -0.011662,
      "general_efficiency": -0.012798,
      "math_efficiency": -0.016908,
      "code_efficiency": 0.004365,
      "reasoning_efficiency": -0.021308,
      "general_scores": [
        42.76,
        40.2925,
        31.0,
        15.9878571,
        43.88,
        40.24,
        30.84,
        19.5707143,
        39.99,
        41.0375,
        30.38,
        15.9671429
      ],
      "math_scores": [
        35.03,
        7.3,
        6.0,
        5.06,
        0.0,
        40.49,
        8.02,
        8.8,
        5.69,
        0.0,
        37.91,
        7.42,
        5.8,
        4.92,
        0.0
      ],
      "code_scores": [
        26.83,
        38.52,
        3.23,
        22.76,
        13.12,
        18.29,
        29.88,
        28.79,
        3.23,
        25.68,
        19.68,
        24.39,
        31.1,
        31.13,
        3.23,
        22.76,
        20.14,
        26.83
      ],
      "reasoning_scores": [
        64.07,
        25.85,
        31.31,
        0.32641304,
        1.34,
        61.69,
        27.84,
        30.81,
        0.32673913,
        1.34,
        63.73,
        22.61,
        26.26,
        0.32184783,
        1.78
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.81
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.73
              },
              {
                "metric": "lcb_test_output",
                "score": 17.65
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 23.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.38,
          "math_avg": -8.43,
          "code_avg": 2.18,
          "reasoning_avg": -10.63,
          "overall_avg": -5.82,
          "overall_efficiency": -0.011662,
          "general_efficiency": -0.012798,
          "math_efficiency": -0.016908,
          "code_efficiency": 0.004365,
          "reasoning_efficiency": -0.021308,
          "general_task_scores": [
            -22.16,
            20.18,
            -5.91,
            -17.65
          ],
          "math_task_scores": [
            -18.6,
            -12.12,
            -12.33,
            0.88,
            0.0
          ],
          "code_task_scores": [
            1.83,
            -21.66,
            -0.35,
            23.52,
            17.65,
            -7.93
          ],
          "reasoning_task_scores": [
            -17.18,
            -37.13,
            -0.34,
            0.01,
            1.49
          ]
        },
        "vs_instruct": {
          "general_avg": -23.88,
          "math_avg": -29.41,
          "code_avg": -18.2,
          "reasoning_avg": -12.27,
          "overall_avg": -20.94,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.69,
            -19.8,
            -10.24,
            -26.78
          ],
          "math_task_scores": [
            -47.86,
            -42.14,
            -41.33,
            -7.4,
            -8.34
          ],
          "code_task_scores": [
            -38.41,
            -38.4,
            -6.45,
            18.72,
            -7.46,
            -37.2
          ],
          "reasoning_task_scores": [
            -16.16,
            -37.42,
            5.22,
            -0.1,
            -12.9
          ]
        }
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 15,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 33.42,
      "math_avg": 10.6,
      "code_avg": 8.93,
      "reasoning_avg": 19.42,
      "overall_avg": 18.09,
      "overall_efficiency": -0.075545,
      "general_efficiency": -0.041774,
      "math_efficiency": -0.069331,
      "code_efficiency": -0.078271,
      "reasoning_efficiency": -0.112804,
      "general_scores": [
        70.9,
        28.2,
        22.83,
        12.525,
        65.14,
        29.3725,
        24.34,
        13.25214,
        67.83,
        29.2975,
        23.73,
        13.6585714
      ],
      "math_scores": [
        21.15,
        12.44,
        10.6,
        6.53,
        0.0,
        22.14,
        13.42,
        12.0,
        6.8,
        0.0,
        22.74,
        12.9,
        11.6,
        6.64,
        0.0
      ],
      "code_scores": [
        11.59,
        38.91,
        0.0,
        0.0,
        0.0,
        4.88,
        10.37,
        32.3,
        0.0,
        0.21,
        0.0,
        5.49,
        15.24,
        29.96,
        0.0,
        0.21,
        0.0,
        11.59
      ],
      "reasoning_scores": [
        50.51,
        24.08,
        21.21,
        0.2076087,
        2.37,
        50.85,
        23.4,
        14.14,
        0.206196,
        3.56,
        51.19,
        24.5,
        21.72,
        0.20478261,
        3.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.96
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.4
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.85
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.02
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -5.62,
          "math_avg": -9.33,
          "code_avg": -10.54,
          "reasoning_avg": -15.18,
          "overall_avg": -10.17,
          "overall_efficiency": -0.075545,
          "general_efficiency": -0.041774,
          "math_efficiency": -0.069331,
          "code_efficiency": -0.078271,
          "reasoning_efficiency": -0.112804,
          "general_task_scores": [
            3.59,
            8.62,
            -13.02,
            -21.68
          ],
          "math_task_scores": [
            -34.4,
            -6.78,
            -7.8,
            2.32,
            0.0
          ],
          "code_task_scores": [
            -15.04,
            -20.75,
            -3.58,
            -0.07,
            0.0,
            -23.78
          ],
          "reasoning_task_scores": [
            -29.49,
            -38.57,
            -10.78,
            -0.1,
            3.02
          ]
        },
        "vs_instruct": {
          "general_avg": -23.12,
          "math_avg": -30.31,
          "code_avg": -30.91,
          "reasoning_avg": -16.83,
          "overall_avg": -25.29,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.94,
            -31.36,
            -17.35,
            -30.81
          ],
          "math_task_scores": [
            -63.66,
            -36.8,
            -36.8,
            -5.96,
            -8.34
          ],
          "code_task_scores": [
            -55.28,
            -37.49,
            -9.68,
            -4.87,
            -25.11,
            -53.05
          ],
          "reasoning_task_scores": [
            -28.47,
            -38.86,
            -5.22,
            -0.21,
            -11.37
          ]
        }
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 16,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 32.75,
      "math_avg": 17.69,
      "code_avg": 3.9,
      "reasoning_avg": 24.85,
      "overall_avg": 19.8,
      "overall_efficiency": -1.13284,
      "general_efficiency": -0.842792,
      "math_efficiency": -0.299745,
      "code_efficiency": -2.083352,
      "reasoning_efficiency": -1.305468,
      "general_scores": [
        68.63,
        19.3875,
        22.56,
        21.9753846,
        69.8,
        18.1275,
        21.12,
        21.4585714,
        69.99,
        16.5275,
        21.87,
        21.5292857
      ],
      "math_scores": [
        61.87,
        9.88,
        9.6,
        5.67,
        0.0,
        63.31,
        10.02,
        10.4,
        5.94,
        0.0,
        61.71,
        10.26,
        11.2,
        5.49,
        0.0
      ],
      "code_scores": [
        0.0,
        24.51,
        0.0,
        0.42,
        0.0,
        0.0,
        0.0,
        14.79,
        0.0,
        0.42,
        0.0,
        0.0,
        0.0,
        29.18,
        0.0,
        0.84,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        59.66,
        50.39,
        16.16,
        0.12402174,
        1.93,
        55.93,
        49.48,
        13.13,
        0.11141304,
        1.04,
        57.29,
        49.66,
        15.15,
        0.12402174,
        2.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.3
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.7
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.56
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.81
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.3,
          "math_avg": -2.24,
          "code_avg": -15.57,
          "reasoning_avg": -9.76,
          "overall_avg": -8.47,
          "overall_efficiency": -1.13284,
          "general_efficiency": -0.842792,
          "math_efficiency": -0.299745,
          "code_efficiency": -2.083352,
          "reasoning_efficiency": -1.305468,
          "general_task_scores": [
            5.1,
            -2.33,
            -14.8,
            -13.18
          ],
          "math_task_scores": [
            5.89,
            -9.65,
            -8.8,
            1.36,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -31.64,
            -3.58,
            0.35,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -22.71,
            -12.72,
            -14.99,
            -0.19,
            1.83
          ]
        },
        "vs_instruct": {
          "general_avg": -23.79,
          "math_avg": -23.22,
          "code_avg": -35.95,
          "reasoning_avg": -11.4,
          "overall_avg": -23.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.43,
            -42.32,
            -19.13,
            -22.31
          ],
          "math_task_scores": [
            -23.37,
            -39.67,
            -37.8,
            -6.92,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -48.38,
            -9.68,
            -4.45,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -21.69,
            -13.01,
            -9.43,
            -0.3,
            -12.56
          ]
        }
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 17,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 39.03,
      "math_avg": 20.16,
      "code_avg": 10.56,
      "reasoning_avg": 30.85,
      "overall_avg": 25.15,
      "overall_efficiency": -0.414741,
      "general_efficiency": -0.001794,
      "math_efficiency": 0.031111,
      "code_efficiency": -1.188,
      "reasoning_efficiency": -0.500284,
      "general_scores": [
        74.15,
        24.5925,
        31.25,
        30.4071429,
        73.27,
        24.1725,
        30.24,
        28.1321429,
        72.39,
        22.2625,
        29.98,
        27.5457143
      ],
      "math_scores": [
        53.75,
        20.0,
        21.0,
        9.67,
        0.0,
        53.83,
        17.14,
        19.0,
        8.85,
        0.0,
        51.48,
        19.48,
        19.4,
        8.85,
        0.0
      ],
      "code_scores": [
        0.0,
        54.86,
        0.72,
        7.72,
        2.26,
        0.0,
        1.22,
        52.92,
        1.43,
        2.09,
        2.04,
        1.22,
        1.83,
        56.03,
        1.43,
        2.51,
        1.13,
        0.61
      ],
      "reasoning_scores": [
        75.25,
        56.37,
        22.22,
        0.17293478,
        2.82,
        72.54,
        52.85,
        23.23,
        0.1675,
        3.71,
        72.88,
        54.25,
        22.73,
        0.15358696,
        3.41
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.19
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.11
              },
              {
                "metric": "lcb_test_output",
                "score": 1.81
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.01,
          "math_avg": 0.23,
          "code_avg": -8.91,
          "reasoning_avg": -3.75,
          "overall_avg": -3.11,
          "overall_efficiency": -0.414741,
          "general_efficiency": -0.001794,
          "math_efficiency": 0.031111,
          "code_efficiency": -1.188,
          "reasoning_efficiency": -0.500284,
          "general_task_scores": [
            8.9,
            3.34,
            -6.16,
            -6.13
          ],
          "math_task_scores": [
            -3.39,
            -0.83,
            0.6,
            4.78,
            0.0
          ],
          "code_task_scores": [
            -26.42,
            0.13,
            -2.39,
            3.9,
            1.81,
            -30.49
          ],
          "reasoning_task_scores": [
            -6.78,
            -8.07,
            -7.07,
            -0.15,
            3.31
          ]
        },
        "vs_instruct": {
          "general_avg": -17.51,
          "math_avg": -20.75,
          "code_avg": -29.29,
          "reasoning_avg": -5.39,
          "overall_avg": -18.23,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.63,
            -36.65,
            -10.49,
            -15.26
          ],
          "math_task_scores": [
            -32.65,
            -30.85,
            -28.4,
            -3.5,
            -8.34
          ],
          "code_task_scores": [
            -66.66,
            -16.61,
            -8.49,
            -0.9,
            -23.3,
            -59.76
          ],
          "reasoning_task_scores": [
            -5.76,
            -8.36,
            -1.51,
            -0.26,
            -11.08
          ]
        }
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 36.91,
      "math_avg": 24.79,
      "code_avg": 10.82,
      "reasoning_avg": 27.92,
      "overall_avg": 25.11,
      "overall_efficiency": -0.003151,
      "general_efficiency": -0.002136,
      "math_efficiency": 0.004863,
      "code_efficiency": -0.008644,
      "reasoning_efficiency": -0.006686,
      "general_scores": [
        69.87,
        24.0,
        32.89,
        19.9392857,
        73.83,
        28.025,
        34.86,
        19.0228571,
        64.83,
        26.545,
        33.23,
        15.8757143
      ],
      "math_scores": [
        63.76,
        20.66,
        21.8,
        18.27,
        3.33,
        62.17,
        18.82,
        17.2,
        18.9,
        3.33,
        64.44,
        20.48,
        20.4,
        18.34,
        0.0
      ],
      "code_scores": [
        3.66,
        40.86,
        0.72,
        0.21,
        4.98,
        7.93,
        9.15,
        37.74,
        1.79,
        0.0,
        9.5,
        2.44,
        8.54,
        36.58,
        2.15,
        0.0,
        9.05,
        19.51
      ],
      "reasoning_scores": [
        62.37,
        24.09,
        21.72,
        0.18771739,
        23.59,
        64.41,
        32.11,
        20.2,
        0.20554348,
        25.37,
        66.78,
        32.63,
        21.72,
        0.20445652,
        23.15
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.99
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 9.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -2.14,
          "math_avg": 4.86,
          "code_avg": -8.64,
          "reasoning_avg": -6.69,
          "overall_avg": -3.15,
          "overall_efficiency": -0.003151,
          "general_efficiency": -0.002136,
          "math_efficiency": 0.004863,
          "code_efficiency": -0.008644,
          "reasoning_efficiency": -0.006686,
          "general_task_scores": [
            5.14,
            5.85,
            -2.99,
            -16.55
          ],
          "math_task_scores": [
            7.05,
            0.29,
            0.6,
            14.16,
            2.22
          ],
          "code_task_scores": [
            -20.32,
            -16.08,
            -2.03,
            -0.14,
            7.84,
            -21.14
          ],
          "reasoning_task_scores": [
            -15.82,
            -32.95,
            -8.59,
            -0.11,
            24.04
          ]
        },
        "vs_instruct": {
          "general_avg": -19.63,
          "math_avg": -16.12,
          "code_avg": -29.02,
          "reasoning_avg": -8.33,
          "overall_avg": -18.27,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.39,
            -34.14,
            -7.32,
            -25.68
          ],
          "math_task_scores": [
            -22.21,
            -29.73,
            -28.4,
            5.88,
            -6.12
          ],
          "code_task_scores": [
            -60.56,
            -32.82,
            -8.13,
            -4.94,
            -17.27,
            -50.41
          ],
          "reasoning_task_scores": [
            -14.8,
            -33.24,
            -3.03,
            -0.22,
            9.65
          ]
        }
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 40.99,
      "math_avg": 41.22,
      "code_avg": 9.31,
      "reasoning_avg": 26.85,
      "overall_avg": 29.59,
      "overall_efficiency": 0.001549,
      "general_efficiency": 0.002266,
      "math_efficiency": 0.024767,
      "code_efficiency": -0.011818,
      "reasoning_efficiency": -0.009018,
      "general_scores": [
        75.33,
        24.8925,
        40.85,
        29.3614286,
        74.09,
        25.495,
        40.3,
        26.8942857,
        67.03,
        23.0425,
        40.26,
        24.3728571
      ],
      "math_scores": [
        83.85,
        50.3,
        49.0,
        19.76,
        0.0,
        82.87,
        50.36,
        52.0,
        19.2,
        3.33,
        83.32,
        51.94,
        49.0,
        19.99,
        3.33
      ],
      "code_scores": [
        11.59,
        40.86,
        0.36,
        0.21,
        2.26,
        6.1,
        7.93,
        40.08,
        0.36,
        0.0,
        0.23,
        4.88,
        4.27,
        36.19,
        0.36,
        0.0,
        0.9,
        10.98
      ],
      "reasoning_scores": [
        66.78,
        24.77,
        18.69,
        0.16913043,
        21.07,
        68.14,
        30.17,
        23.23,
        0.16945652,
        19.29,
        67.8,
        20.88,
        19.19,
        0.16804348,
        22.26
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.48
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.35
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.95,
          "math_avg": 21.29,
          "code_avg": -10.16,
          "reasoning_avg": -7.75,
          "overall_avg": 1.33,
          "overall_efficiency": 0.001549,
          "general_efficiency": 0.002266,
          "math_efficiency": 0.024767,
          "code_efficiency": -0.011818,
          "reasoning_efficiency": -0.009018,
          "general_task_scores": [
            7.78,
            4.14,
            3.82,
            -7.95
          ],
          "math_task_scores": [
            26.94,
            31.17,
            30.8,
            15.31,
            2.22
          ],
          "code_task_scores": [
            -19.51,
            -15.43,
            -3.22,
            -0.14,
            1.13,
            -23.78
          ],
          "reasoning_task_scores": [
            -12.77,
            -37.29,
            -9.43,
            -0.14,
            20.87
          ]
        },
        "vs_instruct": {
          "general_avg": -15.55,
          "math_avg": 0.31,
          "code_avg": -30.53,
          "reasoning_avg": -9.39,
          "overall_avg": -13.79,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.75,
            -35.84,
            -0.51,
            -17.08
          ],
          "math_task_scores": [
            -2.32,
            1.15,
            1.8,
            7.03,
            -6.12
          ],
          "code_task_scores": [
            -59.75,
            -32.17,
            -9.32,
            -4.94,
            -23.98,
            -53.05
          ],
          "reasoning_task_scores": [
            -11.75,
            -37.58,
            -3.87,
            -0.25,
            6.48
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 39.74,
      "math_avg": 21.48,
      "code_avg": 25.38,
      "reasoning_avg": 26.22,
      "overall_avg": 28.2,
      "overall_efficiency": -0.000787,
      "general_efficiency": 0.009575,
      "math_efficiency": 0.021452,
      "code_efficiency": 0.081599,
      "reasoning_efficiency": -0.115772,
      "general_scores": [
        61.66,
        31.35,
        36.58,
        34.4142857,
        53.16,
        31.675,
        37.53,
        35.3457143,
        51.88,
        31.505,
        37.21,
        34.5671429
      ],
      "math_scores": [
        41.39,
        25.5,
        25.0,
        14.86,
        0.0,
        41.47,
        25.56,
        26.0,
        15.15,
        0.0,
        40.86,
        25.66,
        26.0,
        14.81,
        0.0
      ],
      "code_scores": [
        48.17,
        52.53,
        7.17,
        2.3,
        0.23,
        40.85,
        48.17,
        55.25,
        5.02,
        2.3,
        0.0,
        38.41,
        51.83,
        55.25,
        6.09,
        3.76,
        0.45,
        39.02
      ],
      "reasoning_scores": [
        78.98,
        16.99,
        23.74,
        0.17597826,
        9.05,
        77.97,
        13.46,
        27.78,
        0.17934783,
        9.5,
        79.66,
        15.13,
        30.81,
        0.17119565,
        9.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.78
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.79
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 39.43
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.87
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.69,
          "math_avg": 1.55,
          "code_avg": 5.91,
          "reasoning_avg": -8.39,
          "overall_avg": -0.06,
          "overall_efficiency": -0.000787,
          "general_efficiency": 0.009575,
          "math_efficiency": 0.021452,
          "code_efficiency": 0.081599,
          "reasoning_efficiency": -0.115772,
          "general_task_scores": [
            -8.8,
            11.17,
            0.46,
            -0.05
          ],
          "math_task_scores": [
            -15.17,
            5.87,
            6.47,
            10.6,
            0.0
          ],
          "code_task_scores": [
            21.95,
            -0.13,
            2.51,
            2.58,
            0.23,
            8.33
          ],
          "reasoning_task_scores": [
            -1.47,
            -47.37,
            -2.36,
            -0.13,
            9.4
          ]
        },
        "vs_instruct": {
          "general_avg": -16.8,
          "math_avg": -19.42,
          "code_avg": -14.47,
          "reasoning_avg": -10.03,
          "overall_avg": -15.18,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -25.33,
            -28.82,
            -3.87,
            -9.18
          ],
          "math_task_scores": [
            -44.43,
            -24.15,
            -22.53,
            2.32,
            -8.34
          ],
          "code_task_scores": [
            -18.29,
            -16.87,
            -3.59,
            -2.22,
            -24.88,
            -20.94
          ],
          "reasoning_task_scores": [
            -0.45,
            -47.66,
            3.2,
            -0.24,
            -4.99
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 32.05,
      "math_avg": 30.17,
      "code_avg": 3.83,
      "reasoning_avg": 23.14,
      "overall_avg": 22.3,
      "overall_efficiency": -0.015101,
      "general_efficiency": -0.017701,
      "math_efficiency": 0.025914,
      "code_efficiency": -0.039592,
      "reasoning_efficiency": -0.029024,
      "general_scores": [
        65.46,
        22.3325,
        26.7,
        13.9314286,
        67.28,
        23.03,
        27.24,
        13.8142857,
        62.9,
        22.7125,
        26.45,
        12.8
      ],
      "math_scores": [
        74.75,
        31.72,
        32.8,
        10.34,
        0.0,
        76.72,
        32.68,
        32.8,
        9.96,
        0.0,
        76.88,
        32.26,
        31.8,
        9.78,
        0.0
      ],
      "code_scores": [
        0.0,
        19.84,
        0.0,
        0.84,
        0.68,
        0.0,
        0.0,
        17.9,
        0.0,
        4.59,
        0.23,
        0.0,
        0.0,
        20.62,
        0.0,
        3.97,
        0.23,
        0.0
      ],
      "reasoning_scores": [
        52.54,
        35.66,
        22.73,
        0.15347826,
        6.38,
        56.27,
        35.94,
        19.7,
        0.14304348,
        7.42,
        51.53,
        35.44,
        15.66,
        0.16956522,
        7.33
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 19.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.13
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.68
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.36
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.99,
          "math_avg": 10.24,
          "code_avg": -15.64,
          "reasoning_avg": -11.46,
          "overall_avg": -5.96,
          "overall_efficiency": -0.015101,
          "general_efficiency": -0.017701,
          "math_efficiency": 0.025914,
          "code_efficiency": -0.039592,
          "reasoning_efficiency": -0.029024,
          "general_task_scores": [
            0.84,
            2.35,
            -9.85,
            -21.31
          ],
          "math_task_scores": [
            19.71,
            12.52,
            13.27,
            5.69,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -35.02,
            -3.58,
            2.92,
            0.38,
            -31.1
          ],
          "reasoning_task_scores": [
            -26.89,
            -26.88,
            -10.44,
            -0.15,
            7.04
          ]
        },
        "vs_instruct": {
          "general_avg": -24.49,
          "math_avg": -10.74,
          "code_avg": -36.02,
          "reasoning_avg": -13.11,
          "overall_avg": -21.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.69,
            -37.64,
            -14.18,
            -30.44
          ],
          "math_task_scores": [
            -9.55,
            -17.5,
            -15.73,
            -2.59,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -51.76,
            -9.68,
            -1.88,
            -24.73,
            -60.37
          ],
          "reasoning_task_scores": [
            -25.87,
            -27.17,
            -4.88,
            -0.26,
            -7.35
          ]
        }
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 40.23,
      "math_avg": 22.44,
      "code_avg": 17.55,
      "reasoning_avg": 28.29,
      "overall_avg": 27.13,
      "overall_efficiency": -0.004325,
      "general_efficiency": 0.004532,
      "math_efficiency": 0.009582,
      "code_efficiency": -0.007318,
      "reasoning_efficiency": -0.024091,
      "general_scores": [
        63.24,
        34.2625,
        32.89,
        27.605,
        63.57,
        35.44,
        34.86,
        27.0735714,
        70.25,
        32.9325,
        33.23,
        27.45
      ],
      "math_scores": [
        63.76,
        20.66,
        21.8,
        9.08,
        0.0,
        62.17,
        18.82,
        17.2,
        8.81,
        0.0,
        64.44,
        20.48,
        20.4,
        8.99,
        0.0
      ],
      "code_scores": [
        28.05,
        43.97,
        0.72,
        6.05,
        7.01,
        17.68,
        28.66,
        43.58,
        0.0,
        5.85,
        4.07,
        18.29,
        27.44,
        42.41,
        2.51,
        11.9,
        4.52,
        23.17
      ],
      "reasoning_scores": [
        62.37,
        49.76,
        26.26,
        0.25086957,
        3.71,
        64.41,
        50.93,
        18.69,
        0.2573913,
        3.41,
        66.78,
        49.5,
        24.33,
        0.27836957,
        3.41
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.99
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.08
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.93
              },
              {
                "metric": "lcb_test_output",
                "score": 5.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 19.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.19,
          "math_avg": 2.51,
          "code_avg": -1.92,
          "reasoning_avg": -6.31,
          "overall_avg": -1.13,
          "overall_efficiency": -0.004325,
          "general_efficiency": 0.004532,
          "math_efficiency": 0.009582,
          "code_efficiency": -0.007318,
          "reasoning_efficiency": -0.024091,
          "general_task_scores": [
            1.32,
            13.87,
            -2.99,
            -7.45
          ],
          "math_task_scores": [
            7.05,
            0.29,
            0.6,
            4.62,
            0.0
          ],
          "code_task_scores": [
            0.61,
            -11.15,
            -2.5,
            7.72,
            5.2,
            -11.39
          ],
          "reasoning_task_scores": [
            -15.82,
            -12.5,
            -6.71,
            -0.05,
            3.51
          ]
        },
        "vs_instruct": {
          "general_avg": -16.31,
          "math_avg": -18.47,
          "code_avg": -22.29,
          "reasoning_avg": -7.95,
          "overall_avg": -16.26,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.21,
            -26.12,
            -7.32,
            -16.58
          ],
          "math_task_scores": [
            -22.21,
            -29.73,
            -28.4,
            -3.66,
            -8.34
          ],
          "code_task_scores": [
            -39.63,
            -27.89,
            -8.6,
            2.92,
            -19.91,
            -40.66
          ],
          "reasoning_task_scores": [
            -14.8,
            -12.79,
            -1.15,
            -0.16,
            -10.88
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 29.99,
      "math_avg": 29.37,
      "code_avg": 4.68,
      "reasoning_avg": 16.61,
      "overall_avg": 20.16,
      "overall_efficiency": -0.271492,
      "general_efficiency": -0.303553,
      "math_efficiency": 0.316385,
      "code_efficiency": -0.495656,
      "reasoning_efficiency": -0.603144,
      "general_scores": [
        43.68,
        26.495,
        26.7,
        24.1247,
        41.93,
        25.9,
        27.24,
        24.4721429,
        42.24,
        26.5475,
        26.45,
        24.0892857
      ],
      "math_scores": [
        74.75,
        31.72,
        32.8,
        5.85,
        0.0,
        76.72,
        32.68,
        32.8,
        6.12,
        0.0,
        76.88,
        32.26,
        31.8,
        6.17,
        0.0
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        0.42,
        0.0,
        0.0,
        0.0,
        26.46,
        0.0,
        1.25,
        0.0,
        0.0,
        0.0,
        27.63,
        0.0,
        0.42,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        52.54,
        3.9,
        23.56,
        0.13880435,
        2.23,
        56.27,
        3.64,
        23.74,
        0.16217391,
        2.67,
        51.53,
        4.71,
        22.22,
        0.1548913,
        1.63
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.08
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -9.06,
          "math_avg": 9.44,
          "code_avg": -14.79,
          "reasoning_avg": -18.0,
          "overall_avg": -8.1,
          "overall_efficiency": -0.271492,
          "general_efficiency": -0.303553,
          "math_efficiency": 0.316385,
          "code_efficiency": -0.495656,
          "reasoning_efficiency": -0.603144,
          "general_task_scores": [
            -21.75,
            5.97,
            -9.85,
            -10.6
          ],
          "math_task_scores": [
            19.71,
            12.52,
            13.27,
            1.71,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -27.1,
            -3.58,
            0.49,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -26.89,
            -58.48,
            -6.63,
            -0.16,
            2.18
          ]
        },
        "vs_instruct": {
          "general_avg": -26.55,
          "math_avg": -11.54,
          "code_avg": -35.17,
          "reasoning_avg": -19.64,
          "overall_avg": -23.22,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.28,
            -34.02,
            -14.18,
            -19.73
          ],
          "math_task_scores": [
            -9.55,
            -17.5,
            -15.73,
            -6.57,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -43.84,
            -9.68,
            -4.31,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -25.87,
            -58.77,
            -1.07,
            -0.27,
            -12.21
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 39.26,
      "math_avg": 20.44,
      "code_avg": 10.6,
      "reasoning_avg": 30.75,
      "overall_avg": 25.26,
      "overall_efficiency": -0.05995,
      "general_efficiency": 0.004348,
      "math_efficiency": 0.010267,
      "code_efficiency": -0.177366,
      "reasoning_efficiency": -0.077049,
      "general_scores": [
        74.15,
        21.0,
        32.71,
        28.4,
        74.15,
        24.7925,
        33.03,
        28.1971429,
        72.39,
        21.8575,
        33.0,
        27.4857143
      ],
      "math_scores": [
        54.06,
        18.28,
        18.6,
        8.79,
        0.0,
        54.66,
        19.06,
        18.2,
        9.55,
        0.0,
        54.28,
        18.94,
        20.0,
        8.9,
        3.33
      ],
      "code_scores": [
        0.0,
        54.09,
        0.36,
        5.22,
        0.9,
        0.0,
        0.0,
        55.25,
        1.43,
        3.34,
        2.26,
        1.22,
        1.83,
        56.81,
        1.08,
        5.01,
        1.36,
        0.61
      ],
      "reasoning_scores": [
        74.92,
        54.08,
        22.13,
        0.16869565,
        3.71,
        74.58,
        55.61,
        19.19,
        0.16869565,
        3.86,
        73.22,
        52.05,
        22.22,
        0.15163043,
        5.19
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.52
              },
              {
                "metric": "lcb_test_output",
                "score": 1.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.22,
          "math_avg": 0.51,
          "code_avg": -8.87,
          "reasoning_avg": -3.85,
          "overall_avg": -3.0,
          "overall_efficiency": -0.05995,
          "general_efficiency": 0.004348,
          "math_efficiency": 0.010267,
          "code_efficiency": -0.177366,
          "reasoning_efficiency": -0.077049,
          "general_task_scores": [
            9.19,
            2.21,
            -3.74,
            -6.8
          ],
          "math_task_scores": [
            -2.08,
            -0.94,
            -0.27,
            4.74,
            1.11
          ],
          "code_task_scores": [
            -26.83,
            0.91,
            -2.62,
            4.31,
            1.51,
            -30.49
          ],
          "reasoning_task_scores": [
            -6.1,
            -8.65,
            -8.62,
            -0.15,
            4.25
          ]
        },
        "vs_instruct": {
          "general_avg": -17.28,
          "math_avg": -20.47,
          "code_avg": -29.24,
          "reasoning_avg": -5.49,
          "overall_avg": -18.12,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.34,
            -37.78,
            -8.07,
            -15.93
          ],
          "math_task_scores": [
            -31.34,
            -30.96,
            -29.27,
            -3.54,
            -7.22
          ],
          "code_task_scores": [
            -67.07,
            -15.83,
            -8.72,
            -0.49,
            -23.6,
            -59.76
          ],
          "reasoning_task_scores": [
            -5.08,
            -8.94,
            -3.06,
            -0.26,
            -10.14
          ]
        }
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 42.98,
      "math_avg": 26.19,
      "code_avg": 14.34,
      "reasoning_avg": 28.33,
      "overall_avg": 27.96,
      "overall_efficiency": -0.010067,
      "general_efficiency": 0.131764,
      "math_efficiency": 0.209689,
      "code_efficiency": -0.171615,
      "reasoning_efficiency": -0.210109,
      "general_scores": [
        62.36,
        33.68,
        39.18,
        32.2707143,
        71.51,
        32.125,
        39.24,
        32.4614286,
        67.61,
        31.85,
        40.16,
        33.31571
      ],
      "math_scores": [
        57.77,
        32.38,
        29.6,
        11.36,
        0.0,
        56.71,
        31.94,
        30.4,
        11.36,
        0.0,
        55.88,
        32.66,
        31.8,
        11.0,
        0.0
      ],
      "code_scores": [
        2.44,
        55.64,
        2.51,
        3.13,
        1.36,
        25.61,
        1.83,
        52.14,
        2.87,
        4.59,
        2.26,
        24.39,
        1.83,
        54.47,
        2.51,
        4.38,
        1.58,
        14.63
      ],
      "reasoning_scores": [
        75.59,
        39.81,
        24.75,
        0.28271739,
        4.45,
        74.24,
        39.74,
        21.72,
        0.28728261,
        3.71,
        71.19,
        38.89,
        25.76,
        0.28793478,
        4.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.16
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.79
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.73
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 21.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.48
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.93,
          "math_avg": 6.26,
          "code_avg": -5.12,
          "reasoning_avg": -6.27,
          "overall_avg": -0.3,
          "overall_efficiency": -0.010067,
          "general_efficiency": 0.131764,
          "math_efficiency": 0.209689,
          "code_efficiency": -0.171615,
          "reasoning_efficiency": -0.210109,
          "general_task_scores": [
            2.79,
            12.21,
            2.88,
            -2.15
          ],
          "math_task_scores": [
            0.38,
            12.63,
            11.4,
            6.9,
            0.0
          ],
          "code_task_scores": [
            -25.41,
            -0.39,
            -0.95,
            3.82,
            1.73,
            -9.56
          ],
          "reasoning_task_scores": [
            -6.67,
            -23.08,
            -5.72,
            -0.02,
            4.13
          ]
        },
        "vs_instruct": {
          "general_avg": -13.56,
          "math_avg": -14.72,
          "code_avg": -25.5,
          "reasoning_avg": -7.92,
          "overall_avg": -15.42,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -13.74,
            -27.78,
            -1.45,
            -11.28
          ],
          "math_task_scores": [
            -28.88,
            -17.39,
            -17.6,
            -1.38,
            -8.34
          ],
          "code_task_scores": [
            -65.65,
            -17.13,
            -7.05,
            -0.98,
            -23.38,
            -38.83
          ],
          "reasoning_task_scores": [
            -5.65,
            -23.37,
            -0.16,
            -0.13,
            -10.26
          ]
        }
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 26,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 33.93,
      "math_avg": 34.92,
      "code_avg": 24.2,
      "reasoning_avg": 28.01,
      "overall_avg": 30.27,
      "overall_efficiency": 0.013382,
      "general_efficiency": -0.034092,
      "math_efficiency": 0.099965,
      "code_efficiency": 0.031583,
      "reasoning_efficiency": -0.043928,
      "general_scores": [
        42.23,
        31.32,
        36.01,
        18.6878571,
        47.94,
        31.6875,
        36.92,
        26.4385714,
        41.87,
        33.075,
        37.22,
        23.805
      ],
      "math_scores": [
        79.38,
        39.06,
        39.2,
        14.52,
        3.33,
        79.08,
        40.08,
        40.2,
        14.21,
        0.0,
        80.44,
        39.56,
        39.6,
        15.15,
        0.0
      ],
      "code_scores": [
        31.71,
        55.64,
        6.45,
        0.42,
        16.74,
        28.66,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        30.49,
        35.98,
        54.86,
        8.24,
        1.25,
        19.68,
        30.49
      ],
      "reasoning_scores": [
        68.14,
        34.29,
        21.21,
        0.27228261,
        17.06,
        73.22,
        31.44,
        17.68,
        0.25282609,
        14.39,
        72.54,
        32.99,
        19.7,
        0.26967391,
        16.77
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.63
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.74
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 29.88
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -5.11,
          "math_avg": 14.99,
          "code_avg": 4.74,
          "reasoning_avg": -6.59,
          "overall_avg": 2.01,
          "overall_efficiency": 0.013382,
          "general_efficiency": -0.034092,
          "math_efficiency": 0.099965,
          "code_efficiency": 0.031583,
          "reasoning_efficiency": -0.043928,
          "general_task_scores": [
            -20.36,
            11.69,
            0.07,
            -11.85
          ],
          "math_task_scores": [
            23.22,
            19.87,
            20.47,
            10.29,
            1.11
          ],
          "code_task_scores": [
            6.3,
            1.3,
            3.83,
            0.49,
            17.72,
            -1.22
          ],
          "reasoning_task_scores": [
            -9.04,
            -29.65,
            -10.27,
            -0.05,
            16.07
          ]
        },
        "vs_instruct": {
          "general_avg": -22.61,
          "math_avg": -5.99,
          "code_avg": -15.64,
          "reasoning_avg": -8.23,
          "overall_avg": -13.12,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.89,
            -28.3,
            -4.26,
            -20.98
          ],
          "math_task_scores": [
            -6.04,
            -10.15,
            -8.53,
            2.01,
            -7.22
          ],
          "code_task_scores": [
            -33.94,
            -15.44,
            -2.27,
            -4.31,
            -7.39,
            -30.49
          ],
          "reasoning_task_scores": [
            -8.02,
            -29.94,
            -4.71,
            -0.16,
            1.68
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 34.52,
      "math_avg": 27.95,
      "code_avg": 6.6,
      "reasoning_avg": 25.51,
      "overall_avg": 23.65,
      "overall_efficiency": -0.230673,
      "general_efficiency": -0.226198,
      "math_efficiency": 0.401167,
      "code_efficiency": -0.643139,
      "reasoning_efficiency": -0.454521,
      "general_scores": [
        52.17,
        26.94,
        33.7,
        31.1328571,
        43.26,
        25.765,
        33.23,
        26.4592857,
        53.1,
        25.1,
        34.15,
        29.2592857
      ],
      "math_scores": [
        74.53,
        26.16,
        26.0,
        11.5,
        0.0,
        75.36,
        27.6,
        26.6,
        10.84,
        0.0,
        75.51,
        26.7,
        27.6,
        10.9,
        0.0
      ],
      "code_scores": [
        0.0,
        48.64,
        0.0,
        0.0,
        0.68,
        0.0,
        0.0,
        35.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        33.07,
        0.0,
        0.0,
        0.68,
        0.0
      ],
      "reasoning_scores": [
        70.85,
        34.07,
        14.14,
        0.2425,
        7.86,
        67.46,
        38.59,
        16.67,
        0.23369565,
        8.31,
        65.08,
        33.76,
        17.17,
        0.23336957,
        8.01
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.13
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.06
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.52,
          "math_avg": 8.02,
          "code_avg": -12.86,
          "reasoning_avg": -9.09,
          "overall_avg": -4.61,
          "overall_efficiency": -0.230673,
          "general_efficiency": -0.226198,
          "math_efficiency": 0.401167,
          "code_efficiency": -0.643139,
          "reasoning_efficiency": -0.454521,
          "general_task_scores": [
            -14.86,
            5.6,
            -2.96,
            -5.88
          ],
          "math_task_scores": [
            18.72,
            7.12,
            7.53,
            6.74,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -15.3,
            -3.58,
            -0.21,
            0.45,
            -31.1
          ],
          "reasoning_task_scores": [
            -12.54,
            -27.09,
            -13.81,
            -0.07,
            8.06
          ]
        },
        "vs_instruct": {
          "general_avg": -22.02,
          "math_avg": -12.96,
          "code_avg": -33.24,
          "reasoning_avg": -10.73,
          "overall_avg": -19.74,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -31.39,
            -34.39,
            -7.29,
            -15.01
          ],
          "math_task_scores": [
            -10.54,
            -22.9,
            -21.47,
            -1.54,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -32.04,
            -9.68,
            -5.01,
            -24.66,
            -60.37
          ],
          "reasoning_task_scores": [
            -11.52,
            -27.38,
            -8.25,
            -0.18,
            -6.33
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 41.19,
      "math_avg": 48.85,
      "code_avg": 12.82,
      "reasoning_avg": 27.28,
      "overall_avg": 32.53,
      "overall_efficiency": 0.004257,
      "general_efficiency": 0.002132,
      "math_efficiency": 0.028817,
      "code_efficiency": -0.006623,
      "reasoning_efficiency": -0.007298,
      "general_scores": [
        81.82,
        23.6725,
        38.11,
        20.105,
        83.71,
        24.2225,
        38.43,
        20.1028571,
        83.04,
        23.88,
        38.65,
        18.4838462
      ],
      "math_scores": [
        88.48,
        64.58,
        64.4,
        21.0,
        6.67,
        90.67,
        64.6,
        64.0,
        20.57,
        0.0,
        88.55,
        64.42,
        67.2,
        20.89,
        6.67
      ],
      "code_scores": [
        19.51,
        30.74,
        2.51,
        0.0,
        7.47,
        16.46,
        23.17,
        35.02,
        2.15,
        0.0,
        1.81,
        20.73,
        19.51,
        31.91,
        0.36,
        2.3,
        2.49,
        14.63
      ],
      "reasoning_scores": [
        62.71,
        20.33,
        28.79,
        0.18402174,
        28.34,
        62.71,
        22.91,
        22.73,
        0.18641304,
        28.49,
        60.34,
        20.61,
        23.23,
        0.18130435,
        27.45
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.23
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.67
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.77
              },
              {
                "metric": "lcb_test_output",
                "score": 3.92
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 17.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.92
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.14,
          "math_avg": 28.92,
          "code_avg": -6.65,
          "reasoning_avg": -7.32,
          "overall_avg": 4.27,
          "overall_efficiency": 0.004257,
          "general_efficiency": 0.002132,
          "math_efficiency": 0.028817,
          "code_efficiency": -0.006623,
          "reasoning_efficiency": -0.007298,
          "general_task_scores": [
            18.49,
            3.58,
            1.75,
            -15.27
          ],
          "math_task_scores": [
            32.82,
            44.83,
            46.0,
            16.48,
            4.45
          ],
          "code_task_scores": [
            -6.71,
            -21.91,
            -1.91,
            0.56,
            3.92,
            -13.83
          ],
          "reasoning_task_scores": [
            -18.42,
            -41.28,
            -4.88,
            -0.13,
            28.09
          ]
        },
        "vs_instruct": {
          "general_avg": -15.36,
          "math_avg": 7.94,
          "code_avg": -27.02,
          "reasoning_avg": -8.96,
          "overall_avg": -10.85,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.96,
            -36.4,
            -2.58,
            -24.4
          ],
          "math_task_scores": [
            3.56,
            14.81,
            17.0,
            8.2,
            -3.89
          ],
          "code_task_scores": [
            -46.95,
            -38.65,
            -8.01,
            -4.24,
            -21.19,
            -43.1
          ],
          "reasoning_task_scores": [
            -17.4,
            -41.57,
            0.68,
            -0.24,
            13.7
          ]
        }
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 45.22,
      "math_avg": 64.5,
      "code_avg": 3.9,
      "reasoning_avg": 35.37,
      "overall_avg": 37.25,
      "overall_efficiency": 0.029074,
      "general_efficiency": 0.01996,
      "math_efficiency": 0.144202,
      "code_efficiency": -0.05035,
      "reasoning_efficiency": 0.002483,
      "general_scores": [
        78.91,
        29.9,
        31.61,
        40.4407143
      ],
      "math_scores": [
        88.86,
        88.16,
        84.8,
        40.67,
        20.0
      ],
      "code_scores": [
        6.1,
        1.95,
        0.72,
        2.71,
        1.58,
        10.37
      ],
      "reasoning_scores": [
        78.31,
        19.22,
        20.71,
        0.29923913,
        58.31
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.61
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.67
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 1.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.71
              },
              {
                "metric": "lcb_test_output",
                "score": 1.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 10.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.17,
          "math_avg": 44.57,
          "code_avg": -15.56,
          "reasoning_avg": 0.77,
          "overall_avg": 8.99,
          "overall_efficiency": 0.029074,
          "general_efficiency": 0.01996,
          "math_efficiency": 0.144202,
          "code_efficiency": -0.05035,
          "reasoning_efficiency": 0.002483,
          "general_task_scores": [
            14.54,
            9.56,
            -5.04,
            5.61
          ],
          "math_task_scores": [
            32.45,
            68.46,
            65.6,
            36.33,
            20.0
          ],
          "code_task_scores": [
            -21.34,
            -52.52,
            -2.86,
            2.5,
            1.58,
            -20.73
          ],
          "reasoning_task_scores": [
            -2.03,
            -43.34,
            -9.09,
            -0.01,
            58.31
          ]
        },
        "vs_instruct": {
          "general_avg": -11.33,
          "math_avg": 23.59,
          "code_avg": -35.94,
          "reasoning_avg": -0.87,
          "overall_avg": -6.14,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.99,
            -30.43,
            -9.37,
            -3.52
          ],
          "math_task_scores": [
            3.19,
            38.44,
            36.6,
            28.05,
            11.66
          ],
          "code_task_scores": [
            -61.58,
            -69.26,
            -8.96,
            -2.3,
            -23.53,
            -50.0
          ],
          "reasoning_task_scores": [
            -1.01,
            -43.63,
            -3.53,
            -0.12,
            43.92
          ]
        }
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 38.09,
      "math_avg": 39.36,
      "code_avg": 7.81,
      "reasoning_avg": 27.27,
      "overall_avg": 28.13,
      "overall_efficiency": -0.000766,
      "general_efficiency": -0.00565,
      "math_efficiency": 0.114598,
      "code_efficiency": -0.068783,
      "reasoning_efficiency": -0.043226,
      "general_scores": [
        69.02,
        23.81,
        34.83,
        26.6914286,
        62.72,
        24.03,
        36.17,
        21.8414286,
        72.35,
        25.4725,
        35.77,
        24.355
      ],
      "math_scores": [
        73.69,
        56.82,
        47.4,
        16.24,
        3.33,
        74.22,
        57.2,
        49.8,
        16.46,
        3.33,
        72.86,
        56.78,
        46.4,
        15.83,
        0.0
      ],
      "code_scores": [
        0.0,
        43.19,
        0.0,
        0.21,
        0.23,
        0.0,
        0.61,
        45.91,
        0.0,
        0.21,
        0.68,
        0.0,
        0.0,
        43.19,
        0.0,
        6.05,
        0.23,
        0.0
      ],
      "reasoning_scores": [
        70.17,
        34.49,
        15.66,
        0.20608696,
        12.17,
        72.2,
        34.21,
        17.17,
        0.21119565,
        13.65,
        72.54,
        37.13,
        15.15,
        0.20913043,
        13.95
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.44
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.64
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.26
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.96,
          "math_avg": 19.43,
          "code_avg": -11.66,
          "reasoning_avg": -7.33,
          "overall_avg": -0.13,
          "overall_efficiency": -0.000766,
          "general_efficiency": -0.00565,
          "math_efficiency": 0.114598,
          "code_efficiency": -0.068783,
          "reasoning_efficiency": -0.043226,
          "general_task_scores": [
            3.66,
            4.1,
            -1.06,
            -10.53
          ],
          "math_task_scores": [
            17.18,
            37.23,
            28.67,
            11.84,
            2.22
          ],
          "code_task_scores": [
            -27.24,
            -10.37,
            -3.58,
            1.95,
            0.38,
            -31.1
          ],
          "reasoning_task_scores": [
            -8.7,
            -27.28,
            -13.81,
            -0.1,
            13.26
          ]
        },
        "vs_instruct": {
          "general_avg": -18.45,
          "math_avg": -1.55,
          "code_avg": -32.04,
          "reasoning_avg": -8.97,
          "overall_avg": -15.25,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.87,
            -35.89,
            -5.39,
            -19.66
          ],
          "math_task_scores": [
            -12.08,
            7.21,
            -0.33,
            3.56,
            -6.12
          ],
          "code_task_scores": [
            -67.48,
            -27.11,
            -9.68,
            -2.85,
            -24.73,
            -60.37
          ],
          "reasoning_task_scores": [
            -7.68,
            -27.57,
            -8.25,
            -0.21,
            -1.13
          ]
        }
      },
      "affiliation": "Ruben Roy",
      "year": "2025",
      "size": "170k",
      "size_precise": "169527",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 40.6,
      "math_avg": 30.66,
      "code_avg": 13.8,
      "reasoning_avg": 25.75,
      "overall_avg": 27.7,
      "overall_efficiency": -0.002788,
      "general_efficiency": 0.007777,
      "math_efficiency": 0.053654,
      "code_efficiency": -0.028342,
      "reasoning_efficiency": -0.044243,
      "general_scores": [
        73.45,
        29.815,
        33.55,
        24.0585714,
        73.13,
        29.4225,
        34.41,
        25.3292857,
        75.09,
        31.1675,
        32.41,
        25.39
      ],
      "math_scores": [
        85.75,
        27.82,
        27.6,
        11.18,
        0.0,
        85.97,
        27.9,
        28.6,
        10.98,
        0.0,
        86.05,
        28.78,
        28.2,
        11.11,
        0.0
      ],
      "code_scores": [
        20.12,
        36.19,
        1.08,
        5.43,
        11.31,
        4.88,
        17.07,
        40.86,
        1.79,
        1.46,
        14.25,
        3.05,
        17.07,
        39.3,
        1.08,
        14.41,
        12.9,
        6.1
      ],
      "reasoning_scores": [
        58.64,
        43.53,
        18.69,
        0.22565217,
        7.42,
        61.69,
        44.5,
        20.71,
        0.21793478,
        7.12,
        53.56,
        44.49,
        19.19,
        0.2223913,
        6.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.93
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.17
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.1
              },
              {
                "metric": "lcb_test_output",
                "score": 12.82
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 4.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.96
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.56,
          "math_avg": 10.73,
          "code_avg": -5.67,
          "reasoning_avg": -8.85,
          "overall_avg": -0.56,
          "overall_efficiency": -0.002788,
          "general_efficiency": 0.007777,
          "math_efficiency": 0.053654,
          "code_efficiency": -0.028342,
          "reasoning_efficiency": -0.044243,
          "general_task_scores": [
            9.52,
            9.8,
            -3.19,
            -9.9
          ],
          "math_task_scores": [
            29.51,
            8.47,
            8.93,
            6.75,
            0.0
          ],
          "code_task_scores": [
            -9.35,
            -15.69,
            -2.26,
            6.89,
            12.82,
            -26.42
          ],
          "reasoning_task_scores": [
            -22.38,
            -18.39,
            -10.27,
            -0.09,
            6.87
          ]
        },
        "vs_instruct": {
          "general_avg": -15.94,
          "math_avg": -10.25,
          "code_avg": -26.05,
          "reasoning_avg": -10.49,
          "overall_avg": -15.68,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.01,
            -30.18,
            -7.52,
            -19.03
          ],
          "math_task_scores": [
            0.25,
            -21.55,
            -20.07,
            -1.53,
            -8.34
          ],
          "code_task_scores": [
            -49.59,
            -32.43,
            -8.36,
            2.09,
            -12.29,
            -55.69
          ],
          "reasoning_task_scores": [
            -21.36,
            -18.68,
            -4.71,
            -0.2,
            -7.52
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 40.26,
      "math_avg": 39.13,
      "code_avg": 15.65,
      "reasoning_avg": 28.04,
      "overall_avg": 30.77,
      "overall_efficiency": 0.004288,
      "general_efficiency": 0.002079,
      "math_efficiency": 0.032798,
      "code_efficiency": -0.006518,
      "reasoning_efficiency": -0.011208,
      "general_scores": [
        75.63,
        24.375,
        36.13,
        25.4164286,
        74.56,
        25.16,
        36.13,
        25.97,
        74.58,
        23.74,
        35.7,
        25.7685714
      ],
      "math_scores": [
        83.7,
        47.08,
        48.0,
        14.61,
        3.33,
        83.7,
        47.08,
        48.0,
        14.84,
        0.0,
        84.31,
        47.18,
        47.2,
        14.59,
        3.33
      ],
      "code_scores": [
        18.9,
        40.86,
        0.36,
        3.13,
        4.75,
        18.29,
        19.51,
        47.86,
        1.08,
        2.51,
        5.66,
        22.56,
        22.56,
        47.47,
        2.15,
        2.09,
        4.3,
        17.68
      ],
      "reasoning_scores": [
        62.37,
        44.06,
        19.19,
        0.19586957,
        15.73,
        62.37,
        42.44,
        18.18,
        0.21054348,
        16.02,
        62.71,
        43.34,
        18.18,
        0.19315217,
        15.43
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.9
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.68
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.58
              },
              {
                "metric": "lcb_test_output",
                "score": 4.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 19.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.22,
          "math_avg": 19.2,
          "code_avg": -3.82,
          "reasoning_avg": -6.56,
          "overall_avg": 2.51,
          "overall_efficiency": 0.004288,
          "general_efficiency": 0.002079,
          "math_efficiency": 0.032798,
          "code_efficiency": -0.006518,
          "reasoning_efficiency": -0.011208,
          "general_task_scores": [
            10.55,
            4.08,
            -0.66,
            -9.11
          ],
          "math_task_scores": [
            27.49,
            27.41,
            28.53,
            10.34,
            2.22
          ],
          "code_task_scores": [
            -7.12,
            -9.07,
            -2.38,
            2.37,
            4.9,
            -11.59
          ],
          "reasoning_task_scores": [
            -17.86,
            -19.28,
            -11.28,
            -0.11,
            15.73
          ]
        },
        "vs_instruct": {
          "general_avg": -16.28,
          "math_avg": -1.78,
          "code_avg": -24.19,
          "reasoning_avg": -8.2,
          "overall_avg": -12.61,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.98,
            -35.9,
            -4.99,
            -18.24
          ],
          "math_task_scores": [
            -1.77,
            -2.61,
            -0.47,
            2.06,
            -6.12
          ],
          "code_task_scores": [
            -47.36,
            -25.81,
            -8.48,
            -2.43,
            -20.21,
            -40.86
          ],
          "reasoning_task_scores": [
            -16.84,
            -19.57,
            -5.72,
            -0.22,
            1.34
          ]
        }
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 36.29,
      "math_avg": 23.49,
      "code_avg": 7.73,
      "reasoning_avg": 23.42,
      "overall_avg": 22.73,
      "overall_efficiency": -0.234479,
      "general_efficiency": -0.116958,
      "math_efficiency": 0.151187,
      "code_efficiency": -0.497804,
      "reasoning_efficiency": -0.47434,
      "general_scores": [
        76.87,
        22.8025,
        29.56,
        17.1028571,
        76.48,
        21.865,
        31.14,
        17.2092857,
        75.01,
        21.5775,
        30.15,
        15.695
      ],
      "math_scores": [
        74.0,
        17.66,
        16.4,
        7.48,
        0.0,
        73.01,
        18.64,
        18.6,
        7.32,
        0.0,
        72.25,
        17.76,
        18.2,
        7.77,
        3.33
      ],
      "code_scores": [
        0.0,
        41.63,
        0.0,
        0.63,
        0.0,
        0.0,
        0.0,
        47.08,
        0.0,
        1.25,
        0.0,
        0.0,
        0.0,
        47.08,
        0.0,
        1.46,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        71.53,
        25.66,
        14.65,
        0.18434783,
        4.01,
        72.54,
        29.84,
        14.14,
        0.17978261,
        4.75,
        66.1,
        27.13,
        16.67,
        0.18173913,
        3.71
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.06
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.54
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -2.76,
          "math_avg": 3.56,
          "code_avg": -11.74,
          "reasoning_avg": -11.18,
          "overall_avg": -5.53,
          "overall_efficiency": -0.234479,
          "general_efficiency": -0.116958,
          "math_efficiency": 0.151187,
          "code_efficiency": -0.497804,
          "reasoning_efficiency": -0.47434,
          "general_task_scores": [
            11.75,
            1.74,
            -6.37,
            -18.16
          ],
          "math_task_scores": [
            16.68,
            -1.68,
            -1.47,
            3.18,
            1.11
          ],
          "code_task_scores": [
            -27.44,
            -9.21,
            -3.58,
            0.9,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -10.28,
            -35.02,
            -14.65,
            -0.13,
            4.16
          ]
        },
        "vs_instruct": {
          "general_avg": -20.25,
          "math_avg": -17.41,
          "code_avg": -32.11,
          "reasoning_avg": -12.83,
          "overall_avg": -20.65,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.78,
            -38.25,
            -10.7,
            -27.29
          ],
          "math_task_scores": [
            -12.58,
            -31.7,
            -30.47,
            -5.1,
            -7.22
          ],
          "code_task_scores": [
            -67.68,
            -25.95,
            -9.68,
            -3.9,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -9.26,
            -35.31,
            -9.09,
            -0.24,
            -10.23
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 45.58,
      "math_avg": 22.18,
      "code_avg": 39.99,
      "reasoning_avg": 34.27,
      "overall_avg": 35.5,
      "overall_efficiency": 0.007466,
      "general_efficiency": 0.006734,
      "math_efficiency": 0.002317,
      "code_efficiency": 0.021158,
      "reasoning_efficiency": -0.000345,
      "general_scores": [
        68.29,
        42.195,
        37.6,
        32.3742857,
        69.3,
        42.1725,
        37.03,
        32.5442857,
        72.57,
        41.9,
        37.91,
        33.0542857
      ],
      "math_scores": [
        56.33,
        21.68,
        21.6,
        10.89,
        3.33,
        52.62,
        21.88,
        20.8,
        11.11,
        3.33,
        54.66,
        22.34,
        20.4,
        11.7,
        0.0
      ],
      "code_scores": [
        64.02,
        66.93,
        6.81,
        20.88,
        20.59,
        56.1,
        64.02,
        64.98,
        5.02,
        16.08,
        29.19,
        57.32,
        68.29,
        64.2,
        5.02,
        21.71,
        28.28,
        60.37
      ],
      "reasoning_scores": [
        78.98,
        58.95,
        30.81,
        0.3301087,
        3.86,
        77.97,
        58.69,
        28.79,
        0.34315217,
        3.86,
        78.98,
        60.49,
        27.78,
        0.32434783,
        3.86
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.66
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.56
              },
              {
                "metric": "lcb_test_output",
                "score": 26.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.64
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.13
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.86
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.53,
          "math_avg": 2.25,
          "code_avg": 20.52,
          "reasoning_avg": -0.33,
          "overall_avg": 7.24,
          "overall_efficiency": 0.007466,
          "general_efficiency": 0.006734,
          "math_efficiency": 0.002317,
          "code_efficiency": 0.021158,
          "reasoning_efficiency": -0.000345,
          "general_task_scores": [
            5.68,
            21.75,
            0.86,
            -2.17
          ],
          "math_task_scores": [
            -1.87,
            2.27,
            1.73,
            6.89,
            2.22
          ],
          "code_task_scores": [
            38.0,
            10.9,
            2.04,
            19.35,
            26.02,
            26.83
          ],
          "reasoning_task_scores": [
            -1.7,
            -3.18,
            -0.67,
            0.02,
            3.86
          ]
        },
        "vs_instruct": {
          "general_avg": -10.96,
          "math_avg": -18.73,
          "code_avg": 0.15,
          "reasoning_avg": -1.98,
          "overall_avg": -7.88,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.85,
            -18.24,
            -3.47,
            -11.3
          ],
          "math_task_scores": [
            -31.13,
            -27.75,
            -27.27,
            -1.39,
            -6.12
          ],
          "code_task_scores": [
            -2.24,
            -5.84,
            -4.06,
            14.55,
            0.91,
            -2.44
          ],
          "reasoning_task_scores": [
            -0.68,
            -3.47,
            4.89,
            -0.09,
            -10.53
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 30.56,
      "math_avg": 22.09,
      "code_avg": 19.36,
      "reasoning_avg": 20.57,
      "overall_avg": 23.15,
      "overall_efficiency": -0.00522,
      "general_efficiency": -0.00866,
      "math_efficiency": 0.002207,
      "code_efficiency": -0.000107,
      "reasoning_efficiency": -0.014318,
      "general_scores": [
        49.43,
        32.3675,
        25.81,
        14.0,
        35.65,
        28.535,
        29.6,
        13.3678571,
        57.92,
        28.1425,
        29.6,
        22.2878571
      ],
      "math_scores": [
        58.45,
        19.4,
        20.8,
        9.85,
        0.0,
        63.61,
        20.3,
        16.6,
        10.34,
        0.0,
        63.61,
        20.3,
        16.6,
        11.52,
        0.0
      ],
      "code_scores": [
        9.76,
        45.53,
        2.87,
        20.67,
        11.31,
        26.83,
        19.51,
        45.53,
        3.23,
        14.2,
        14.03,
        23.78,
        12.8,
        46.3,
        2.87,
        6.26,
        17.42,
        25.61
      ],
      "reasoning_scores": [
        63.39,
        18.38,
        21.21,
        0.22717391,
        5.04,
        62.03,
        15.63,
        14.65,
        0.21304348,
        4.01,
        62.03,
        19.87,
        17.68,
        0.2198913,
        4.01
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.71
              },
              {
                "metric": "lcb_test_output",
                "score": 14.25
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 25.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.35
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.49,
          "math_avg": 2.16,
          "code_avg": -0.1,
          "reasoning_avg": -14.03,
          "overall_avg": -5.11,
          "overall_efficiency": -0.00522,
          "general_efficiency": -0.00866,
          "math_efficiency": 0.002207,
          "code_efficiency": -0.000107,
          "reasoning_efficiency": -0.014318,
          "general_task_scores": [
            -16.7,
            9.34,
            -8.31,
            -18.28
          ],
          "math_task_scores": [
            5.48,
            0.3,
            -1.2,
            6.23,
            0.0
          ],
          "code_task_scores": [
            -13.42,
            -8.68,
            -0.59,
            13.5,
            14.25,
            -5.69
          ],
          "reasoning_task_scores": [
            -17.86,
            -44.6,
            -11.95,
            -0.09,
            4.35
          ]
        },
        "vs_instruct": {
          "general_avg": -25.98,
          "math_avg": -18.82,
          "code_avg": -20.48,
          "reasoning_avg": -15.67,
          "overall_avg": -20.24,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -33.23,
            -30.65,
            -12.64,
            -27.41
          ],
          "math_task_scores": [
            -23.78,
            -29.72,
            -30.2,
            -2.05,
            -8.34
          ],
          "code_task_scores": [
            -53.66,
            -25.42,
            -6.69,
            8.7,
            -10.86,
            -34.96
          ],
          "reasoning_task_scores": [
            -16.84,
            -44.89,
            -6.39,
            -0.2,
            -10.04
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 38.38,
      "math_avg": 30.34,
      "code_avg": 22.51,
      "reasoning_avg": 26.96,
      "overall_avg": 29.55,
      "overall_efficiency": 0.025743,
      "general_efficiency": -0.013237,
      "math_efficiency": 0.208213,
      "code_efficiency": 0.060811,
      "reasoning_efficiency": -0.152814,
      "general_scores": [
        48.18,
        27.6075,
        37.34,
        35.5157143,
        60.17,
        28.75,
        38.25,
        36.5564286,
        47.39,
        28.1825,
        37.67,
        34.9992857
      ],
      "math_scores": [
        66.64,
        33.3,
        31.8,
        16.78,
        3.33,
        66.41,
        34.06,
        35.0,
        17.1,
        0.0,
        67.17,
        32.86,
        34.4,
        16.26,
        0.0
      ],
      "code_scores": [
        35.37,
        60.7,
        3.58,
        0.0,
        9.5,
        25.61,
        40.85,
        61.87,
        2.87,
        0.0,
        9.73,
        18.9,
        34.15,
        61.48,
        3.94,
        0.0,
        7.92,
        28.66
      ],
      "reasoning_scores": [
        75.59,
        24.23,
        21.21,
        0.235,
        10.53,
        75.59,
        22.58,
        30.81,
        0.24684783,
        9.79,
        75.25,
        26.43,
        20.71,
        0.24369565,
        10.98
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.79
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 24.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.66,
          "math_avg": 10.41,
          "code_avg": 3.04,
          "reasoning_avg": -7.64,
          "overall_avg": 1.29,
          "overall_efficiency": 0.025743,
          "general_efficiency": -0.013237,
          "math_efficiency": 0.208213,
          "code_efficiency": 0.060811,
          "reasoning_efficiency": -0.152814,
          "general_task_scores": [
            -12.46,
            7.84,
            1.1,
            0.86
          ],
          "math_task_scores": [
            10.33,
            13.71,
            14.53,
            12.37,
            1.11
          ],
          "code_task_scores": [
            9.35,
            6.88,
            -0.12,
            -0.21,
            9.05,
            -6.71
          ],
          "reasoning_task_scores": [
            -4.86,
            -38.15,
            -5.56,
            -0.07,
            10.43
          ]
        },
        "vs_instruct": {
          "general_avg": -18.16,
          "math_avg": -10.57,
          "code_avg": -17.34,
          "reasoning_avg": -9.28,
          "overall_avg": -13.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -28.99,
            -32.15,
            -3.23,
            -8.27
          ],
          "math_task_scores": [
            -18.93,
            -16.31,
            -14.47,
            4.09,
            -7.22
          ],
          "code_task_scores": [
            -30.89,
            -9.86,
            -6.22,
            -5.01,
            -16.06,
            -35.98
          ],
          "reasoning_task_scores": [
            -3.84,
            -38.44,
            0.0,
            -0.18,
            -3.96
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 37,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 44.78,
      "math_avg": 35.76,
      "code_avg": 24.9,
      "reasoning_avg": 32.58,
      "overall_avg": 34.51,
      "overall_efficiency": 0.006984,
      "general_efficiency": 0.006415,
      "math_efficiency": 0.017711,
      "code_efficiency": 0.006074,
      "reasoning_efficiency": -0.002262,
      "general_scores": [
        77.43,
        33.9675,
        38.04,
        32.0078571,
        78.58,
        31.5525,
        36.28,
        31.13,
        78.3,
        34.055,
        36.53,
        29.4914286
      ],
      "math_scores": [
        87.41,
        38.42,
        36.6,
        13.14,
        0.0,
        86.13,
        41.04,
        40.2,
        13.71,
        0.0,
        85.97,
        40.54,
        39.8,
        13.48,
        0.0
      ],
      "code_scores": [
        44.51,
        60.7,
        3.58,
        0.63,
        9.73,
        33.54,
        40.24,
        56.03,
        4.3,
        1.04,
        11.99,
        32.32,
        35.37,
        60.31,
        3.58,
        1.46,
        12.22,
        36.59
      ],
      "reasoning_scores": [
        76.61,
        55.0,
        21.21,
        0.28184783,
        11.28,
        77.63,
        51.36,
        21.21,
        0.27869565,
        13.06,
        78.64,
        50.53,
        20.2,
        0.28467391,
        11.13
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.82
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.04
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 34.15
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.82
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.73,
          "math_avg": 15.83,
          "code_avg": 5.43,
          "reasoning_avg": -2.02,
          "overall_avg": 6.24,
          "overall_efficiency": 0.006984,
          "general_efficiency": 0.006415,
          "math_efficiency": 0.017711,
          "code_efficiency": 0.006074,
          "reasoning_efficiency": -0.002262,
          "general_task_scores": [
            13.73,
            12.85,
            0.3,
            -3.95
          ],
          "math_task_scores": [
            30.09,
            20.3,
            19.67,
            9.1,
            0.0
          ],
          "code_task_scores": [
            12.6,
            4.54,
            0.24,
            0.83,
            11.31,
            3.05
          ],
          "reasoning_task_scores": [
            -2.71,
            -10.26,
            -8.93,
            -0.03,
            11.82
          ]
        },
        "vs_instruct": {
          "general_avg": -11.76,
          "math_avg": -5.15,
          "code_avg": -14.95,
          "reasoning_avg": -3.66,
          "overall_avg": -8.88,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.8,
            -27.14,
            -4.03,
            -13.08
          ],
          "math_task_scores": [
            0.83,
            -9.72,
            -9.33,
            0.82,
            -8.34
          ],
          "code_task_scores": [
            -27.64,
            -12.2,
            -5.86,
            -3.97,
            -13.8,
            -26.22
          ],
          "reasoning_task_scores": [
            -1.69,
            -10.55,
            -3.37,
            -0.14,
            -2.57
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 38,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 44.01,
      "math_avg": 38.64,
      "code_avg": 28.61,
      "reasoning_avg": 25.89,
      "overall_avg": 34.29,
      "overall_efficiency": 0.030122,
      "general_efficiency": 0.024813,
      "math_efficiency": 0.093523,
      "code_efficiency": 0.045717,
      "reasoning_efficiency": -0.043567,
      "general_scores": [
        78.62,
        33.4775,
        42.71,
        22.755,
        78.94,
        33.3025,
        42.78,
        20.2385714,
        78.89,
        33.2825,
        42.44,
        20.68
      ],
      "math_scores": [
        88.7,
        42.24,
        42.4,
        14.68,
        6.67,
        88.78,
        41.68,
        42.0,
        15.2,
        3.33,
        89.01,
        41.68,
        41.6,
        14.93,
        6.67
      ],
      "code_scores": [
        44.51,
        55.25,
        5.38,
        0.84,
        17.87,
        49.39,
        46.95,
        55.64,
        6.81,
        0.21,
        14.71,
        40.24,
        50.61,
        55.64,
        7.89,
        0.21,
        16.52,
        46.34
      ],
      "reasoning_scores": [
        74.24,
        21.41,
        20.2,
        0.27847826,
        15.58,
        73.22,
        20.92,
        16.16,
        0.27195652,
        16.62,
        70.51,
        21.02,
        23.23,
        0.26173913,
        14.39
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.82
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.83
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 16.37
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 45.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.12
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.96,
          "math_avg": 18.71,
          "code_avg": 9.14,
          "reasoning_avg": -8.71,
          "overall_avg": 6.03,
          "overall_efficiency": 0.030122,
          "general_efficiency": 0.024813,
          "math_efficiency": 0.093523,
          "code_efficiency": 0.045717,
          "reasoning_efficiency": -0.043567,
          "general_task_scores": [
            14.45,
            13.01,
            5.99,
            -13.61
          ],
          "math_task_scores": [
            32.42,
            22.17,
            22.8,
            10.6,
            5.56
          ],
          "code_task_scores": [
            19.92,
            1.04,
            3.11,
            0.21,
            16.37,
            14.22
          ],
          "reasoning_task_scores": [
            -7.68,
            -41.44,
            -9.94,
            -0.04,
            15.53
          ]
        },
        "vs_instruct": {
          "general_avg": -12.53,
          "math_avg": -2.27,
          "code_avg": -11.23,
          "reasoning_avg": -10.36,
          "overall_avg": -9.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.08,
            -26.98,
            1.66,
            -22.74
          ],
          "math_task_scores": [
            3.16,
            -7.85,
            -6.2,
            2.32,
            -2.78
          ],
          "code_task_scores": [
            -20.32,
            -15.7,
            -2.99,
            -4.59,
            -8.74,
            -15.05
          ],
          "reasoning_task_scores": [
            -6.66,
            -41.73,
            -4.38,
            -0.15,
            1.14
          ]
        }
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 40.28,
      "math_avg": 42.69,
      "code_avg": 9.05,
      "reasoning_avg": 29.12,
      "overall_avg": 30.28,
      "overall_efficiency": 0.002255,
      "general_efficiency": 0.001372,
      "math_efficiency": 0.025395,
      "code_efficiency": -0.011629,
      "reasoning_efficiency": -0.006116,
      "general_scores": [
        74.77,
        25.6175,
        36.82,
        25.775,
        71.43,
        25.19,
        35.96,
        26.4435714,
        75.07,
        26.26,
        35.46,
        24.51
      ],
      "math_scores": [
        83.47,
        54.86,
        54.8,
        19.49,
        10.0,
        82.56,
        53.28,
        53.0,
        19.83,
        0.0,
        82.64,
        53.9,
        49.6,
        19.58,
        3.33
      ],
      "code_scores": [
        6.71,
        36.96,
        0.36,
        0.21,
        0.23,
        6.71,
        7.93,
        33.46,
        1.43,
        0.21,
        0.68,
        9.15,
        8.54,
        40.86,
        1.08,
        0.0,
        1.58,
        6.71
      ],
      "reasoning_scores": [
        62.37,
        39.04,
        21.21,
        0.21836957,
        25.07,
        62.03,
        32.48,
        19.19,
        0.20858696,
        24.63,
        67.12,
        35.3,
        25.76,
        0.21782609,
        21.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 37.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 0.83
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.89
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.23,
          "math_avg": 22.76,
          "code_avg": -10.42,
          "reasoning_avg": -5.48,
          "overall_avg": 2.02,
          "overall_efficiency": 0.002255,
          "general_efficiency": 0.001372,
          "math_efficiency": 0.025395,
          "code_efficiency": -0.011629,
          "reasoning_efficiency": -0.006116,
          "general_task_scores": [
            9.39,
            5.35,
            -0.57,
            -9.25
          ],
          "math_task_scores": [
            26.48,
            34.31,
            33.27,
            15.29,
            4.44
          ],
          "code_task_scores": [
            -19.71,
            -17.38,
            -2.62,
            -0.07,
            0.83,
            -23.58
          ],
          "reasoning_task_scores": [
            -16.5,
            -26.95,
            -7.75,
            -0.1,
            23.89
          ]
        },
        "vs_instruct": {
          "general_avg": -16.27,
          "math_avg": 1.78,
          "code_avg": -30.8,
          "reasoning_avg": -7.12,
          "overall_avg": -13.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.14,
            -34.64,
            -4.9,
            -18.38
          ],
          "math_task_scores": [
            -2.78,
            4.29,
            4.27,
            7.01,
            -3.9
          ],
          "code_task_scores": [
            -59.95,
            -34.12,
            -8.72,
            -4.87,
            -24.28,
            -52.85
          ],
          "reasoning_task_scores": [
            -15.48,
            -27.24,
            -2.19,
            -0.21,
            9.5
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 44.06,
      "math_avg": 33.85,
      "code_avg": 17.01,
      "reasoning_avg": 31.46,
      "overall_avg": 31.6,
      "overall_efficiency": 0.055664,
      "general_efficiency": 0.083716,
      "math_efficiency": 0.232474,
      "code_efficiency": -0.041064,
      "reasoning_efficiency": -0.052472,
      "general_scores": [
        69.86,
        35.1975,
        37.65,
        31.24071,
        72.88,
        34.855,
        37.24,
        31.8835714,
        73.24,
        34.1825,
        38.54,
        31.9521429
      ],
      "math_scores": [
        82.71,
        39.2,
        36.6,
        11.04,
        0.0,
        79.91,
        37.46,
        34.0,
        10.75,
        3.33,
        81.43,
        37.98,
        39.2,
        10.86,
        3.33
      ],
      "code_scores": [
        12.8,
        49.03,
        2.51,
        6.89,
        18.1,
        7.93,
        14.02,
        51.36,
        2.51,
        0.21,
        17.19,
        17.68,
        19.51,
        53.31,
        1.08,
        0.0,
        14.93,
        17.07
      ],
      "reasoning_scores": [
        66.1,
        44.42,
        23.74,
        0.283913,
        18.1,
        67.12,
        51.66,
        22.73,
        0.27717391,
        17.66,
        69.83,
        54.27,
        17.17,
        0.28543478,
        18.25
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.35
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.88
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.23
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.03
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.37
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 14.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.12
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.01,
          "math_avg": 13.92,
          "code_avg": -2.46,
          "reasoning_avg": -3.14,
          "overall_avg": 3.33,
          "overall_efficiency": 0.055664,
          "general_efficiency": 0.083716,
          "math_efficiency": 0.232474,
          "code_efficiency": -0.041064,
          "reasoning_efficiency": -0.052472,
          "general_task_scores": [
            7.62,
            14.4,
            1.16,
            -3.14
          ],
          "math_task_scores": [
            24.94,
            18.51,
            17.4,
            6.54,
            2.22
          ],
          "code_task_scores": [
            -12.0,
            -3.24,
            -1.55,
            2.16,
            16.74,
            -16.87
          ],
          "reasoning_task_scores": [
            -12.66,
            -12.44,
            -8.59,
            -0.03,
            18.0
          ]
        },
        "vs_instruct": {
          "general_avg": -12.48,
          "math_avg": -7.06,
          "code_avg": -22.84,
          "reasoning_avg": -4.78,
          "overall_avg": -11.79,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.91,
            -25.58,
            -3.17,
            -12.27
          ],
          "math_task_scores": [
            -4.32,
            -11.51,
            -11.6,
            -1.74,
            -6.12
          ],
          "code_task_scores": [
            -52.24,
            -19.98,
            -7.65,
            -2.64,
            -8.37,
            -46.14
          ],
          "reasoning_task_scores": [
            -11.64,
            -12.73,
            -3.03,
            -0.14,
            3.61
          ]
        }
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 46.22,
      "math_avg": 27.29,
      "code_avg": 40.38,
      "reasoning_avg": 34.67,
      "overall_avg": 37.14,
      "overall_efficiency": 0.01918,
      "general_efficiency": 0.0155,
      "math_efficiency": 0.015902,
      "code_efficiency": 0.045176,
      "reasoning_efficiency": 0.000145,
      "general_scores": [
        53.26,
        54.035,
        40.41,
        35.9864286,
        56.15,
        53.8975,
        39.84,
        35.0885714,
        55.79,
        54.4775,
        40.3,
        35.4385714
      ],
      "math_scores": [
        70.43,
        26.8,
        28.0,
        11.99,
        3.33,
        72.86,
        26.04,
        25.0,
        11.09,
        0.0,
        71.65,
        26.0,
        25.0,
        11.2,
        0.0
      ],
      "code_scores": [
        68.29,
        65.37,
        7.17,
        19.42,
        25.79,
        56.1,
        68.9,
        65.37,
        7.53,
        19.62,
        25.57,
        56.1,
        67.68,
        66.93,
        7.17,
        17.95,
        26.47,
        55.49
      ],
      "reasoning_scores": [
        78.64,
        64.32,
        26.26,
        0.34152174,
        6.38,
        80.0,
        62.75,
        25.76,
        0.38271739,
        5.64,
        77.63,
        62.72,
        24.24,
        0.37793478,
        4.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.07
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.43
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.29
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.0
              },
              {
                "metric": "lcb_test_output",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.76
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.26
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.54
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.18,
          "math_avg": 7.36,
          "code_avg": 20.92,
          "reasoning_avg": 0.07,
          "overall_avg": 8.88,
          "overall_efficiency": 0.01918,
          "general_efficiency": 0.0155,
          "math_efficiency": 0.015902,
          "code_efficiency": 0.045176,
          "reasoning_efficiency": 0.000145,
          "general_task_scores": [
            -9.3,
            33.8,
            3.53,
            0.67
          ],
          "math_task_scores": [
            15.24,
            6.58,
            6.8,
            7.09,
            1.11
          ],
          "code_task_scores": [
            40.85,
            11.42,
            3.71,
            18.79,
            25.94,
            24.8
          ],
          "reasoning_task_scores": [
            -1.58,
            0.7,
            -4.38,
            0.06,
            5.54
          ]
        },
        "vs_instruct": {
          "general_avg": -10.32,
          "math_avg": -13.62,
          "code_avg": 0.54,
          "reasoning_avg": -1.57,
          "overall_avg": -6.24,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -25.83,
            -6.19,
            -0.8,
            -8.46
          ],
          "math_task_scores": [
            -14.02,
            -23.44,
            -22.2,
            -1.19,
            -7.22
          ],
          "code_task_scores": [
            0.61,
            -5.32,
            -2.39,
            13.99,
            0.83,
            -4.47
          ],
          "reasoning_task_scores": [
            -0.56,
            0.41,
            1.18,
            -0.05,
            -8.85
          ]
        }
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 42,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 39.95,
      "math_avg": 10.51,
      "code_avg": 24.96,
      "reasoning_avg": 29.25,
      "overall_avg": 26.17,
      "overall_efficiency": -0.104478,
      "general_efficiency": 0.045249,
      "math_efficiency": -0.470582,
      "code_efficiency": 0.274614,
      "reasoning_efficiency": -0.267193,
      "general_scores": [
        55.04,
        45.455,
        35.44,
        19.9164286,
        55.95,
        44.5575,
        36.77,
        25.7914286,
        57.15,
        44.5625,
        35.3,
        23.4928571
      ],
      "math_scores": [
        24.94,
        8.98,
        8.2,
        6.39,
        0.0,
        27.98,
        9.1,
        8.4,
        6.28,
        3.33,
        26.46,
        10.4,
        7.6,
        6.23,
        3.33
      ],
      "code_scores": [
        34.74,
        46.69,
        0.0,
        26.1,
        23.08,
        23.17,
        29.27,
        42.41,
        0.0,
        25.05,
        23.08,
        26.83,
        29.27,
        47.08,
        0.0,
        27.14,
        25.34,
        20.12
      ],
      "reasoning_scores": [
        69.83,
        45.78,
        25.76,
        0.29,
        3.41,
        72.2,
        49.66,
        25.76,
        0.29076087,
        2.08,
        71.53,
        45.28,
        24.24,
        0.30923913,
        2.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.1
              },
              {
                "metric": "lcb_test_output",
                "score": 23.83
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 23.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.19
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.62
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.91,
          "math_avg": -9.42,
          "code_avg": 5.5,
          "reasoning_avg": -5.35,
          "overall_avg": -2.09,
          "overall_efficiency": -0.104478,
          "general_efficiency": 0.045249,
          "math_efficiency": -0.470582,
          "code_efficiency": 0.274614,
          "reasoning_efficiency": -0.267193,
          "general_task_scores": [
            -8.32,
            24.52,
            -0.81,
            -11.76
          ],
          "math_task_scores": [
            -29.95,
            -10.21,
            -11.13,
            1.96,
            2.22
          ],
          "code_task_scores": [
            3.65,
            -9.08,
            -3.58,
            25.89,
            23.83,
            -7.73
          ],
          "reasoning_task_scores": [
            -9.15,
            -15.65,
            -4.55,
            -0.01,
            2.62
          ]
        },
        "vs_instruct": {
          "general_avg": -16.59,
          "math_avg": -30.4,
          "code_avg": -14.88,
          "reasoning_avg": -6.99,
          "overall_avg": -17.22,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.85,
            -15.47,
            -5.14,
            -20.89
          ],
          "math_task_scores": [
            -59.21,
            -40.23,
            -40.13,
            -6.32,
            -6.12
          ],
          "code_task_scores": [
            -36.59,
            -25.82,
            -9.68,
            21.09,
            -1.28,
            -37.0
          ],
          "reasoning_task_scores": [
            -8.13,
            -15.94,
            1.01,
            -0.12,
            -11.77
          ]
        }
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 43,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 35.21,
      "math_avg": 18.11,
      "code_avg": 41.53,
      "reasoning_avg": 29.34,
      "overall_avg": 31.05,
      "overall_efficiency": 0.006385,
      "general_efficiency": -0.008785,
      "math_efficiency": -0.004173,
      "code_efficiency": 0.050571,
      "reasoning_efficiency": -0.012071,
      "general_scores": [
        36.87,
        37.875,
        29.04,
        28.7871429,
        43.8,
        41.305,
        33.12,
        26.9528571,
        42.1,
        39.6725,
        33.77,
        29.2584615
      ],
      "math_scores": [
        48.07,
        12.94,
        15.0,
        14.77,
        0.0,
        47.23,
        12.88,
        11.8,
        15.45,
        6.67,
        43.97,
        13.84,
        14.0,
        15.02,
        0.0
      ],
      "code_scores": [
        78.66,
        63.81,
        7.53,
        18.79,
        26.7,
        66.46,
        76.3,
        67.7,
        7.53,
        17.12,
        22.4,
        53.66,
        75.6,
        65.76,
        6.81,
        18.79,
        19.68,
        54.3
      ],
      "reasoning_scores": [
        68.81,
        48.99,
        26.26,
        0.27619565,
        2.97,
        71.53,
        48.14,
        25.25,
        0.30434783,
        2.82,
        71.53,
        46.37,
        23.23,
        0.28576087,
        3.26
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.33
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.23
              },
              {
                "metric": "lcb_test_output",
                "score": 22.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.83
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -3.83,
          "math_avg": -1.82,
          "code_avg": 22.07,
          "reasoning_avg": -5.27,
          "overall_avg": 2.79,
          "overall_efficiency": 0.006385,
          "general_efficiency": -0.008785,
          "math_efficiency": -0.004173,
          "code_efficiency": 0.050571,
          "reasoning_efficiency": -0.012071,
          "general_task_scores": [
            -23.45,
            19.28,
            -4.67,
            -6.5
          ],
          "math_task_scores": [
            -9.99,
            -6.48,
            -5.6,
            10.74,
            2.22
          ],
          "code_task_scores": [
            49.41,
            11.29,
            3.71,
            18.02,
            22.93,
            27.04
          ],
          "reasoning_task_scores": [
            -9.72,
            -14.73,
            -4.89,
            -0.02,
            3.02
          ]
        },
        "vs_instruct": {
          "general_avg": -21.33,
          "math_avg": -22.8,
          "code_avg": 1.69,
          "reasoning_avg": -6.91,
          "overall_avg": -12.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -39.98,
            -20.71,
            -9.0,
            -15.63
          ],
          "math_task_scores": [
            -39.25,
            -36.5,
            -34.6,
            2.46,
            -6.12
          ],
          "code_task_scores": [
            9.17,
            -5.45,
            -2.39,
            13.22,
            -2.18,
            -2.23
          ],
          "reasoning_task_scores": [
            -8.7,
            -15.02,
            0.67,
            -0.13,
            -11.37
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 38.78,
      "math_avg": 17.88,
      "code_avg": 37.61,
      "reasoning_avg": 30.87,
      "overall_avg": 31.29,
      "overall_efficiency": 0.019318,
      "general_efficiency": -0.001718,
      "math_efficiency": -0.013093,
      "code_efficiency": 0.115916,
      "reasoning_efficiency": -0.023836,
      "general_scores": [
        41.45,
        45.0025,
        35.87,
        32.2007143,
        47.87,
        45.9125,
        33.26,
        31.435,
        43.18,
        44.7925,
        33.85,
        30.505
      ],
      "math_scores": [
        49.81,
        14.4,
        12.6,
        11.4,
        0.0,
        47.23,
        14.52,
        14.2,
        10.23,
        3.33,
        50.57,
        13.96,
        14.8,
        11.16,
        0.0
      ],
      "code_scores": [
        64.63,
        57.59,
        6.45,
        19.42,
        20.14,
        56.71,
        67.78,
        53.31,
        7.53,
        17.12,
        20.81,
        56.1,
        68.29,
        57.59,
        5.38,
        15.66,
        19.68,
        62.8
      ],
      "reasoning_scores": [
        74.58,
        54.55,
        24.75,
        0.26608696,
        2.82,
        72.88,
        53.7,
        27.27,
        0.24445652,
        2.23,
        72.2,
        50.86,
        24.24,
        0.25206522,
        2.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.33
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.29
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.4
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.27,
          "math_avg": -2.05,
          "code_avg": 18.14,
          "reasoning_avg": -3.73,
          "overall_avg": 3.02,
          "overall_efficiency": 0.019318,
          "general_efficiency": -0.001718,
          "math_efficiency": -0.013093,
          "code_efficiency": 0.115916,
          "reasoning_efficiency": -0.023836,
          "general_task_scores": [
            -20.2,
            24.9,
            -2.32,
            -3.45
          ],
          "math_task_scores": [
            -7.21,
            -5.41,
            -5.33,
            6.59,
            1.11
          ],
          "code_task_scores": [
            39.46,
            1.69,
            2.87,
            17.19,
            20.21,
            27.44
          ],
          "reasoning_task_scores": [
            -7.12,
            -9.52,
            -4.38,
            -0.06,
            2.43
          ]
        },
        "vs_instruct": {
          "general_avg": -17.76,
          "math_avg": -23.03,
          "code_avg": -2.23,
          "reasoning_avg": -5.37,
          "overall_avg": -12.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.73,
            -15.08,
            -6.65,
            -12.58
          ],
          "math_task_scores": [
            -36.47,
            -35.43,
            -34.33,
            -1.69,
            -7.22
          ],
          "code_task_scores": [
            -0.78,
            -15.05,
            -3.23,
            12.39,
            -4.9,
            -1.83
          ],
          "reasoning_task_scores": [
            -6.1,
            -9.81,
            1.18,
            -0.17,
            -11.96
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 38.34,
      "math_avg": 20.14,
      "code_avg": 41.02,
      "reasoning_avg": 30.79,
      "overall_avg": 32.57,
      "overall_efficiency": 0.03876,
      "general_efficiency": -0.006352,
      "math_efficiency": 0.001917,
      "code_efficiency": 0.193704,
      "reasoning_efficiency": -0.03423,
      "general_scores": [
        46.68,
        39.87,
        34.55,
        31.7035714,
        43.66,
        41.98,
        35.5,
        31.7228571,
        44.31,
        42.24,
        35.17,
        32.685
      ],
      "math_scores": [
        52.69,
        15.34,
        16.6,
        16.06,
        0.0,
        54.89,
        14.64,
        11.4,
        15.29,
        6.67,
        54.59,
        15.14,
        14.0,
        14.84,
        0.0
      ],
      "code_scores": [
        68.29,
        58.37,
        5.02,
        31.11,
        18.78,
        61.59,
        73.17,
        59.14,
        5.38,
        36.95,
        16.74,
        59.15,
        67.07,
        59.14,
        6.81,
        28.6,
        23.3,
        59.76
      ],
      "reasoning_scores": [
        74.58,
        53.03,
        19.19,
        0.25380435,
        3.26,
        77.29,
        53.36,
        23.23,
        0.27521739,
        3.12,
        76.61,
        54.76,
        19.7,
        0.27380435,
        2.97
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.06
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.74
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.22
              },
              {
                "metric": "lcb_test_output",
                "score": 19.61
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.16
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.72
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.71,
          "math_avg": 0.21,
          "code_avg": 21.55,
          "reasoning_avg": -3.81,
          "overall_avg": 4.31,
          "overall_efficiency": 0.03876,
          "general_efficiency": -0.006352,
          "math_efficiency": 0.001917,
          "code_efficiency": 0.193704,
          "reasoning_efficiency": -0.03423,
          "general_task_scores": [
            -19.49,
            21.02,
            -1.58,
            -2.79
          ],
          "math_task_scores": [
            -2.35,
            -4.66,
            -5.2,
            11.06,
            2.22
          ],
          "code_task_scores": [
            42.07,
            4.41,
            2.16,
            32.01,
            19.61,
            29.07
          ],
          "reasoning_task_scores": [
            -4.18,
            -8.84,
            -9.09,
            -0.04,
            3.12
          ]
        },
        "vs_instruct": {
          "general_avg": -18.2,
          "math_avg": -20.77,
          "code_avg": 1.18,
          "reasoning_avg": -5.45,
          "overall_avg": -10.81,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.02,
            -18.97,
            -5.91,
            -11.92
          ],
          "math_task_scores": [
            -31.61,
            -34.68,
            -34.2,
            2.78,
            -6.12
          ],
          "code_task_scores": [
            1.83,
            -12.33,
            -3.94,
            27.21,
            -5.5,
            -0.2
          ],
          "reasoning_task_scores": [
            -3.16,
            -9.13,
            -3.53,
            -0.15,
            -11.27
          ]
        }
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 37.97,
      "math_avg": 17.77,
      "code_avg": 24.35,
      "reasoning_avg": 27.23,
      "overall_avg": 26.83,
      "overall_efficiency": -0.019059,
      "general_efficiency": -0.014371,
      "math_efficiency": -0.028742,
      "code_efficiency": 0.064903,
      "reasoning_efficiency": -0.098026,
      "general_scores": [
        51.47,
        35.24,
        36.01,
        31.8564286,
        44.26,
        35.8975,
        35.28,
        32.9446154,
        51.16,
        34.9925,
        34.57,
        31.9057143
      ],
      "math_scores": [
        51.4,
        14.6,
        13.8,
        8.15,
        0.0,
        50.87,
        15.44,
        15.8,
        8.58,
        0.0,
        49.28,
        15.36,
        14.8,
        8.45,
        0.0
      ],
      "code_scores": [
        48.17,
        52.53,
        2.51,
        0.21,
        0.23,
        44.51,
        43.9,
        52.53,
        3.94,
        0.0,
        0.0,
        45.12,
        46.95,
        53.7,
        2.87,
        0.84,
        0.0,
        40.24
      ],
      "reasoning_scores": [
        77.29,
        27.74,
        22.73,
        0.22836957,
        3.56,
        76.61,
        28.16,
        24.24,
        0.2373913,
        2.52,
        80.0,
        37.2,
        24.75,
        0.23119565,
        2.97
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.13
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.03
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.08,
          "math_avg": -2.16,
          "code_avg": 4.88,
          "reasoning_avg": -7.37,
          "overall_avg": -1.43,
          "overall_efficiency": -0.019059,
          "general_efficiency": -0.014371,
          "math_efficiency": -0.028742,
          "code_efficiency": 0.064903,
          "reasoning_efficiency": -0.098026,
          "general_task_scores": [
            -15.41,
            15.04,
            -1.36,
            -2.59
          ],
          "math_task_scores": [
            -5.89,
            -4.57,
            -4.4,
            4.05,
            0.0
          ],
          "code_task_scores": [
            18.9,
            -1.55,
            -0.47,
            0.14,
            0.08,
            12.19
          ],
          "reasoning_task_scores": [
            -2.37,
            -31.53,
            -5.89,
            -0.08,
            3.02
          ]
        },
        "vs_instruct": {
          "general_avg": -18.58,
          "math_avg": -23.14,
          "code_avg": -15.5,
          "reasoning_avg": -9.01,
          "overall_avg": -16.56,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -31.94,
            -24.94,
            -5.69,
            -11.72
          ],
          "math_task_scores": [
            -35.15,
            -34.59,
            -33.4,
            -4.23,
            -8.34
          ],
          "code_task_scores": [
            -21.34,
            -18.29,
            -6.57,
            -4.66,
            -25.03,
            -17.08
          ],
          "reasoning_task_scores": [
            -1.35,
            -31.82,
            -0.33,
            -0.19,
            -11.37
          ]
        }
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 34.49,
      "math_avg": 20.2,
      "code_avg": 32.22,
      "reasoning_avg": 27.26,
      "overall_avg": 28.54,
      "overall_efficiency": 0.004243,
      "general_efficiency": -0.068673,
      "math_efficiency": 0.004138,
      "code_efficiency": 0.192142,
      "reasoning_efficiency": -0.110636,
      "general_scores": [
        35.06,
        42.0775,
        32.84,
        27.8492857,
        41.69,
        37.0,
        33.39,
        25.5164286,
        38.37,
        37.8975,
        32.65,
        29.5085714
      ],
      "math_scores": [
        62.4,
        10.1,
        10.6,
        17.48,
        6.67,
        59.21,
        9.58,
        11.0,
        17.66,
        0.0,
        58.68,
        9.82,
        9.2,
        17.34,
        3.33
      ],
      "code_scores": [
        48.78,
        59.14,
        8.96,
        0.42,
        20.36,
        47.56,
        59.76,
        58.37,
        6.81,
        0.21,
        24.89,
        49.39,
        56.1,
        56.42,
        7.53,
        0.21,
        20.81,
        54.27
      ],
      "reasoning_scores": [
        76.95,
        29.31,
        30.81,
        0.22,
        1.78,
        74.24,
        32.06,
        28.28,
        0.23684783,
        2.82,
        70.51,
        32.29,
        26.77,
        0.22423913,
        2.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.88
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.77
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 22.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 50.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.56,
          "math_avg": 0.27,
          "code_avg": 12.76,
          "reasoning_avg": -7.34,
          "overall_avg": 0.28,
          "overall_efficiency": 0.004243,
          "general_efficiency": -0.068673,
          "math_efficiency": 0.004138,
          "code_efficiency": 0.192142,
          "reasoning_efficiency": -0.110636,
          "general_task_scores": [
            -26.0,
            18.65,
            -3.69,
            -7.21
          ],
          "math_task_scores": [
            3.69,
            -9.87,
            -8.93,
            13.15,
            3.33
          ],
          "code_task_scores": [
            27.44,
            3.51,
            4.19,
            0.07,
            22.02,
            19.31
          ],
          "reasoning_task_scores": [
            -6.44,
            -31.34,
            -1.18,
            -0.08,
            2.32
          ]
        },
        "vs_instruct": {
          "general_avg": -22.05,
          "math_avg": -20.7,
          "code_avg": -7.62,
          "reasoning_avg": -8.99,
          "overall_avg": -14.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -42.53,
            -21.34,
            -8.02,
            -16.34
          ],
          "math_task_scores": [
            -25.57,
            -39.89,
            -37.93,
            4.87,
            -5.01
          ],
          "code_task_scores": [
            -12.8,
            -13.23,
            -1.91,
            -4.73,
            -3.09,
            -9.96
          ],
          "reasoning_task_scores": [
            -5.42,
            -31.63,
            4.38,
            -0.19,
            -12.07
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 23.59,
      "math_avg": 7.53,
      "code_avg": 14.39,
      "reasoning_avg": 21.09,
      "overall_avg": 16.65,
      "overall_efficiency": -0.210668,
      "general_efficiency": -0.280435,
      "math_efficiency": -0.225,
      "code_efficiency": -0.092147,
      "reasoning_efficiency": -0.245089,
      "general_scores": [
        34.19,
        24.61,
        23.52,
        16.5578571,
        30.04,
        23.915,
        21.53,
        17.355,
        28.96,
        23.7825,
        21.33,
        17.2828571
      ],
      "math_scores": [
        22.29,
        6.62,
        6.6,
        2.76,
        0.0,
        22.74,
        6.54,
        4.2,
        3.21,
        0.0,
        22.14,
        7.52,
        5.8,
        2.51,
        0.0
      ],
      "code_scores": [
        18.9,
        47.47,
        0.36,
        12.94,
        0.45,
        12.8,
        15.24,
        46.3,
        0.72,
        10.44,
        4.3,
        13.41,
        12.2,
        46.69,
        1.08,
        4.18,
        1.13,
        10.37
      ],
      "reasoning_scores": [
        46.44,
        48.76,
        18.18,
        0.15815217,
        1.04,
        39.32,
        37.61,
        18.69,
        0.14782609,
        0.59,
        48.47,
        40.61,
        15.66,
        0.14130435,
        0.59
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.06
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.45
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.19
              },
              {
                "metric": "lcb_test_output",
                "score": 1.96
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 12.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.74
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.74
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -15.46,
          "math_avg": -12.4,
          "code_avg": -5.08,
          "reasoning_avg": -13.51,
          "overall_avg": -11.61,
          "overall_efficiency": -0.210668,
          "general_efficiency": -0.280435,
          "math_efficiency": -0.225,
          "code_efficiency": -0.092147,
          "reasoning_efficiency": -0.245089,
          "general_task_scores": [
            -33.31,
            3.76,
            -14.52,
            -17.76
          ],
          "math_task_scores": [
            -34.02,
            -12.81,
            -13.67,
            -1.51,
            0.0
          ],
          "code_task_scores": [
            -11.99,
            -7.65,
            -2.86,
            8.98,
            1.96,
            -18.91
          ],
          "reasoning_task_scores": [
            -35.6,
            -20.23,
            -12.29,
            -0.16,
            0.74
          ]
        },
        "vs_instruct": {
          "general_avg": -32.95,
          "math_avg": -33.38,
          "code_avg": -25.46,
          "reasoning_avg": -15.15,
          "overall_avg": -26.73,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -49.84,
            -36.22,
            -18.85,
            -26.89
          ],
          "math_task_scores": [
            -63.28,
            -42.83,
            -42.67,
            -9.79,
            -8.34
          ],
          "code_task_scores": [
            -52.23,
            -24.39,
            -8.96,
            4.18,
            -23.15,
            -48.18
          ],
          "reasoning_task_scores": [
            -34.58,
            -20.52,
            -6.73,
            -0.27,
            -13.65
          ]
        }
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 25.81,
      "math_avg": 16.14,
      "code_avg": 25.79,
      "reasoning_avg": 22.54,
      "overall_avg": 22.57,
      "overall_efficiency": -0.112315,
      "general_efficiency": -0.261324,
      "math_efficiency": -0.074732,
      "code_efficiency": 0.124893,
      "reasoning_efficiency": -0.238098,
      "general_scores": [
        27.07,
        30.26,
        20.75,
        15.8507143,
        52.11,
        34.065,
        22.92,
        16.8221429,
        18.22,
        29.7375,
        22.11,
        19.7714286
      ],
      "math_scores": [
        48.75,
        9.76,
        10.0,
        15.65,
        0.0,
        42.15,
        9.64,
        8.6,
        16.46,
        0.0,
        47.31,
        9.3,
        8.8,
        15.74,
        0.0
      ],
      "code_scores": [
        50.61,
        55.64,
        3.58,
        0.21,
        18.78,
        25.61,
        47.56,
        55.25,
        3.94,
        0.21,
        19.91,
        25.0,
        46.34,
        57.2,
        3.23,
        0.0,
        21.95,
        29.27
      ],
      "reasoning_scores": [
        60.68,
        26.58,
        22.22,
        0.14956522,
        1.04,
        64.75,
        33.66,
        19.19,
        0.15380435,
        1.93,
        58.98,
        29.33,
        18.69,
        0.15793478,
        0.59
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.48
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.03
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 26.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.47
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -13.24,
          "math_avg": -3.79,
          "code_avg": 6.33,
          "reasoning_avg": -12.06,
          "overall_avg": -5.69,
          "overall_efficiency": -0.112315,
          "general_efficiency": -0.261324,
          "math_efficiency": -0.074732,
          "code_efficiency": 0.124893,
          "reasoning_efficiency": -0.238098,
          "general_task_scores": [
            -31.9,
            11.01,
            -14.72,
            -17.35
          ],
          "math_task_scores": [
            -10.34,
            -10.13,
            -10.07,
            11.61,
            0.0
          ],
          "code_task_scores": [
            20.73,
            1.56,
            0.0,
            -0.07,
            20.21,
            -4.47
          ],
          "reasoning_task_scores": [
            -18.87,
            -32.7,
            -9.77,
            -0.16,
            1.19
          ]
        },
        "vs_instruct": {
          "general_avg": -30.73,
          "math_avg": -24.76,
          "code_avg": -14.05,
          "reasoning_avg": -13.7,
          "overall_avg": -20.81,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -48.43,
            -28.98,
            -19.05,
            -26.48
          ],
          "math_task_scores": [
            -39.6,
            -40.15,
            -39.07,
            3.33,
            -8.34
          ],
          "code_task_scores": [
            -19.51,
            -15.18,
            -6.1,
            -4.87,
            -4.9,
            -33.74
          ],
          "reasoning_task_scores": [
            -17.85,
            -32.99,
            -4.21,
            -0.27,
            -13.2
          ]
        }
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 37.29,
      "math_avg": 18.29,
      "code_avg": 36.47,
      "reasoning_avg": 29.2,
      "overall_avg": 30.31,
      "overall_efficiency": 0.001994,
      "general_efficiency": -0.001713,
      "math_efficiency": -0.001599,
      "code_efficiency": 0.016551,
      "reasoning_efficiency": -0.005261,
      "general_scores": [
        39.63,
        46.4425,
        31.6,
        27.8085714,
        43.81,
        47.25,
        32.2,
        28.68,
        42.1,
        46.66,
        33.35,
        27.9064286
      ],
      "math_scores": [
        55.12,
        14.24,
        14.2,
        7.0,
        0.0,
        56.86,
        13.82,
        14.4,
        7.45,
        0.0,
        55.12,
        14.52,
        14.0,
        7.59,
        0.0
      ],
      "code_scores": [
        57.93,
        32.3,
        3.23,
        29.23,
        19.23,
        55.49,
        64.63,
        59.92,
        2.87,
        30.48,
        24.66,
        53.66,
        62.8,
        54.47,
        2.51,
        31.73,
        16.97,
        54.27
      ],
      "reasoning_scores": [
        67.12,
        51.65,
        25.76,
        0.39184783,
        2.82,
        63.05,
        52.43,
        25.76,
        0.40597826,
        2.82,
        67.8,
        50.81,
        24.75,
        0.35032609,
        2.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.79
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 20.29
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 54.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.76,
          "math_avg": -1.64,
          "code_avg": 17.0,
          "reasoning_avg": -5.4,
          "overall_avg": 2.05,
          "overall_efficiency": 0.001994,
          "general_efficiency": -0.001713,
          "math_efficiency": -0.001599,
          "code_efficiency": 0.016551,
          "reasoning_efficiency": -0.005261,
          "general_task_scores": [
            -22.52,
            26.44,
            -4.27,
            -6.7
          ],
          "math_task_scores": [
            -0.71,
            -5.51,
            -5.0,
            3.01,
            0.0
          ],
          "code_task_scores": [
            34.35,
            -5.57,
            -0.71,
            30.27,
            20.29,
            23.37
          ],
          "reasoning_task_scores": [
            -14.35,
            -10.93,
            -4.38,
            0.07,
            2.57
          ]
        },
        "vs_instruct": {
          "general_avg": -19.26,
          "math_avg": -22.62,
          "code_avg": -3.38,
          "reasoning_avg": -7.04,
          "overall_avg": -13.07,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -39.05,
            -13.55,
            -8.6,
            -15.83
          ],
          "math_task_scores": [
            -29.97,
            -35.53,
            -34.0,
            -5.27,
            -8.34
          ],
          "code_task_scores": [
            -5.89,
            -22.31,
            -6.81,
            25.47,
            -4.82,
            -5.9
          ],
          "reasoning_task_scores": [
            -13.33,
            -11.22,
            1.18,
            -0.04,
            -11.82
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 28.85,
      "math_avg": 18.12,
      "code_avg": 21.46,
      "reasoning_avg": 21.11,
      "overall_avg": 22.38,
      "overall_efficiency": -0.167927,
      "general_efficiency": -0.291447,
      "math_efficiency": -0.051735,
      "code_efficiency": 0.056875,
      "reasoning_efficiency": -0.385402,
      "general_scores": [
        33.99,
        37.9425,
        20.11,
        20.1121429,
        37.28,
        35.685,
        20.67,
        14.6814286,
        40.29,
        39.87,
        23.66,
        21.8585714
      ],
      "math_scores": [
        70.51,
        2.98,
        2.4,
        14.54,
        0.0,
        68.08,
        2.92,
        2.0,
        16.62,
        0.0,
        72.63,
        2.98,
        3.8,
        12.33,
        0.0
      ],
      "code_scores": [
        40.85,
        50.97,
        1.43,
        1.25,
        0.23,
        32.93,
        40.24,
        47.47,
        1.43,
        4.59,
        0.9,
        37.2,
        42.07,
        47.86,
        0.36,
        0.63,
        0.45,
        35.37
      ],
      "reasoning_scores": [
        46.1,
        46.75,
        10.61,
        0.10391304,
        0.59,
        40.34,
        44.4,
        12.63,
        0.10847826,
        1.34,
        49.15,
        47.29,
        16.16,
        0.09326087,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.48
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.53
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 35.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.2
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.13
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -10.2,
          "math_avg": -1.81,
          "code_avg": 1.99,
          "reasoning_avg": -13.49,
          "overall_avg": -5.88,
          "overall_efficiency": -0.167927,
          "general_efficiency": -0.291447,
          "math_efficiency": -0.051735,
          "code_efficiency": 0.056875,
          "reasoning_efficiency": -0.385402,
          "general_task_scores": [
            -27.18,
            17.49,
            -15.17,
            -15.95
          ],
          "math_task_scores": [
            14.0,
            -16.74,
            -16.47,
            10.16,
            0.0
          ],
          "code_task_scores": [
            13.61,
            -5.7,
            -2.51,
            1.95,
            0.53,
            4.07
          ],
          "reasoning_task_scores": [
            -35.14,
            -16.41,
            -16.67,
            -0.21,
            0.99
          ]
        },
        "vs_instruct": {
          "general_avg": -27.7,
          "math_avg": -22.79,
          "code_avg": -18.39,
          "reasoning_avg": -15.13,
          "overall_avg": -21.0,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -43.71,
            -22.5,
            -19.5,
            -25.08
          ],
          "math_task_scores": [
            -15.26,
            -46.76,
            -45.47,
            1.88,
            -8.34
          ],
          "code_task_scores": [
            -26.63,
            -22.44,
            -8.61,
            -2.85,
            -24.58,
            -25.2
          ],
          "reasoning_task_scores": [
            -34.12,
            -16.7,
            -11.11,
            -0.32,
            -13.4
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 18.64,
      "math_avg": 19.35,
      "code_avg": 31.36,
      "reasoning_avg": 12.47,
      "overall_avg": 20.45,
      "overall_efficiency": -0.016125,
      "general_efficiency": -0.042159,
      "math_efficiency": -0.001205,
      "code_efficiency": 0.024576,
      "reasoning_efficiency": -0.045716,
      "general_scores": [
        21.96,
        30.5625,
        22.17,
        3.50333333,
        25.95,
        29.95,
        22.18,
        3.58285714,
        13.6,
        31.2475,
        16.33,
        2.61214286
      ],
      "math_scores": [
        72.4,
        2.88,
        3.0,
        16.73,
        0.0,
        76.57,
        2.76,
        2.2,
        18.34,
        0.0,
        74.45,
        2.62,
        2.4,
        15.85,
        0.0
      ],
      "code_scores": [
        63.41,
        60.7,
        0.0,
        0.42,
        0.0,
        60.37,
        68.29,
        57.2,
        0.0,
        0.21,
        0.23,
        62.8,
        67.24,
        61.48,
        0.0,
        0.0,
        0.0,
        62.2
      ],
      "reasoning_scores": [
        43.05,
        3.87,
        18.69,
        0.14717391,
        1.19,
        42.03,
        3.95,
        11.62,
        0.14663043,
        0.15,
        43.39,
        4.39,
        14.14,
        0.15869565,
        0.15
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.47
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.75
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.31
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 61.79
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.5
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -20.41,
          "math_avg": -0.58,
          "code_avg": 11.9,
          "reasoning_avg": -22.13,
          "overall_avg": -7.81,
          "overall_efficiency": -0.016125,
          "general_efficiency": -0.042159,
          "math_efficiency": -0.001205,
          "code_efficiency": 0.024576,
          "reasoning_efficiency": -0.045716,
          "general_task_scores": [
            -43.87,
            10.25,
            -16.42,
            -31.6
          ],
          "math_task_scores": [
            18.06,
            -16.95,
            -16.67,
            12.63,
            0.0
          ],
          "code_task_scores": [
            38.87,
            5.32,
            -3.58,
            0.0,
            0.08,
            30.69
          ],
          "reasoning_task_scores": [
            -37.52,
            -58.49,
            -14.98,
            -0.16,
            0.5
          ]
        },
        "vs_instruct": {
          "general_avg": -37.9,
          "math_avg": -21.56,
          "code_avg": -8.48,
          "reasoning_avg": -23.77,
          "overall_avg": -22.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -60.4,
            -29.74,
            -20.75,
            -40.73
          ],
          "math_task_scores": [
            -11.2,
            -46.97,
            -45.67,
            4.35,
            -8.34
          ],
          "code_task_scores": [
            -1.37,
            -11.42,
            -9.68,
            -4.8,
            -25.03,
            1.42
          ],
          "reasoning_task_scores": [
            -36.5,
            -58.78,
            -9.42,
            -0.27,
            -13.89
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 31.46,
      "math_avg": 14.61,
      "code_avg": 14.96,
      "reasoning_avg": 24.91,
      "overall_avg": 21.48,
      "overall_efficiency": -0.082202,
      "general_efficiency": -0.091999,
      "math_efficiency": -0.064557,
      "code_efficiency": -0.05466,
      "reasoning_efficiency": -0.117591,
      "general_scores": [
        44.28,
        37.3625,
        22.41,
        14.2771429,
        44.74,
        39.5775,
        20.8,
        45.48,
        38.46,
        19.59,
        19.1035714
      ],
      "math_scores": [
        59.21,
        3.64,
        2.2,
        13.53,
        0.0,
        50.49,
        4.9,
        3.6,
        8.42,
        0.0,
        53.6,
        4.92,
        4.4,
        10.21,
        0.0
      ],
      "code_scores": [
        23.78,
        37.74,
        0.0,
        11.06,
        1.13,
        15.85,
        20.73,
        39.69,
        0.0,
        19.21,
        0.9,
        16.46,
        18.9,
        36.96,
        0.0,
        13.78,
        0.9,
        12.2
      ],
      "reasoning_scores": [
        48.81,
        50.86,
        24.75,
        0.23043478,
        1.93,
        43.73,
        49.59,
        0.26793478,
        1.93,
        42.37,
        51.84,
        29.8,
        0.23804348,
        2.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.43
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.14
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.68
              },
              {
                "metric": "lcb_test_output",
                "score": 0.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 14.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.76
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -7.58,
          "math_avg": -5.32,
          "code_avg": -4.51,
          "reasoning_avg": -9.69,
          "overall_avg": -6.78,
          "overall_efficiency": -0.082202,
          "general_efficiency": -0.091999,
          "math_efficiency": -0.064557,
          "code_efficiency": -0.05466,
          "reasoning_efficiency": -0.117591,
          "general_task_scores": [
            -19.54,
            18.13,
            -15.72,
            -18.14
          ],
          "math_task_scores": [
            -1.98,
            -15.21,
            -15.8,
            6.38,
            0.0
          ],
          "code_task_scores": [
            -6.3,
            -16.34,
            -3.58,
            14.47,
            0.98,
            -16.26
          ],
          "reasoning_task_scores": [
            -35.37,
            -11.8,
            -2.52,
            -0.06,
            2.08
          ]
        },
        "vs_instruct": {
          "general_avg": -25.08,
          "math_avg": -26.3,
          "code_avg": -24.88,
          "reasoning_avg": -11.34,
          "overall_avg": -21.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.07,
            -21.86,
            -20.05,
            -27.27
          ],
          "math_task_scores": [
            -31.24,
            -45.23,
            -44.8,
            -1.9,
            -8.34
          ],
          "code_task_scores": [
            -46.54,
            -33.08,
            -9.68,
            9.67,
            -24.13,
            -45.53
          ],
          "reasoning_task_scores": [
            -34.35,
            -12.09,
            3.04,
            -0.17,
            -12.31
          ]
        }
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 41.29,
      "math_avg": 16.15,
      "code_avg": 31.37,
      "reasoning_avg": 32.23,
      "overall_avg": 30.26,
      "overall_efficiency": 0.025553,
      "general_efficiency": 0.028661,
      "math_efficiency": -0.048281,
      "code_efficiency": 0.152085,
      "reasoning_efficiency": -0.030252,
      "general_scores": [
        47.25,
        48.8325,
        37.03,
        30.0128571,
        51.56,
        47.2175,
        36.46,
        29.7723077,
        51.69,
        47.5275,
        36.54,
        31.5785714
      ],
      "math_scores": [
        50.87,
        12.76,
        10.6,
        7.0,
        0.0,
        47.31,
        12.94,
        10.8,
        6.71,
        0.0,
        51.33,
        12.7,
        11.8,
        7.45,
        0.0
      ],
      "code_scores": [
        48.17,
        53.7,
        1.08,
        23.8,
        19.68,
        43.29,
        45.73,
        53.7,
        2.51,
        29.02,
        12.9,
        45.12,
        42.68,
        53.31,
        1.08,
        26.72,
        17.65,
        44.51
      ],
      "reasoning_scores": [
        77.63,
        54.67,
        26.77,
        0.35402174,
        2.08,
        77.63,
        53.19,
        26.26,
        0.36195652,
        2.52,
        77.97,
        55.03,
        26.77,
        0.35543478,
        1.93
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.56
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.51
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 44.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.74
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.6
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.24,
          "math_avg": -3.78,
          "code_avg": 11.9,
          "reasoning_avg": -2.37,
          "overall_avg": 2.0,
          "overall_efficiency": 0.025553,
          "general_efficiency": 0.028661,
          "math_efficiency": -0.048281,
          "code_efficiency": 0.152085,
          "reasoning_efficiency": -0.030252,
          "general_task_scores": [
            -14.2,
            27.52,
            0.03,
            -4.38
          ],
          "math_task_scores": [
            -6.57,
            -6.9,
            -8.13,
            2.71,
            0.0
          ],
          "code_task_scores": [
            18.09,
            -0.9,
            -2.02,
            26.3,
            16.74,
            13.21
          ],
          "reasoning_task_scores": [
            -2.6,
            -8.26,
            -3.2,
            0.05,
            2.18
          ]
        },
        "vs_instruct": {
          "general_avg": -15.25,
          "math_avg": -24.76,
          "code_avg": -8.47,
          "reasoning_avg": -4.01,
          "overall_avg": -13.12,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -30.73,
            -12.47,
            -4.3,
            -13.51
          ],
          "math_task_scores": [
            -35.83,
            -36.92,
            -37.13,
            -5.57,
            -8.34
          ],
          "code_task_scores": [
            -22.15,
            -17.64,
            -8.12,
            21.5,
            -8.37,
            -16.06
          ],
          "reasoning_task_scores": [
            -1.58,
            -8.55,
            2.36,
            -0.06,
            -12.21
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 38.18,
      "math_avg": 11.95,
      "code_avg": 24.22,
      "reasoning_avg": 29.12,
      "overall_avg": 25.86,
      "overall_efficiency": -0.019666,
      "general_efficiency": -0.007142,
      "math_efficiency": -0.065471,
      "code_efficiency": 0.038934,
      "reasoning_efficiency": -0.044986,
      "general_scores": [
        48.07,
        42.0,
        33.69,
        28.0976923,
        49.7,
        44.71,
        31.14,
        27.8978571,
        46.98,
        43.045,
        33.47,
        29.3007143
      ],
      "math_scores": [
        38.51,
        10.58,
        8.6,
        6.59,
        0.0,
        30.02,
        9.4,
        8.6,
        6.66,
        0.0,
        34.8,
        9.56,
        9.4,
        6.46,
        0.0
      ],
      "code_scores": [
        45.12,
        50.58,
        0.0,
        15.24,
        6.33,
        32.32,
        39.02,
        54.09,
        0.0,
        13.78,
        4.07,
        32.93,
        39.63,
        45.53,
        0.0,
        5.64,
        15.61,
        35.98
      ],
      "reasoning_scores": [
        68.47,
        45.04,
        29.8,
        0.28347826,
        2.82,
        68.47,
        46.26,
        25.76,
        0.275,
        1.93,
        71.53,
        46.6,
        26.26,
        0.27206522,
        2.97
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.55
              },
              {
                "metric": "lcb_test_output",
                "score": 8.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 33.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.49
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.87,
          "math_avg": -7.98,
          "code_avg": 4.75,
          "reasoning_avg": -5.49,
          "overall_avg": -2.4,
          "overall_efficiency": -0.019666,
          "general_efficiency": -0.007142,
          "math_efficiency": -0.065471,
          "code_efficiency": 0.038934,
          "reasoning_efficiency": -0.044986,
          "general_task_scores": [
            -16.12,
            22.91,
            -3.88,
            -6.4
          ],
          "math_task_scores": [
            -21.97,
            -9.85,
            -10.33,
            2.23,
            0.0
          ],
          "code_task_scores": [
            13.82,
            -4.4,
            -3.58,
            11.34,
            8.67,
            2.64
          ],
          "reasoning_task_scores": [
            -10.85,
            -16.59,
            -2.53,
            -0.03,
            2.57
          ]
        },
        "vs_instruct": {
          "general_avg": -18.37,
          "math_avg": -28.96,
          "code_avg": -15.63,
          "reasoning_avg": -7.13,
          "overall_avg": -17.52,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -32.65,
            -17.08,
            -8.21,
            -15.53
          ],
          "math_task_scores": [
            -51.23,
            -39.87,
            -39.33,
            -6.05,
            -8.34
          ],
          "code_task_scores": [
            -26.42,
            -21.14,
            -9.68,
            6.54,
            -16.44,
            -26.63
          ],
          "reasoning_task_scores": [
            -9.83,
            -16.88,
            3.03,
            -0.14,
            -11.82
          ]
        }
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 40.96,
      "math_avg": 13.89,
      "code_avg": 24.44,
      "reasoning_avg": 31.05,
      "overall_avg": 27.58,
      "overall_efficiency": -0.036354,
      "general_efficiency": 0.102904,
      "math_efficiency": -0.324594,
      "code_efficiency": 0.267062,
      "reasoning_efficiency": -0.190789,
      "general_scores": [
        43.48,
        46.8975,
        37.88,
        33.3314286,
        48.21,
        48.0325,
        37.1,
        31.7071429,
        47.06,
        47.0875,
        37.92,
        32.8307143
      ],
      "math_scores": [
        35.56,
        13.48,
        14.4,
        7.36,
        0.0,
        35.18,
        12.76,
        11.2,
        7.38,
        3.33,
        33.66,
        12.38,
        11.4,
        6.91,
        3.33
      ],
      "code_scores": [
        36.59,
        54.47,
        0.0,
        19.62,
        9.05,
        31.1,
        37.2,
        52.53,
        0.0,
        10.86,
        11.31,
        32.93,
        37.8,
        54.47,
        0.0,
        6.05,
        13.57,
        32.32
      ],
      "reasoning_scores": [
        73.22,
        53.47,
        25.76,
        0.30804348,
        3.26,
        73.22,
        53.88,
        24.75,
        0.32304348,
        2.97,
        74.58,
        53.76,
        23.74,
        0.30043478,
        2.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.18
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 32.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.7
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.82
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.92,
          "math_avg": -6.04,
          "code_avg": 4.97,
          "reasoning_avg": -3.55,
          "overall_avg": -0.68,
          "overall_efficiency": -0.036354,
          "general_efficiency": 0.102904,
          "math_efficiency": -0.324594,
          "code_efficiency": 0.267062,
          "reasoning_efficiency": -0.190789,
          "general_task_scores": [
            -18.12,
            27.0,
            0.98,
            -2.21
          ],
          "math_task_scores": [
            -21.61,
            -6.83,
            -6.87,
            2.88,
            2.22
          ],
          "code_task_scores": [
            9.76,
            -0.65,
            -3.58,
            11.97,
            11.31,
            1.02
          ],
          "reasoning_task_scores": [
            -6.67,
            -8.86,
            -5.05,
            -0.0,
            2.82
          ]
        },
        "vs_instruct": {
          "general_avg": -15.58,
          "math_avg": -27.02,
          "code_avg": -15.41,
          "reasoning_avg": -5.19,
          "overall_avg": -15.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.65,
            -12.98,
            -3.35,
            -11.34
          ],
          "math_task_scores": [
            -50.87,
            -36.85,
            -35.87,
            -5.4,
            -6.12
          ],
          "code_task_scores": [
            -30.48,
            -17.39,
            -9.68,
            7.17,
            -13.8,
            -28.25
          ],
          "reasoning_task_scores": [
            -5.65,
            -9.15,
            0.51,
            -0.11,
            -11.57
          ]
        }
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.78,
      "math_avg": 17.73,
      "code_avg": 31.44,
      "reasoning_avg": 31.5,
      "overall_avg": 30.86,
      "overall_efficiency": 0.115074,
      "general_efficiency": 0.16532,
      "math_efficiency": -0.097399,
      "code_efficiency": 0.529554,
      "reasoning_efficiency": -0.137179,
      "general_scores": [
        59.33,
        45.2575,
        36.23,
        32.4157143,
        56.05,
        44.4375,
        37.2,
        33.3942857,
        55.872,
        44.0875,
        35.85,
        33.28
      ],
      "math_scores": [
        47.92,
        15.18,
        13.8,
        8.2,
        0.0,
        51.71,
        15.84,
        14.2,
        8.31,
        0.0,
        50.11,
        15.58,
        16.8,
        8.27,
        0.0
      ],
      "code_scores": [
        54.27,
        56.42,
        7.89,
        16.91,
        22.17,
        46.95,
        51.83,
        55.64,
        8.24,
        5.22,
        25.34,
        40.85,
        54.09,
        6.45,
        14.61,
        23.08,
        44.5
      ],
      "reasoning_scores": [
        73.56,
        57.22,
        25.25,
        0.29184783,
        2.23,
        73.9,
        57.96,
        22.73,
        0.29641304,
        2.23,
        73.56,
        57.88,
        22.73,
        0.3075,
        2.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.43
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.91
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.25
              },
              {
                "metric": "lcb_test_output",
                "score": 23.53
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 44.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.57
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.74,
          "math_avg": -2.2,
          "code_avg": 11.97,
          "reasoning_avg": -3.1,
          "overall_avg": 2.6,
          "overall_efficiency": 0.115074,
          "general_efficiency": 0.16532,
          "math_efficiency": -0.097399,
          "code_efficiency": 0.529554,
          "reasoning_efficiency": -0.137179,
          "general_task_scores": [
            -7.29,
            24.25,
            -0.22,
            -1.8
          ],
          "math_task_scores": [
            -6.5,
            -4.17,
            -4.27,
            3.92,
            0.0
          ],
          "code_task_scores": [
            25.61,
            0.91,
            3.95,
            12.04,
            23.53,
            13.0
          ],
          "reasoning_task_scores": [
            -6.67,
            -4.87,
            -6.23,
            -0.01,
            2.28
          ]
        },
        "vs_instruct": {
          "general_avg": -13.76,
          "math_avg": -23.18,
          "code_avg": -8.4,
          "reasoning_avg": -4.74,
          "overall_avg": -12.52,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.82,
            -15.74,
            -4.55,
            -10.93
          ],
          "math_task_scores": [
            -35.76,
            -34.19,
            -33.27,
            -4.36,
            -8.34
          ],
          "code_task_scores": [
            -14.63,
            -15.83,
            -2.15,
            7.24,
            -1.58,
            -16.27
          ],
          "reasoning_task_scores": [
            -5.65,
            -5.16,
            -0.67,
            -0.12,
            -12.11
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 22.8,
      "math_avg": 19.1,
      "code_avg": 8.09,
      "reasoning_avg": 20.67,
      "overall_avg": 17.67,
      "overall_efficiency": -0.034771,
      "general_efficiency": -0.053316,
      "math_efficiency": -0.002708,
      "code_efficiency": -0.037337,
      "reasoning_efficiency": -0.045721,
      "general_scores": [
        41.82,
        23.7175,
        17.69,
        5.67,
        42.42,
        24.685,
        18.65,
        5.56214286,
        42.4,
        27.2425,
        18.09,
        5.66785714
      ],
      "math_scores": [
        66.72,
        10.48,
        9.8,
        6.35,
        3.33,
        61.79,
        10.22,
        10.6,
        6.75,
        3.33,
        62.32,
        11.22,
        10.6,
        6.39,
        6.67
      ],
      "code_scores": [
        3.66,
        39.3,
        0.0,
        0.0,
        0.0,
        7.32,
        3.66,
        39.3,
        0.0,
        0.0,
        0.0,
        6.1,
        2.44,
        42.02,
        0.0,
        0.0,
        0.0,
        1.83
      ],
      "reasoning_scores": [
        50.51,
        29.34,
        0.20933333,
        2.67,
        56.27,
        28.57,
        12.12,
        0.226,
        2.67,
        58.31,
        32.73,
        13.64,
        0.20533333,
        1.93
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.61
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.21
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 5.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.21
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.88
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.42
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -16.24,
          "math_avg": -0.83,
          "code_avg": -11.38,
          "reasoning_avg": -13.93,
          "overall_avg": -10.59,
          "overall_efficiency": -0.034771,
          "general_efficiency": -0.053316,
          "math_efficiency": -0.002708,
          "code_efficiency": -0.037337,
          "reasoning_efficiency": -0.045721,
          "general_task_scores": [
            -22.16,
            4.88,
            -18.51,
            -29.2
          ],
          "math_task_scores": [
            7.2,
            -9.06,
            -8.87,
            2.16,
            4.44
          ],
          "code_task_scores": [
            -24.19,
            -14.26,
            -3.58,
            -0.21,
            0.0,
            -26.02
          ],
          "reasoning_task_scores": [
            -25.31,
            -32.35,
            -16.92,
            -0.1,
            2.42
          ]
        },
        "vs_instruct": {
          "general_avg": -33.74,
          "math_avg": -21.8,
          "code_avg": -31.75,
          "reasoning_avg": -15.57,
          "overall_avg": -25.72,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.69,
            -35.11,
            -22.84,
            -38.33
          ],
          "math_task_scores": [
            -22.06,
            -39.08,
            -37.87,
            -6.12,
            -3.9
          ],
          "code_task_scores": [
            -64.43,
            -31.0,
            -9.68,
            -5.01,
            -25.11,
            -55.29
          ],
          "reasoning_task_scores": [
            -24.29,
            -32.64,
            -11.36,
            -0.21,
            -11.97
          ]
        }
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 16.07,
      "math_avg": 17.88,
      "code_avg": 29.23,
      "reasoning_avg": 11.79,
      "overall_avg": 18.74,
      "overall_efficiency": -0.02505,
      "general_efficiency": -0.060474,
      "math_efficiency": -0.005401,
      "code_efficiency": 0.025705,
      "reasoning_efficiency": -0.060027,
      "general_scores": [
        7.37,
        31.015,
        20.31,
        7.53642857,
        3.54,
        31.8075,
        19.98,
        6.67461538,
        6.37,
        30.205,
        19.7,
        8.28214286
      ],
      "math_scores": [
        42.99,
        9.54,
        7.8,
        31.82,
        0.0,
        42.91,
        9.12,
        8.0,
        30.76,
        0.0,
        41.7,
        6.44,
        4.6,
        32.48,
        0.0
      ],
      "code_scores": [
        60.37,
        21.4,
        8.6,
        0.0,
        6.79,
        54.27,
        57.93,
        54.47,
        10.39,
        0.0,
        10.86,
        56.1,
        61.59,
        54.09,
        8.6,
        0.0,
        5.88,
        54.88
      ],
      "reasoning_scores": [
        40.34,
        6.76,
        7.07,
        0.13891304,
        0.59,
        47.12,
        7.43,
        7.07,
        0.12956522,
        0.74,
        44.75,
        6.15,
        7.58,
        0.12315217,
        0.89
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.96
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.74
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -22.98,
          "math_avg": -2.05,
          "code_avg": 9.77,
          "reasoning_avg": -22.81,
          "overall_avg": -9.52,
          "overall_efficiency": -0.02505,
          "general_efficiency": -0.060474,
          "math_efficiency": -0.005401,
          "code_efficiency": 0.025705,
          "reasoning_efficiency": -0.060027,
          "general_task_scores": [
            -58.61,
            10.67,
            -16.65,
            -27.33
          ],
          "math_task_scores": [
            -13.88,
            -11.33,
            -12.4,
            27.35,
            0.0
          ],
          "code_task_scores": [
            32.52,
            -11.15,
            5.62,
            -0.21,
            7.84,
            23.98
          ],
          "reasoning_task_scores": [
            -36.27,
            -55.78,
            -22.56,
            -0.18,
            0.74
          ]
        },
        "vs_instruct": {
          "general_avg": -40.48,
          "math_avg": -23.03,
          "code_avg": -10.61,
          "reasoning_avg": -24.45,
          "overall_avg": -24.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -75.14,
            -29.32,
            -20.98,
            -36.46
          ],
          "math_task_scores": [
            -43.14,
            -41.35,
            -41.4,
            19.07,
            -8.34
          ],
          "code_task_scores": [
            -7.72,
            -27.89,
            -0.48,
            -5.01,
            -17.27,
            -5.29
          ],
          "reasoning_task_scores": [
            -35.25,
            -56.07,
            -17.0,
            -0.29,
            -13.65
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 42.92,
      "math_avg": 23.27,
      "code_avg": 32.6,
      "reasoning_avg": 32.16,
      "overall_avg": 32.74,
      "overall_efficiency": 0.041292,
      "general_efficiency": 0.035702,
      "math_efficiency": 0.030796,
      "code_efficiency": 0.121202,
      "reasoning_efficiency": -0.022531,
      "general_scores": [
        66.6,
        45.01,
        33.82,
        27.085,
        63.37,
        44.8675,
        32.8,
        29.2685714,
        67.32,
        43.4825,
        33.89,
        27.4778571
      ],
      "math_scores": [
        63.0,
        21.32,
        20.2,
        12.08,
        0.0,
        63.15,
        21.3,
        20.2,
        11.34,
        3.33,
        60.96,
        21.06,
        19.2,
        11.88,
        0.0
      ],
      "code_scores": [
        52.4,
        57.59,
        1.08,
        14.82,
        28.51,
        39.63,
        50.61,
        61.09,
        3.94,
        17.33,
        29.64,
        41.46,
        53.05,
        59.53,
        1.08,
        6.05,
        27.6,
        41.46
      ],
      "reasoning_scores": [
        74.92,
        55.67,
        23.74,
        0.17163043,
        4.3,
        75.93,
        56.42,
        25.25,
        0.18043478,
        4.01,
        74.24,
        57.16,
        24.75,
        0.17152174,
        5.49
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.03
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.73
              },
              {
                "metric": "lcb_test_output",
                "score": 28.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 40.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.58
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.87,
          "math_avg": 3.34,
          "code_avg": 13.14,
          "reasoning_avg": -2.44,
          "overall_avg": 4.48,
          "overall_efficiency": 0.041292,
          "general_efficiency": 0.035702,
          "math_efficiency": 0.030796,
          "code_efficiency": 0.121202,
          "reasoning_efficiency": -0.022531,
          "general_task_scores": [
            1.39,
            24.11,
            -3.15,
            -6.89
          ],
          "math_task_scores": [
            5.96,
            1.53,
            0.67,
            7.43,
            1.11
          ],
          "code_task_scores": [
            24.58,
            4.93,
            -1.55,
            12.52,
            28.58,
            9.75
          ],
          "reasoning_task_scores": [
            -5.31,
            -6.14,
            -5.22,
            -0.14,
            4.6
          ]
        },
        "vs_instruct": {
          "general_avg": -13.63,
          "math_avg": -17.64,
          "code_avg": -7.24,
          "reasoning_avg": -4.08,
          "overall_avg": -10.65,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.14,
            -15.88,
            -7.48,
            -16.02
          ],
          "math_task_scores": [
            -23.3,
            -28.49,
            -28.33,
            -0.85,
            -7.22
          ],
          "code_task_scores": [
            -15.66,
            -11.81,
            -7.65,
            7.72,
            3.47,
            -19.52
          ],
          "reasoning_task_scores": [
            -4.29,
            -6.43,
            0.34,
            -0.25,
            -9.79
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 21.12,
      "math_avg": 20.37,
      "code_avg": 20.53,
      "reasoning_avg": 16.8,
      "overall_avg": 19.71,
      "overall_efficiency": -0.008202,
      "general_efficiency": -0.017178,
      "math_efficiency": 0.000422,
      "code_efficiency": 0.001015,
      "reasoning_efficiency": -0.017065,
      "general_scores": [
        28.92,
        29.495,
        24.82,
        13.8064286,
        24.57,
        30.8075,
        10.19,
        7.05785714,
        17.23,
        33.365,
        17.7,
        15.5371429
      ],
      "math_scores": [
        80.67,
        1.62,
        1.4,
        20.82,
        0.0,
        80.21,
        1.34,
        1.4,
        21.54,
        0.0,
        73.92,
        1.4,
        2.0,
        19.24,
        0.0
      ],
      "code_scores": [
        36.59,
        47.08,
        0.0,
        0.0,
        0.0,
        34.15,
        35.37,
        55.25,
        0.0,
        0.0,
        0.0,
        29.88,
        39.02,
        53.7,
        0.0,
        0.63,
        0.0,
        37.8
      ],
      "reasoning_scores": [
        64.41,
        5.96,
        22.73,
        0.03206522,
        0.0,
        60.34,
        4.53,
        15.15,
        0.01586957,
        0.59,
        63.39,
        2.91,
        11.62,
        0.01293478,
        0.3
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.99
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 33.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -17.92,
          "math_avg": 0.44,
          "code_avg": 1.06,
          "reasoning_avg": -17.8,
          "overall_avg": -8.56,
          "overall_efficiency": -0.008202,
          "general_efficiency": -0.017178,
          "math_efficiency": 0.000422,
          "code_efficiency": 0.001015,
          "reasoning_efficiency": -0.017065,
          "general_task_scores": [
            -40.8,
            10.88,
            -19.08,
            -22.7
          ],
          "math_task_scores": [
            21.86,
            -18.25,
            -17.6,
            16.19,
            0.0
          ],
          "code_task_scores": [
            9.55,
            -2.46,
            -3.58,
            0.0,
            0.0,
            2.84
          ],
          "reasoning_task_scores": [
            -17.63,
            -58.09,
            -13.3,
            -0.29,
            0.3
          ]
        },
        "vs_instruct": {
          "general_avg": -35.42,
          "math_avg": -20.54,
          "code_avg": -19.32,
          "reasoning_avg": -19.44,
          "overall_avg": -23.68,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -57.33,
            -29.11,
            -23.41,
            -31.83
          ],
          "math_task_scores": [
            -7.4,
            -48.27,
            -46.6,
            7.91,
            -8.34
          ],
          "code_task_scores": [
            -30.69,
            -19.2,
            -9.68,
            -4.8,
            -25.11,
            -26.43
          ],
          "reasoning_task_scores": [
            -16.61,
            -58.38,
            -7.74,
            -0.4,
            -14.09
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 15.44,
      "math_avg": 14.85,
      "code_avg": 0.04,
      "reasoning_avg": 6.08,
      "overall_avg": 9.1,
      "overall_efficiency": -0.191576,
      "general_efficiency": -0.236023,
      "math_efficiency": -0.050807,
      "code_efficiency": -0.194234,
      "reasoning_efficiency": -0.285241,
      "general_scores": [
        24.26,
        24.49,
        5.83,
        2.715714,
        28.25,
        25.09,
        7.58,
        1.91357143,
        29.72,
        24.2225,
        9.21,
        2.045
      ],
      "math_scores": [
        66.63,
        0.34,
        0.4,
        2.62,
        0.0,
        74.15,
        1.06,
        0.6,
        3.0,
        0.0,
        69.6,
        0.38,
        0.8,
        3.16,
        0.0
      ],
      "code_scores": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.39,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.39,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        18.31,
        6.18,
        1.52,
        0.02,
        0.0,
        21.69,
        6.22,
        5.56,
        0.0401087,
        0.0,
        20.0,
        7.57,
        4.04,
        0.02380435,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.6
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 7.54
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.13
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.66
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -23.6,
          "math_avg": -5.08,
          "code_avg": -19.42,
          "reasoning_avg": -28.52,
          "overall_avg": -19.16,
          "overall_efficiency": -0.191576,
          "general_efficiency": -0.236023,
          "math_efficiency": -0.050807,
          "code_efficiency": -0.194234,
          "reasoning_efficiency": -0.285241,
          "general_task_scores": [
            -36.96,
            4.26,
            -29.11,
            -32.61
          ],
          "math_task_scores": [
            13.72,
            -19.11,
            -18.6,
            -1.41,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -54.21,
            -3.58,
            -0.21,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -60.34,
            -55.9,
            -26.09,
            -0.28,
            0.0
          ]
        },
        "vs_instruct": {
          "general_avg": -41.1,
          "math_avg": -26.06,
          "code_avg": -39.8,
          "reasoning_avg": -30.17,
          "overall_avg": -34.28,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -53.49,
            -35.72,
            -33.44,
            -41.74
          ],
          "math_task_scores": [
            -15.54,
            -49.13,
            -47.6,
            -9.69,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -70.95,
            -9.68,
            -5.01,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -59.32,
            -56.19,
            -20.53,
            -0.39,
            -14.39
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 30.24,
      "math_avg": 15.58,
      "code_avg": 17.3,
      "reasoning_avg": 21.19,
      "overall_avg": 21.08,
      "overall_efficiency": -0.319199,
      "general_efficiency": -0.391398,
      "math_efficiency": -0.193126,
      "code_efficiency": -0.096345,
      "reasoning_efficiency": -0.595928,
      "general_scores": [
        35.39,
        41.3275,
        26.68,
        16.3792857,
        45.86,
        45.1325,
        31.03,
        14.3221429,
        38.58,
        40.27,
        21.11,
        6.795
      ],
      "math_scores": [
        46.85,
        8.62,
        7.2,
        17.66,
        3.33,
        45.49,
        9.96,
        9.0,
        13.98,
        0.0,
        41.62,
        7.48,
        5.6,
        16.98,
        0.0
      ],
      "code_scores": [
        32.32,
        42.41,
        0.0,
        20.46,
        2.49,
        22.56,
        28.66,
        35.02,
        0.0,
        13.99,
        2.26,
        18.9,
        24.39,
        44.75,
        0.0,
        5.22,
        2.71,
        15.24
      ],
      "reasoning_scores": [
        68.81,
        29.31,
        13.13,
        0.08576087,
        2.08,
        63.05,
        31.61,
        10.61,
        0.08815217,
        2.82,
        63.73,
        20.22,
        11.11,
        0.06619565,
        1.19
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.69
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.22
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 18.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.81,
          "math_avg": -4.35,
          "code_avg": -2.17,
          "reasoning_avg": -13.41,
          "overall_avg": -7.18,
          "overall_efficiency": -0.319199,
          "general_efficiency": -0.391398,
          "math_efficiency": -0.193126,
          "code_efficiency": -0.096345,
          "reasoning_efficiency": -0.595928,
          "general_task_scores": [
            -24.43,
            21.9,
            -10.38,
            -22.33
          ],
          "math_task_scores": [
            -11.76,
            -11.01,
            -11.93,
            11.87,
            1.11
          ],
          "code_task_scores": [
            1.02,
            -13.74,
            -3.58,
            13.01,
            2.49,
            -12.2
          ],
          "reasoning_task_scores": [
            -15.14,
            -35.51,
            -18.18,
            -0.23,
            2.03
          ]
        },
        "vs_instruct": {
          "general_avg": -26.3,
          "math_avg": -25.32,
          "code_avg": -22.54,
          "reasoning_avg": -15.05,
          "overall_avg": -22.31,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -40.96,
            -18.08,
            -14.71,
            -31.46
          ],
          "math_task_scores": [
            -41.02,
            -41.03,
            -40.93,
            3.59,
            -7.22
          ],
          "code_task_scores": [
            -39.22,
            -30.48,
            -9.68,
            8.21,
            -22.62,
            -41.47
          ],
          "reasoning_task_scores": [
            -14.12,
            -35.8,
            -12.62,
            -0.34,
            -12.36
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 15.16,
      "math_avg": 18.94,
      "code_avg": 7.69,
      "reasoning_avg": 6.02,
      "overall_avg": 11.95,
      "overall_efficiency": -0.207579,
      "general_efficiency": -0.304029,
      "math_efficiency": -0.012583,
      "code_efficiency": -0.149895,
      "reasoning_efficiency": -0.36381,
      "general_scores": [
        20.94,
        26.68,
        11.57,
        0.75785714,
        23.43,
        23.56,
        10.83,
        7.24928571,
        25.96,
        21.7775,
        7.57,
        1.55357143
      ],
      "math_scores": [
        88.55,
        2.94,
        2.6,
        4.04,
        0.0,
        79.98,
        5.28,
        4.8,
        3.84,
        0.0,
        85.82,
        1.88,
        2.4,
        1.99,
        0.0
      ],
      "code_scores": [
        0.0,
        42.41,
        0.0,
        1.04,
        0.0,
        0.0,
        0.0,
        46.3,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        48.64,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        22.71,
        0.09,
        5.05,
        0.11967391,
        0.15,
        26.44,
        2.62,
        8.59,
        0.14521739,
        0.3,
        20.68,
        0.02,
        3.03,
        0.13413043,
        0.15
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 9.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.78
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -23.89,
          "math_avg": -0.99,
          "code_avg": -11.78,
          "reasoning_avg": -28.59,
          "overall_avg": -16.31,
          "overall_efficiency": -0.207579,
          "general_efficiency": -0.304029,
          "math_efficiency": -0.012583,
          "code_efficiency": -0.149895,
          "reasoning_efficiency": -0.36381,
          "general_task_scores": [
            -40.93,
            3.67,
            -26.66,
            -31.64
          ],
          "math_task_scores": [
            28.37,
            -16.33,
            -15.93,
            -1.05,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -8.69,
            -3.58,
            0.14,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -57.06,
            -61.65,
            -24.24,
            -0.18,
            0.2
          ]
        },
        "vs_instruct": {
          "general_avg": -41.39,
          "math_avg": -21.97,
          "code_avg": -32.16,
          "reasoning_avg": -30.23,
          "overall_avg": -31.43,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -57.46,
            -36.32,
            -30.99,
            -40.77
          ],
          "math_task_scores": [
            -0.89,
            -46.35,
            -44.93,
            -9.33,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -25.43,
            -9.68,
            -4.66,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -56.04,
            -61.94,
            -18.68,
            -0.29,
            -14.19
          ]
        }
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 6.52,
      "math_avg": 10.14,
      "code_avg": 5.92,
      "reasoning_avg": 5.58,
      "overall_avg": 7.04,
      "overall_efficiency": -0.193966,
      "general_efficiency": -0.297304,
      "math_efficiency": -0.089511,
      "code_efficiency": -0.123794,
      "reasoning_efficiency": -0.265257,
      "general_scores": [
        0.39,
        16.575,
        1.19,
        7.99214286,
        0.36,
        16.575,
        1.25,
        7.864286,
        0.36,
        16.575,
        1.25,
        7.864286
      ],
      "math_scores": [
        50.49,
        0.14,
        0.0,
        0.09,
        0.0,
        50.42,
        0.14,
        0.0,
        0.11,
        0.0,
        50.42,
        0.14,
        0.0,
        0.11,
        0.0
      ],
      "code_scores": [
        0.0,
        36.58,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        35.02,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        35.02,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        28.14,
        0.0,
        0.0,
        0.00076087,
        0.0,
        27.8,
        0.0,
        0.0,
        0.000761,
        0.0,
        27.8,
        0.0,
        0.0,
        0.000761,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.58
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 1.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 35.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -32.53,
          "math_avg": -9.79,
          "code_avg": -13.54,
          "reasoning_avg": -29.02,
          "overall_avg": -21.22,
          "overall_efficiency": -0.193966,
          "general_efficiency": -0.297304,
          "math_efficiency": -0.089511,
          "code_efficiency": -0.123794,
          "reasoning_efficiency": -0.265257,
          "general_task_scores": [
            -64.0,
            -3.76,
            -35.42,
            -26.92
          ],
          "math_task_scores": [
            -5.97,
            -19.56,
            -19.2,
            -4.24,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -18.93,
            -3.58,
            -0.21,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -52.43,
            -62.56,
            -29.8,
            -0.31,
            0.0
          ]
        },
        "vs_instruct": {
          "general_avg": -50.02,
          "math_avg": -30.77,
          "code_avg": -33.92,
          "reasoning_avg": -30.66,
          "overall_avg": -36.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -80.53,
            -43.74,
            -39.75,
            -36.05
          ],
          "math_task_scores": [
            -35.23,
            -49.58,
            -48.2,
            -12.52,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -35.67,
            -9.68,
            -5.01,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -51.41,
            -62.85,
            -24.24,
            -0.42,
            -14.39
          ]
        }
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 45.01,
      "math_avg": 22.93,
      "code_avg": 39.84,
      "reasoning_avg": 34.28,
      "overall_avg": 35.52,
      "overall_efficiency": 0.025098,
      "general_efficiency": 0.020639,
      "math_efficiency": 0.010388,
      "code_efficiency": 0.070488,
      "reasoning_efficiency": -0.001125,
      "general_scores": [
        66.51,
        41.69,
        36.43,
        32.37,
        67.97,
        43.42,
        37.33,
        31.4957143,
        70.04,
        41.3525,
        37.77,
        33.7771429
      ],
      "math_scores": [
        55.34,
        22.5,
        23.0,
        11.56,
        6.67,
        52.92,
        22.84,
        22.8,
        11.34,
        3.33,
        57.16,
        22.0,
        20.8,
        11.74,
        0.0
      ],
      "code_scores": [
        67.68,
        65.37,
        7.17,
        25.89,
        23.3,
        54.88,
        66.46,
        64.98,
        3.58,
        13.99,
        25.79,
        55.49,
        66.46,
        65.37,
        6.81,
        19.83,
        28.05,
        56.1
      ],
      "reasoning_scores": [
        79.32,
        60.15,
        27.78,
        0.32402174,
        4.01,
        79.32,
        59.12,
        27.78,
        0.33869565,
        5.34,
        77.97,
        61.17,
        26.77,
        0.31847826,
        4.45
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.85
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.9
              },
              {
                "metric": "lcb_test_output",
                "score": 25.71
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.87
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.97,
          "math_avg": 3.0,
          "code_avg": 20.38,
          "reasoning_avg": -0.32,
          "overall_avg": 7.26,
          "overall_efficiency": 0.025098,
          "general_efficiency": 0.020639,
          "math_efficiency": 0.010388,
          "code_efficiency": 0.070488,
          "reasoning_efficiency": -0.001125,
          "general_task_scores": [
            3.8,
            21.81,
            0.53,
            -2.28
          ],
          "math_task_scores": [
            -1.27,
            2.75,
            3.0,
            7.21,
            3.33
          ],
          "code_task_scores": [
            39.43,
            10.77,
            2.27,
            19.69,
            25.71,
            24.39
          ],
          "reasoning_task_scores": [
            -1.47,
            -2.41,
            -2.36,
            0.02,
            4.6
          ]
        },
        "vs_instruct": {
          "general_avg": -11.53,
          "math_avg": -17.98,
          "code_avg": 0.0,
          "reasoning_avg": -1.97,
          "overall_avg": -7.87,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.73,
            -18.18,
            -3.8,
            -11.41
          ],
          "math_task_scores": [
            -30.53,
            -27.27,
            -26.0,
            -1.07,
            -5.01
          ],
          "code_task_scores": [
            -0.81,
            -5.97,
            -3.83,
            14.89,
            0.6,
            -4.88
          ],
          "reasoning_task_scores": [
            -0.45,
            -2.7,
            3.2,
            -0.09,
            -9.79
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 46.64,
      "math_avg": 22.34,
      "code_avg": 36.23,
      "reasoning_avg": 32.96,
      "overall_avg": 34.54,
      "overall_efficiency": 0.084983,
      "general_efficiency": 0.102776,
      "math_efficiency": 0.032653,
      "code_efficiency": 0.226722,
      "reasoning_efficiency": -0.022218,
      "general_scores": [
        70.78,
        49.31,
        37.89,
        24.9842857,
        71.56,
        48.2975,
        36.72,
        32.6592857,
        71.3,
        47.68,
        36.33,
        32.2192857
      ],
      "math_scores": [
        59.74,
        20.92,
        19.0,
        11.33,
        0.0,
        55.72,
        22.92,
        22.2,
        11.02,
        0.0,
        55.65,
        22.5,
        22.6,
        11.56,
        0.0
      ],
      "code_scores": [
        50.61,
        59.92,
        3.94,
        8.35,
        0.23,
        43.9,
        62.8,
        65.37,
        7.89,
        26.93,
        24.66,
        56.71,
        65.24,
        62.26,
        8.24,
        23.8,
        21.49,
        59.76
      ],
      "reasoning_scores": [
        75.93,
        43.83,
        23.74,
        0.21619565,
        3.12,
        79.32,
        62.48,
        28.28,
        0.31043478,
        4.15,
        79.32,
        62.85,
        26.26,
        0.29043478,
        4.3
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.55
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 62.52
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.69
              },
              {
                "metric": "lcb_test_output",
                "score": 15.46
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 53.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.19
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.39
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.86
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.6,
          "math_avg": 2.41,
          "code_avg": 16.76,
          "reasoning_avg": -1.64,
          "overall_avg": 6.28,
          "overall_efficiency": 0.084983,
          "general_efficiency": 0.102776,
          "math_efficiency": 0.032653,
          "code_efficiency": 0.226722,
          "reasoning_efficiency": -0.022218,
          "general_task_scores": [
            6.84,
            28.09,
            0.33,
            -4.88
          ],
          "math_task_scores": [
            0.63,
            2.41,
            2.07,
            6.96,
            0.0
          ],
          "code_task_scores": [
            32.11,
            8.05,
            3.11,
            19.48,
            15.46,
            22.36
          ],
          "reasoning_task_scores": [
            -2.15,
            -6.17,
            -3.71,
            -0.04,
            3.86
          ]
        },
        "vs_instruct": {
          "general_avg": -9.9,
          "math_avg": -18.56,
          "code_avg": -3.62,
          "reasoning_avg": -3.28,
          "overall_avg": -8.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.69,
            -11.9,
            -4.0,
            -14.01
          ],
          "math_task_scores": [
            -28.63,
            -27.61,
            -26.93,
            -1.32,
            -8.34
          ],
          "code_task_scores": [
            -8.13,
            -8.69,
            -2.99,
            14.68,
            -9.65,
            -6.91
          ],
          "reasoning_task_scores": [
            -1.13,
            -6.46,
            1.85,
            -0.15,
            -10.53
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 20.96,
      "math_avg": 14.47,
      "code_avg": 25.62,
      "reasoning_avg": 16.68,
      "overall_avg": 19.43,
      "overall_efficiency": -0.194264,
      "general_efficiency": -0.397976,
      "math_efficiency": -0.120122,
      "code_efficiency": 0.135491,
      "reasoning_efficiency": -0.394448,
      "general_scores": [
        26.05,
        30.395,
        17.23,
        10.1828571,
        19.66,
        28.015,
        22.5,
        12.6657143,
        29.38,
        26.9925,
        20.71,
        7.72642857
      ],
      "math_scores": [
        33.74,
        7.28,
        6.8,
        20.3,
        3.33,
        35.48,
        7.9,
        9.0,
        19.47,
        3.33,
        35.41,
        6.72,
        6.6,
        21.7,
        0.0
      ],
      "code_scores": [
        37.8,
        48.64,
        5.02,
        10.02,
        24.21,
        29.88,
        35.37,
        54.09,
        5.38,
        10.02,
        23.08,
        31.71,
        34.15,
        50.58,
        5.02,
        4.38,
        22.62,
        29.27
      ],
      "reasoning_scores": [
        46.1,
        9.88,
        15.66,
        0.17923913,
        1.19,
        69.49,
        14.92,
        12.63,
        0.18532609,
        1.34,
        53.22,
        9.84,
        14.14,
        0.16793478,
        1.19
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.88
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.14
              },
              {
                "metric": "lcb_test_output",
                "score": 23.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -18.09,
          "math_avg": -5.46,
          "code_avg": 6.16,
          "reasoning_avg": -17.93,
          "overall_avg": -8.83,
          "overall_efficiency": -0.194264,
          "general_efficiency": -0.397976,
          "math_efficiency": -0.120122,
          "code_efficiency": 0.135491,
          "reasoning_efficiency": -0.394448,
          "general_task_scores": [
            -39.34,
            8.13,
            -16.5,
            -24.64
          ],
          "math_task_scores": [
            -21.53,
            -12.4,
            -11.73,
            16.15,
            2.22
          ],
          "code_task_scores": [
            8.33,
            -3.37,
            1.56,
            7.93,
            23.3,
            -0.81
          ],
          "reasoning_task_scores": [
            -24.07,
            -51.01,
            -15.66,
            -0.13,
            1.24
          ]
        },
        "vs_instruct": {
          "general_avg": -35.58,
          "math_avg": -26.44,
          "code_avg": -14.22,
          "reasoning_avg": -19.57,
          "overall_avg": -23.95,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -55.87,
            -31.86,
            -20.83,
            -33.77
          ],
          "math_task_scores": [
            -50.79,
            -42.42,
            -40.73,
            7.87,
            -6.12
          ],
          "code_task_scores": [
            -31.91,
            -20.11,
            -4.54,
            3.13,
            -1.81,
            -30.08
          ],
          "reasoning_task_scores": [
            -23.05,
            -51.3,
            -10.1,
            -0.24,
            -13.15
          ]
        }
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 13.79,
      "math_avg": 13.58,
      "code_avg": 6.16,
      "reasoning_avg": 14.42,
      "overall_avg": 11.99,
      "overall_efficiency": -0.038879,
      "general_efficiency": -0.060334,
      "math_efficiency": -0.015168,
      "code_efficiency": -0.031782,
      "reasoning_efficiency": -0.04823,
      "general_scores": [
        1.18,
        31.1675,
        10.81,
        16.3792857,
        0.04,
        31.3975,
        11.81,
        14.3221429,
        0.07,
        30.2225,
        11.33,
        6.795
      ],
      "math_scores": [
        48.29,
        4.48,
        3.4,
        6.32,
        0.0,
        45.49,
        4.72,
        5.8,
        9.28,
        0.0,
        53.75,
        3.36,
        2.2,
        13.3,
        3.33
      ],
      "code_scores": [
        0.0,
        38.13,
        0.0,
        0.84,
        0.0,
        0.0,
        0.0,
        35.41,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        36.58,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        37.97,
        22.24,
        13.13,
        0.18326087,
        2.08,
        39.32,
        23.48,
        10.61,
        0.19141304,
        2.82,
        36.95,
        14.76,
        11.11,
        0.20543478,
        1.19
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.93
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.18
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 36.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.16
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -25.25,
          "math_avg": -6.35,
          "code_avg": -13.3,
          "reasoning_avg": -20.19,
          "overall_avg": -16.27,
          "overall_efficiency": -0.038879,
          "general_efficiency": -0.060334,
          "math_efficiency": -0.015168,
          "code_efficiency": -0.031782,
          "reasoning_efficiency": -0.04823,
          "general_task_scores": [
            -63.94,
            10.59,
            -25.33,
            -22.33
          ],
          "math_task_scores": [
            -7.23,
            -15.51,
            -15.4,
            5.29,
            1.11
          ],
          "code_task_scores": [
            -27.44,
            -17.76,
            -3.58,
            0.07,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -42.26,
            -42.4,
            -18.18,
            -0.12,
            2.03
          ]
        },
        "vs_instruct": {
          "general_avg": -42.75,
          "math_avg": -27.33,
          "code_avg": -33.68,
          "reasoning_avg": -21.83,
          "overall_avg": -31.4,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -80.47,
            -29.4,
            -29.66,
            -31.46
          ],
          "math_task_scores": [
            -36.49,
            -45.53,
            -44.4,
            -2.99,
            -7.22
          ],
          "code_task_scores": [
            -67.68,
            -34.5,
            -9.68,
            -4.73,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -41.24,
            -42.69,
            -12.62,
            -0.23,
            -12.36
          ]
        }
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "Open-Platypus",
      "domain": "reasoning",
      "general_avg": 42.07,
      "math_avg": 27.99,
      "code_avg": 13.74,
      "reasoning_avg": 30.14,
      "overall_avg": 28.49,
      "overall_efficiency": 0.00906,
      "general_efficiency": 0.121334,
      "math_efficiency": 0.323557,
      "code_efficiency": -0.229679,
      "reasoning_efficiency": -0.178974,
      "general_scores": [
        56.95,
        41.805,
        41.43,
        28.0971429
      ],
      "math_scores": [
        60.05,
        35.4,
        32.2,
        10.66,
        1.665
      ],
      "code_scores": [
        19.51,
        26.07,
        2.15,
        13.99,
        0.0,
        20.73
      ],
      "reasoning_scores": [
        72.88,
        51.2,
        23.23,
        0.27652174,
        3.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.43
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.4
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 19.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 26.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.15
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.99
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 20.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.2
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.02,
          "math_avg": 8.06,
          "code_avg": -5.72,
          "reasoning_avg": -4.46,
          "overall_avg": 0.23,
          "overall_efficiency": 0.00906,
          "general_efficiency": 0.121334,
          "math_efficiency": 0.323557,
          "code_efficiency": -0.229679,
          "reasoning_efficiency": -0.178974,
          "general_task_scores": [
            -7.42,
            21.46,
            4.78,
            -6.73
          ],
          "math_task_scores": [
            3.64,
            15.7,
            13.0,
            6.32,
            1.66
          ],
          "code_task_scores": [
            -7.93,
            -28.4,
            -1.43,
            13.78,
            0.0,
            -10.37
          ],
          "reasoning_task_scores": [
            -7.46,
            -11.36,
            -6.57,
            -0.03,
            3.12
          ]
        },
        "vs_instruct": {
          "general_avg": -14.47,
          "math_avg": -12.91,
          "code_avg": -26.1,
          "reasoning_avg": -6.1,
          "overall_avg": -14.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.95,
            -18.53,
            0.45,
            -15.86
          ],
          "math_task_scores": [
            -25.62,
            -14.32,
            -16.0,
            -1.96,
            -6.68
          ],
          "code_task_scores": [
            -48.17,
            -45.14,
            -7.53,
            8.98,
            -25.11,
            -39.64
          ],
          "reasoning_task_scores": [
            -6.44,
            -11.65,
            -1.01,
            -0.14,
            -11.27
          ]
        }
      },
      "affiliation": "Boston University",
      "year": "2023",
      "size": "24.9k",
      "size_precise": "24926",
      "link": "https://huggingface.co/datasets/garage-bAInd/Open-Platypus",
      "paper_link": "https://arxiv.org/abs/2308.07317",
      "tag": "general,math,code,science"
    },
    {
      "id": 71,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 38.69,
      "math_avg": 59.77,
      "code_avg": 5.31,
      "reasoning_avg": 30.82,
      "overall_avg": 33.65,
      "overall_efficiency": 0.057449,
      "general_efficiency": -0.003831,
      "math_efficiency": 0.424995,
      "code_efficiency": -0.151014,
      "reasoning_efficiency": -0.040354,
      "general_scores": [
        72.19,
        28.8825,
        26.56,
        27.1158333
      ],
      "math_scores": [
        88.02,
        79.38,
        78.2,
        35.73,
        17.5
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        1.88,
        1.36,
        0.61
      ],
      "reasoning_scores": [
        71.53,
        19.23,
        11.62,
        0.23956522,
        51.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.56
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.38
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.73
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.5
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.88
              },
              {
                "metric": "lcb_test_output",
                "score": 1.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.36,
          "math_avg": 39.84,
          "code_avg": -14.15,
          "reasoning_avg": -3.78,
          "overall_avg": 5.38,
          "overall_efficiency": 0.057449,
          "general_efficiency": -0.003831,
          "math_efficiency": 0.424995,
          "code_efficiency": -0.151014,
          "reasoning_efficiency": -0.040354,
          "general_task_scores": [
            7.82,
            8.54,
            -10.09,
            -7.71
          ],
          "math_task_scores": [
            31.61,
            59.68,
            59.0,
            31.39,
            17.5
          ],
          "code_task_scores": [
            -27.44,
            -26.45,
            -3.58,
            1.67,
            1.36,
            -30.49
          ],
          "reasoning_task_scores": [
            -8.81,
            -43.33,
            -18.18,
            -0.07,
            51.48
          ]
        },
        "vs_instruct": {
          "general_avg": -17.85,
          "math_avg": 18.86,
          "code_avg": -34.53,
          "reasoning_avg": -5.42,
          "overall_avg": -9.74,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.71,
            -31.45,
            -14.42,
            -16.84
          ],
          "math_task_scores": [
            2.35,
            29.66,
            30.0,
            23.11,
            9.16
          ],
          "code_task_scores": [
            -67.68,
            -43.19,
            -9.68,
            -3.13,
            -23.75,
            -59.76
          ],
          "reasoning_task_scores": [
            -7.79,
            -43.62,
            -12.62,
            -0.18,
            37.09
          ]
        }
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 72,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 38.07,
      "math_avg": 48.22,
      "code_avg": 16.61,
      "reasoning_avg": 28.67,
      "overall_avg": 32.89,
      "overall_efficiency": 0.034791,
      "general_efficiency": -0.007324,
      "math_efficiency": 0.212529,
      "code_efficiency": -0.02145,
      "reasoning_efficiency": -0.044592,
      "general_scores": [
        57.36,
        29.295,
        33.35,
        32.28
      ],
      "math_scores": [
        88.7,
        63.72,
        64.0,
        21.34,
        3.33
      ],
      "code_scores": [
        28.66,
        44.36,
        5.02,
        1.46,
        2.49,
        17.68
      ],
      "reasoning_scores": [
        76.61,
        24.07,
        12.12,
        0.26532609,
        30.27
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 44.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 17.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.61
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.07
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.27
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.97,
          "math_avg": 28.29,
          "code_avg": -2.86,
          "reasoning_avg": -5.94,
          "overall_avg": 4.63,
          "overall_efficiency": 0.034791,
          "general_efficiency": -0.007324,
          "math_efficiency": 0.212529,
          "code_efficiency": -0.02145,
          "reasoning_efficiency": -0.044592,
          "general_task_scores": [
            -7.01,
            8.96,
            -3.3,
            -2.55
          ],
          "math_task_scores": [
            32.29,
            44.02,
            44.8,
            17.0,
            3.33
          ],
          "code_task_scores": [
            1.22,
            -10.11,
            1.44,
            1.25,
            2.49,
            -13.42
          ],
          "reasoning_task_scores": [
            -3.73,
            -38.49,
            -17.68,
            -0.04,
            30.27
          ]
        },
        "vs_instruct": {
          "general_avg": -18.47,
          "math_avg": 7.31,
          "code_avg": -23.23,
          "reasoning_avg": -7.58,
          "overall_avg": -10.49,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.54,
            -31.02,
            -7.63,
            -11.68
          ],
          "math_task_scores": [
            3.03,
            14.0,
            15.8,
            8.72,
            -5.01
          ],
          "code_task_scores": [
            -39.02,
            -26.85,
            -4.66,
            -3.55,
            -22.62,
            -42.69
          ],
          "reasoning_task_scores": [
            -2.71,
            -38.78,
            -12.12,
            -0.15,
            15.88
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 73,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 36.31,
      "math_avg": 31.76,
      "code_avg": 16.92,
      "reasoning_avg": 24.64,
      "overall_avg": 27.41,
      "overall_efficiency": -0.006199,
      "general_efficiency": -0.019833,
      "math_efficiency": 0.08571,
      "code_efficiency": -0.018454,
      "reasoning_efficiency": -0.072219,
      "general_scores": [
        71.2,
        28.945,
        23.19,
        21.9018182
      ],
      "math_scores": [
        78.7,
        31.04,
        31.2,
        11.18,
        6.67
      ],
      "code_scores": [
        21.34,
        46.3,
        4.3,
        0.63,
        0.9,
        28.05
      ],
      "reasoning_scores": [
        75.25,
        29.51,
        8.08,
        0.25086957,
        10.09
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.19
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 28.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.25
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -2.74,
          "math_avg": 11.83,
          "code_avg": -2.55,
          "reasoning_avg": -9.97,
          "overall_avg": -0.86,
          "overall_efficiency": -0.006199,
          "general_efficiency": -0.019833,
          "math_efficiency": 0.08571,
          "code_efficiency": -0.018454,
          "reasoning_efficiency": -0.072219,
          "general_task_scores": [
            6.83,
            8.6,
            -13.46,
            -12.93
          ],
          "math_task_scores": [
            22.29,
            11.34,
            12.0,
            6.84,
            6.67
          ],
          "code_task_scores": [
            -6.1,
            -8.17,
            0.72,
            0.42,
            0.9,
            -3.05
          ],
          "reasoning_task_scores": [
            -5.09,
            -33.05,
            -21.72,
            -0.06,
            10.09
          ]
        },
        "vs_instruct": {
          "general_avg": -20.23,
          "math_avg": -9.15,
          "code_avg": -22.92,
          "reasoning_avg": -11.61,
          "overall_avg": -15.98,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.7,
            -31.38,
            -17.79,
            -22.06
          ],
          "math_task_scores": [
            -6.97,
            -18.68,
            -17.0,
            -1.44,
            -1.67
          ],
          "code_task_scores": [
            -46.34,
            -24.91,
            -5.38,
            -4.38,
            -24.21,
            -32.32
          ],
          "reasoning_task_scores": [
            -4.07,
            -33.34,
            -16.16,
            -0.17,
            -4.3
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 74,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 22.83,
      "math_avg": 39.21,
      "code_avg": 11.63,
      "reasoning_avg": 17.94,
      "overall_avg": 22.9,
      "overall_efficiency": -0.320764,
      "general_efficiency": -0.970548,
      "math_efficiency": 1.153561,
      "code_efficiency": -0.46918,
      "reasoning_efficiency": -0.996887,
      "general_scores": [
        36.57,
        27.0825,
        16.93,
        10.7307143
      ],
      "math_scores": [
        85.22,
        45.2,
        46.2,
        14.41,
        5.0
      ],
      "code_scores": [
        24.39,
        15.95,
        5.38,
        2.09,
        0.0,
        21.95
      ],
      "reasoning_scores": [
        45.08,
        20.34,
        9.09,
        0.22206522,
        14.99
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 16.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.41
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.09
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 21.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -16.22,
          "math_avg": 19.28,
          "code_avg": -7.84,
          "reasoning_avg": -16.66,
          "overall_avg": -5.36,
          "overall_efficiency": -0.320764,
          "general_efficiency": -0.970548,
          "math_efficiency": 1.153561,
          "code_efficiency": -0.46918,
          "reasoning_efficiency": -0.996887,
          "general_task_scores": [
            -27.8,
            6.74,
            -19.72,
            -24.1
          ],
          "math_task_scores": [
            28.81,
            25.5,
            27.0,
            10.07,
            5.0
          ],
          "code_task_scores": [
            -3.05,
            -38.52,
            1.8,
            1.88,
            0.0,
            -9.15
          ],
          "reasoning_task_scores": [
            -35.26,
            -42.22,
            -20.71,
            -0.09,
            14.99
          ]
        },
        "vs_instruct": {
          "general_avg": -33.71,
          "math_avg": -1.7,
          "code_avg": -28.22,
          "reasoning_avg": -18.3,
          "overall_avg": -20.48,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -44.33,
            -33.25,
            -24.05,
            -33.23
          ],
          "math_task_scores": [
            -0.45,
            -4.52,
            -2.0,
            1.79,
            -3.34
          ],
          "code_task_scores": [
            -43.29,
            -55.26,
            -4.3,
            -2.92,
            -25.11,
            -38.42
          ],
          "reasoning_task_scores": [
            -34.24,
            -42.51,
            -15.15,
            -0.2,
            0.6
          ]
        }
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "size_precise": "16710",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k",
      "paper_link": "https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation",
      "tag": "general,code,math,science"
    },
    {
      "id": 75,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 34.54,
      "math_avg": 66.58,
      "code_avg": 24.44,
      "reasoning_avg": 34.57,
      "overall_avg": 40.03,
      "overall_efficiency": 0.103288,
      "general_efficiency": -0.039549,
      "math_efficiency": 0.409347,
      "code_efficiency": 0.043613,
      "reasoning_efficiency": -0.000258,
      "general_scores": [
        57.92,
        31.1125,
        33.53,
        15.5942857
      ],
      "math_scores": [
        88.86,
        88.16,
        84.8,
        37.74,
        33.33
      ],
      "code_scores": [
        37.8,
        49.42,
        10.39,
        8.77,
        0.0,
        40.24
      ],
      "reasoning_scores": [
        77.97,
        21.26,
        17.68,
        0.31456522,
        55.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.77
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 40.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.26
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.51,
          "math_avg": 46.65,
          "code_avg": 4.97,
          "reasoning_avg": -0.03,
          "overall_avg": 11.77,
          "overall_efficiency": 0.103288,
          "general_efficiency": -0.039549,
          "math_efficiency": 0.409347,
          "code_efficiency": 0.043613,
          "reasoning_efficiency": -0.000258,
          "general_task_scores": [
            -6.45,
            10.77,
            -3.12,
            -19.24
          ],
          "math_task_scores": [
            32.45,
            68.46,
            65.6,
            33.4,
            33.33
          ],
          "code_task_scores": [
            10.36,
            -5.05,
            6.81,
            8.56,
            0.0,
            9.14
          ],
          "reasoning_task_scores": [
            -2.37,
            -41.3,
            -12.12,
            -0.0,
            55.64
          ]
        },
        "vs_instruct": {
          "general_avg": -22.0,
          "math_avg": 25.67,
          "code_avg": -15.41,
          "reasoning_avg": -1.67,
          "overall_avg": -3.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -22.98,
            -29.22,
            -7.45,
            -28.37
          ],
          "math_task_scores": [
            3.19,
            38.44,
            36.6,
            25.12,
            24.99
          ],
          "code_task_scores": [
            -29.88,
            -21.79,
            0.71,
            3.76,
            -25.11,
            -20.13
          ],
          "reasoning_task_scores": [
            -1.35,
            -41.59,
            -6.56,
            -0.11,
            41.25
          ]
        }
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,code,math,science"
    },
    {
      "id": 76,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 30.45,
      "math_avg": 29.34,
      "code_avg": 2.92,
      "reasoning_avg": 20.25,
      "overall_avg": 20.74,
      "overall_efficiency": -0.043831,
      "general_efficiency": -0.050105,
      "math_efficiency": 0.054802,
      "code_efficiency": -0.09638,
      "reasoning_efficiency": -0.083639,
      "general_scores": [
        51.28,
        22.535,
        29.96,
        18.0078571
      ],
      "math_scores": [
        67.85,
        34.22,
        31.4,
        12.38,
        0.8325
      ],
      "code_scores": [
        0.0,
        17.12,
        0.0,
        0.42,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        46.78,
        22.55,
        22.22,
        0.17945652,
        9.5
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.28
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.85
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.83
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.5
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.6,
          "math_avg": 9.41,
          "code_avg": -16.54,
          "reasoning_avg": -14.36,
          "overall_avg": -7.52,
          "overall_efficiency": -0.043831,
          "general_efficiency": -0.050105,
          "math_efficiency": 0.054802,
          "code_efficiency": -0.09638,
          "reasoning_efficiency": -0.083639,
          "general_task_scores": [
            -13.09,
            2.2,
            -6.69,
            -16.82
          ],
          "math_task_scores": [
            11.44,
            14.52,
            12.2,
            8.04,
            0.83
          ],
          "code_task_scores": [
            -27.44,
            -37.35,
            -3.58,
            0.21,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -33.56,
            -40.01,
            -7.58,
            -0.13,
            9.5
          ]
        },
        "vs_instruct": {
          "general_avg": -26.1,
          "math_avg": -11.57,
          "code_avg": -36.92,
          "reasoning_avg": -16.0,
          "overall_avg": -22.65,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -29.62,
            -37.79,
            -11.02,
            -25.95
          ],
          "math_task_scores": [
            -17.82,
            -15.5,
            -16.8,
            -0.24,
            -7.51
          ],
          "code_task_scores": [
            -67.68,
            -54.09,
            -9.68,
            -4.59,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -32.54,
            -40.3,
            -2.02,
            -0.24,
            -4.89
          ]
        }
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 77,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 43.82,
      "math_avg": 34.05,
      "code_avg": 32.56,
      "reasoning_avg": 24.84,
      "overall_avg": 33.82,
      "overall_efficiency": 0.071529,
      "general_efficiency": 0.06142,
      "math_efficiency": 0.181785,
      "code_efficiency": 0.168586,
      "reasoning_efficiency": -0.125673,
      "general_scores": [
        77.46,
        34.5225,
        36.95,
        26.3378571
      ],
      "math_scores": [
        77.48,
        39.36,
        40.8,
        12.62,
        0.0
      ],
      "code_scores": [
        51.83,
        51.36,
        4.3,
        16.28,
        24.66,
        46.95
      ],
      "reasoning_scores": [
        68.14,
        20.78,
        23.23,
        0.32717391,
        11.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.46
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.48
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.36
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.28
              },
              {
                "metric": "lcb_test_output",
                "score": 24.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 46.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.14
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.77,
          "math_avg": 14.12,
          "code_avg": 13.1,
          "reasoning_avg": -9.76,
          "overall_avg": 5.56,
          "overall_efficiency": 0.071529,
          "general_efficiency": 0.06142,
          "math_efficiency": 0.181785,
          "code_efficiency": 0.168586,
          "reasoning_efficiency": -0.125673,
          "general_task_scores": [
            13.09,
            14.18,
            0.3,
            -8.49
          ],
          "math_task_scores": [
            21.07,
            19.66,
            21.6,
            8.28,
            0.0
          ],
          "code_task_scores": [
            24.39,
            -3.11,
            0.72,
            16.07,
            24.66,
            15.85
          ],
          "reasoning_task_scores": [
            -12.2,
            -41.78,
            -6.57,
            0.02,
            11.72
          ]
        },
        "vs_instruct": {
          "general_avg": -12.72,
          "math_avg": -6.86,
          "code_avg": -7.28,
          "reasoning_avg": -11.4,
          "overall_avg": -9.57,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.44,
            -25.8,
            -4.03,
            -17.62
          ],
          "math_task_scores": [
            -8.19,
            -10.36,
            -7.4,
            0.0,
            -8.34
          ],
          "code_task_scores": [
            -15.85,
            -19.85,
            -5.38,
            11.27,
            -0.45,
            -13.42
          ],
          "reasoning_task_scores": [
            -11.18,
            -42.07,
            -1.01,
            -0.09,
            -2.67
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 78,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 14.72,
      "math_avg": 21.35,
      "code_avg": 7.91,
      "reasoning_avg": 9.84,
      "overall_avg": 13.45,
      "overall_efficiency": -45.279337,
      "general_efficiency": -74.397935,
      "math_efficiency": 4.348624,
      "code_efficiency": -35.336392,
      "reasoning_efficiency": -75.731645,
      "general_scores": [
        25.5,
        15.86,
        10.02,
        7.49214286
      ],
      "math_scores": [
        93.1,
        1.12,
        1.2,
        11.34,
        0.0
      ],
      "code_scores": [
        0.0,
        47.47,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        24.41,
        17.89,
        6.57,
        0.02076087,
        0.3
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 10.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.49
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.89
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -24.33,
          "math_avg": 1.42,
          "code_avg": -11.56,
          "reasoning_avg": -24.76,
          "overall_avg": -14.81,
          "overall_efficiency": -45.279337,
          "general_efficiency": -74.397935,
          "math_efficiency": 4.348624,
          "code_efficiency": -35.336392,
          "reasoning_efficiency": -75.731645,
          "general_task_scores": [
            -38.87,
            -4.48,
            -26.63,
            -27.34
          ],
          "math_task_scores": [
            36.69,
            -18.58,
            -18.0,
            7.0,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -7.0,
            -3.58,
            -0.21,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -55.93,
            -44.67,
            -23.23,
            -0.29,
            0.3
          ]
        },
        "vs_instruct": {
          "general_avg": -41.82,
          "math_avg": -19.56,
          "code_avg": -31.93,
          "reasoning_avg": -26.41,
          "overall_avg": -29.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -55.4,
            -44.46,
            -30.96,
            -36.47
          ],
          "math_task_scores": [
            7.43,
            -48.6,
            -47.0,
            -1.28,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -23.74,
            -9.68,
            -5.01,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -54.91,
            -44.96,
            -17.67,
            -0.4,
            -14.09
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 79,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 47.57,
      "math_avg": 30.75,
      "code_avg": 12.51,
      "reasoning_avg": 36.71,
      "overall_avg": 31.88,
      "overall_efficiency": 0.057587,
      "general_efficiency": 0.135493,
      "math_efficiency": 0.171876,
      "code_efficiency": -0.110581,
      "reasoning_efficiency": 0.03356,
      "general_scores": [
        78.48,
        35.2975,
        32.45,
        44.0607143
      ],
      "math_scores": [
        78.62,
        31.48,
        30.2,
        13.01,
        0.41625
      ],
      "code_scores": [
        0.0,
        56.42,
        4.66,
        6.05,
        7.92,
        0.0
      ],
      "reasoning_scores": [
        86.44,
        69.15,
        19.19,
        0.33076087,
        8.46
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.48
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.01
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.05
              },
              {
                "metric": "lcb_test_output",
                "score": 7.92
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.46
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.53,
          "math_avg": 10.82,
          "code_avg": -6.96,
          "reasoning_avg": 2.11,
          "overall_avg": 3.62,
          "overall_efficiency": 0.057587,
          "general_efficiency": 0.135493,
          "math_efficiency": 0.171876,
          "code_efficiency": -0.110581,
          "reasoning_efficiency": 0.03356,
          "general_task_scores": [
            14.11,
            14.96,
            -4.2,
            9.23
          ],
          "math_task_scores": [
            22.21,
            11.78,
            11.0,
            8.67,
            0.42
          ],
          "code_task_scores": [
            -27.44,
            1.95,
            1.08,
            5.84,
            7.92,
            -31.1
          ],
          "reasoning_task_scores": [
            6.1,
            6.59,
            -10.61,
            0.02,
            8.46
          ]
        },
        "vs_instruct": {
          "general_avg": -8.97,
          "math_avg": -10.16,
          "code_avg": -27.34,
          "reasoning_avg": 0.47,
          "overall_avg": -11.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.42,
            -25.03,
            -8.53,
            0.1
          ],
          "math_task_scores": [
            -7.05,
            -18.24,
            -18.0,
            0.39,
            -7.92
          ],
          "code_task_scores": [
            -67.68,
            -14.79,
            -5.02,
            1.04,
            -17.19,
            -60.37
          ],
          "reasoning_task_scores": [
            7.12,
            6.3,
            -5.05,
            -0.09,
            -5.93
          ]
        }
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 80,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 48.88,
      "math_avg": 47.2,
      "code_avg": 17.95,
      "reasoning_avg": 30.34,
      "overall_avg": 36.09,
      "overall_efficiency": 0.031336,
      "general_efficiency": 0.039354,
      "math_efficiency": 0.109106,
      "code_efficiency": -0.006075,
      "reasoning_efficiency": -0.017038,
      "general_scores": [
        84.91,
        30.11,
        39.91,
        40.5964286
      ],
      "math_scores": [
        83.93,
        61.2,
        60.4,
        20.46,
        10.0
      ],
      "code_scores": [
        35.98,
        24.9,
        4.66,
        15.03,
        0.9,
        26.22
      ],
      "reasoning_scores": [
        88.14,
        14.65,
        17.17,
        0.31163043,
        31.45
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.6
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.46
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 24.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 15.03
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 26.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.14
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.84,
          "math_avg": 27.27,
          "code_avg": -1.52,
          "reasoning_avg": -4.26,
          "overall_avg": 7.83,
          "overall_efficiency": 0.031336,
          "general_efficiency": 0.039354,
          "math_efficiency": 0.109106,
          "code_efficiency": -0.006075,
          "reasoning_efficiency": -0.017038,
          "general_task_scores": [
            20.54,
            9.77,
            3.26,
            5.77
          ],
          "math_task_scores": [
            27.52,
            41.5,
            41.2,
            16.12,
            10.0
          ],
          "code_task_scores": [
            8.54,
            -29.57,
            1.08,
            14.82,
            0.9,
            -4.88
          ],
          "reasoning_task_scores": [
            7.8,
            -47.91,
            -12.63,
            -0.0,
            31.45
          ]
        },
        "vs_instruct": {
          "general_avg": -7.66,
          "math_avg": 6.29,
          "code_avg": -21.9,
          "reasoning_avg": -5.9,
          "overall_avg": -7.29,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.01,
            -30.22,
            -1.07,
            -3.36
          ],
          "math_task_scores": [
            -1.74,
            11.48,
            12.2,
            7.84,
            1.66
          ],
          "code_task_scores": [
            -31.7,
            -46.31,
            -5.02,
            10.02,
            -24.21,
            -34.15
          ],
          "reasoning_task_scores": [
            8.82,
            -48.2,
            -7.07,
            -0.11,
            17.06
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 47.38,
      "math_avg": 30.68,
      "code_avg": 34.94,
      "reasoning_avg": 32.54,
      "overall_avg": 36.39,
      "overall_efficiency": 0.054184,
      "general_efficiency": 0.055569,
      "math_efficiency": 0.071693,
      "code_efficiency": 0.103189,
      "reasoning_efficiency": -0.013717,
      "general_scores": [
        56.44,
        54.23,
        43.13,
        35.7264286
      ],
      "math_scores": [
        79.76,
        29.16,
        29.4,
        11.77,
        3.33
      ],
      "code_scores": [
        55.49,
        64.98,
        10.39,
        5.85,
        19.91,
        53.05
      ],
      "reasoning_scores": [
        82.03,
        46.23,
        26.26,
        0.34467391000000003,
        7.86
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.23
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.85
              },
              {
                "metric": "lcb_test_output",
                "score": 19.91
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 53.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.26
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.86
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.34,
          "math_avg": 10.75,
          "code_avg": 15.48,
          "reasoning_avg": -2.06,
          "overall_avg": 8.13,
          "overall_efficiency": 0.054184,
          "general_efficiency": 0.055569,
          "math_efficiency": 0.071693,
          "code_efficiency": 0.103189,
          "reasoning_efficiency": -0.013717,
          "general_task_scores": [
            -7.93,
            33.89,
            6.48,
            0.9
          ],
          "math_task_scores": [
            23.35,
            9.46,
            10.2,
            7.43,
            3.33
          ],
          "code_task_scores": [
            28.05,
            10.51,
            6.81,
            5.64,
            19.91,
            21.95
          ],
          "reasoning_task_scores": [
            1.69,
            -16.33,
            -3.54,
            0.03,
            7.86
          ]
        },
        "vs_instruct": {
          "general_avg": -9.16,
          "math_avg": -10.22,
          "code_avg": -4.9,
          "reasoning_avg": -3.7,
          "overall_avg": -7.0,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.46,
            -6.1,
            2.15,
            -8.23
          ],
          "math_task_scores": [
            -5.91,
            -20.56,
            -18.8,
            -0.85,
            -5.01
          ],
          "code_task_scores": [
            -12.19,
            -6.23,
            0.71,
            0.84,
            -5.2,
            -7.32
          ],
          "reasoning_task_scores": [
            2.71,
            -16.62,
            2.02,
            -0.08,
            -6.53
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 46.92,
      "math_avg": 46.81,
      "code_avg": 19.46,
      "reasoning_avg": 29.54,
      "overall_avg": 35.68,
      "overall_efficiency": 0.029694,
      "general_efficiency": 0.031499,
      "math_efficiency": 0.107537,
      "code_efficiency": -2e-05,
      "reasoning_efficiency": -0.02024,
      "general_scores": [
        82.95,
        32.085,
        31.8,
        40.8385714
      ],
      "math_scores": [
        90.07,
        57.58,
        58.0,
        18.38,
        10.0
      ],
      "code_scores": [
        27.44,
        55.25,
        3.23,
        4.38,
        4.52,
        21.95
      ],
      "reasoning_scores": [
        82.71,
        21.8,
        14.14,
        0.2898913,
        28.78
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.38
              },
              {
                "metric": "lcb_test_output",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 21.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.8
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.78
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.87,
          "math_avg": 26.88,
          "code_avg": -0.0,
          "reasoning_avg": -5.06,
          "overall_avg": 7.42,
          "overall_efficiency": 0.029694,
          "general_efficiency": 0.031499,
          "math_efficiency": 0.107537,
          "code_efficiency": -2e-05,
          "reasoning_efficiency": -0.02024,
          "general_task_scores": [
            18.58,
            11.74,
            -4.85,
            6.01
          ],
          "math_task_scores": [
            33.66,
            37.88,
            38.8,
            14.04,
            10.0
          ],
          "code_task_scores": [
            0.0,
            0.78,
            -0.35,
            4.17,
            4.52,
            -9.15
          ],
          "reasoning_task_scores": [
            2.37,
            -40.76,
            -15.66,
            -0.02,
            28.78
          ]
        },
        "vs_instruct": {
          "general_avg": -9.62,
          "math_avg": 5.9,
          "code_avg": -20.38,
          "reasoning_avg": -6.7,
          "overall_avg": -7.7,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.05,
            -28.25,
            -9.18,
            -3.12
          ],
          "math_task_scores": [
            4.4,
            7.86,
            9.8,
            5.76,
            1.66
          ],
          "code_task_scores": [
            -40.24,
            -15.96,
            -6.45,
            -0.63,
            -20.59,
            -38.42
          ],
          "reasoning_task_scores": [
            3.39,
            -41.05,
            -10.1,
            -0.13,
            14.39
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 35.37,
      "math_avg": 24.29,
      "code_avg": 18.64,
      "reasoning_avg": 25.0,
      "overall_avg": 25.82,
      "overall_efficiency": -0.12218,
      "general_efficiency": -0.184145,
      "math_efficiency": 0.218412,
      "code_efficiency": -0.041449,
      "reasoning_efficiency": -0.481536,
      "general_scores": [
        47.59,
        28.35,
        36.25,
        29.3042857
      ],
      "math_scores": [
        65.73,
        23.16,
        22.6,
        9.94,
        0.0
      ],
      "code_scores": [
        18.9,
        59.92,
        2.87,
        0.0,
        2.71,
        27.44
      ],
      "reasoning_scores": [
        80.0,
        12.22,
        27.78,
        0.24326087,
        4.75
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 2.71
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 27.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.75
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -3.67,
          "math_avg": 4.36,
          "code_avg": -0.83,
          "reasoning_avg": -9.6,
          "overall_avg": -2.44,
          "overall_efficiency": -0.12218,
          "general_efficiency": -0.184145,
          "math_efficiency": 0.218412,
          "code_efficiency": -0.041449,
          "reasoning_efficiency": -0.481536,
          "general_task_scores": [
            -16.78,
            8.01,
            -0.4,
            -5.53
          ],
          "math_task_scores": [
            9.32,
            3.46,
            3.4,
            5.6,
            0.0
          ],
          "code_task_scores": [
            -8.54,
            5.45,
            -0.71,
            -0.21,
            2.71,
            -3.66
          ],
          "reasoning_task_scores": [
            -0.34,
            -50.34,
            -2.02,
            -0.07,
            4.75
          ]
        },
        "vs_instruct": {
          "general_avg": -21.17,
          "math_avg": -16.62,
          "code_avg": -21.2,
          "reasoning_avg": -11.25,
          "overall_avg": -17.56,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -33.31,
            -31.98,
            -4.73,
            -14.66
          ],
          "math_task_scores": [
            -19.94,
            -26.56,
            -25.6,
            -2.68,
            -8.34
          ],
          "code_task_scores": [
            -48.78,
            -11.29,
            -6.81,
            -5.01,
            -22.4,
            -32.93
          ],
          "reasoning_task_scores": [
            0.68,
            -50.63,
            3.54,
            -0.18,
            -9.64
          ]
        }
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,code,math,science"
    },
    {
      "id": 84,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 5.3,
      "math_avg": 4.73,
      "code_avg": 5.64,
      "reasoning_avg": 0.34,
      "overall_avg": 4.0,
      "overall_efficiency": -1.231102,
      "general_efficiency": -1.712583,
      "math_efficiency": -0.771519,
      "code_efficiency": -0.701634,
      "reasoning_efficiency": -1.738673,
      "general_scores": [
        9.69,
        11.15,
        0.0,
        0.36571429
      ],
      "math_scores": [
        22.29,
        0.18,
        0.2,
        0.97,
        0.0
      ],
      "code_scores": [
        0.0,
        33.85,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        1.36,
        0.16,
        0.0,
        0.19793478,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.29
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.85
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -33.74,
          "math_avg": -15.2,
          "code_avg": -13.82,
          "reasoning_avg": -34.26,
          "overall_avg": -24.26,
          "overall_efficiency": -1.231102,
          "general_efficiency": -1.712583,
          "math_efficiency": -0.771519,
          "code_efficiency": -0.701634,
          "reasoning_efficiency": -1.738673,
          "general_task_scores": [
            -54.68,
            -9.19,
            -36.65,
            -34.46
          ],
          "math_task_scores": [
            -34.12,
            -19.52,
            -19.0,
            -3.37,
            0.0
          ],
          "code_task_scores": [
            -27.44,
            -20.62,
            -3.58,
            -0.21,
            0.0,
            -31.1
          ],
          "reasoning_task_scores": [
            -78.98,
            -62.4,
            -29.8,
            -0.11,
            0.0
          ]
        },
        "vs_instruct": {
          "general_avg": -51.24,
          "math_avg": -36.18,
          "code_avg": -34.2,
          "reasoning_avg": -35.9,
          "overall_avg": -39.38,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -71.21,
            -49.18,
            -40.98,
            -43.59
          ],
          "math_task_scores": [
            -63.38,
            -49.54,
            -48.0,
            -11.65,
            -8.34
          ],
          "code_task_scores": [
            -67.68,
            -37.36,
            -9.68,
            -5.01,
            -25.11,
            -60.37
          ],
          "reasoning_task_scores": [
            -77.96,
            -62.69,
            -24.24,
            -0.22,
            -14.39
          ]
        }
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 85,
      "name": "SCP-116K",
      "domain": "reasoning",
      "general_avg": 35.59,
      "math_avg": 52.1,
      "code_avg": 10.71,
      "reasoning_avg": 31.36,
      "overall_avg": 32.44,
      "overall_efficiency": 0.015241,
      "general_efficiency": -0.012621,
      "math_efficiency": 0.117334,
      "code_efficiency": -0.031933,
      "reasoning_efficiency": -0.011815,
      "general_scores": [
        68.04,
        32.8325,
        33.65,
        7.82142857
      ],
      "math_scores": [
        87.11,
        68.8,
        68.4,
        27.85,
        8.335
      ],
      "code_scores": [
        20.12,
        23.74,
        2.15,
        1.46,
        3.39,
        13.41
      ],
      "reasoning_scores": [
        87.12,
        14.97,
        18.69,
        0.27554348,
        35.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.04
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.11
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 23.74
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.15
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 13.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.12
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -3.46,
          "math_avg": 32.17,
          "code_avg": -8.75,
          "reasoning_avg": -3.24,
          "overall_avg": 4.18,
          "overall_efficiency": 0.015241,
          "general_efficiency": -0.012621,
          "math_efficiency": 0.117334,
          "code_efficiency": -0.031933,
          "reasoning_efficiency": -0.011815,
          "general_task_scores": [
            3.67,
            12.49,
            -3.0,
            -27.01
          ],
          "math_task_scores": [
            30.7,
            49.1,
            49.2,
            23.51,
            8.34
          ],
          "code_task_scores": [
            -7.32,
            -30.73,
            -1.43,
            1.25,
            3.39,
            -17.69
          ],
          "reasoning_task_scores": [
            6.78,
            -47.59,
            -11.11,
            -0.03,
            35.76
          ]
        },
        "vs_instruct": {
          "general_avg": -20.96,
          "math_avg": 11.19,
          "code_avg": -29.13,
          "reasoning_avg": -4.88,
          "overall_avg": -10.94,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.86,
            -27.5,
            -7.33,
            -36.14
          ],
          "math_task_scores": [
            1.44,
            19.08,
            20.2,
            15.23,
            0.0
          ],
          "code_task_scores": [
            -47.56,
            -47.47,
            -7.53,
            -3.55,
            -21.72,
            -46.96
          ],
          "reasoning_task_scores": [
            7.8,
            -47.88,
            -5.55,
            -0.14,
            21.37
          ]
        }
      },
      "affiliation": "Fudan University",
      "year": "2025",
      "size": "274k",
      "size_precise": "274166",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K",
      "paper_link": "https://arxiv.org/abs/2501.15587",
      "tag": "general,math,science"
    },
    {
      "id": 86,
      "name": "AM-Thinking-v1-Distilled-code",
      "domain": "reasoning",
      "general_avg": 39.53,
      "math_avg": 39.59,
      "code_avg": 47.25,
      "reasoning_avg": 29.76,
      "overall_avg": 39.03,
      "overall_efficiency": 0.033252,
      "general_efficiency": 0.001488,
      "math_efficiency": 0.060698,
      "code_efficiency": 0.08577,
      "reasoning_efficiency": -0.014949,
      "general_scores": [
        71.98,
        31.85,
        25.14,
        29.1428571
      ],
      "math_scores": [
        78.32,
        43.72,
        45.0,
        20.93,
        10.0
      ],
      "code_scores": [
        83.54,
        62.26,
        35.84,
        14.82,
        12.67,
        74.39
      ],
      "reasoning_scores": [
        75.93,
        43.87,
        7.07,
        0.26728261,
        21.66
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 62.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 35.84
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.82
              },
              {
                "metric": "lcb_test_output",
                "score": 12.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 74.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.87
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.07
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.66
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.48,
          "math_avg": 19.66,
          "code_avg": 27.79,
          "reasoning_avg": -4.84,
          "overall_avg": 10.77,
          "overall_efficiency": 0.033252,
          "general_efficiency": 0.001488,
          "math_efficiency": 0.060698,
          "code_efficiency": 0.08577,
          "reasoning_efficiency": -0.014949,
          "general_task_scores": [
            7.61,
            11.51,
            -11.51,
            -5.69
          ],
          "math_task_scores": [
            21.91,
            24.02,
            25.8,
            16.59,
            10.0
          ],
          "code_task_scores": [
            56.1,
            7.79,
            32.26,
            14.61,
            12.67,
            43.29
          ],
          "reasoning_task_scores": [
            -4.41,
            -18.69,
            -22.73,
            -0.04,
            21.66
          ]
        },
        "vs_instruct": {
          "general_avg": -17.01,
          "math_avg": -1.31,
          "code_avg": 7.41,
          "reasoning_avg": -6.48,
          "overall_avg": -4.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.92,
            -28.48,
            -15.84,
            -14.82
          ],
          "math_task_scores": [
            -7.35,
            -6.0,
            -3.2,
            8.31,
            1.66
          ],
          "code_task_scores": [
            15.86,
            -8.95,
            26.16,
            9.81,
            -12.44,
            14.02
          ],
          "reasoning_task_scores": [
            -3.39,
            -18.98,
            -17.17,
            -0.15,
            7.27
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "324k",
      "size_precise": "323965",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/code.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "code"
    },
    {
      "id": 87,
      "name": "AM-Thinking-v1-Distilled-math",
      "domain": "reasoning",
      "general_avg": 50.52,
      "math_avg": 79.52,
      "code_avg": 17.61,
      "reasoning_avg": 48.29,
      "overall_avg": 48.99,
      "overall_efficiency": 0.037131,
      "general_efficiency": 0.020566,
      "math_efficiency": 0.106764,
      "code_efficiency": -0.003329,
      "reasoning_efficiency": 0.024523,
      "general_scores": [
        90.6,
        31.54,
        37.67,
        42.2878571
      ],
      "math_scores": [
        93.78,
        94.1,
        94.6,
        59.28,
        55.8325
      ],
      "code_scores": [
        24.39,
        48.25,
        2.15,
        5.43,
        2.26,
        23.17
      ],
      "reasoning_scores": [
        84.07,
        70.95,
        17.17,
        0.26619565,
        68.99
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.29
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.78
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 94.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 94.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.28
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.83
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.15
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.43
              },
              {
                "metric": "lcb_test_output",
                "score": 2.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 23.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 11.48,
          "math_avg": 59.59,
          "code_avg": -1.86,
          "reasoning_avg": 13.69,
          "overall_avg": 20.72,
          "overall_efficiency": 0.037131,
          "general_efficiency": 0.020566,
          "math_efficiency": 0.106764,
          "code_efficiency": -0.003329,
          "reasoning_efficiency": 0.024523,
          "general_task_scores": [
            26.23,
            11.2,
            1.02,
            7.46
          ],
          "math_task_scores": [
            37.37,
            74.4,
            75.4,
            54.94,
            55.83
          ],
          "code_task_scores": [
            -3.05,
            -6.22,
            -1.43,
            5.22,
            2.26,
            -7.93
          ],
          "reasoning_task_scores": [
            3.73,
            8.39,
            -12.63,
            -0.04,
            68.99
          ]
        },
        "vs_instruct": {
          "general_avg": -6.02,
          "math_avg": 38.61,
          "code_avg": -22.24,
          "reasoning_avg": 12.04,
          "overall_avg": 5.6,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.7,
            -28.79,
            -3.31,
            -1.67
          ],
          "math_task_scores": [
            8.11,
            44.38,
            46.4,
            46.66,
            47.5
          ],
          "code_task_scores": [
            -43.29,
            -22.96,
            -7.53,
            0.42,
            -22.85,
            -37.2
          ],
          "reasoning_task_scores": [
            4.75,
            8.1,
            -7.07,
            -0.15,
            54.6
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "558k",
      "size_precise": "558129",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/math.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "math"
    },
    {
      "id": 88,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 46.64,
      "math_avg": 59.85,
      "code_avg": 7.32,
      "reasoning_avg": 34.15,
      "overall_avg": 36.99,
      "overall_efficiency": 0.109863,
      "general_efficiency": 0.095565,
      "math_efficiency": 0.502562,
      "code_efficiency": -0.152969,
      "reasoning_efficiency": -0.005707,
      "general_scores": [
        83.4,
        29.915,
        32.22,
        41.0161538
      ],
      "math_scores": [
        88.02,
        76.84,
        76.4,
        36.34,
        21.665
      ],
      "code_scores": [
        3.66,
        20.62,
        0.0,
        7.31,
        4.98,
        7.32
      ],
      "reasoning_scores": [
        81.69,
        18.06,
        19.7,
        0.25532609,
        51.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.31
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.69
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.59,
          "math_avg": 39.92,
          "code_avg": -12.15,
          "reasoning_avg": -0.45,
          "overall_avg": 8.73,
          "overall_efficiency": 0.109863,
          "general_efficiency": 0.095565,
          "math_efficiency": 0.502562,
          "code_efficiency": -0.152969,
          "reasoning_efficiency": -0.005707,
          "general_task_scores": [
            19.03,
            9.58,
            -4.43,
            6.19
          ],
          "math_task_scores": [
            31.61,
            57.14,
            57.2,
            32.0,
            21.66
          ],
          "code_task_scores": [
            -23.78,
            -33.85,
            -3.58,
            7.1,
            4.98,
            -23.78
          ],
          "reasoning_task_scores": [
            1.35,
            -44.5,
            -10.1,
            -0.05,
            51.04
          ]
        },
        "vs_instruct": {
          "general_avg": -9.9,
          "math_avg": 18.94,
          "code_avg": -32.53,
          "reasoning_avg": -2.1,
          "overall_avg": -6.4,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.5,
            -30.4,
            -8.76,
            -2.94
          ],
          "math_task_scores": [
            2.35,
            27.12,
            28.2,
            23.72,
            13.32
          ],
          "code_task_scores": [
            -64.02,
            -50.59,
            -9.68,
            2.3,
            -20.13,
            -53.05
          ],
          "reasoning_task_scores": [
            2.37,
            -44.79,
            -4.54,
            -0.16,
            36.65
          ]
        }
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,code,math,science"
    },
    {
      "id": 89,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 41.46,
      "math_avg": 41.51,
      "code_avg": 7.36,
      "reasoning_avg": 25.13,
      "overall_avg": 28.87,
      "overall_efficiency": 0.076542,
      "general_efficiency": 0.305233,
      "math_efficiency": 2.731645,
      "code_efficiency": -1.531857,
      "reasoning_efficiency": -1.198854,
      "general_scores": [
        84.6,
        26.995,
        23.4,
        30.835
      ],
      "math_scores": [
        82.03,
        48.16,
        47.8,
        21.23,
        8.33
      ],
      "code_scores": [
        9.76,
        22.18,
        1.43,
        1.46,
        2.04,
        7.32
      ],
      "reasoning_scores": [
        70.17,
        16.93,
        14.14,
        0.23728261,
        24.18
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.18
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 2.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 16.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.41,
          "math_avg": 21.58,
          "code_avg": -12.1,
          "reasoning_avg": -9.47,
          "overall_avg": 0.6,
          "overall_efficiency": 0.076542,
          "general_efficiency": 0.305233,
          "math_efficiency": 2.731645,
          "code_efficiency": -1.531857,
          "reasoning_efficiency": -1.198854,
          "general_task_scores": [
            20.23,
            6.66,
            -13.25,
            -3.99
          ],
          "math_task_scores": [
            25.62,
            28.46,
            28.6,
            16.89,
            8.33
          ],
          "code_task_scores": [
            -17.68,
            -32.29,
            -2.15,
            1.25,
            2.04,
            -23.78
          ],
          "reasoning_task_scores": [
            -10.17,
            -45.63,
            -15.66,
            -0.07,
            24.18
          ]
        },
        "vs_instruct": {
          "general_avg": -15.08,
          "math_avg": 0.6,
          "code_avg": -32.48,
          "reasoning_avg": -11.11,
          "overall_avg": -14.52,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.7,
            -33.33,
            -17.58,
            -13.12
          ],
          "math_task_scores": [
            -3.64,
            -1.56,
            -0.4,
            8.61,
            -0.01
          ],
          "code_task_scores": [
            -57.92,
            -49.03,
            -8.25,
            -3.55,
            -23.07,
            -53.05
          ],
          "reasoning_task_scores": [
            -9.15,
            -45.92,
            -10.1,
            -0.18,
            9.79
          ]
        }
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ],
  "qwen": [
    {
      "id": 1,
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "domain": "instruct",
      "general_avg": 66.21,
      "math_avg": 56.81,
      "code_avg": 48.37,
      "reasoning_avg": 34.45,
      "overall_avg": 51.46,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        67.2,
        72.96,
        67.44,
        57.2342857
      ],
      "math_task_scores": [
        92.27,
        75.02,
        77.8,
        29.4,
        9.58375
      ],
      "code_task_scores": [
        64.02,
        74.71,
        11.11,
        43.01,
        41.86,
        55.49
      ],
      "reasoning_task_scores": [
        25.42,
        71.62,
        33.84,
        0.42445652,
        40.95
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.96
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.44
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 57.2342857
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.58375
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.01
              },
              {
                "metric": "lcb_test_output",
                "score": 41.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.62
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42445652
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.95
              }
            ]
          }
        ]
      }
    },
    {
      "id": 0,
      "name": "Qwen/Qwen2.5-7B",
      "domain": "base",
      "general_avg": 51.44,
      "math_avg": 42.8,
      "code_avg": 39.78,
      "reasoning_avg": 35.44,
      "overall_avg": 42.37,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        68.31,
        35.49,
        57.74,
        44.2378571
      ],
      "math_task_scores": [
        79.98,
        51.12,
        50.2,
        26.04,
        6.67
      ],
      "code_task_scores": [
        77.44,
        71.6,
        8.24,
        1.04,
        37.1,
        43.29
      ],
      "reasoning_task_scores": [
        36.6,
        69.46,
        34.85,
        0.392,
        35.91
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.2378571
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.98
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.24
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.04
              },
              {
                "metric": "lcb_test_output",
                "score": 37.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.46
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.392
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.91
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 54.23,
      "math_avg": 40.66,
      "code_avg": 49.82,
      "reasoning_avg": 28.37,
      "overall_avg": 43.27,
      "overall_efficiency": 0.001113,
      "general_efficiency": 0.003439,
      "math_efficiency": -0.002644,
      "code_efficiency": 0.012405,
      "reasoning_efficiency": -0.008745,
      "general_scores": [
        65.09,
        43.28,
        59.22,
        47.3492857,
        65.36,
        45.345,
        59.49,
        47.2757143,
        65.71,
        46.2225,
        59.54,
        46.8328571
      ],
      "math_scores": [
        82.79,
        50.76,
        51.6,
        19.04,
        3.33,
        83.09,
        49.9,
        48.2,
        18.9,
        0.0,
        81.5,
        51.02,
        49.8,
        20.03,
        0.0
      ],
      "code_scores": [
        75.61,
        73.93,
        12.9,
        41.54,
        41.63,
        66.46,
        75.0,
        74.71,
        12.54,
        42.17,
        30.09,
        50.61,
        71.95,
        73.93,
        12.54,
        41.34,
        35.75,
        64.02
      ],
      "reasoning_scores": [
        17.63,
        66.9,
        36.36,
        0.46956522,
        17.51,
        18.64,
        67.1,
        36.87,
        0.44793478,
        17.66,
        21.69,
        67.55,
        38.89,
        0.46315217,
        17.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.56
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.68
              },
              {
                "metric": "lcb_test_output",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.18
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.78,
          "math_avg": -2.14,
          "code_avg": 10.03,
          "reasoning_avg": -7.07,
          "overall_avg": 0.9,
          "overall_efficiency": 0.001113,
          "general_efficiency": 0.003439,
          "math_efficiency": -0.002644,
          "code_efficiency": 0.012405,
          "reasoning_efficiency": -0.008745,
          "general_task_scores": [
            -2.92,
            9.46,
            1.68,
            2.91
          ],
          "math_task_scores": [
            2.48,
            -0.56,
            -0.33,
            -6.72,
            -5.56
          ],
          "code_task_scores": [
            -3.25,
            2.59,
            4.42,
            40.64,
            -1.28,
            17.07
          ],
          "reasoning_task_scores": [
            -17.28,
            -2.28,
            2.52,
            0.07,
            -18.4
          ]
        },
        "vs_instruct": {
          "general_avg": -11.98,
          "math_avg": -16.15,
          "code_avg": 1.45,
          "reasoning_avg": -6.08,
          "overall_avg": -8.19,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.81,
            -28.01,
            -8.02,
            -10.08
          ],
          "math_task_scores": [
            -9.81,
            -24.46,
            -27.93,
            -10.08,
            -8.47
          ],
          "code_task_scores": [
            10.17,
            -0.52,
            1.55,
            -1.33,
            -6.04,
            4.87
          ],
          "reasoning_task_scores": [
            -6.1,
            -4.44,
            3.53,
            0.04,
            -23.44
          ]
        }
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.37,
      "math_avg": 37.86,
      "code_avg": 49.9,
      "reasoning_avg": 26.49,
      "overall_avg": 42.41,
      "overall_efficiency": 0.00073,
      "general_efficiency": 0.075531,
      "math_efficiency": -0.094945,
      "code_efficiency": 0.194576,
      "reasoning_efficiency": -0.172241,
      "general_scores": [
        58.52,
        54.5275,
        60.09,
        46.6914286,
        60.11,
        54.805,
        60.45,
        46.3707143,
        60.47,
        55.52,
        60.57,
        46.3421429
      ],
      "math_scores": [
        81.5,
        44.64,
        43.6,
        13.28,
        3.33,
        81.73,
        44.24,
        44.2,
        13.62,
        6.67,
        80.89,
        44.98,
        45.0,
        13.62,
        6.67
      ],
      "code_scores": [
        72.56,
        71.98,
        10.75,
        42.38,
        37.1,
        64.63,
        71.34,
        72.76,
        11.11,
        42.59,
        33.26,
        67.07,
        70.73,
        72.37,
        9.68,
        41.13,
        39.14,
        67.68
      ],
      "reasoning_scores": [
        14.92,
        68.37,
        31.82,
        0.38934783,
        13.65,
        15.59,
        68.65,
        34.85,
        0.39,
        13.8,
        18.64,
        68.45,
        32.83,
        0.39347826,
        14.54
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.7
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.62
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 36.5
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.93,
          "math_avg": -4.94,
          "code_avg": 10.12,
          "reasoning_avg": -8.96,
          "overall_avg": 0.04,
          "overall_efficiency": 0.00073,
          "general_efficiency": 0.075531,
          "math_efficiency": -0.094945,
          "code_efficiency": 0.194576,
          "reasoning_efficiency": -0.172241,
          "general_task_scores": [
            -8.61,
            19.46,
            2.63,
            2.23
          ],
          "math_task_scores": [
            1.39,
            -6.5,
            -5.93,
            -12.53,
            -1.11
          ],
          "code_task_scores": [
            -5.9,
            0.77,
            2.27,
            40.99,
            -0.6,
            23.17
          ],
          "reasoning_task_scores": [
            -20.22,
            -0.97,
            -1.68,
            -0.0,
            -21.91
          ]
        },
        "vs_instruct": {
          "general_avg": -10.84,
          "math_avg": -18.95,
          "code_avg": 1.54,
          "reasoning_avg": -7.97,
          "overall_avg": -9.05,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.5,
            -18.01,
            -7.07,
            -10.76
          ],
          "math_task_scores": [
            -10.9,
            -30.4,
            -33.53,
            -15.89,
            -4.02
          ],
          "code_task_scores": [
            7.52,
            -2.34,
            -0.6,
            -0.98,
            -5.36,
            10.97
          ],
          "reasoning_task_scores": [
            -9.04,
            -3.13,
            -0.67,
            -0.03,
            -26.95
          ]
        }
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 56.85,
      "math_avg": 46.31,
      "code_avg": 52.98,
      "reasoning_avg": 28.22,
      "overall_avg": 46.09,
      "overall_efficiency": 0.071585,
      "general_efficiency": 0.104036,
      "math_efficiency": 0.067497,
      "code_efficiency": 0.253719,
      "reasoning_efficiency": -0.13891,
      "general_scores": [
        62.8,
        51.095,
        63.73,
        49.105,
        64.71,
        50.86,
        63.63,
        48.9664286,
        62.89,
        51.565,
        63.75,
        49.1535714
      ],
      "math_scores": [
        90.07,
        57.1,
        59.0,
        21.3,
        6.67,
        89.16,
        56.86,
        57.0,
        21.48,
        6.67,
        88.48,
        56.74,
        56.0,
        21.48,
        6.67
      ],
      "code_scores": [
        78.05,
        72.76,
        12.54,
        37.16,
        46.61,
        66.46,
        80.49,
        75.1,
        11.47,
        39.04,
        47.51,
        64.63,
        79.88,
        73.15,
        13.26,
        41.54,
        47.51,
        66.46
      ],
      "reasoning_scores": [
        21.36,
        66.0,
        32.83,
        0.45934783,
        22.26,
        16.95,
        65.93,
        33.33,
        0.45108696,
        21.36,
        20.34,
        66.24,
        34.85,
        0.45119565,
        20.47
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.42
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.25
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.41,
          "math_avg": 3.51,
          "code_avg": 13.19,
          "reasoning_avg": -7.22,
          "overall_avg": 3.72,
          "overall_efficiency": 0.071585,
          "general_efficiency": 0.104036,
          "math_efficiency": 0.067497,
          "code_efficiency": 0.253719,
          "reasoning_efficiency": -0.13891,
          "general_task_scores": [
            -4.84,
            15.68,
            5.96,
            4.84
          ],
          "math_task_scores": [
            9.26,
            5.78,
            7.13,
            -4.62,
            0.0
          ],
          "code_task_scores": [
            2.03,
            2.07,
            4.18,
            38.21,
            10.11,
            22.56
          ],
          "reasoning_task_scores": [
            -17.05,
            -3.4,
            -1.18,
            0.06,
            -14.55
          ]
        },
        "vs_instruct": {
          "general_avg": -9.35,
          "math_avg": -10.5,
          "code_avg": 4.61,
          "reasoning_avg": -6.23,
          "overall_avg": -5.37,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.73,
            -21.79,
            -3.74,
            -8.15
          ],
          "math_task_scores": [
            -3.03,
            -18.12,
            -20.47,
            -7.98,
            -2.91
          ],
          "code_task_scores": [
            15.45,
            -1.04,
            1.31,
            -3.76,
            5.35,
            10.36
          ],
          "reasoning_task_scores": [
            -5.87,
            -5.56,
            -0.17,
            0.03,
            -19.59
          ]
        }
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 54.72,
      "math_avg": 43.46,
      "code_avg": 45.63,
      "reasoning_avg": 28.28,
      "overall_avg": 43.02,
      "overall_efficiency": 0.043514,
      "general_efficiency": 0.217905,
      "math_efficiency": 0.043923,
      "code_efficiency": 0.389677,
      "reasoning_efficiency": -0.477449,
      "general_scores": [
        64.79,
        45.8825,
        61.43,
        47.5907143,
        66.29,
        44.0925,
        61.58,
        47.0185714,
        65.39,
        43.6075,
        61.02,
        47.8935714
      ],
      "math_scores": [
        82.56,
        52.26,
        49.8,
        20.91,
        10.0,
        81.88,
        52.66,
        53.2,
        20.75,
        13.33,
        82.18,
        52.6,
        49.6,
        20.19,
        10.0
      ],
      "code_scores": [
        54.88,
        73.15,
        11.11,
        41.96,
        30.77,
        65.85,
        47.56,
        71.21,
        10.04,
        44.05,
        26.92,
        65.85,
        59.76,
        72.37,
        11.83,
        42.38,
        24.66,
        67.07
      ],
      "reasoning_scores": [
        22.71,
        65.45,
        33.33,
        0.4548913,
        18.55,
        22.03,
        65.32,
        30.81,
        0.44652174,
        17.95,
        31.53,
        65.98,
        28.79,
        0.44967391,
        20.33
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 27.45
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.26
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.58
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.98
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.94
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.27,
          "math_avg": 0.66,
          "code_avg": 5.85,
          "reasoning_avg": -7.17,
          "overall_avg": 0.65,
          "overall_efficiency": 0.043514,
          "general_efficiency": 0.217905,
          "math_efficiency": 0.043923,
          "code_efficiency": 0.389677,
          "reasoning_efficiency": -0.477449,
          "general_task_scores": [
            -2.82,
            9.04,
            3.6,
            3.26
          ],
          "math_task_scores": [
            2.23,
            1.39,
            0.67,
            -5.42,
            4.44
          ],
          "code_task_scores": [
            -23.37,
            0.64,
            2.75,
            41.76,
            -9.65,
            22.97
          ],
          "reasoning_task_scores": [
            -11.18,
            -3.88,
            -3.87,
            0.06,
            -16.97
          ]
        },
        "vs_instruct": {
          "general_avg": -11.49,
          "math_avg": -13.35,
          "code_avg": -2.73,
          "reasoning_avg": -6.18,
          "overall_avg": -8.44,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.71,
            -28.43,
            -6.1,
            -9.73
          ],
          "math_task_scores": [
            -10.06,
            -22.51,
            -26.93,
            -8.78,
            1.53
          ],
          "code_task_scores": [
            -9.95,
            -2.47,
            -0.12,
            -0.21,
            -14.41,
            10.77
          ],
          "reasoning_task_scores": [
            0.0,
            -6.04,
            -2.86,
            0.03,
            -22.01
          ]
        }
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 54.86,
      "math_avg": 44.64,
      "code_avg": 53.18,
      "reasoning_avg": 27.32,
      "overall_avg": 45.0,
      "overall_efficiency": 0.037608,
      "general_efficiency": 0.048812,
      "math_efficiency": 0.0262,
      "code_efficiency": 0.191413,
      "reasoning_efficiency": -0.115991,
      "general_scores": [
        56.3,
        53.35,
        59.68,
        49.1671429,
        57.25,
        52.2825,
        59.95,
        48.9921429,
        56.77,
        55.37,
        60.51,
        48.7142857
      ],
      "math_scores": [
        84.91,
        49.62,
        49.8,
        18.54,
        20.0,
        84.91,
        49.86,
        51.6,
        19.11,
        20.0,
        83.78,
        49.78,
        52.4,
        18.56,
        16.67
      ],
      "code_scores": [
        77.44,
        73.54,
        10.75,
        43.01,
        43.21,
        68.29,
        79.27,
        73.15,
        12.19,
        42.59,
        47.29,
        67.68,
        78.66,
        73.93,
        11.47,
        42.38,
        46.61,
        65.85
      ],
      "reasoning_scores": [
        20.34,
        66.88,
        35.35,
        0.45163043,
        17.51,
        18.64,
        67.34,
        31.82,
        0.44543478,
        16.91,
        17.63,
        67.74,
        32.32,
        0.44858696,
        16.02
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.77
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.67
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.96
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.75
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.66
              },
              {
                "metric": "lcb_test_output",
                "score": 45.7
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.42,
          "math_avg": 1.83,
          "code_avg": 13.4,
          "reasoning_avg": -8.12,
          "overall_avg": 2.63,
          "overall_efficiency": 0.037608,
          "general_efficiency": 0.048812,
          "math_efficiency": 0.0262,
          "code_efficiency": 0.191413,
          "reasoning_efficiency": -0.115991,
          "general_task_scores": [
            -11.54,
            18.18,
            2.31,
            4.72
          ],
          "math_task_scores": [
            4.55,
            -1.37,
            1.07,
            -7.3,
            12.22
          ],
          "code_task_scores": [
            1.02,
            1.94,
            3.23,
            41.62,
            8.6,
            23.98
          ],
          "reasoning_task_scores": [
            -17.73,
            -2.14,
            -1.69,
            0.06,
            -19.1
          ]
        },
        "vs_instruct": {
          "general_avg": -11.35,
          "math_avg": -12.18,
          "code_avg": 4.82,
          "reasoning_avg": -7.13,
          "overall_avg": -6.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.43,
            -19.29,
            -7.39,
            -8.27
          ],
          "math_task_scores": [
            -7.74,
            -25.27,
            -26.53,
            -10.66,
            9.31
          ],
          "code_task_scores": [
            14.44,
            -1.17,
            0.36,
            -0.35,
            3.84,
            11.78
          ],
          "reasoning_task_scores": [
            -6.55,
            -4.3,
            -0.68,
            0.03,
            -24.14
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 55.44,
      "math_avg": 39.77,
      "code_avg": 51.52,
      "reasoning_avg": 27.85,
      "overall_avg": 43.65,
      "overall_efficiency": 0.00895,
      "general_efficiency": 0.027959,
      "math_efficiency": -0.02117,
      "code_efficiency": 0.082082,
      "reasoning_efficiency": -0.053071,
      "general_scores": [
        60.49,
        52.5275,
        61.55,
        47.5457143,
        61.62,
        53.2075,
        61.59,
        47.8192857,
        59.34,
        51.6175,
        61.45,
        46.5530769
      ],
      "math_scores": [
        83.24,
        48.72,
        48.0,
        19.26,
        0.0,
        81.65,
        49.74,
        49.8,
        19.31,
        0.0,
        81.73,
        48.64,
        47.4,
        19.13,
        0.0
      ],
      "code_scores": [
        78.66,
        74.71,
        11.11,
        40.71,
        42.99,
        62.8,
        78.66,
        74.32,
        10.75,
        41.96,
        42.08,
        60.98,
        77.44,
        72.76,
        11.83,
        40.71,
        44.57,
        60.37
      ],
      "reasoning_scores": [
        20.0,
        67.14,
        33.33,
        0.41554348,
        16.02,
        25.42,
        66.9,
        33.33,
        0.41228261,
        16.02,
        23.39,
        67.14,
        33.33,
        0.41163043,
        14.54
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.13
              },
              {
                "metric": "lcb_test_output",
                "score": 43.21
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 61.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.0,
          "math_avg": -3.03,
          "code_avg": 11.74,
          "reasoning_avg": -7.59,
          "overall_avg": 1.28,
          "overall_efficiency": 0.00895,
          "general_efficiency": 0.027959,
          "math_efficiency": -0.02117,
          "code_efficiency": 0.082082,
          "reasoning_efficiency": -0.053071,
          "general_task_scores": [
            -7.83,
            16.96,
            3.79,
            3.07
          ],
          "math_task_scores": [
            2.23,
            -2.09,
            -1.8,
            -6.81,
            -6.67
          ],
          "code_task_scores": [
            0.81,
            2.33,
            2.99,
            40.09,
            6.11,
            18.09
          ],
          "reasoning_task_scores": [
            -13.66,
            -2.4,
            -1.52,
            0.02,
            -20.38
          ]
        },
        "vs_instruct": {
          "general_avg": -10.77,
          "math_avg": -17.04,
          "code_avg": 3.16,
          "reasoning_avg": -6.6,
          "overall_avg": -7.81,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -6.72,
            -20.51,
            -5.91,
            -9.92
          ],
          "math_task_scores": [
            -10.06,
            -25.99,
            -29.4,
            -10.17,
            -9.58
          ],
          "code_task_scores": [
            14.23,
            -0.78,
            0.12,
            -1.88,
            1.35,
            5.89
          ],
          "reasoning_task_scores": [
            -2.48,
            -4.56,
            -0.51,
            -0.01,
            -25.42
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 56.86,
      "math_avg": 45.51,
      "code_avg": 51.91,
      "reasoning_avg": 27.64,
      "overall_avg": 45.48,
      "overall_efficiency": 2.869716,
      "general_efficiency": 4.998764,
      "math_efficiency": 2.49508,
      "code_efficiency": 11.186449,
      "reasoning_efficiency": -7.201429,
      "general_scores": [
        72.32,
        43.3625,
        62.55,
        48.2692857,
        73.1,
        41.78,
        63.13,
        48.0121429,
        74.5,
        43.195,
        63.01,
        49.1285714
      ],
      "math_scores": [
        83.62,
        53.76,
        55.4,
        22.99,
        10.0,
        81.8,
        53.56,
        54.6,
        22.9,
        16.67,
        83.09,
        53.64,
        54.0,
        23.24,
        13.33
      ],
      "code_scores": [
        73.17,
        70.82,
        11.47,
        45.93,
        42.08,
        64.63,
        73.17,
        69.65,
        10.75,
        46.35,
        51.13,
        64.02,
        69.51,
        69.65,
        12.19,
        45.93,
        49.32,
        64.63
      ],
      "reasoning_scores": [
        21.02,
        67.42,
        32.32,
        0.40869565,
        15.88,
        20.68,
        67.46,
        31.31,
        0.39793478,
        17.8,
        20.68,
        66.94,
        32.83,
        0.40413043,
        18.99
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.95
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.07
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.43
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": 2.7,
          "code_avg": 12.13,
          "reasoning_avg": -7.81,
          "overall_avg": 3.11,
          "overall_efficiency": 2.869716,
          "general_efficiency": 4.998764,
          "math_efficiency": 2.49508,
          "code_efficiency": 11.186449,
          "reasoning_efficiency": -7.201429,
          "general_task_scores": [
            5.0,
            7.29,
            5.16,
            4.23
          ],
          "math_task_scores": [
            2.86,
            2.53,
            4.47,
            -3.0,
            6.66
          ],
          "code_task_scores": [
            -5.49,
            -1.56,
            3.23,
            45.03,
            10.41,
            21.14
          ],
          "reasoning_task_scores": [
            -15.81,
            -2.19,
            -2.7,
            0.01,
            -18.35
          ]
        },
        "vs_instruct": {
          "general_avg": -9.35,
          "math_avg": -11.31,
          "code_avg": 3.54,
          "reasoning_avg": -6.81,
          "overall_avg": -5.98,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            6.11,
            -30.18,
            -4.54,
            -8.76
          ],
          "math_task_scores": [
            -9.43,
            -21.37,
            -23.13,
            -6.36,
            3.75
          ],
          "code_task_scores": [
            7.93,
            -4.67,
            0.36,
            3.06,
            5.65,
            8.94
          ],
          "reasoning_task_scores": [
            -4.63,
            -4.35,
            -1.69,
            -0.02,
            -23.39
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 8,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.26,
      "math_avg": 48.95,
      "code_avg": 50.27,
      "reasoning_avg": 42.89,
      "overall_avg": 50.09,
      "overall_efficiency": 0.007771,
      "general_efficiency": 0.006859,
      "math_efficiency": 0.006184,
      "code_efficiency": 0.010552,
      "reasoning_efficiency": 0.007488,
      "general_scores": [
        62.02,
        57.77,
        61.42,
        48.4685714,
        65.53,
        58.145,
        60.78,
        48.4246154,
        68.06,
        60.0675,
        60.56,
        47.91
      ],
      "math_scores": [
        89.92,
        61.4,
        61.6,
        21.27,
        10.0,
        89.84,
        61.92,
        61.2,
        21.18,
        13.33,
        89.84,
        62.7,
        62.0,
        21.36,
        6.67
      ],
      "code_scores": [
        75.61,
        71.98,
        12.54,
        39.67,
        40.95,
        59.15,
        78.05,
        74.32,
        12.9,
        34.86,
        40.95,
        60.37,
        75.0,
        74.32,
        14.34,
        40.71,
        40.05,
        59.15
      ],
      "reasoning_scores": [
        89.49,
        69.34,
        32.32,
        0.37869565,
        20.92,
        88.47,
        70.42,
        32.32,
        0.37706522,
        23.74,
        88.47,
        69.39,
        33.84,
        0.37978261,
        23.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.92
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 59.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.72
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.7
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.82,
          "math_avg": 6.15,
          "code_avg": 10.49,
          "reasoning_avg": 7.44,
          "overall_avg": 7.72,
          "overall_efficiency": 0.007771,
          "general_efficiency": 0.006859,
          "math_efficiency": 0.006184,
          "code_efficiency": 0.010552,
          "reasoning_efficiency": 0.007488,
          "general_task_scores": [
            -3.11,
            23.17,
            3.18,
            4.03
          ],
          "math_task_scores": [
            9.89,
            10.89,
            11.4,
            -4.77,
            3.33
          ],
          "code_task_scores": [
            -1.22,
            1.94,
            5.02,
            37.37,
            3.55,
            16.27
          ],
          "reasoning_task_scores": [
            52.21,
            0.26,
            -2.02,
            -0.01,
            -13.21
          ]
        },
        "vs_instruct": {
          "general_avg": -7.95,
          "math_avg": -7.87,
          "code_avg": 1.91,
          "reasoning_avg": 8.44,
          "overall_avg": -1.37,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.0,
            -14.3,
            -6.52,
            -8.96
          ],
          "math_task_scores": [
            -2.4,
            -13.01,
            -16.2,
            -8.13,
            0.42
          ],
          "code_task_scores": [
            12.2,
            -1.17,
            2.15,
            -4.6,
            -1.21,
            4.07
          ],
          "reasoning_task_scores": [
            63.39,
            -1.9,
            -1.01,
            -0.04,
            -18.25
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 57.1,
      "math_avg": 43.07,
      "code_avg": 49.0,
      "reasoning_avg": 28.18,
      "overall_avg": 44.34,
      "overall_efficiency": 0.001966,
      "general_efficiency": 0.005646,
      "math_efficiency": 0.000267,
      "code_efficiency": 0.009205,
      "reasoning_efficiency": -0.007254,
      "general_scores": [
        64.42,
        56.3125,
        60.51,
        47.53,
        63.72,
        55.5125,
        61.32,
        47.2142857,
        64.95,
        54.6775,
        61.66,
        47.3614286
      ],
      "math_scores": [
        86.43,
        54.84,
        54.8,
        19.76,
        3.33,
        85.9,
        54.76,
        50.0,
        19.67,
        0.0,
        86.96,
        54.54,
        52.4,
        19.33,
        3.33
      ],
      "code_scores": [
        66.46,
        70.82,
        12.9,
        36.95,
        43.67,
        61.59,
        78.05,
        71.21,
        12.19,
        33.82,
        41.18,
        63.41,
        69.51,
        69.65,
        12.9,
        33.4,
        40.95,
        63.41
      ],
      "reasoning_scores": [
        21.02,
        67.31,
        28.28,
        0.41141304,
        17.95,
        24.75,
        67.85,
        32.83,
        0.41532609,
        18.1,
        23.73,
        66.4,
        34.34,
        0.4398913,
        18.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.43
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.71
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.72
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 62.8
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.3
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.65,
          "math_avg": 0.27,
          "code_avg": 9.22,
          "reasoning_avg": -7.26,
          "overall_avg": 1.97,
          "overall_efficiency": 0.001966,
          "general_efficiency": 0.005646,
          "math_efficiency": 0.000267,
          "code_efficiency": 0.009205,
          "reasoning_efficiency": -0.007254,
          "general_task_scores": [
            -3.95,
            20.01,
            3.42,
            3.13
          ],
          "math_task_scores": [
            6.45,
            3.59,
            2.2,
            -6.45,
            -4.45
          ],
          "code_task_scores": [
            -6.1,
            -1.04,
            4.42,
            33.68,
            4.83,
            19.51
          ],
          "reasoning_task_scores": [
            -13.43,
            -2.27,
            -3.03,
            0.03,
            -17.61
          ]
        },
        "vs_instruct": {
          "general_avg": -9.11,
          "math_avg": -13.74,
          "code_avg": 0.64,
          "reasoning_avg": -6.27,
          "overall_avg": -7.12,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.84,
            -17.46,
            -6.28,
            -9.86
          ],
          "math_task_scores": [
            -5.84,
            -20.31,
            -25.4,
            -9.81,
            -7.36
          ],
          "code_task_scores": [
            7.32,
            -4.15,
            1.55,
            -8.29,
            0.07,
            7.31
          ],
          "reasoning_task_scores": [
            -2.25,
            -4.43,
            -2.02,
            -0.0,
            -22.65
          ]
        }
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.63,
      "math_avg": 23.62,
      "code_avg": 14.58,
      "reasoning_avg": 21.42,
      "overall_avg": 24.81,
      "overall_efficiency": -0.212961,
      "general_efficiency": -0.143343,
      "math_efficiency": -0.232689,
      "code_efficiency": -0.305701,
      "reasoning_efficiency": -0.170111,
      "general_scores": [
        43.73,
        34.8875,
        48.77,
        30.055,
        43.54,
        34.605,
        48.93,
        32.25,
        42.39,
        36.355,
        47.24,
        32.7764286
      ],
      "math_scores": [
        65.13,
        20.48,
        20.0,
        8.72,
        6.67,
        65.05,
        20.02,
        19.8,
        8.47,
        6.67,
        61.18,
        19.48,
        18.0,
        7.95,
        6.67
      ],
      "code_scores": [
        17.07,
        28.4,
        0.0,
        31.73,
        0.0,
        12.8,
        15.85,
        28.4,
        0.0,
        30.69,
        0.0,
        12.2,
        15.24,
        27.63,
        0.0,
        30.9,
        0.0,
        11.59
      ],
      "reasoning_scores": [
        19.32,
        54.05,
        26.77,
        0.33826087,
        2.97,
        19.66,
        54.92,
        30.3,
        0.33630435,
        2.67,
        23.73,
        54.57,
        28.79,
        0.33554348,
        2.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.31
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.79
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.99
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 12.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -11.82,
          "math_avg": -19.18,
          "code_avg": -25.2,
          "reasoning_avg": -14.02,
          "overall_avg": -17.56,
          "overall_efficiency": -0.212961,
          "general_efficiency": -0.143343,
          "math_efficiency": -0.232689,
          "code_efficiency": -0.305701,
          "reasoning_efficiency": -0.170111,
          "general_task_scores": [
            -25.09,
            -0.21,
            -9.43,
            -12.55
          ],
          "math_task_scores": [
            -16.19,
            -31.13,
            -30.93,
            -17.66,
            0.0
          ],
          "code_task_scores": [
            -61.39,
            -43.46,
            -8.24,
            30.07,
            -37.1,
            -31.09
          ],
          "reasoning_task_scores": [
            -15.7,
            -14.95,
            -6.23,
            -0.05,
            -33.19
          ]
        },
        "vs_instruct": {
          "general_avg": -26.58,
          "math_avg": -33.2,
          "code_avg": -33.78,
          "reasoning_avg": -13.03,
          "overall_avg": -26.65,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.98,
            -37.68,
            -19.13,
            -25.54
          ],
          "math_task_scores": [
            -28.48,
            -55.03,
            -58.53,
            -21.02,
            -2.91
          ],
          "code_task_scores": [
            -47.97,
            -46.57,
            -11.11,
            -11.9,
            -41.86,
            -43.29
          ],
          "reasoning_task_scores": [
            -4.52,
            -17.11,
            -5.22,
            -0.08,
            -38.23
          ]
        }
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 60.54,
      "math_avg": 44.61,
      "code_avg": 52.3,
      "reasoning_avg": 41.14,
      "overall_avg": 49.64,
      "overall_efficiency": 0.007747,
      "general_efficiency": 0.009678,
      "math_efficiency": 0.001925,
      "code_efficiency": 0.01332,
      "reasoning_efficiency": 0.006063,
      "general_scores": [
        61.67,
        71.4675,
        62.15,
        46.0707143,
        62.37,
        72.1525,
        61.15,
        46.0535714,
        62.84,
        72.685,
        62.09,
        45.7214286
      ],
      "math_scores": [
        88.1,
        57.48,
        58.0,
        19.49,
        3.33,
        87.11,
        56.62,
        55.4,
        19.99,
        6.67,
        85.82,
        55.78,
        53.2,
        18.83,
        3.33
      ],
      "code_scores": [
        80.49,
        71.21,
        11.83,
        39.04,
        45.93,
        68.9,
        78.05,
        72.76,
        11.47,
        38.62,
        44.34,
        66.46,
        78.66,
        71.6,
        13.62,
        37.58,
        46.15,
        64.63
      ],
      "reasoning_scores": [
        82.03,
        62.09,
        35.86,
        0.44315217,
        27.89,
        83.73,
        61.85,
        33.33,
        0.44576087,
        26.85,
        76.27,
        61.82,
        31.82,
        0.43684783,
        32.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.86
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 45.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.66
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.68
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.98
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.09,
          "math_avg": 1.81,
          "code_avg": 12.51,
          "reasoning_avg": 5.7,
          "overall_avg": 7.28,
          "overall_efficiency": 0.007747,
          "general_efficiency": 0.009678,
          "math_efficiency": 0.001925,
          "code_efficiency": 0.01332,
          "reasoning_efficiency": 0.006063,
          "general_task_scores": [
            -6.02,
            36.61,
            4.06,
            1.71
          ],
          "math_task_scores": [
            7.03,
            5.51,
            5.33,
            -6.6,
            -2.23
          ],
          "code_task_scores": [
            1.63,
            0.26,
            4.07,
            37.37,
            8.37,
            23.37
          ],
          "reasoning_task_scores": [
            44.08,
            -7.54,
            -1.18,
            0.05,
            -6.93
          ]
        },
        "vs_instruct": {
          "general_avg": -5.67,
          "math_avg": -12.2,
          "code_avg": 3.93,
          "reasoning_avg": 6.69,
          "overall_avg": -1.82,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.91,
            -0.86,
            -5.64,
            -11.28
          ],
          "math_task_scores": [
            -5.26,
            -18.39,
            -22.27,
            -9.96,
            -5.14
          ],
          "code_task_scores": [
            15.05,
            -2.85,
            1.2,
            -4.6,
            3.61,
            11.17
          ],
          "reasoning_task_scores": [
            55.26,
            -9.7,
            -0.17,
            0.02,
            -11.97
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 12,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 57.92,
      "math_avg": 46.76,
      "code_avg": 37.36,
      "reasoning_avg": 32.87,
      "overall_avg": 43.73,
      "overall_efficiency": 0.001526,
      "general_efficiency": 0.007258,
      "math_efficiency": 0.004441,
      "code_efficiency": -0.002714,
      "reasoning_efficiency": -0.002882,
      "general_scores": [
        69.25,
        49.3625,
        63.18,
        48.7314286,
        70.53,
        49.5325,
        62.99,
        48.8142857,
        70.21,
        50.65,
        62.86,
        48.9
      ],
      "math_scores": [
        88.25,
        59.7,
        58.0,
        22.0,
        3.33,
        89.16,
        60.3,
        56.6,
        22.36,
        3.33,
        89.31,
        60.96,
        59.2,
        22.27,
        6.67
      ],
      "code_scores": [
        39.63,
        74.71,
        7.17,
        40.71,
        44.34,
        14.63,
        46.95,
        75.1,
        7.17,
        42.8,
        42.76,
        7.93,
        45.54,
        75.1,
        11.47,
        40.29,
        41.63,
        14.63
      ],
      "reasoning_scores": [
        40.34,
        69.87,
        31.31,
        0.42967391,
        25.82,
        41.36,
        69.82,
        34.34,
        0.43793478,
        25.96,
        24.07,
        69.0,
        33.92,
        0.44043478,
        25.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.91
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.6
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.27
              },
              {
                "metric": "lcb_test_output",
                "score": 42.91
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 12.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.26
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.91
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.47,
          "math_avg": 3.96,
          "code_avg": -2.42,
          "reasoning_avg": -2.57,
          "overall_avg": 1.36,
          "overall_efficiency": 0.001526,
          "general_efficiency": 0.007258,
          "math_efficiency": 0.004441,
          "code_efficiency": -0.002714,
          "reasoning_efficiency": -0.002882,
          "general_task_scores": [
            1.69,
            14.36,
            5.27,
            4.58
          ],
          "math_task_scores": [
            8.93,
            9.2,
            7.73,
            -3.83,
            -2.23
          ],
          "code_task_scores": [
            -33.4,
            3.37,
            0.36,
            40.23,
            5.81,
            -30.89
          ],
          "reasoning_task_scores": [
            -1.34,
            0.1,
            -1.66,
            0.05,
            -10.0
          ]
        },
        "vs_instruct": {
          "general_avg": -8.29,
          "math_avg": -10.05,
          "code_avg": -11.0,
          "reasoning_avg": -1.58,
          "overall_avg": -7.73,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.8,
            -23.11,
            -4.43,
            -8.41
          ],
          "math_task_scores": [
            -3.36,
            -14.7,
            -19.87,
            -7.19,
            -5.14
          ],
          "code_task_scores": [
            -19.98,
            0.26,
            -2.51,
            -1.74,
            1.05,
            -43.09
          ],
          "reasoning_task_scores": [
            9.84,
            -2.06,
            -0.65,
            0.02,
            -15.04
          ]
        }
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 13,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 59.76,
      "math_avg": 57.53,
      "code_avg": 53.57,
      "reasoning_avg": 32.55,
      "overall_avg": 50.85,
      "overall_efficiency": 0.848318,
      "general_efficiency": 0.831649,
      "math_efficiency": 1.472333,
      "code_efficiency": 1.378222,
      "reasoning_efficiency": -0.288932,
      "general_scores": [
        64.52,
        51.6575,
        68.62,
        54.6642857,
        63.1,
        51.7175,
        68.88,
        54.3228571,
        63.88,
        52.435,
        68.67,
        54.6642857
      ],
      "math_scores": [
        91.81,
        74.42,
        75.8,
        28.16,
        16.67,
        91.58,
        74.3,
        74.4,
        28.68,
        16.67,
        92.27,
        74.18,
        75.8,
        28.14,
        20.0
      ],
      "code_scores": [
        82.32,
        72.76,
        12.54,
        43.63,
        46.83,
        60.98,
        83.54,
        73.15,
        12.9,
        44.05,
        44.12,
        64.63,
        83.46,
        73.54,
        12.54,
        43.63,
        45.48,
        64.11
      ],
      "reasoning_scores": [
        18.64,
        70.64,
        33.84,
        0.44032609,
        38.13,
        21.69,
        70.41,
        33.33,
        0.43695652,
        39.61,
        16.95,
        71.03,
        33.84,
        0.43891304,
        38.87
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.11
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 45.48
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 63.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.09
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.69
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.32,
          "math_avg": 14.72,
          "code_avg": 13.78,
          "reasoning_avg": -2.89,
          "overall_avg": 8.48,
          "overall_efficiency": 0.848318,
          "general_efficiency": 0.831649,
          "math_efficiency": 1.472333,
          "code_efficiency": 1.378222,
          "reasoning_efficiency": -0.288932,
          "general_task_scores": [
            -4.48,
            16.45,
            10.98,
            10.31
          ],
          "math_task_scores": [
            11.91,
            23.18,
            25.13,
            2.29,
            11.11
          ],
          "code_task_scores": [
            5.67,
            1.55,
            4.42,
            42.73,
            8.38,
            19.95
          ],
          "reasoning_task_scores": [
            -17.51,
            1.23,
            -1.18,
            0.05,
            2.96
          ]
        },
        "vs_instruct": {
          "general_avg": -6.45,
          "math_avg": 0.71,
          "code_avg": 5.2,
          "reasoning_avg": -1.9,
          "overall_avg": -0.61,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.37,
            -21.02,
            1.28,
            -2.68
          ],
          "math_task_scores": [
            -0.38,
            -0.72,
            -2.47,
            -1.07,
            8.2
          ],
          "code_task_scores": [
            19.09,
            -1.56,
            1.55,
            0.76,
            3.62,
            7.75
          ],
          "reasoning_task_scores": [
            -6.33,
            -0.93,
            -0.17,
            0.02,
            -2.08
          ]
        }
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 52.62,
      "math_avg": 38.9,
      "code_avg": 47.69,
      "reasoning_avg": 28.66,
      "overall_avg": 41.97,
      "overall_efficiency": -0.000798,
      "general_efficiency": 0.002365,
      "math_efficiency": -0.007819,
      "code_efficiency": 0.015855,
      "reasoning_efficiency": -0.01359,
      "general_scores": [
        53.25,
        50.49,
        57.52,
        45.4307143,
        59.84,
        51.0625,
        57.37,
        45.8185714,
        56.45,
        51.04,
        58.92,
        44.30214
      ],
      "math_scores": [
        80.59,
        47.4,
        47.2,
        17.39,
        3.33,
        77.86,
        46.48,
        45.0,
        16.31,
        3.33,
        80.29,
        47.6,
        49.8,
        17.62,
        3.33
      ],
      "code_scores": [
        62.2,
        70.43,
        11.83,
        41.96,
        42.53,
        58.54,
        69.51,
        71.21,
        10.04,
        41.13,
        41.86,
        57.32,
        65.24,
        66.93,
        10.04,
        38.62,
        42.99,
        56.1
      ],
      "reasoning_scores": [
        24.41,
        65.5,
        30.81,
        0.4026087,
        17.06,
        28.47,
        64.31,
        32.83,
        0.41565217,
        16.77,
        34.24,
        64.13,
        32.83,
        0.410109,
        17.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.11
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.65
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.52
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.64
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.57
              },
              {
                "metric": "lcb_test_output",
                "score": 42.46
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.04
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.06
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.18,
          "math_avg": -3.9,
          "code_avg": 7.91,
          "reasoning_avg": -6.78,
          "overall_avg": -0.4,
          "overall_efficiency": -0.000798,
          "general_efficiency": 0.002365,
          "math_efficiency": -0.007819,
          "code_efficiency": 0.015855,
          "reasoning_efficiency": -0.01359,
          "general_task_scores": [
            -11.8,
            15.37,
            0.2,
            0.94
          ],
          "math_task_scores": [
            -0.4,
            -3.96,
            -2.87,
            -8.93,
            -3.34
          ],
          "code_task_scores": [
            -11.79,
            -2.08,
            2.4,
            39.53,
            5.36,
            14.03
          ],
          "reasoning_task_scores": [
            -7.56,
            -4.81,
            -2.69,
            0.02,
            -18.85
          ]
        },
        "vs_instruct": {
          "general_avg": -13.58,
          "math_avg": -17.91,
          "code_avg": -0.67,
          "reasoning_avg": -5.79,
          "overall_avg": -9.49,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.69,
            -22.1,
            -9.5,
            -12.05
          ],
          "math_task_scores": [
            -12.69,
            -27.86,
            -30.47,
            -12.29,
            -6.25
          ],
          "code_task_scores": [
            1.63,
            -5.19,
            -0.47,
            -2.44,
            0.6,
            1.83
          ],
          "reasoning_task_scores": [
            3.62,
            -6.97,
            -1.68,
            -0.01,
            -23.89
          ]
        }
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 15,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 50.58,
      "math_avg": 20.45,
      "code_avg": 40.48,
      "reasoning_avg": 25.08,
      "overall_avg": 34.15,
      "overall_efficiency": -0.06106,
      "general_efficiency": -0.006394,
      "math_efficiency": -0.16603,
      "code_efficiency": 0.005154,
      "reasoning_efficiency": -0.076971,
      "general_scores": [
        56.13,
        47.1375,
        49.86,
        43.3035714,
        63.44,
        47.0325,
        49.48,
        43.5214286,
        65.54,
        46.9625,
        50.09,
        44.5071429
      ],
      "math_scores": [
        38.67,
        26.98,
        25.6,
        10.93,
        0.0,
        38.44,
        26.84,
        25.2,
        10.95,
        0.0,
        37.3,
        26.54,
        25.4,
        10.61,
        3.33
      ],
      "code_scores": [
        55.67,
        66.54,
        7.17,
        36.74,
        28.96,
        53.11,
        53.66,
        64.98,
        7.17,
        29.65,
        31.45,
        53.66,
        56.1,
        65.37,
        5.73,
        31.94,
        28.28,
        52.44
      ],
      "reasoning_scores": [
        26.44,
        63.46,
        29.29,
        0.3225,
        5.79,
        27.12,
        62.6,
        28.28,
        0.31532609,
        5.34,
        29.15,
        62.2,
        29.8,
        0.32271739,
        5.79
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.7
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.04
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.78
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.14
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 29.56
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 53.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.75
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.86,
          "math_avg": -22.35,
          "code_avg": 0.69,
          "reasoning_avg": -10.36,
          "overall_avg": -8.22,
          "overall_efficiency": -0.06106,
          "general_efficiency": -0.006394,
          "math_efficiency": -0.16603,
          "code_efficiency": 0.005154,
          "reasoning_efficiency": -0.076971,
          "general_task_scores": [
            -6.61,
            11.55,
            -7.93,
            -0.46
          ],
          "math_task_scores": [
            -41.84,
            -24.33,
            -24.8,
            -15.21,
            -5.56
          ],
          "code_task_scores": [
            -22.3,
            -5.97,
            -1.55,
            31.74,
            -7.54,
            9.78
          ],
          "reasoning_task_scores": [
            -9.03,
            -6.71,
            -5.73,
            -0.07,
            -30.27
          ]
        },
        "vs_instruct": {
          "general_avg": -15.62,
          "math_avg": -36.36,
          "code_avg": -7.89,
          "reasoning_avg": -9.37,
          "overall_avg": -17.31,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.5,
            -25.92,
            -17.63,
            -13.45
          ],
          "math_task_scores": [
            -54.13,
            -48.23,
            -52.4,
            -18.57,
            -8.47
          ],
          "code_task_scores": [
            -8.88,
            -9.08,
            -4.42,
            -10.23,
            -12.3,
            -2.42
          ],
          "reasoning_task_scores": [
            2.15,
            -8.87,
            -4.72,
            -0.1,
            -35.31
          ]
        }
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 16,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 54.64,
      "math_avg": 39.19,
      "code_avg": 47.14,
      "reasoning_avg": 28.59,
      "overall_avg": 42.39,
      "overall_efficiency": 0.003058,
      "general_efficiency": 0.42744,
      "math_efficiency": -0.483162,
      "code_efficiency": 0.984285,
      "reasoning_efficiency": -0.916331,
      "general_scores": [
        63.41,
        47.84,
        59.24,
        48.5021429,
        63.5,
        48.6625,
        59.61,
        48.1521429,
        62.63,
        46.19,
        59.51,
        48.4178571
      ],
      "math_scores": [
        75.89,
        48.3,
        47.6,
        18.72,
        13.3,
        77.41,
        48.22,
        46.6,
        18.25,
        3.33,
        75.97,
        47.76,
        45.8,
        17.39,
        3.33
      ],
      "code_scores": [
        75.0,
        71.6,
        11.83,
        29.23,
        29.41,
        68.29,
        73.78,
        70.43,
        10.75,
        26.51,
        24.66,
        66.46,
        75.0,
        72.37,
        11.83,
        26.51,
        35.97,
        68.9
      ],
      "reasoning_scores": [
        25.76,
        68.77,
        35.35,
        0.42119565,
        14.09,
        24.07,
        68.88,
        35.86,
        0.40652174,
        14.84,
        21.02,
        68.92,
        34.85,
        0.40217391,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.65
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.59
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.42
              },
              {
                "metric": "lcb_test_output",
                "score": 30.01
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.88
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.74
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.19,
          "math_avg": -3.61,
          "code_avg": 7.36,
          "reasoning_avg": -6.85,
          "overall_avg": 0.02,
          "overall_efficiency": 0.003058,
          "general_efficiency": 0.42744,
          "math_efficiency": -0.483162,
          "code_efficiency": 0.984285,
          "reasoning_efficiency": -0.916331,
          "general_task_scores": [
            -5.13,
            12.07,
            1.71,
            4.12
          ],
          "math_task_scores": [
            -3.56,
            -3.03,
            -3.53,
            -7.92,
            -0.02
          ],
          "code_task_scores": [
            -2.85,
            -0.13,
            3.23,
            26.38,
            -7.09,
            24.59
          ],
          "reasoning_task_scores": [
            -12.98,
            -0.6,
            0.5,
            0.02,
            -21.17
          ]
        },
        "vs_instruct": {
          "general_avg": -11.57,
          "math_avg": -17.62,
          "code_avg": -1.23,
          "reasoning_avg": -5.86,
          "overall_avg": -9.07,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.02,
            -25.4,
            -7.99,
            -8.87
          ],
          "math_task_scores": [
            -15.85,
            -26.93,
            -31.13,
            -11.28,
            -2.93
          ],
          "code_task_scores": [
            10.57,
            -3.24,
            0.36,
            -15.59,
            -11.85,
            12.39
          ],
          "reasoning_task_scores": [
            -1.8,
            -2.76,
            1.51,
            -0.01,
            -26.21
          ]
        }
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 17,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 54.25,
      "math_avg": 39.28,
      "code_avg": 43.94,
      "reasoning_avg": 27.23,
      "overall_avg": 41.17,
      "overall_efficiency": -0.159143,
      "general_efficiency": 0.37436,
      "math_efficiency": -0.468977,
      "code_efficiency": 0.553555,
      "reasoning_efficiency": -1.09551,
      "general_scores": [
        67.69,
        45.2625,
        58.99,
        47.8485714,
        63.07,
        44.47,
        58.47,
        46.4884615,
        66.42,
        45.575,
        59.11,
        47.6314286
      ],
      "math_scores": [
        83.62,
        46.92,
        45.0,
        18.04,
        6.67,
        84.53,
        46.58,
        46.6,
        18.07,
        0.0,
        83.55,
        47.18,
        44.4,
        18.11,
        0.0
      ],
      "code_scores": [
        76.22,
        69.65,
        11.11,
        43.22,
        2.26,
        61.59,
        74.39,
        71.6,
        9.68,
        43.84,
        0.9,
        64.63,
        73.78,
        69.65,
        9.32,
        43.42,
        1.58,
        64.02
      ],
      "reasoning_scores": [
        20.34,
        68.47,
        33.84,
        0.42847826,
        13.95,
        17.97,
        68.23,
        32.83,
        0.42815217,
        14.68,
        20.34,
        68.2,
        32.83,
        0.42445652,
        15.43
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.9
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 1.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 63.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.81,
          "math_avg": -3.52,
          "code_avg": 4.15,
          "reasoning_avg": -8.22,
          "overall_avg": -1.19,
          "overall_efficiency": -0.159143,
          "general_efficiency": 0.37436,
          "math_efficiency": -0.468977,
          "code_efficiency": 0.553555,
          "reasoning_efficiency": -1.09551,
          "general_task_scores": [
            -2.58,
            9.61,
            1.12,
            3.08
          ],
          "math_task_scores": [
            3.92,
            -4.23,
            -4.87,
            -7.97,
            -4.45
          ],
          "code_task_scores": [
            -2.64,
            -1.3,
            1.8,
            42.45,
            -35.52,
            20.12
          ],
          "reasoning_task_scores": [
            -17.05,
            -1.16,
            -1.68,
            0.04,
            -21.22
          ]
        },
        "vs_instruct": {
          "general_avg": -11.96,
          "math_avg": -17.53,
          "code_avg": -4.43,
          "reasoning_avg": -7.22,
          "overall_avg": -10.29,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.47,
            -27.86,
            -8.58,
            -9.91
          ],
          "math_task_scores": [
            -8.37,
            -28.13,
            -32.47,
            -11.33,
            -7.36
          ],
          "code_task_scores": [
            10.78,
            -4.41,
            -1.07,
            0.48,
            -40.28,
            7.92
          ],
          "reasoning_task_scores": [
            -5.87,
            -3.32,
            -0.67,
            0.01,
            -26.26
          ]
        }
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 56.72,
      "math_avg": 50.69,
      "code_avg": 49.01,
      "reasoning_avg": 30.4,
      "overall_avg": 46.71,
      "overall_efficiency": 0.004338,
      "general_efficiency": 0.005278,
      "math_efficiency": 0.007884,
      "code_efficiency": 0.009228,
      "reasoning_efficiency": -0.00504,
      "general_scores": [
        70.22,
        45.74,
        61.7,
        50.2571429,
        69.21,
        45.3225,
        61.01,
        50.6442857,
        69.21,
        46.115,
        60.59,
        50.6442857
      ],
      "math_scores": [
        91.36,
        66.48,
        65.4,
        22.56,
        6.67,
        91.81,
        66.92,
        66.4,
        22.47,
        6.67,
        91.58,
        66.76,
        66.0,
        22.54,
        6.67
      ],
      "code_scores": [
        67.68,
        73.15,
        14.7,
        34.66,
        40.05,
        69.51,
        73.17,
        73.54,
        13.26,
        30.06,
        43.21,
        54.88,
        73.17,
        72.37,
        13.62,
        37.79,
        42.53,
        54.88
      ],
      "reasoning_scores": [
        26.1,
        65.67,
        31.31,
        0.3223913,
        31.6,
        21.69,
        66.25,
        29.8,
        0.32119565,
        30.27,
        24.75,
        67.55,
        29.8,
        0.31902174,
        30.27
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.55
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.17
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 59.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.28,
          "math_avg": 7.88,
          "code_avg": 9.23,
          "reasoning_avg": -5.04,
          "overall_avg": 4.34,
          "overall_efficiency": 0.004338,
          "general_efficiency": 0.005278,
          "math_efficiency": 0.007884,
          "code_efficiency": 0.009228,
          "reasoning_efficiency": -0.00504,
          "general_task_scores": [
            1.24,
            10.24,
            3.36,
            6.28
          ],
          "math_task_scores": [
            11.6,
            15.6,
            15.73,
            -3.52,
            0.0
          ],
          "code_task_scores": [
            -6.1,
            1.42,
            5.62,
            33.13,
            4.83,
            16.47
          ],
          "reasoning_task_scores": [
            -12.42,
            -2.97,
            -4.55,
            -0.07,
            -5.2
          ]
        },
        "vs_instruct": {
          "general_avg": -9.49,
          "math_avg": -6.13,
          "code_avg": 0.65,
          "reasoning_avg": -4.05,
          "overall_avg": -4.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.35,
            -27.23,
            -6.34,
            -6.71
          ],
          "math_task_scores": [
            -0.69,
            -8.3,
            -11.87,
            -6.88,
            -2.91
          ],
          "code_task_scores": [
            7.32,
            -1.69,
            2.75,
            -8.84,
            0.07,
            4.27
          ],
          "reasoning_task_scores": [
            -1.24,
            -5.13,
            -3.54,
            -0.1,
            -10.24
          ]
        }
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 58.47,
      "math_avg": 49.67,
      "code_avg": 41.44,
      "reasoning_avg": 33.81,
      "overall_avg": 45.85,
      "overall_efficiency": 0.004048,
      "general_efficiency": 0.008171,
      "math_efficiency": 0.007993,
      "code_efficiency": 0.001929,
      "reasoning_efficiency": -0.001899,
      "general_scores": [
        69.15,
        46.485,
        64.45,
        52.1621429,
        68.99,
        47.7125,
        65.79,
        52.36,
        69.68,
        47.54,
        65.46,
        51.8207143
      ],
      "math_scores": [
        87.87,
        63.66,
        62.6,
        26.22,
        10.0,
        88.25,
        64.32,
        63.8,
        25.86,
        13.33,
        87.41,
        63.78,
        61.2,
        26.78,
        0.0
      ],
      "code_scores": [
        53.05,
        68.09,
        10.04,
        20.04,
        40.27,
        60.37,
        47.56,
        68.48,
        5.73,
        23.38,
        39.37,
        54.88,
        48.78,
        67.7,
        4.66,
        30.9,
        42.31,
        60.37
      ],
      "reasoning_scores": [
        32.54,
        59.51,
        30.81,
        0.37402174,
        32.49,
        49.49,
        61.59,
        34.34,
        0.38934783,
        32.2,
        43.05,
        59.03,
        38.89,
        0.39336957,
        32.05
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.77
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.69
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.02,
          "math_avg": 6.87,
          "code_avg": 1.66,
          "reasoning_avg": -1.63,
          "overall_avg": 3.48,
          "overall_efficiency": 0.004048,
          "general_efficiency": 0.008171,
          "math_efficiency": 0.007993,
          "code_efficiency": 0.001929,
          "reasoning_efficiency": -0.001899,
          "general_task_scores": [
            0.96,
            11.76,
            7.49,
            7.87
          ],
          "math_task_scores": [
            7.86,
            12.8,
            12.33,
            0.25,
            1.11
          ],
          "code_task_scores": [
            -27.64,
            -3.51,
            -1.43,
            23.73,
            3.55,
            15.25
          ],
          "reasoning_task_scores": [
            5.09,
            -9.42,
            -0.17,
            -0.0,
            -3.66
          ]
        },
        "vs_instruct": {
          "general_avg": -7.74,
          "math_avg": -7.14,
          "code_avg": -6.92,
          "reasoning_avg": -0.64,
          "overall_avg": -5.61,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.07,
            -25.71,
            -2.21,
            -5.12
          ],
          "math_task_scores": [
            -4.43,
            -11.1,
            -15.27,
            -3.11,
            -1.8
          ],
          "code_task_scores": [
            -14.22,
            -6.62,
            -4.3,
            -18.24,
            -1.21,
            3.05
          ],
          "reasoning_task_scores": [
            16.27,
            -11.58,
            0.84,
            -0.03,
            -8.7
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 60.45,
      "math_avg": 38.66,
      "code_avg": 52.32,
      "reasoning_avg": 31.05,
      "overall_avg": 45.62,
      "overall_efficiency": 0.044909,
      "general_efficiency": 0.124378,
      "math_efficiency": -0.057177,
      "code_efficiency": 0.173052,
      "reasoning_efficiency": -0.060615,
      "general_scores": [
        76.92,
        52.5125,
        59.41,
        51.2678571,
        76.8,
        53.2775,
        61.24,
        51.7264286,
        76.79,
        53.16,
        60.4,
        51.9507143
      ],
      "math_scores": [
        75.21,
        48.7,
        48.6,
        18.95,
        0.0,
        75.36,
        48.3,
        48.4,
        19.63,
        6.67,
        74.0,
        48.8,
        48.2,
        19.08,
        0.0
      ],
      "code_scores": [
        78.66,
        72.76,
        14.34,
        38.41,
        44.12,
        66.46,
        79.27,
        73.54,
        13.62,
        40.29,
        41.63,
        65.85,
        80.49,
        72.76,
        13.62,
        38.41,
        42.31,
        65.24
      ],
      "reasoning_scores": [
        18.64,
        68.96,
        30.3,
        0.37423913,
        34.27,
        21.02,
        68.44,
        34.34,
        0.37815217,
        31.01,
        23.05,
        68.52,
        34.34,
        0.37880435,
        31.75
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.04
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.64
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.34
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.01,
          "math_avg": -4.14,
          "code_avg": 12.54,
          "reasoning_avg": -4.39,
          "overall_avg": 3.25,
          "overall_efficiency": 0.044909,
          "general_efficiency": 0.124378,
          "math_efficiency": -0.057177,
          "code_efficiency": 0.173052,
          "reasoning_efficiency": -0.060615,
          "general_task_scores": [
            8.53,
            17.49,
            2.61,
            7.41
          ],
          "math_task_scores": [
            -5.12,
            -2.52,
            -1.8,
            -6.82,
            -4.45
          ],
          "code_task_scores": [
            2.03,
            1.42,
            5.62,
            38.0,
            5.59,
            22.56
          ],
          "reasoning_task_scores": [
            -15.7,
            -0.82,
            -1.86,
            -0.01,
            -3.57
          ]
        },
        "vs_instruct": {
          "general_avg": -5.75,
          "math_avg": -18.15,
          "code_avg": 3.95,
          "reasoning_avg": -3.4,
          "overall_avg": -5.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.64,
            -19.98,
            -7.09,
            -5.58
          ],
          "math_task_scores": [
            -17.41,
            -26.42,
            -29.4,
            -10.18,
            -7.36
          ],
          "code_task_scores": [
            15.45,
            -1.69,
            2.75,
            -3.97,
            0.83,
            10.36
          ],
          "reasoning_task_scores": [
            -4.52,
            -2.98,
            -0.85,
            -0.04,
            -8.61
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 55.69,
      "math_avg": 41.14,
      "code_avg": 46.97,
      "reasoning_avg": 29.45,
      "overall_avg": 43.31,
      "overall_efficiency": 0.002388,
      "general_efficiency": 0.010742,
      "math_efficiency": -0.004207,
      "code_efficiency": 0.018195,
      "reasoning_efficiency": -0.015178,
      "general_scores": [
        72.14,
        47.59,
        54.62,
        47.3307143,
        70.88,
        48.8025,
        55.29,
        47.7228571,
        72.7,
        47.615,
        55.97,
        47.5878571
      ],
      "math_scores": [
        84.61,
        50.88,
        51.4,
        19.44,
        0.0,
        84.84,
        50.44,
        50.2,
        18.2,
        3.33,
        84.76,
        50.1,
        50.0,
        18.9,
        0.0
      ],
      "code_scores": [
        75.61,
        71.98,
        9.32,
        44.05,
        38.46,
        46.95,
        75.61,
        69.65,
        9.68,
        42.38,
        35.75,
        46.34,
        76.83,
        68.48,
        10.04,
        40.29,
        41.4,
        42.68
      ],
      "reasoning_scores": [
        31.53,
        67.34,
        35.86,
        0.435,
        16.32,
        27.12,
        68.13,
        34.85,
        0.43043478,
        17.95,
        22.03,
        67.16,
        35.35,
        0.43597826,
        16.77
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.74
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.24
              },
              {
                "metric": "lcb_test_output",
                "score": 38.54
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 45.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.89
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.54
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.01
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.24,
          "math_avg": -1.66,
          "code_avg": 7.19,
          "reasoning_avg": -5.99,
          "overall_avg": 0.94,
          "overall_efficiency": 0.002388,
          "general_efficiency": 0.010742,
          "math_efficiency": -0.004207,
          "code_efficiency": 0.018195,
          "reasoning_efficiency": -0.015178,
          "general_task_scores": [
            3.6,
            12.51,
            -2.45,
            3.31
          ],
          "math_task_scores": [
            4.76,
            -0.65,
            0.33,
            -7.19,
            -5.56
          ],
          "code_task_scores": [
            -1.42,
            -1.56,
            1.44,
            41.2,
            1.44,
            2.03
          ],
          "reasoning_task_scores": [
            -9.71,
            -1.92,
            0.5,
            0.04,
            -18.9
          ]
        },
        "vs_instruct": {
          "general_avg": -10.52,
          "math_avg": -15.67,
          "code_avg": -1.39,
          "reasoning_avg": -5.0,
          "overall_avg": -8.15,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.71,
            -24.96,
            -12.15,
            -9.68
          ],
          "math_task_scores": [
            -7.53,
            -24.55,
            -27.27,
            -10.55,
            -8.47
          ],
          "code_task_scores": [
            12.0,
            -4.67,
            -1.43,
            -0.77,
            -3.32,
            -10.17
          ],
          "reasoning_task_scores": [
            1.47,
            -4.08,
            1.51,
            0.01,
            -23.94
          ]
        }
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 58.34,
      "math_avg": 38.51,
      "code_avg": 50.55,
      "reasoning_avg": 28.14,
      "overall_avg": 43.89,
      "overall_efficiency": 0.005794,
      "general_efficiency": 0.026332,
      "math_efficiency": -0.016377,
      "code_efficiency": 0.041081,
      "reasoning_efficiency": -0.027861,
      "general_scores": [
        73.18,
        50.54,
        61.04,
        50.1792857,
        70.186,
        50.905,
        60.9,
        49.7235714,
        70.71,
        51.535,
        61.38,
        49.8564286
      ],
      "math_scores": [
        80.74,
        46.74,
        44.8,
        18.61,
        0.0,
        81.65,
        47.82,
        46.4,
        19.4,
        0.0,
        78.17,
        47.84,
        46.2,
        19.29,
        0.0
      ],
      "code_scores": [
        75.0,
        71.21,
        13.26,
        41.54,
        41.63,
        57.93,
        77.439,
        73.93,
        11.11,
        41.34,
        42.08,
        63.4,
        76.22,
        72.37,
        10.39,
        42.8,
        42.76,
        55.49
      ],
      "reasoning_scores": [
        20.0,
        67.91,
        34.85,
        0.47076087,
        17.95,
        18.31,
        67.21,
        35.86,
        0.44173913,
        18.25,
        17.97,
        68.22,
        35.86,
        0.42152174,
        18.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.59
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.89
              },
              {
                "metric": "lcb_test_output",
                "score": 42.16
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.76
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.9,
          "math_avg": -4.29,
          "code_avg": 10.76,
          "reasoning_avg": -7.3,
          "overall_avg": 1.52,
          "overall_efficiency": 0.005794,
          "general_efficiency": 0.026332,
          "math_efficiency": -0.016377,
          "code_efficiency": 0.041081,
          "reasoning_efficiency": -0.027861,
          "general_task_scores": [
            3.05,
            15.5,
            3.37,
            5.68
          ],
          "math_task_scores": [
            0.21,
            -3.65,
            -4.4,
            -6.94,
            -6.67
          ],
          "code_task_scores": [
            -1.22,
            0.9,
            3.35,
            40.85,
            5.06,
            15.65
          ],
          "reasoning_task_scores": [
            -17.84,
            -1.68,
            0.67,
            0.05,
            -17.71
          ]
        },
        "vs_instruct": {
          "general_avg": -7.86,
          "math_avg": -18.3,
          "code_avg": 2.18,
          "reasoning_avg": -6.31,
          "overall_avg": -7.57,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.16,
            -21.97,
            -6.33,
            -7.31
          ],
          "math_task_scores": [
            -12.08,
            -27.55,
            -32.0,
            -10.3,
            -9.58
          ],
          "code_task_scores": [
            12.2,
            -2.21,
            0.48,
            -1.12,
            0.3,
            3.45
          ],
          "reasoning_task_scores": [
            -6.66,
            -3.84,
            1.68,
            0.02,
            -22.75
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 55.61,
      "math_avg": 39.04,
      "code_avg": 50.83,
      "reasoning_avg": 27.16,
      "overall_avg": 43.16,
      "overall_efficiency": 0.026512,
      "general_efficiency": 0.139468,
      "math_efficiency": -0.126108,
      "code_efficiency": 0.370233,
      "reasoning_efficiency": -0.277545,
      "general_scores": [
        63.76,
        50.355,
        61.44,
        45.5242857,
        63.74,
        52.5325,
        61.16,
        46.0421429,
        63.81,
        51.1575,
        61.46,
        46.2878571
      ],
      "math_scores": [
        75.97,
        45.32,
        43.4,
        18.93,
        13.33,
        75.82,
        45.66,
        44.0,
        20.01,
        10.0,
        75.66,
        45.96,
        45.8,
        19.06,
        6.67
      ],
      "code_scores": [
        75.61,
        69.26,
        11.83,
        44.26,
        45.25,
        61.59,
        78.05,
        69.65,
        10.04,
        42.59,
        38.91,
        60.37,
        76.22,
        68.09,
        12.19,
        43.63,
        47.06,
        60.37
      ],
      "reasoning_scores": [
        23.73,
        66.98,
        31.31,
        0.42391304,
        16.91,
        18.64,
        66.58,
        32.32,
        0.4251087,
        16.02,
        22.03,
        66.67,
        31.31,
        0.42032609,
        13.65
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.77
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.35
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 43.74
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.78
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.47
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.16,
          "math_avg": -3.76,
          "code_avg": 11.05,
          "reasoning_avg": -8.28,
          "overall_avg": 0.79,
          "overall_efficiency": 0.026512,
          "general_efficiency": 0.139468,
          "math_efficiency": -0.126108,
          "code_efficiency": 0.370233,
          "reasoning_efficiency": -0.277545,
          "general_task_scores": [
            -4.54,
            15.86,
            3.61,
            1.71
          ],
          "math_task_scores": [
            -4.16,
            -5.47,
            -5.8,
            -6.71,
            3.33
          ],
          "code_task_scores": [
            -0.81,
            -2.6,
            3.11,
            42.45,
            6.64,
            17.49
          ],
          "reasoning_task_scores": [
            -15.13,
            -2.72,
            -3.2,
            0.03,
            -20.38
          ]
        },
        "vs_instruct": {
          "general_avg": -10.6,
          "math_avg": -17.78,
          "code_avg": 2.46,
          "reasoning_avg": -7.29,
          "overall_avg": -8.3,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.43,
            -21.61,
            -6.09,
            -11.28
          ],
          "math_task_scores": [
            -16.45,
            -29.37,
            -33.4,
            -10.07,
            0.42
          ],
          "code_task_scores": [
            12.61,
            -5.71,
            0.24,
            0.48,
            1.88,
            5.29
          ],
          "reasoning_task_scores": [
            -3.95,
            -4.88,
            -2.19,
            -0.0,
            -25.42
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 53.99,
      "math_avg": 39.4,
      "code_avg": 50.17,
      "reasoning_avg": 27.63,
      "overall_avg": 42.8,
      "overall_efficiency": 0.00858,
      "general_efficiency": 0.050832,
      "math_efficiency": -0.06796,
      "code_efficiency": 0.207667,
      "reasoning_efficiency": -0.15622,
      "general_scores": [
        63.64,
        46.9175,
        58.25,
        49.3621429,
        63.07,
        46.815,
        57.68,
        46.4884615,
        63.31,
        45.7325,
        57.58,
        48.9871429
      ],
      "math_scores": [
        82.94,
        46.72,
        44.8,
        18.68,
        6.67,
        84.31,
        47.48,
        46.4,
        18.97,
        0.0,
        83.09,
        47.26,
        45.4,
        18.34,
        0.0
      ],
      "code_scores": [
        76.22,
        71.6,
        12.19,
        43.84,
        29.64,
        66.46,
        74.39,
        73.15,
        11.83,
        43.42,
        35.29,
        64.63,
        74.39,
        70.43,
        12.19,
        44.05,
        35.29,
        64.02
      ],
      "reasoning_scores": [
        20.68,
        67.88,
        33.84,
        0.4225,
        13.2,
        21.36,
        68.62,
        32.83,
        0.42228261,
        14.39,
        21.69,
        68.98,
        35.35,
        0.41619565,
        14.39
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.45
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 33.41
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.04
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.54,
          "math_avg": -3.4,
          "code_avg": 10.38,
          "reasoning_avg": -7.81,
          "overall_avg": 0.43,
          "overall_efficiency": 0.00858,
          "general_efficiency": 0.050832,
          "math_efficiency": -0.06796,
          "code_efficiency": 0.207667,
          "reasoning_efficiency": -0.15622,
          "general_task_scores": [
            -4.97,
            11.0,
            0.1,
            4.04
          ],
          "math_task_scores": [
            3.47,
            -3.97,
            -4.67,
            -7.38,
            -4.45
          ],
          "code_task_scores": [
            -2.44,
            0.13,
            3.83,
            42.73,
            -3.69,
            21.75
          ],
          "reasoning_task_scores": [
            -15.36,
            -0.97,
            -0.84,
            0.03,
            -21.92
          ]
        },
        "vs_instruct": {
          "general_avg": -12.22,
          "math_avg": -17.41,
          "code_avg": 1.8,
          "reasoning_avg": -6.82,
          "overall_avg": -8.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.86,
            -26.47,
            -9.6,
            -8.95
          ],
          "math_task_scores": [
            -8.82,
            -27.87,
            -32.27,
            -10.74,
            -7.36
          ],
          "code_task_scores": [
            10.98,
            -2.98,
            0.96,
            0.76,
            -8.45,
            9.55
          ],
          "reasoning_task_scores": [
            -4.18,
            -3.13,
            0.17,
            -0.0,
            -26.96
          ]
        }
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 54.13,
      "math_avg": 43.39,
      "code_avg": 26.26,
      "reasoning_avg": 26.8,
      "overall_avg": 37.65,
      "overall_efficiency": -0.158135,
      "general_efficiency": 0.089991,
      "math_efficiency": 0.019738,
      "code_efficiency": -0.452918,
      "reasoning_efficiency": -0.289353,
      "general_scores": [
        73.1,
        37.445,
        58.32,
        48.265,
        72.98,
        37.19,
        57.61,
        48.6414286,
        71.47,
        38.0825,
        58.44,
        48.0321429
      ],
      "math_scores": [
        88.25,
        55.36,
        53.4,
        19.29,
        0.0,
        86.81,
        54.72,
        54.2,
        19.38,
        0.0,
        88.63,
        55.82,
        55.0,
        20.01,
        0.0
      ],
      "code_scores": [
        6.1,
        72.76,
        8.6,
        6.47,
        38.69,
        33.54,
        4.88,
        71.6,
        6.45,
        7.52,
        31.22,
        25.0,
        7.93,
        71.6,
        8.24,
        4.8,
        34.39,
        32.93
      ],
      "reasoning_scores": [
        18.64,
        61.45,
        30.3,
        0.30858696,
        17.06,
        24.75,
        60.35,
        32.83,
        0.30521739,
        17.66,
        29.15,
        61.07,
        30.81,
        0.30413043,
        17.06
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.52
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.12
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.9
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.76
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.26
              },
              {
                "metric": "lcb_test_output",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.26
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.69,
          "math_avg": 0.59,
          "code_avg": -13.52,
          "reasoning_avg": -8.64,
          "overall_avg": -4.72,
          "overall_efficiency": -0.158135,
          "general_efficiency": 0.089991,
          "math_efficiency": 0.019738,
          "code_efficiency": -0.452918,
          "reasoning_efficiency": -0.289353,
          "general_task_scores": [
            4.21,
            2.08,
            0.38,
            4.07
          ],
          "math_task_scores": [
            7.92,
            4.18,
            4.0,
            -6.48,
            -6.67
          ],
          "code_task_scores": [
            -71.14,
            0.39,
            -0.48,
            5.22,
            -2.33,
            -12.8
          ],
          "reasoning_task_scores": [
            -12.42,
            -8.5,
            -3.54,
            -0.08,
            -18.65
          ]
        },
        "vs_instruct": {
          "general_avg": -12.08,
          "math_avg": -13.42,
          "code_avg": -22.1,
          "reasoning_avg": -7.65,
          "overall_avg": -13.81,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            5.32,
            -35.39,
            -9.32,
            -8.92
          ],
          "math_task_scores": [
            -4.37,
            -19.72,
            -23.6,
            -9.84,
            -9.58
          ],
          "code_task_scores": [
            -57.72,
            -2.72,
            -3.35,
            -36.75,
            -7.09,
            -25.0
          ],
          "reasoning_task_scores": [
            -1.24,
            -10.66,
            -2.53,
            -0.11,
            -23.69
          ]
        }
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 26,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 59.4,
      "math_avg": 55.21,
      "code_avg": 48.14,
      "reasoning_avg": 33.69,
      "overall_avg": 49.11,
      "overall_efficiency": 0.044948,
      "general_efficiency": 0.053045,
      "math_efficiency": 0.082751,
      "code_efficiency": 0.055682,
      "reasoning_efficiency": -0.011687,
      "general_scores": [
        72.29,
        47.58,
        64.9,
        51.665,
        72.84,
        48.9925,
        63.95,
        51.5114286,
        68.12,
        49.7,
        66.93,
        54.3107692
      ],
      "math_scores": [
        91.21,
        72.28,
        71.2,
        26.45,
        6.67,
        91.43,
        72.34,
        72.6,
        27.35,
        16.67,
        92.27,
        73.66,
        74.2,
        26.51,
        13.33
      ],
      "code_scores": [
        60.98,
        70.04,
        12.9,
        40.71,
        42.99,
        67.68,
        52.44,
        71.98,
        12.19,
        40.92,
        44.8,
        66.46,
        58.54,
        73.15,
        13.98,
        25.89,
        46.15,
        64.63
      ],
      "reasoning_scores": [
        25.76,
        68.7,
        35.35,
        0.35880435,
        39.76,
        22.71,
        68.11,
        34.85,
        0.36271739,
        38.43,
        25.76,
        70.3,
        35.35,
        0.37706522,
        39.17
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.64
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.26
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.74
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.95,
          "math_avg": 12.41,
          "code_avg": 8.35,
          "reasoning_avg": -1.75,
          "overall_avg": 6.74,
          "overall_efficiency": 0.044948,
          "general_efficiency": 0.053045,
          "math_efficiency": 0.082751,
          "code_efficiency": 0.055682,
          "reasoning_efficiency": -0.011687,
          "general_task_scores": [
            2.77,
            13.27,
            7.52,
            8.26
          ],
          "math_task_scores": [
            11.66,
            21.64,
            22.47,
            0.73,
            5.55
          ],
          "code_task_scores": [
            -20.12,
            0.12,
            4.78,
            34.8,
            7.55,
            22.97
          ],
          "reasoning_task_scores": [
            -11.86,
            -0.42,
            0.33,
            -0.02,
            3.21
          ]
        },
        "vs_instruct": {
          "general_avg": -6.81,
          "math_avg": -1.6,
          "code_avg": -0.23,
          "reasoning_avg": -0.76,
          "overall_avg": -2.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.88,
            -24.2,
            -2.18,
            -4.73
          ],
          "math_task_scores": [
            -0.63,
            -2.26,
            -5.13,
            -2.63,
            2.64
          ],
          "code_task_scores": [
            -6.7,
            -2.99,
            1.91,
            -7.17,
            2.79,
            10.77
          ],
          "reasoning_task_scores": [
            -0.68,
            -2.58,
            1.34,
            -0.05,
            -1.83
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 61.09,
      "math_avg": 55.6,
      "code_avg": 47.56,
      "reasoning_avg": 32.74,
      "overall_avg": 49.25,
      "overall_efficiency": 0.343922,
      "general_efficiency": 0.482189,
      "math_efficiency": 0.639733,
      "code_efficiency": 0.388667,
      "reasoning_efficiency": -0.134901,
      "general_scores": [
        76.84,
        49.64,
        65.94,
        52.6142857,
        76.89,
        48.8,
        65.33,
        53.0192857,
        77.1,
        48.3725,
        65.49,
        53.0228571
      ],
      "math_scores": [
        91.51,
        73.08,
        72.8,
        27.46,
        13.33,
        91.74,
        72.94,
        73.2,
        27.35,
        13.33,
        91.74,
        73.0,
        72.2,
        26.94,
        13.33
      ],
      "code_scores": [
        81.71,
        73.93,
        12.9,
        6.47,
        47.06,
        64.63,
        80.49,
        73.93,
        12.19,
        7.93,
        47.29,
        65.85,
        76.83,
        73.93,
        12.19,
        7.1,
        46.38,
        65.24
      ],
      "reasoning_scores": [
        25.42,
        67.6,
        35.86,
        0.37391304,
        38.28,
        19.66,
        69.85,
        34.34,
        0.37108696,
        39.91,
        20.34,
        68.1,
        34.34,
        0.37076087,
        36.35
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.59
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.17
              },
              {
                "metric": "lcb_test_output",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.52
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.18
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.64,
          "math_avg": 12.79,
          "code_avg": 7.77,
          "reasoning_avg": -2.7,
          "overall_avg": 6.88,
          "overall_efficiency": 0.343922,
          "general_efficiency": 0.482189,
          "math_efficiency": 0.639733,
          "code_efficiency": 0.388667,
          "reasoning_efficiency": -0.134901,
          "general_task_scores": [
            8.63,
            13.45,
            7.85,
            8.65
          ],
          "math_task_scores": [
            11.68,
            21.89,
            22.53,
            1.21,
            6.66
          ],
          "code_task_scores": [
            2.24,
            2.33,
            4.19,
            6.13,
            9.81,
            21.95
          ],
          "reasoning_task_scores": [
            -14.79,
            -0.94,
            0.0,
            -0.02,
            2.27
          ]
        },
        "vs_instruct": {
          "general_avg": -5.12,
          "math_avg": -1.22,
          "code_avg": -0.81,
          "reasoning_avg": -1.71,
          "overall_avg": -2.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.74,
            -24.02,
            -1.85,
            -4.34
          ],
          "math_task_scores": [
            -0.61,
            -2.01,
            -5.07,
            -2.15,
            3.75
          ],
          "code_task_scores": [
            15.66,
            -0.78,
            1.32,
            -35.84,
            5.05,
            9.75
          ],
          "reasoning_task_scores": [
            -3.61,
            -3.1,
            1.01,
            -0.05,
            -2.77
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 57.96,
      "math_avg": 55.42,
      "code_avg": 40.91,
      "reasoning_avg": 31.62,
      "overall_avg": 46.48,
      "overall_efficiency": 0.004094,
      "general_efficiency": 0.006497,
      "math_efficiency": 0.01257,
      "code_efficiency": 0.00112,
      "reasoning_efficiency": -0.003812,
      "general_scores": [
        87.79,
        35.41,
        60.46,
        48.2128571,
        87.38,
        36.31,
        60.17,
        47.69,
        87.88,
        37.18,
        59.84,
        47.2535714
      ],
      "math_scores": [
        90.75,
        74.18,
        74.0,
        26.78,
        13.33,
        91.05,
        73.76,
        74.0,
        27.12,
        10.0,
        91.13,
        73.66,
        73.6,
        27.87,
        10.0
      ],
      "code_scores": [
        65.85,
        73.54,
        9.68,
        20.88,
        13.57,
        62.2,
        65.24,
        72.76,
        11.47,
        20.46,
        11.76,
        64.02,
        66.46,
        73.54,
        9.32,
        18.79,
        14.03,
        62.8
      ],
      "reasoning_scores": [
        25.76,
        63.97,
        31.82,
        0.22065217,
        38.13,
        22.71,
        64.9,
        32.32,
        0.2201087,
        38.58,
        21.69,
        64.72,
        31.31,
        0.21913043,
        37.69
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.98
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.16
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.04
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 63.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.52,
          "math_avg": 12.61,
          "code_avg": 1.12,
          "reasoning_avg": -3.83,
          "overall_avg": 4.11,
          "overall_efficiency": 0.004094,
          "general_efficiency": 0.006497,
          "math_efficiency": 0.01257,
          "code_efficiency": 0.00112,
          "reasoning_efficiency": -0.003812,
          "general_task_scores": [
            19.37,
            0.81,
            2.42,
            3.48
          ],
          "math_task_scores": [
            11.0,
            22.75,
            23.67,
            1.22,
            4.44
          ],
          "code_task_scores": [
            -11.59,
            1.68,
            1.92,
            19.0,
            -23.98,
            19.72
          ],
          "reasoning_task_scores": [
            -13.21,
            -4.93,
            -3.03,
            -0.17,
            2.22
          ]
        },
        "vs_instruct": {
          "general_avg": -8.24,
          "math_avg": -1.4,
          "code_avg": -7.46,
          "reasoning_avg": -2.83,
          "overall_avg": -4.98,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            20.48,
            -36.66,
            -7.28,
            -9.51
          ],
          "math_task_scores": [
            -1.29,
            -1.15,
            -3.93,
            -2.14,
            1.53
          ],
          "code_task_scores": [
            1.83,
            -1.43,
            -0.95,
            -22.97,
            -28.74,
            7.52
          ],
          "reasoning_task_scores": [
            -2.03,
            -7.09,
            -2.02,
            -0.2,
            -2.82
          ]
        }
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 49.64,
      "math_avg": 71.28,
      "code_avg": 24.57,
      "reasoning_avg": 45.89,
      "overall_avg": 47.84,
      "overall_efficiency": 0.017715,
      "general_efficiency": -0.00584,
      "math_efficiency": 0.092128,
      "code_efficiency": -0.049219,
      "reasoning_efficiency": 0.033788,
      "general_scores": [
        82.23,
        28.82,
        40.87,
        46.6371429
      ],
      "math_scores": [
        92.12,
        92.72,
        92.0,
        45.37,
        34.1675
      ],
      "code_scores": [
        31.71,
        49.42,
        2.87,
        19.83,
        13.12,
        30.49
      ],
      "reasoning_scores": [
        82.03,
        62.11,
        24.75,
        0.29652174,
        60.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.87
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.17
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.71
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.83
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.81,
          "math_avg": 28.47,
          "code_avg": -15.21,
          "reasoning_avg": 10.44,
          "overall_avg": 5.47,
          "overall_efficiency": 0.017715,
          "general_efficiency": -0.00584,
          "math_efficiency": 0.092128,
          "code_efficiency": -0.049219,
          "reasoning_efficiency": 0.033788,
          "general_task_scores": [
            13.92,
            -6.67,
            -16.87,
            2.4
          ],
          "math_task_scores": [
            12.14,
            41.6,
            41.8,
            19.33,
            27.5
          ],
          "code_task_scores": [
            -45.73,
            -22.18,
            -5.37,
            18.79,
            -23.98,
            -12.8
          ],
          "reasoning_task_scores": [
            45.43,
            -7.35,
            -10.1,
            -0.09,
            24.33
          ]
        },
        "vs_instruct": {
          "general_avg": -16.57,
          "math_avg": 14.46,
          "code_avg": -23.79,
          "reasoning_avg": 11.43,
          "overall_avg": -3.62,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            15.03,
            -44.14,
            -26.57,
            -10.59
          ],
          "math_task_scores": [
            -0.15,
            17.7,
            14.2,
            15.97,
            24.59
          ],
          "code_task_scores": [
            -32.31,
            -25.29,
            -8.24,
            -23.18,
            -28.74,
            -25.0
          ],
          "reasoning_task_scores": [
            56.61,
            -9.51,
            -9.09,
            -0.12,
            19.29
          ]
        }
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 56.3,
      "math_avg": 47.28,
      "code_avg": 41.79,
      "reasoning_avg": 29.81,
      "overall_avg": 43.8,
      "overall_efficiency": 0.008419,
      "general_efficiency": 0.028651,
      "math_efficiency": 0.026426,
      "code_efficiency": 0.011821,
      "reasoning_efficiency": -0.033222,
      "general_scores": [
        76.22,
        44.9875,
        56.38,
        50.0785714,
        74.61,
        42.615,
        56.6,
        48.8342857,
        77.1,
        44.3375,
        55.22,
        48.63714
      ],
      "math_scores": [
        88.1,
        64.52,
        59.2,
        21.3,
        0.0,
        88.1,
        62.82,
        60.4,
        22.52,
        3.33,
        85.6,
        63.16,
        60.6,
        21.25,
        8.33
      ],
      "code_scores": [
        56.1,
        71.98,
        10.75,
        36.33,
        11.09,
        60.98,
        55.37,
        73.54,
        8.96,
        41.96,
        26.92,
        57.93,
        56.71,
        71.98,
        11.83,
        39.25,
        7.47,
        53.05
      ],
      "reasoning_scores": [
        30.51,
        65.72,
        34.34,
        0.31782609,
        25.07,
        25.42,
        66.5,
        29.29,
        0.34163043,
        21.36,
        26.1,
        66.8,
        31.31,
        0.335109,
        23.74
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 56.06
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.18
              },
              {
                "metric": "lcb_test_output",
                "score": 15.16
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.86,
          "math_avg": 4.48,
          "code_avg": 2.0,
          "reasoning_avg": -5.63,
          "overall_avg": 1.43,
          "overall_efficiency": 0.008419,
          "general_efficiency": 0.028651,
          "math_efficiency": 0.026426,
          "code_efficiency": 0.011821,
          "reasoning_efficiency": -0.033222,
          "general_task_scores": [
            7.67,
            8.49,
            -1.67,
            4.94
          ],
          "math_task_scores": [
            7.29,
            12.38,
            9.87,
            -4.35,
            -2.78
          ],
          "code_task_scores": [
            -21.38,
            0.9,
            2.27,
            38.14,
            -21.94,
            14.03
          ],
          "reasoning_task_scores": [
            -9.26,
            -3.12,
            -3.2,
            -0.06,
            -12.52
          ]
        },
        "vs_instruct": {
          "general_avg": -9.91,
          "math_avg": -9.53,
          "code_avg": -6.58,
          "reasoning_avg": -4.64,
          "overall_avg": -7.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.78,
            -28.98,
            -11.37,
            -8.05
          ],
          "math_task_scores": [
            -5.0,
            -11.52,
            -17.73,
            -7.71,
            -5.69
          ],
          "code_task_scores": [
            -7.96,
            -2.21,
            -0.6,
            -3.83,
            -26.7,
            1.83
          ],
          "reasoning_task_scores": [
            1.92,
            -5.28,
            -2.19,
            -0.09,
            -17.56
          ]
        }
      },
      "affiliation": "Ruben Roy",
      "year": "2025",
      "size": "170k",
      "size_precise": "169527",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 60.81,
      "math_avg": 50.32,
      "code_avg": 50.85,
      "reasoning_avg": 30.62,
      "overall_avg": 48.15,
      "overall_efficiency": 0.028907,
      "general_efficiency": 0.046802,
      "math_efficiency": 0.037583,
      "code_efficiency": 0.055327,
      "reasoning_efficiency": -0.024086,
      "general_scores": [
        78.55,
        52.4675,
        64.68,
        50.4071429,
        76.01,
        53.2375,
        64.97,
        49.8278571,
        75.77,
        52.4825,
        65.15,
        46.1257143
      ],
      "math_scores": [
        89.76,
        63.96,
        64.8,
        23.4,
        10.0,
        89.39,
        64.38,
        63.2,
        24.77,
        6.67,
        89.61,
        63.94,
        63.0,
        24.59,
        13.33
      ],
      "code_scores": [
        66.46,
        73.54,
        13.62,
        43.01,
        47.74,
        60.98,
        70.12,
        75.88,
        13.62,
        44.47,
        49.77,
        54.88,
        67.07,
        73.54,
        12.9,
        43.22,
        48.42,
        56.1
      ],
      "reasoning_scores": [
        20.68,
        68.52,
        35.86,
        0.41097826,
        31.31,
        20.0,
        67.72,
        33.84,
        0.40532608999999997,
        30.71,
        21.02,
        67.55,
        33.33,
        0.40902174,
        27.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.79
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.88
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.57
              },
              {
                "metric": "lcb_test_output",
                "score": 48.64
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.36,
          "math_avg": 7.52,
          "code_avg": 11.07,
          "reasoning_avg": -4.82,
          "overall_avg": 5.78,
          "overall_efficiency": 0.028907,
          "general_efficiency": 0.046802,
          "math_efficiency": 0.037583,
          "code_efficiency": 0.055327,
          "reasoning_efficiency": -0.024086,
          "general_task_scores": [
            8.47,
            17.24,
            7.19,
            4.55
          ],
          "math_task_scores": [
            9.61,
            12.97,
            13.47,
            -1.79,
            3.33
          ],
          "code_task_scores": [
            -9.56,
            2.72,
            5.14,
            42.53,
            11.54,
            14.03
          ],
          "reasoning_task_scores": [
            -16.03,
            -1.53,
            -0.51,
            0.02,
            -6.04
          ]
        },
        "vs_instruct": {
          "general_avg": -5.4,
          "math_avg": -6.49,
          "code_avg": 2.49,
          "reasoning_avg": -3.83,
          "overall_avg": -3.31,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.58,
            -20.23,
            -2.51,
            -8.44
          ],
          "math_task_scores": [
            -2.68,
            -10.93,
            -14.13,
            -5.15,
            0.42
          ],
          "code_task_scores": [
            3.86,
            -0.39,
            2.27,
            0.56,
            6.78,
            1.83
          ],
          "reasoning_task_scores": [
            -4.85,
            -3.69,
            0.5,
            -0.01,
            -11.08
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 57.37,
      "math_avg": 46.96,
      "code_avg": 48.5,
      "reasoning_avg": 24.75,
      "overall_avg": 44.4,
      "overall_efficiency": 0.003465,
      "general_efficiency": 0.010116,
      "math_efficiency": 0.007111,
      "code_efficiency": 0.014896,
      "reasoning_efficiency": -0.018263,
      "general_scores": [
        76.01,
        43.0725,
        62.67,
        47.535,
        76.01,
        43.3675,
        63.54,
        47.3207143,
        75.77,
        42.605,
        63.07,
        47.4235714
      ],
      "math_scores": [
        89.46,
        60.4,
        60.8,
        20.51,
        6.67,
        90.45,
        59.64,
        58.2,
        20.17,
        0.0,
        90.6,
        59.46,
        61.0,
        20.44,
        6.67
      ],
      "code_scores": [
        70.12,
        69.26,
        14.34,
        41.34,
        43.44,
        54.88,
        70.12,
        69.26,
        14.7,
        41.13,
        42.76,
        54.88,
        67.07,
        67.32,
        13.98,
        40.08,
        42.31,
        56.1
      ],
      "reasoning_scores": [
        25.76,
        40.46,
        30.81,
        0.4423913,
        25.96,
        26.78,
        40.35,
        29.8,
        0.43956522,
        24.18,
        29.15,
        40.22,
        30.81,
        0.44532609,
        25.67
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.93
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.61
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.34
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.85
              },
              {
                "metric": "lcb_test_output",
                "score": 42.84
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.27
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.92,
          "math_avg": 4.16,
          "code_avg": 8.72,
          "reasoning_avg": -10.69,
          "overall_avg": 2.03,
          "overall_efficiency": 0.003465,
          "general_efficiency": 0.010116,
          "math_efficiency": 0.007111,
          "code_efficiency": 0.014896,
          "reasoning_efficiency": -0.018263,
          "general_task_scores": [
            7.62,
            7.52,
            5.35,
            3.19
          ],
          "math_task_scores": [
            10.19,
            8.71,
            9.8,
            -5.67,
            -2.22
          ],
          "code_task_scores": [
            -8.34,
            -2.99,
            6.1,
            39.81,
            5.74,
            12.0
          ],
          "reasoning_task_scores": [
            -9.37,
            -29.12,
            -4.38,
            0.05,
            -10.64
          ]
        },
        "vs_instruct": {
          "general_avg": -8.84,
          "math_avg": -9.85,
          "code_avg": 0.14,
          "reasoning_avg": -9.7,
          "overall_avg": -7.06,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.73,
            -29.95,
            -4.35,
            -9.8
          ],
          "math_task_scores": [
            -2.1,
            -15.19,
            -17.8,
            -9.03,
            -5.13
          ],
          "code_task_scores": [
            5.08,
            -6.1,
            3.23,
            -2.16,
            0.98,
            -0.2
          ],
          "reasoning_task_scores": [
            1.81,
            -31.28,
            -3.37,
            0.02,
            -15.68
          ]
        }
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 57.18,
      "math_avg": 46.67,
      "code_avg": 38.62,
      "reasoning_avg": 29.91,
      "overall_avg": 43.09,
      "overall_efficiency": 0.030793,
      "general_efficiency": 0.243162,
      "math_efficiency": 0.16408,
      "code_efficiency": -0.049387,
      "reasoning_efficiency": -0.234682,
      "general_scores": [
        75.66,
        48.0325,
        60.95,
        43.2571429,
        75.61,
        47.8875,
        60.81,
        43.1,
        74.95,
        48.885,
        61.11,
        45.8807143
      ],
      "math_scores": [
        88.86,
        58.5,
        59.6,
        19.67,
        6.67,
        89.99,
        59.64,
        60.2,
        18.99,
        6.67,
        89.54,
        59.16,
        60.0,
        19.24,
        3.33
      ],
      "code_scores": [
        53.66,
        72.76,
        13.98,
        21.09,
        21.04,
        64.63,
        50.0,
        73.15,
        12.9,
        20.04,
        22.62,
        58.54,
        28.05,
        73.15,
        11.83,
        21.29,
        22.17,
        54.27
      ],
      "reasoning_scores": [
        29.15,
        70.71,
        26.26,
        0.31369565,
        27.0,
        25.08,
        70.27,
        22.73,
        0.31293478,
        25.37,
        29.49,
        70.68,
        26.77,
        0.31934783,
        24.18
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.9
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.81
              },
              {
                "metric": "lcb_test_output",
                "score": 21.94
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 59.15
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.73,
          "math_avg": 3.87,
          "code_avg": -1.16,
          "reasoning_avg": -5.53,
          "overall_avg": 0.73,
          "overall_efficiency": 0.030793,
          "general_efficiency": 0.243162,
          "math_efficiency": 0.16408,
          "code_efficiency": -0.049387,
          "reasoning_efficiency": -0.234682,
          "general_task_scores": [
            7.1,
            12.78,
            3.22,
            -0.16
          ],
          "math_task_scores": [
            9.48,
            7.98,
            9.73,
            -6.74,
            -1.11
          ],
          "code_task_scores": [
            -33.54,
            1.42,
            4.66,
            19.77,
            -15.16,
            15.86
          ],
          "reasoning_task_scores": [
            -8.69,
            1.09,
            -9.6,
            -0.07,
            -10.39
          ]
        },
        "vs_instruct": {
          "general_avg": -9.03,
          "math_avg": -10.14,
          "code_avg": -9.75,
          "reasoning_avg": -4.54,
          "overall_avg": -8.37,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.21,
            -24.69,
            -6.48,
            -13.15
          ],
          "math_task_scores": [
            -2.81,
            -15.92,
            -17.87,
            -10.1,
            -4.02
          ],
          "code_task_scores": [
            -20.12,
            -1.69,
            1.79,
            -22.2,
            -19.92,
            3.66
          ],
          "reasoning_task_scores": [
            2.49,
            -1.07,
            -8.59,
            -0.1,
            -15.43
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 57.61,
      "math_avg": 44.62,
      "code_avg": 49.04,
      "reasoning_avg": 28.67,
      "overall_avg": 44.99,
      "overall_efficiency": 0.002698,
      "general_efficiency": 0.006361,
      "math_efficiency": 0.001871,
      "code_efficiency": 0.009538,
      "reasoning_efficiency": -0.006977,
      "general_scores": [
        72.66,
        49.795,
        59.75,
        49.3178571,
        70.17,
        50.0625,
        61.25,
        49.2214286,
        70.97,
        48.6925,
        59.75,
        49.7342857
      ],
      "math_scores": [
        83.4,
        53.64,
        52.8,
        21.66,
        13.33,
        80.82,
        53.3,
        54.4,
        22.06,
        10.0,
        81.5,
        53.48,
        51.2,
        21.0,
        16.67
      ],
      "code_scores": [
        78.66,
        68.48,
        12.19,
        12.53,
        42.31,
        67.68,
        78.05,
        67.7,
        13.26,
        32.15,
        41.86,
        68.9,
        76.83,
        69.26,
        12.19,
        29.23,
        41.86,
        69.51
      ],
      "reasoning_scores": [
        25.08,
        66.36,
        34.85,
        0.39804348,
        20.33,
        21.69,
        65.98,
        35.35,
        0.37967391,
        19.29,
        18.64,
        66.31,
        35.35,
        0.37902174,
        19.73
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.42
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.91
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.64
              },
              {
                "metric": "lcb_test_output",
                "score": 42.01
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.78
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.17,
          "math_avg": 1.82,
          "code_avg": 9.25,
          "reasoning_avg": -6.77,
          "overall_avg": 2.62,
          "overall_efficiency": 0.002698,
          "general_efficiency": 0.006361,
          "math_efficiency": 0.001871,
          "code_efficiency": 0.009538,
          "reasoning_efficiency": -0.006977,
          "general_task_scores": [
            2.96,
            14.03,
            2.51,
            5.18
          ],
          "math_task_scores": [
            1.93,
            2.35,
            2.6,
            -4.47,
            6.66
          ],
          "code_task_scores": [
            0.41,
            -3.12,
            4.31,
            23.6,
            4.91,
            25.41
          ],
          "reasoning_task_scores": [
            -14.8,
            -3.24,
            0.33,
            -0.0,
            -16.13
          ]
        },
        "vs_instruct": {
          "general_avg": -8.59,
          "math_avg": -12.2,
          "code_avg": 0.67,
          "reasoning_avg": -5.78,
          "overall_avg": -6.47,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.07,
            -23.44,
            -7.19,
            -7.81
          ],
          "math_task_scores": [
            -10.36,
            -21.55,
            -25.0,
            -7.83,
            3.75
          ],
          "code_task_scores": [
            13.83,
            -6.23,
            1.44,
            -18.37,
            0.15,
            13.21
          ],
          "reasoning_task_scores": [
            -3.62,
            -5.4,
            1.34,
            -0.03,
            -21.17
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 54.29,
      "math_avg": 37.9,
      "code_avg": 33.46,
      "reasoning_avg": 26.2,
      "overall_avg": 37.96,
      "overall_efficiency": -0.004494,
      "general_efficiency": 0.002905,
      "math_efficiency": -0.004997,
      "code_efficiency": -0.006455,
      "reasoning_efficiency": -0.009428,
      "general_scores": [
        67.37,
        46.36,
        58.91,
        44.1564286,
        68.11,
        46.2175,
        58.86,
        44.3985714,
        68.24,
        44.9275,
        59.13,
        44.8185714
      ],
      "math_scores": [
        81.12,
        42.94,
        45.6,
        17.73,
        0.0,
        81.2,
        43.32,
        45.8,
        16.76,
        3.33,
        81.73,
        43.0,
        45.0,
        17.71,
        3.33
      ],
      "code_scores": [
        16.46,
        66.54,
        9.32,
        37.16,
        35.75,
        33.54,
        17.07,
        67.7,
        11.11,
        36.53,
        34.16,
        38.41,
        15.85,
        66.54,
        10.75,
        36.95,
        37.33,
        31.1
      ],
      "reasoning_scores": [
        23.05,
        60.83,
        33.33,
        0.48076087,
        13.5,
        24.07,
        60.9,
        32.32,
        0.4851087,
        12.76,
        21.69,
        62.62,
        33.33,
        0.48771739,
        13.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.46
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.35
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.88
              },
              {
                "metric": "lcb_test_output",
                "score": 35.75
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 34.35
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.45
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.48
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.85,
          "math_avg": -4.9,
          "code_avg": -6.33,
          "reasoning_avg": -9.24,
          "overall_avg": -4.4,
          "overall_efficiency": -0.004494,
          "general_efficiency": 0.002905,
          "math_efficiency": -0.004997,
          "code_efficiency": -0.006455,
          "reasoning_efficiency": -0.009428,
          "general_task_scores": [
            -0.4,
            10.35,
            1.23,
            0.22
          ],
          "math_task_scores": [
            1.37,
            -8.03,
            -4.73,
            -8.64,
            -4.45
          ],
          "code_task_scores": [
            -60.98,
            -4.67,
            2.15,
            35.84,
            -1.35,
            -8.94
          ],
          "reasoning_task_scores": [
            -13.66,
            -8.01,
            -1.86,
            0.09,
            -22.76
          ]
        },
        "vs_instruct": {
          "general_avg": -11.92,
          "math_avg": -18.91,
          "code_avg": -14.91,
          "reasoning_avg": -8.25,
          "overall_avg": -13.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.71,
            -27.12,
            -8.47,
            -12.77
          ],
          "math_task_scores": [
            -10.92,
            -31.93,
            -32.33,
            -12.0,
            -7.36
          ],
          "code_task_scores": [
            -47.56,
            -7.78,
            -0.72,
            -6.13,
            -6.11,
            -21.14
          ],
          "reasoning_task_scores": [
            -2.48,
            -10.17,
            -0.85,
            0.06,
            -27.8
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 60.23,
      "math_avg": 55.24,
      "code_avg": 41.37,
      "reasoning_avg": 33.92,
      "overall_avg": 47.69,
      "overall_efficiency": 0.106421,
      "general_efficiency": 0.175629,
      "math_efficiency": 0.248853,
      "code_efficiency": 0.031611,
      "reasoning_efficiency": -0.03041,
      "general_scores": [
        66.54,
        51.715,
        67.69,
        53.7535714,
        66.66,
        53.0675,
        67.8,
        53.9378571,
        66.48,
        53.955,
        67.49,
        53.6221429
      ],
      "math_scores": [
        89.84,
        71.96,
        73.4,
        26.74,
        20.0,
        89.99,
        72.22,
        72.6,
        27.01,
        13.33,
        90.75,
        72.12,
        72.2,
        26.51,
        10.0
      ],
      "code_scores": [
        14.63,
        73.15,
        14.34,
        42.38,
        42.99,
        57.32,
        15.24,
        73.54,
        15.05,
        41.75,
        44.12,
        60.37,
        18.9,
        72.76,
        13.26,
        42.8,
        43.44,
        58.54
      ],
      "reasoning_scores": [
        24.07,
        69.49,
        37.88,
        0.42576087,
        38.58,
        21.02,
        69.24,
        38.89,
        0.41304348,
        38.43,
        24.07,
        69.3,
        37.88,
        0.41934783,
        38.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.91
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.26
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 43.52
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.05
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.58
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.78,
          "math_avg": 12.44,
          "code_avg": 1.58,
          "reasoning_avg": -1.52,
          "overall_avg": 5.32,
          "overall_efficiency": 0.106421,
          "general_efficiency": 0.175629,
          "math_efficiency": 0.248853,
          "code_efficiency": 0.031611,
          "reasoning_efficiency": -0.03041,
          "general_task_scores": [
            -1.75,
            17.42,
            9.92,
            9.53
          ],
          "math_task_scores": [
            10.21,
            20.98,
            22.53,
            0.71,
            7.77
          ],
          "code_task_scores": [
            -61.18,
            1.55,
            5.98,
            41.27,
            6.42,
            15.45
          ],
          "reasoning_task_scores": [
            -13.55,
            -0.12,
            3.37,
            0.03,
            2.67
          ]
        },
        "vs_instruct": {
          "general_avg": -5.98,
          "math_avg": -1.57,
          "code_avg": -7.0,
          "reasoning_avg": -0.53,
          "overall_avg": -3.77,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.64,
            -20.05,
            0.22,
            -3.46
          ],
          "math_task_scores": [
            -2.08,
            -2.92,
            -5.07,
            -2.65,
            4.86
          ],
          "code_task_scores": [
            -47.76,
            -1.56,
            3.11,
            -0.7,
            1.66,
            3.25
          ],
          "reasoning_task_scores": [
            -2.37,
            -2.28,
            4.38,
            -0.0,
            -2.37
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 37,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 55.57,
      "math_avg": 41.99,
      "code_avg": 48.51,
      "reasoning_avg": 28.84,
      "overall_avg": 43.73,
      "overall_efficiency": 0.00152,
      "general_efficiency": 0.004614,
      "math_efficiency": -0.000906,
      "code_efficiency": 0.009755,
      "reasoning_efficiency": -0.007384,
      "general_scores": [
        66.39,
        49.585,
        57.63,
        49.16,
        66.78,
        49.68,
        57.88,
        49.4557143,
        65.86,
        49.875,
        55.96,
        48.5714286
      ],
      "math_scores": [
        87.57,
        50.94,
        51.0,
        18.81,
        0.0,
        87.57,
        50.94,
        50.6,
        18.41,
        3.33,
        88.48,
        51.3,
        52.8,
        18.13,
        0.0
      ],
      "code_scores": [
        55.49,
        71.21,
        7.53,
        43.01,
        42.53,
        55.49,
        65.24,
        68.87,
        12.9,
        42.59,
        41.63,
        67.07,
        69.51,
        74.71,
        12.19,
        40.92,
        43.67,
        58.54
      ],
      "reasoning_scores": [
        23.39,
        67.4,
        34.34,
        0.410109,
        18.1,
        21.36,
        65.8,
        37.37,
        0.42076087,
        20.77,
        25.08,
        66.39,
        32.83,
        0.41130435,
        18.55
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.06
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.17
              },
              {
                "metric": "lcb_test_output",
                "score": 42.61
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.14
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.12,
          "math_avg": -0.81,
          "code_avg": 8.72,
          "reasoning_avg": -6.6,
          "overall_avg": 1.36,
          "overall_efficiency": 0.00152,
          "general_efficiency": 0.004614,
          "math_efficiency": -0.000906,
          "code_efficiency": 0.009755,
          "reasoning_efficiency": -0.007384,
          "general_task_scores": [
            -1.97,
            14.22,
            -0.58,
            4.82
          ],
          "math_task_scores": [
            7.89,
            -0.06,
            1.27,
            -7.59,
            -5.56
          ],
          "code_task_scores": [
            -14.03,
            0.0,
            2.63,
            41.13,
            5.51,
            17.08
          ],
          "reasoning_task_scores": [
            -13.32,
            -2.93,
            0.0,
            0.02,
            -16.77
          ]
        },
        "vs_instruct": {
          "general_avg": -10.64,
          "math_avg": -14.82,
          "code_avg": 0.14,
          "reasoning_avg": -5.61,
          "overall_avg": -7.73,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.86,
            -23.25,
            -10.28,
            -8.17
          ],
          "math_task_scores": [
            -4.4,
            -23.96,
            -26.33,
            -10.95,
            -8.47
          ],
          "code_task_scores": [
            -0.61,
            -3.11,
            -0.24,
            -0.84,
            0.75,
            4.88
          ],
          "reasoning_task_scores": [
            -2.14,
            -5.09,
            1.01,
            -0.01,
            -21.81
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 38,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 62.48,
      "math_avg": 56.24,
      "code_avg": 54.51,
      "reasoning_avg": 33.66,
      "overall_avg": 51.72,
      "overall_efficiency": 0.046773,
      "general_efficiency": 0.055176,
      "math_efficiency": 0.067201,
      "code_efficiency": 0.073601,
      "reasoning_efficiency": -0.00889,
      "general_scores": [
        74.58,
        51.7625,
        68.76,
        53.3578571,
        74.64,
        52.635,
        68.91,
        54.1521429,
        75.31,
        52.88,
        68.79,
        54.0014286
      ],
      "math_scores": [
        92.42,
        74.32,
        74.6,
        28.27,
        13.33,
        91.36,
        74.08,
        77.0,
        27.46,
        6.67,
        91.51,
        73.74,
        77.4,
        28.18,
        13.33
      ],
      "code_scores": [
        79.27,
        75.1,
        13.98,
        41.34,
        45.93,
        67.68,
        79.27,
        75.1,
        14.34,
        42.38,
        47.06,
        69.51,
        78.66,
        75.1,
        15.05,
        43.22,
        48.64,
        69.51
      ],
      "reasoning_scores": [
        23.73,
        69.07,
        39.9,
        0.42130435,
        39.17,
        17.63,
        69.06,
        38.89,
        0.4251087,
        38.58,
        20.0,
        68.98,
        40.4,
        0.42630435,
        38.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 75.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.68
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 11.04,
          "math_avg": 13.44,
          "code_avg": 14.72,
          "reasoning_avg": -1.78,
          "overall_avg": 9.36,
          "overall_efficiency": 0.046773,
          "general_efficiency": 0.055176,
          "math_efficiency": 0.067201,
          "code_efficiency": 0.073601,
          "reasoning_efficiency": -0.00889,
          "general_task_scores": [
            6.53,
            16.94,
            11.08,
            9.6
          ],
          "math_task_scores": [
            11.78,
            22.93,
            26.13,
            1.93,
            4.44
          ],
          "code_task_scores": [
            1.63,
            3.5,
            6.22,
            41.27,
            10.11,
            25.61
          ],
          "reasoning_task_scores": [
            -16.15,
            -0.42,
            4.88,
            0.03,
            2.77
          ]
        },
        "vs_instruct": {
          "general_avg": -3.73,
          "math_avg": -0.57,
          "code_avg": 6.14,
          "reasoning_avg": -0.79,
          "overall_avg": 0.26,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            7.64,
            -20.53,
            1.38,
            -3.39
          ],
          "math_task_scores": [
            -0.51,
            -0.97,
            -1.47,
            -1.43,
            1.53
          ],
          "code_task_scores": [
            15.05,
            0.39,
            3.35,
            -0.7,
            5.35,
            13.41
          ],
          "reasoning_task_scores": [
            -4.97,
            -2.58,
            5.89,
            -0.0,
            -2.27
          ]
        }
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 56.41,
      "math_avg": 51.0,
      "code_avg": 42.93,
      "reasoning_avg": 31.05,
      "overall_avg": 45.35,
      "overall_efficiency": 0.003322,
      "general_efficiency": 0.00554,
      "math_efficiency": 0.009144,
      "code_efficiency": 0.003506,
      "reasoning_efficiency": -0.004901,
      "general_scores": [
        64.79,
        48.38,
        61.54,
        52.225,
        65.65,
        46.18,
        61.32,
        52.2571429,
        63.72,
        47.225,
        61.18,
        52.4428571
      ],
      "math_scores": [
        89.46,
        67.74,
        69.4,
        24.16,
        10.0,
        88.25,
        68.48,
        69.2,
        23.26,
        6.67,
        89.23,
        67.08,
        66.8,
        25.23,
        0.0
      ],
      "code_scores": [
        56.1,
        66.54,
        7.89,
        40.92,
        36.65,
        50.0,
        53.66,
        67.32,
        11.83,
        34.86,
        23.98,
        52.44,
        58.54,
        69.26,
        9.32,
        38.0,
        34.39,
        60.98
      ],
      "reasoning_scores": [
        18.98,
        61.3,
        35.35,
        0.38054348,
        24.63,
        32.2,
        62.08,
        39.9,
        0.37902174,
        27.15,
        42.71,
        59.93,
        34.85,
        0.38913043,
        25.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.72
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.98
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 56.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.93
              },
              {
                "metric": "lcb_test_output",
                "score": 31.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 54.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.96,
          "math_avg": 8.2,
          "code_avg": 3.14,
          "reasoning_avg": -4.39,
          "overall_avg": 2.98,
          "overall_efficiency": 0.003322,
          "general_efficiency": 0.00554,
          "math_efficiency": 0.009144,
          "code_efficiency": 0.003506,
          "reasoning_efficiency": -0.004901,
          "general_task_scores": [
            -3.59,
            11.77,
            3.61,
            8.07
          ],
          "math_task_scores": [
            9.0,
            16.65,
            18.27,
            -1.82,
            -1.11
          ],
          "code_task_scores": [
            -21.34,
            -3.89,
            1.44,
            36.89,
            -5.43,
            11.18
          ],
          "reasoning_task_scores": [
            -5.3,
            -8.36,
            1.85,
            -0.01,
            -10.14
          ]
        },
        "vs_instruct": {
          "general_avg": -9.8,
          "math_avg": -5.82,
          "code_avg": -5.44,
          "reasoning_avg": -3.4,
          "overall_avg": -6.11,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.48,
            -25.7,
            -6.09,
            -4.92
          ],
          "math_task_scores": [
            -3.29,
            -7.25,
            -9.33,
            -5.18,
            -4.02
          ],
          "code_task_scores": [
            -7.92,
            -7.0,
            -1.43,
            -5.08,
            -10.19,
            -1.02
          ],
          "reasoning_task_scores": [
            5.88,
            -10.52,
            2.86,
            -0.04,
            -15.18
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 60.62,
      "math_avg": 51.43,
      "code_avg": 52.81,
      "reasoning_avg": 31.93,
      "overall_avg": 49.2,
      "overall_efficiency": 0.114061,
      "general_efficiency": 0.153262,
      "math_efficiency": 0.144115,
      "code_efficiency": 0.217512,
      "reasoning_efficiency": -0.058648,
      "general_scores": [
        70.54,
        52.4775,
        67.57,
        52.005,
        70.99,
        52.055,
        67.62,
        51.8535714,
        69.88,
        52.395,
        67.7,
        52.3978571
      ],
      "math_scores": [
        89.99,
        65.58,
        64.4,
        24.12,
        13.33,
        89.76,
        66.48,
        65.8,
        24.46,
        10.0,
        89.99,
        66.3,
        67.4,
        23.89,
        10.0
      ],
      "code_scores": [
        79.27,
        72.37,
        14.7,
        43.01,
        48.87,
        59.15,
        78.66,
        71.98,
        12.19,
        44.89,
        47.51,
        60.37,
        79.88,
        73.93,
        13.98,
        44.05,
        49.1,
        56.71
      ],
      "reasoning_scores": [
        14.92,
        70.05,
        31.82,
        0.4301087,
        39.17,
        18.64,
        69.93,
        30.81,
        0.42826087,
        39.32,
        20.68,
        69.87,
        32.83,
        0.43923913,
        39.61
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.91
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.98
              },
              {
                "metric": "lcb_test_output",
                "score": 48.49
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 58.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.95
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.18,
          "math_avg": 8.63,
          "code_avg": 13.03,
          "reasoning_avg": -3.51,
          "overall_avg": 6.83,
          "overall_efficiency": 0.114061,
          "general_efficiency": 0.153262,
          "math_efficiency": 0.144115,
          "code_efficiency": 0.217512,
          "reasoning_efficiency": -0.058648,
          "general_task_scores": [
            2.16,
            16.82,
            9.89,
            7.85
          ],
          "math_task_scores": [
            9.93,
            15.0,
            15.67,
            -1.88,
            4.44
          ],
          "code_task_scores": [
            1.83,
            1.16,
            5.38,
            42.94,
            11.39,
            15.45
          ],
          "reasoning_task_scores": [
            -18.52,
            0.49,
            -3.03,
            0.04,
            3.46
          ]
        },
        "vs_instruct": {
          "general_avg": -5.58,
          "math_avg": -5.38,
          "code_avg": 4.45,
          "reasoning_avg": -2.52,
          "overall_avg": -2.26,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.27,
            -20.65,
            0.19,
            -5.14
          ],
          "math_task_scores": [
            -2.36,
            -8.9,
            -11.93,
            -5.24,
            1.53
          ],
          "code_task_scores": [
            15.25,
            -1.95,
            2.51,
            0.97,
            6.63,
            3.25
          ],
          "reasoning_task_scores": [
            -7.34,
            -1.67,
            -2.02,
            0.01,
            -1.58
          ]
        }
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 57.27,
      "math_avg": 42.05,
      "code_avg": 52.11,
      "reasoning_avg": 28.6,
      "overall_avg": 45.01,
      "overall_efficiency": 0.005698,
      "general_efficiency": 0.012588,
      "math_efficiency": -0.001621,
      "code_efficiency": 0.026609,
      "reasoning_efficiency": -0.014787,
      "general_scores": [
        66.11,
        53.1125,
        61.95,
        46.8838462,
        66.31,
        52.2825,
        61.93,
        48.51286,
        65.68,
        53.95,
        61.78,
        48.7771429
      ],
      "math_scores": [
        86.35,
        50.62,
        50.6,
        18.88,
        3.33,
        87.26,
        49.9,
        51.8,
        20.1,
        1.665,
        86.35,
        49.18,
        48.4,
        19.67,
        6.67
      ],
      "code_scores": [
        80.49,
        72.76,
        11.83,
        38.83,
        42.08,
        67.68,
        78.05,
        73.54,
        10.75,
        39.67,
        38.91,
        67.07,
        80.49,
        73.54,
        10.75,
        39.87,
        42.08,
        69.51
      ],
      "reasoning_scores": [
        21.69,
        69.31,
        33.33,
        0.43934783,
        17.21,
        22.03,
        68.91,
        34.34,
        0.440435,
        16.02,
        24.41,
        68.6,
        34.85,
        0.44413043,
        16.91
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.89
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.46
              },
              {
                "metric": "lcb_test_output",
                "score": 41.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.09
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.94
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.83,
          "math_avg": -0.75,
          "code_avg": 12.32,
          "reasoning_avg": -6.85,
          "overall_avg": 2.64,
          "overall_efficiency": 0.005698,
          "general_efficiency": 0.012588,
          "math_efficiency": -0.001621,
          "code_efficiency": 0.026609,
          "reasoning_efficiency": -0.014787,
          "general_task_scores": [
            -2.28,
            17.63,
            4.15,
            3.82
          ],
          "math_task_scores": [
            6.67,
            -1.22,
            0.07,
            -6.49,
            -2.78
          ],
          "code_task_scores": [
            2.24,
            1.68,
            2.87,
            38.42,
            3.92,
            24.8
          ],
          "reasoning_task_scores": [
            -13.89,
            -0.52,
            -0.68,
            0.05,
            -19.2
          ]
        },
        "vs_instruct": {
          "general_avg": -8.94,
          "math_avg": -14.76,
          "code_avg": 3.74,
          "reasoning_avg": -5.86,
          "overall_avg": -6.45,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.17,
            -19.84,
            -5.55,
            -9.17
          ],
          "math_task_scores": [
            -5.62,
            -25.12,
            -27.53,
            -9.85,
            -5.69
          ],
          "code_task_scores": [
            15.66,
            -1.43,
            0.0,
            -3.55,
            -0.84,
            12.6
          ],
          "reasoning_task_scores": [
            -2.71,
            -2.68,
            0.33,
            0.02,
            -24.24
          ]
        }
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 42,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 55.26,
      "math_avg": 43.83,
      "code_avg": 51.41,
      "reasoning_avg": 27.53,
      "overall_avg": 44.51,
      "overall_efficiency": 0.10674,
      "general_efficiency": 0.190409,
      "math_efficiency": 0.051211,
      "code_efficiency": 0.580417,
      "reasoning_efficiency": -0.395074,
      "general_scores": [
        58.5,
        52.305,
        59.65,
        49.2307143,
        57.64,
        53.4925,
        59.79,
        50.0714286,
        59.56,
        53.3425,
        59.94,
        49.56
      ],
      "math_scores": [
        85.82,
        52.64,
        54.6,
        20.48,
        6.67,
        84.61,
        51.38,
        52.4,
        18.61,
        13.33,
        85.37,
        51.28,
        52.6,
        17.62,
        10.0
      ],
      "code_scores": [
        74.39,
        71.98,
        8.96,
        44.05,
        44.8,
        68.29,
        73.78,
        70.82,
        6.45,
        42.59,
        43.67,
        68.29,
        73.78,
        71.6,
        3.94,
        44.68,
        44.34,
        68.9
      ],
      "reasoning_scores": [
        17.29,
        67.9,
        34.34,
        0.38032609,
        18.55,
        15.59,
        68.84,
        32.83,
        0.38195652,
        18.1,
        17.97,
        68.41,
        33.33,
        0.38130435,
        18.69
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.95
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.81,
          "math_avg": 1.03,
          "code_avg": 11.62,
          "reasoning_avg": -7.91,
          "overall_avg": 2.14,
          "overall_efficiency": 0.10674,
          "general_efficiency": 0.190409,
          "math_efficiency": 0.051211,
          "code_efficiency": 0.580417,
          "reasoning_efficiency": -0.395074,
          "general_task_scores": [
            -9.74,
            17.56,
            2.05,
            5.38
          ],
          "math_task_scores": [
            5.29,
            0.65,
            3.0,
            -7.14,
            3.33
          ],
          "code_task_scores": [
            -3.46,
            -0.13,
            -1.79,
            42.73,
            7.17,
            25.2
          ],
          "reasoning_task_scores": [
            -19.65,
            -1.08,
            -1.35,
            -0.01,
            -17.46
          ]
        },
        "vs_instruct": {
          "general_avg": -10.95,
          "math_avg": -12.99,
          "code_avg": 3.04,
          "reasoning_avg": -6.92,
          "overall_avg": -6.95,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.63,
            -19.91,
            -7.65,
            -7.61
          ],
          "math_task_scores": [
            -7.0,
            -23.25,
            -24.6,
            -10.5,
            0.42
          ],
          "code_task_scores": [
            9.96,
            -3.24,
            -4.66,
            0.76,
            2.41,
            13.0
          ],
          "reasoning_task_scores": [
            -8.47,
            -3.24,
            -0.34,
            -0.04,
            -22.5
          ]
        }
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 43,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 59.17,
      "math_avg": 48.09,
      "code_avg": 48.92,
      "reasoning_avg": 30.53,
      "overall_avg": 46.68,
      "overall_efficiency": 0.009875,
      "general_efficiency": 0.017703,
      "math_efficiency": 0.012125,
      "code_efficiency": 0.020931,
      "reasoning_efficiency": -0.011259,
      "general_scores": [
        69.46,
        54.0475,
        61.73,
        49.3685714,
        71.3,
        55.2325,
        62.19,
        49.23,
        70.06,
        55.42,
        62.35,
        49.6428571
      ],
      "math_scores": [
        87.64,
        54.34,
        54.4,
        21.12,
        26.67,
        87.11,
        54.92,
        54.4,
        21.43,
        20.0,
        85.44,
        53.96,
        55.2,
        21.43,
        23.33
      ],
      "code_scores": [
        79.27,
        75.88,
        13.26,
        40.92,
        16.52,
        65.24,
        78.05,
        72.76,
        13.98,
        41.13,
        17.65,
        65.24,
        78.66,
        75.49,
        14.34,
        38.0,
        28.28,
        65.85
      ],
      "reasoning_scores": [
        24.41,
        67.17,
        36.36,
        0.44717391,
        23.29,
        25.08,
        67.49,
        38.89,
        0.43336957,
        23.44,
        24.07,
        68.17,
        35.86,
        0.43282609,
        22.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.73
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.02
              },
              {
                "metric": "lcb_test_output",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.72,
          "math_avg": 5.29,
          "code_avg": 9.13,
          "reasoning_avg": -4.91,
          "overall_avg": 4.31,
          "overall_efficiency": 0.009875,
          "general_efficiency": 0.017703,
          "math_efficiency": 0.012125,
          "code_efficiency": 0.020931,
          "reasoning_efficiency": -0.011259,
          "general_task_scores": [
            1.96,
            19.41,
            4.35,
            5.17
          ],
          "math_task_scores": [
            6.75,
            3.29,
            4.47,
            -4.71,
            16.66
          ],
          "code_task_scores": [
            1.22,
            3.11,
            5.62,
            38.98,
            -16.28,
            22.15
          ],
          "reasoning_task_scores": [
            -12.08,
            -1.85,
            2.19,
            0.05,
            -12.87
          ]
        },
        "vs_instruct": {
          "general_avg": -7.04,
          "math_avg": -8.72,
          "code_avg": 0.55,
          "reasoning_avg": -3.92,
          "overall_avg": -4.78,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.07,
            -18.06,
            -5.35,
            -7.82
          ],
          "math_task_scores": [
            -5.54,
            -20.61,
            -23.13,
            -8.07,
            13.75
          ],
          "code_task_scores": [
            14.64,
            0.0,
            2.75,
            -2.99,
            -21.04,
            9.95
          ],
          "reasoning_task_scores": [
            -0.9,
            -4.01,
            3.2,
            0.02,
            -17.91
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 57.82,
      "math_avg": 43.56,
      "code_avg": 52.67,
      "reasoning_avg": 29.4,
      "overall_avg": 45.87,
      "overall_efficiency": 0.022342,
      "general_efficiency": 0.04075,
      "math_efficiency": 0.004864,
      "code_efficiency": 0.08234,
      "reasoning_efficiency": -0.038584,
      "general_scores": [
        70.6,
        51.1425,
        62.08,
        49.5023077,
        69.12,
        50.4425,
        62.41,
        48.7285714,
        68.23,
        50.2575,
        62.22,
        49.1421429
      ],
      "math_scores": [
        83.55,
        52.6,
        53.8,
        20.57,
        10.0,
        82.79,
        53.0,
        52.4,
        20.37,
        6.67,
        82.87,
        51.8,
        52.8,
        20.23,
        10.0
      ],
      "code_scores": [
        81.71,
        74.32,
        13.62,
        39.04,
        43.67,
        70.73,
        78.66,
        70.04,
        14.7,
        38.41,
        44.34,
        68.9,
        79.88,
        70.82,
        13.98,
        38.2,
        41.86,
        65.24
      ],
      "reasoning_scores": [
        22.71,
        67.62,
        36.87,
        0.41380435,
        20.62,
        21.02,
        67.7,
        36.87,
        0.41195652,
        20.47,
        20.68,
        68.4,
        36.36,
        0.42804348,
        20.47
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.24
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.47
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.38,
          "math_avg": 0.76,
          "code_avg": 12.89,
          "reasoning_avg": -6.04,
          "overall_avg": 3.5,
          "overall_efficiency": 0.022342,
          "general_efficiency": 0.04075,
          "math_efficiency": 0.004864,
          "code_efficiency": 0.08234,
          "reasoning_efficiency": -0.038584,
          "general_task_scores": [
            1.01,
            15.12,
            4.5,
            4.88
          ],
          "math_task_scores": [
            3.09,
            1.35,
            2.8,
            -5.65,
            2.22
          ],
          "code_task_scores": [
            2.64,
            0.13,
            5.86,
            37.51,
            6.19,
            25.0
          ],
          "reasoning_task_scores": [
            -15.13,
            -1.55,
            1.85,
            0.03,
            -15.39
          ]
        },
        "vs_instruct": {
          "general_avg": -8.39,
          "math_avg": -13.25,
          "code_avg": 4.31,
          "reasoning_avg": -5.05,
          "overall_avg": -5.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.12,
            -22.35,
            -5.2,
            -8.11
          ],
          "math_task_scores": [
            -9.2,
            -22.55,
            -24.8,
            -9.01,
            -0.69
          ],
          "code_task_scores": [
            16.06,
            -2.98,
            2.99,
            -4.46,
            1.43,
            12.8
          ],
          "reasoning_task_scores": [
            -3.95,
            -3.71,
            2.86,
            -0.0,
            -20.43
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 58.18,
      "math_avg": 44.33,
      "code_avg": 52.02,
      "reasoning_avg": 29.93,
      "overall_avg": 46.12,
      "overall_efficiency": 0.033685,
      "general_efficiency": 0.060571,
      "math_efficiency": 0.01372,
      "code_efficiency": 0.109986,
      "reasoning_efficiency": -0.049535,
      "general_scores": [
        68.26,
        50.4625,
        62.86,
        48.8964286,
        69.85,
        51.2425,
        63.57,
        49.0857143,
        69.48,
        52.2175,
        63.09,
        49.1971429
      ],
      "math_scores": [
        86.35,
        53.96,
        55.0,
        21.03,
        6.67,
        86.2,
        54.8,
        54.4,
        20.05,
        3.33,
        85.6,
        54.74,
        55.6,
        20.53,
        6.67
      ],
      "code_scores": [
        78.66,
        72.37,
        11.83,
        37.58,
        42.76,
        67.07,
        77.44,
        71.6,
        12.19,
        36.95,
        43.21,
        69.51,
        79.27,
        73.15,
        11.47,
        38.41,
        43.44,
        69.51
      ],
      "reasoning_scores": [
        23.39,
        67.97,
        37.88,
        0.43880435,
        20.18,
        23.05,
        68.39,
        35.35,
        0.43891304,
        20.33,
        23.73,
        68.34,
        37.37,
        0.4401087,
        21.66
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.74,
          "math_avg": 1.53,
          "code_avg": 12.24,
          "reasoning_avg": -5.51,
          "overall_avg": 3.75,
          "overall_efficiency": 0.033685,
          "general_efficiency": 0.060571,
          "math_efficiency": 0.01372,
          "code_efficiency": 0.109986,
          "reasoning_efficiency": -0.049535,
          "general_task_scores": [
            0.89,
            15.82,
            5.43,
            4.82
          ],
          "math_task_scores": [
            6.07,
            3.38,
            4.8,
            -5.5,
            -1.11
          ],
          "code_task_scores": [
            1.02,
            0.77,
            3.59,
            36.61,
            6.04,
            25.41
          ],
          "reasoning_task_scores": [
            -13.21,
            -1.23,
            2.02,
            0.05,
            -15.19
          ]
        },
        "vs_instruct": {
          "general_avg": -8.02,
          "math_avg": -12.49,
          "code_avg": 3.66,
          "reasoning_avg": -4.52,
          "overall_avg": -5.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.0,
            -21.65,
            -4.27,
            -8.17
          ],
          "math_task_scores": [
            -6.22,
            -20.52,
            -22.8,
            -8.86,
            -4.02
          ],
          "code_task_scores": [
            14.44,
            -2.34,
            0.72,
            -5.36,
            1.28,
            13.21
          ],
          "reasoning_task_scores": [
            -2.03,
            -3.39,
            3.03,
            0.02,
            -20.23
          ]
        }
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 56.86,
      "math_avg": 45.97,
      "code_avg": 48.49,
      "reasoning_avg": 29.89,
      "overall_avg": 45.3,
      "overall_efficiency": 0.039023,
      "general_efficiency": 0.07208,
      "math_efficiency": 0.042162,
      "code_efficiency": 0.11574,
      "reasoning_efficiency": -0.073891,
      "general_scores": [
        65.37,
        50.5175,
        64.19,
        49.625,
        70.6,
        45.6125,
        60.99,
        48.52429,
        71.56,
        46.1625,
        60.64,
        48.58429
      ],
      "math_scores": [
        87.19,
        64.52,
        63.6,
        26.31,
        16.67,
        84.69,
        51.82,
        52.0,
        23.4,
        0.0,
        85.22,
        51.8,
        49.8,
        24.23,
        8.33625
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        42.8,
        35.29,
        69.51,
        71.95,
        70.43,
        14.7,
        28.81,
        27.6,
        66.46,
        72.56,
        71.98,
        15.05,
        28.39,
        24.89,
        67.68
      ],
      "reasoning_scores": [
        23.39,
        69.52,
        36.36,
        0.41336957,
        28.49,
        22.03,
        68.46,
        36.87,
        0.395217,
        16.62,
        25.42,
        68.76,
        34.85,
        0.391304,
        16.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.59
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 33.33
              },
              {
                "metric": "lcb_test_output",
                "score": 29.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.88
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.61
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": 3.17,
          "code_avg": 8.7,
          "reasoning_avg": -5.56,
          "overall_avg": 2.93,
          "overall_efficiency": 0.039023,
          "general_efficiency": 0.07208,
          "math_efficiency": 0.042162,
          "code_efficiency": 0.11574,
          "reasoning_efficiency": -0.073891,
          "general_task_scores": [
            0.87,
            11.94,
            4.2,
            4.67
          ],
          "math_task_scores": [
            5.72,
            4.93,
            4.93,
            -1.39,
            1.67
          ],
          "code_task_scores": [
            -2.44,
            -0.01,
            5.62,
            32.29,
            -7.84,
            24.59
          ],
          "reasoning_task_scores": [
            -12.99,
            -0.55,
            1.18,
            0.01,
            -15.43
          ]
        },
        "vs_instruct": {
          "general_avg": -9.34,
          "math_avg": -10.84,
          "code_avg": 0.12,
          "reasoning_avg": -4.56,
          "overall_avg": -6.16,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.98,
            -25.53,
            -5.5,
            -8.32
          ],
          "math_task_scores": [
            -6.57,
            -18.97,
            -22.67,
            -4.75,
            -1.24
          ],
          "code_task_scores": [
            10.98,
            -3.12,
            2.75,
            -9.68,
            -12.6,
            12.39
          ],
          "reasoning_task_scores": [
            -1.81,
            -2.71,
            2.19,
            -0.02,
            -20.47
          ]
        }
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 55.83,
      "math_avg": 40.52,
      "code_avg": 50.46,
      "reasoning_avg": 28.08,
      "overall_avg": 43.72,
      "overall_efficiency": 0.020392,
      "general_efficiency": 0.066052,
      "math_efficiency": -0.034417,
      "code_efficiency": 0.160868,
      "reasoning_efficiency": -0.110936,
      "general_scores": [
        70.25,
        48.3275,
        58.8,
        47.0585714,
        70.16,
        48.3925,
        58.86,
        47.2607143,
        69.17,
        45.895,
        58.28,
        47.4957143
      ],
      "math_scores": [
        85.97,
        41.6,
        41.4,
        22.34,
        10.0,
        85.44,
        41.24,
        42.0,
        23.06,
        13.33,
        84.38,
        41.08,
        41.0,
        21.59,
        13.33
      ],
      "code_scores": [
        78.05,
        73.15,
        12.19,
        24.01,
        44.34,
        73.17,
        75.61,
        73.54,
        12.9,
        26.1,
        41.4,
        75.0,
        75.61,
        71.98,
        13.26,
        27.35,
        41.18,
        69.51
      ],
      "reasoning_scores": [
        16.61,
        68.02,
        37.37,
        0.32086957,
        16.77,
        18.64,
        67.36,
        36.36,
        0.32782609,
        17.95,
        20.34,
        67.51,
        36.36,
        0.32369565,
        16.91
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.31
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.42
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.82
              },
              {
                "metric": "lcb_test_output",
                "score": 42.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 72.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.38,
          "math_avg": -2.28,
          "code_avg": 10.68,
          "reasoning_avg": -7.36,
          "overall_avg": 1.35,
          "overall_efficiency": 0.020392,
          "general_efficiency": 0.066052,
          "math_efficiency": -0.034417,
          "code_efficiency": 0.160868,
          "reasoning_efficiency": -0.110936,
          "general_task_scores": [
            1.55,
            12.05,
            0.91,
            3.03
          ],
          "math_task_scores": [
            5.28,
            -9.81,
            -8.73,
            -3.71,
            5.55
          ],
          "code_task_scores": [
            -1.02,
            1.29,
            4.54,
            24.78,
            5.21,
            29.27
          ],
          "reasoning_task_scores": [
            -18.07,
            -1.83,
            1.85,
            -0.07,
            -18.7
          ]
        },
        "vs_instruct": {
          "general_avg": -10.38,
          "math_avg": -16.3,
          "code_avg": 2.1,
          "reasoning_avg": -6.37,
          "overall_avg": -7.74,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.66,
            -25.42,
            -8.79,
            -9.96
          ],
          "math_task_scores": [
            -7.01,
            -33.71,
            -36.33,
            -7.07,
            2.64
          ],
          "code_task_scores": [
            12.4,
            -1.82,
            1.67,
            -17.19,
            0.45,
            17.07
          ],
          "reasoning_task_scores": [
            -6.89,
            -3.99,
            2.86,
            -0.1,
            -23.74
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 44.61,
      "code_avg": 37.73,
      "reasoning_avg": 28.59,
      "overall_avg": 40.6,
      "overall_efficiency": -0.032066,
      "general_efficiency": 0.000437,
      "math_efficiency": 0.032876,
      "code_efficiency": -0.037214,
      "reasoning_efficiency": -0.124358,
      "general_scores": [
        70.49,
        39.445,
        47.31,
        48.3821429,
        71.05,
        39.645,
        46.62,
        48.6735714,
        71.48,
        39.16,
        46.71,
        48.6564286
      ],
      "math_scores": [
        87.64,
        50.16,
        49.2,
        25.09,
        6.67,
        87.04,
        51.24,
        50.4,
        25.43,
        10.0,
        86.2,
        50.02,
        48.2,
        25.25,
        16.67
      ],
      "code_scores": [
        51.83,
        68.87,
        9.32,
        40.71,
        0.45,
        54.47,
        46.34,
        66.93,
        11.83,
        46.76,
        3.17,
        53.66,
        43.9,
        66.54,
        8.24,
        48.43,
        8.37,
        49.39
      ],
      "reasoning_scores": [
        24.75,
        68.44,
        31.31,
        0.38967391,
        18.55,
        24.75,
        68.19,
        33.33,
        0.38804348,
        15.58,
        23.05,
        67.94,
        34.85,
        0.39413043,
        16.91
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.96
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 45.3
              },
              {
                "metric": "lcb_test_output",
                "score": 4.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 52.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.01
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.02,
          "math_avg": 1.81,
          "code_avg": -2.05,
          "reasoning_avg": -6.85,
          "overall_avg": -1.77,
          "overall_efficiency": -0.032066,
          "general_efficiency": 0.000437,
          "math_efficiency": 0.032876,
          "code_efficiency": -0.037214,
          "reasoning_efficiency": -0.124358,
          "general_task_scores": [
            2.7,
            3.93,
            -10.86,
            4.33
          ],
          "math_task_scores": [
            6.98,
            -0.65,
            -0.93,
            -0.78,
            4.44
          ],
          "code_task_scores": [
            -30.08,
            -4.15,
            1.56,
            44.26,
            -33.1,
            9.22
          ],
          "reasoning_task_scores": [
            -12.42,
            -1.27,
            -1.69,
            -0.0,
            -18.9
          ]
        },
        "vs_instruct": {
          "general_avg": -14.74,
          "math_avg": -12.2,
          "code_avg": -10.63,
          "reasoning_avg": -5.86,
          "overall_avg": -10.86,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.81,
            -33.54,
            -20.56,
            -8.66
          ],
          "math_task_scores": [
            -5.31,
            -24.55,
            -28.53,
            -4.14,
            1.53
          ],
          "code_task_scores": [
            -16.66,
            -7.26,
            -1.31,
            2.29,
            -37.86,
            -2.98
          ],
          "reasoning_task_scores": [
            -1.24,
            -3.43,
            -0.68,
            -0.03,
            -23.94
          ]
        }
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 56.9,
      "math_avg": 43.88,
      "code_avg": 46.77,
      "reasoning_avg": 28.47,
      "overall_avg": 44.01,
      "overall_efficiency": 0.032305,
      "general_efficiency": 0.107613,
      "math_efficiency": 0.021318,
      "code_efficiency": 0.137866,
      "reasoning_efficiency": -0.137579,
      "general_scores": [
        72.33,
        45.205,
        61.36,
        48.8171429,
        72.0,
        46.14,
        60.68,
        49.1314286,
        72.58,
        45.295,
        60.26,
        48.9564286
      ],
      "math_scores": [
        84.53,
        52.14,
        50.6,
        23.28,
        6.67,
        85.52,
        51.8,
        51.8,
        23.01,
        6.67,
        85.29,
        51.6,
        51.4,
        23.92,
        10.0
      ],
      "code_scores": [
        72.56,
        73.54,
        13.98,
        24.43,
        28.73,
        70.73,
        71.95,
        71.21,
        13.98,
        25.47,
        26.47,
        67.07,
        74.39,
        71.21,
        14.34,
        27.14,
        29.41,
        65.24
      ],
      "reasoning_scores": [
        21.02,
        68.64,
        33.84,
        0.38173913,
        16.77,
        21.69,
        67.61,
        35.35,
        0.38108696,
        17.95,
        24.75,
        68.38,
        33.33,
        0.37456522,
        16.62
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.11
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.97
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.68
              },
              {
                "metric": "lcb_test_output",
                "score": 28.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.49
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.21
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.45,
          "math_avg": 1.08,
          "code_avg": 6.98,
          "reasoning_avg": -6.97,
          "overall_avg": 1.64,
          "overall_efficiency": 0.032305,
          "general_efficiency": 0.107613,
          "math_efficiency": 0.021318,
          "code_efficiency": 0.137866,
          "reasoning_efficiency": -0.137579,
          "general_task_scores": [
            3.99,
            10.06,
            3.03,
            4.73
          ],
          "math_task_scores": [
            5.13,
            0.73,
            1.07,
            -2.64,
            1.11
          ],
          "code_task_scores": [
            -4.47,
            0.39,
            5.86,
            24.64,
            -8.9,
            24.39
          ],
          "reasoning_task_scores": [
            -14.11,
            -1.25,
            -0.68,
            -0.01,
            -18.8
          ]
        },
        "vs_instruct": {
          "general_avg": -9.31,
          "math_avg": -12.93,
          "code_avg": -1.6,
          "reasoning_avg": -5.98,
          "overall_avg": -7.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            5.1,
            -27.41,
            -6.67,
            -8.26
          ],
          "math_task_scores": [
            -7.16,
            -23.17,
            -26.53,
            -6.0,
            -1.8
          ],
          "code_task_scores": [
            8.95,
            -2.72,
            2.99,
            -17.33,
            -13.66,
            12.19
          ],
          "reasoning_task_scores": [
            -2.93,
            -3.41,
            0.33,
            -0.04,
            -23.84
          ]
        }
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.6,
      "math_avg": 43.26,
      "code_avg": 49.82,
      "reasoning_avg": 27.84,
      "overall_avg": 44.38,
      "overall_efficiency": 0.001959,
      "general_efficiency": 0.005017,
      "math_efficiency": 0.000448,
      "code_efficiency": 0.009767,
      "reasoning_efficiency": -0.007397,
      "general_scores": [
        62.51,
        51.775,
        63.3,
        48.4585714,
        64.79,
        50.195,
        62.82,
        48.9107143,
        64.46,
        51.02,
        62.4,
        48.5342857
      ],
      "math_scores": [
        86.43,
        53.08,
        51.6,
        20.53,
        3.33,
        84.76,
        53.74,
        53.8,
        21.5,
        3.33,
        86.35,
        53.54,
        52.0,
        21.61,
        3.33
      ],
      "code_scores": [
        78.66,
        71.6,
        13.26,
        37.16,
        43.21,
        65.24,
        79.88,
        71.6,
        12.19,
        27.97,
        42.76,
        65.85,
        78.66,
        69.26,
        11.47,
        18.58,
        44.12,
        65.24
      ],
      "reasoning_scores": [
        18.31,
        65.39,
        36.36,
        0.43847826,
        20.03,
        22.71,
        65.57,
        32.32,
        0.43271739,
        16.62,
        20.0,
        66.51,
        33.84,
        0.4475,
        18.69
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.85
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.9
              },
              {
                "metric": "lcb_test_output",
                "score": 43.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.15,
          "math_avg": 0.46,
          "code_avg": 10.03,
          "reasoning_avg": -7.6,
          "overall_avg": 2.01,
          "overall_efficiency": 0.001959,
          "general_efficiency": 0.005017,
          "math_efficiency": 0.000448,
          "code_efficiency": 0.009767,
          "reasoning_efficiency": -0.007397,
          "general_task_scores": [
            -4.39,
            15.51,
            5.1,
            4.39
          ],
          "math_task_scores": [
            5.87,
            2.33,
            2.27,
            -4.83,
            -3.34
          ],
          "code_task_scores": [
            1.63,
            -0.78,
            4.07,
            26.86,
            6.26,
            22.15
          ],
          "reasoning_task_scores": [
            -16.26,
            -3.64,
            -0.68,
            0.05,
            -17.46
          ]
        },
        "vs_instruct": {
          "general_avg": -9.61,
          "math_avg": -13.55,
          "code_avg": 1.45,
          "reasoning_avg": -6.61,
          "overall_avg": -7.08,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.28,
            -21.96,
            -4.6,
            -8.6
          ],
          "math_task_scores": [
            -6.42,
            -21.57,
            -25.33,
            -8.19,
            -6.25
          ],
          "code_task_scores": [
            15.05,
            -3.89,
            1.2,
            -15.11,
            1.5,
            9.95
          ],
          "reasoning_task_scores": [
            -5.08,
            -5.8,
            0.33,
            0.02,
            -22.5
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 57.3,
      "math_avg": 53.99,
      "code_avg": 45.67,
      "reasoning_avg": 32.2,
      "overall_avg": 47.29,
      "overall_efficiency": 0.140618,
      "general_efficiency": 0.167394,
      "math_efficiency": 0.319628,
      "code_efficiency": 0.168211,
      "reasoning_efficiency": -0.092763,
      "general_scores": [
        62.16,
        51.76,
        65.13,
        50.87,
        61.34,
        51.2125,
        64.68,
        50.6557143,
        61.64,
        51.0075,
        65.07,
        52.1115385
      ],
      "math_scores": [
        90.3,
        69.24,
        68.0,
        27.48,
        13.33,
        89.31,
        70.02,
        72.0,
        27.08,
        13.33,
        89.46,
        70.24,
        69.2,
        27.51,
        13.33
      ],
      "code_scores": [
        78.05,
        73.93,
        10.04,
        43.01,
        0.0,
        73.78,
        76.22,
        73.54,
        11.47,
        40.92,
        0.0,
        68.9,
        78.66,
        73.54,
        9.68,
        38.41,
        0.0,
        71.95
      ],
      "reasoning_scores": [
        21.36,
        69.63,
        34.85,
        0.42565217,
        32.79,
        22.37,
        69.84,
        37.37,
        0.41673913,
        33.83,
        18.31,
        69.71,
        38.38,
        0.42467391,
        33.23
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.71
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.69
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.64
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.78
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 71.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.68
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.86,
          "math_avg": 11.19,
          "code_avg": 5.89,
          "reasoning_avg": -3.25,
          "overall_avg": 4.92,
          "overall_efficiency": 0.140618,
          "general_efficiency": 0.167394,
          "math_efficiency": 0.319628,
          "code_efficiency": 0.168211,
          "reasoning_efficiency": -0.092763,
          "general_task_scores": [
            -6.6,
            15.84,
            7.22,
            6.97
          ],
          "math_task_scores": [
            9.71,
            18.71,
            19.53,
            1.32,
            6.66
          ],
          "code_task_scores": [
            0.2,
            2.07,
            2.16,
            39.74,
            -37.1,
            28.25
          ],
          "reasoning_task_scores": [
            -15.92,
            0.27,
            2.02,
            0.03,
            -2.63
          ]
        },
        "vs_instruct": {
          "general_avg": -8.91,
          "math_avg": -2.83,
          "code_avg": -2.69,
          "reasoning_avg": -2.26,
          "overall_avg": -4.17,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.49,
            -21.63,
            -2.48,
            -6.02
          ],
          "math_task_scores": [
            -2.58,
            -5.19,
            -8.07,
            -2.04,
            3.75
          ],
          "code_task_scores": [
            13.62,
            -1.04,
            -0.71,
            -2.23,
            -41.86,
            16.05
          ],
          "reasoning_task_scores": [
            -4.74,
            -1.89,
            3.03,
            -0.0,
            -7.67
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 50.66,
      "math_avg": 51.81,
      "code_avg": 41.34,
      "reasoning_avg": 29.46,
      "overall_avg": 43.32,
      "overall_efficiency": 0.001963,
      "general_efficiency": -0.001611,
      "math_efficiency": 0.018611,
      "code_efficiency": 0.003205,
      "reasoning_efficiency": -0.012353,
      "general_scores": [
        62.79,
        56.4025,
        58.34,
        27.26,
        63.02,
        56.725,
        59.04,
        21.8107143,
        61.83,
        55.93,
        59.31,
        25.51571
      ],
      "math_scores": [
        88.32,
        65.74,
        65.4,
        26.51,
        6.67,
        89.46,
        64.12,
        66.4,
        25.56,
        20.0,
        89.08,
        64.1,
        66.4,
        26.08,
        13.335
      ],
      "code_scores": [
        79.27,
        76.65,
        0.0,
        22.76,
        0.0,
        72.56,
        76.83,
        74.32,
        0.0,
        19.42,
        0.45,
        71.34,
        81.1,
        75.1,
        0.0,
        19.42,
        0.45,
        74.39
      ],
      "reasoning_scores": [
        24.07,
        55.51,
        28.79,
        0.19467391,
        35.91,
        23.05,
        63.21,
        32.32,
        0.1926087,
        32.94,
        24.41,
        62.97,
        28.79,
        0.192391,
        29.38
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.55
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 56.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.95
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 75.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.53
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 72.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.74
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.78,
          "math_avg": 9.01,
          "code_avg": 1.55,
          "reasoning_avg": -5.98,
          "overall_avg": 0.95,
          "overall_efficiency": 0.001963,
          "general_efficiency": -0.001611,
          "math_efficiency": 0.018611,
          "code_efficiency": 0.003205,
          "reasoning_efficiency": -0.012353,
          "general_task_scores": [
            -5.76,
            20.86,
            1.16,
            -19.38
          ],
          "math_task_scores": [
            8.97,
            13.53,
            15.87,
            0.01,
            6.67
          ],
          "code_task_scores": [
            1.63,
            3.76,
            -8.24,
            19.49,
            -36.8,
            29.47
          ],
          "reasoning_task_scores": [
            -12.76,
            -8.9,
            -4.88,
            -0.2,
            -3.17
          ]
        },
        "vs_instruct": {
          "general_avg": -15.54,
          "math_avg": -5.0,
          "code_avg": -7.03,
          "reasoning_avg": -4.99,
          "overall_avg": -8.14,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.65,
            -16.61,
            -8.54,
            -32.37
          ],
          "math_task_scores": [
            -3.32,
            -10.37,
            -11.73,
            -3.35,
            3.76
          ],
          "code_task_scores": [
            15.05,
            0.65,
            -11.11,
            -22.48,
            -41.56,
            17.27
          ],
          "reasoning_task_scores": [
            -1.58,
            -11.06,
            -3.87,
            -0.23,
            -8.21
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 47.27,
      "math_avg": 35.69,
      "code_avg": 26.69,
      "reasoning_avg": 24.64,
      "overall_avg": 33.57,
      "overall_efficiency": -0.106695,
      "general_efficiency": -0.050627,
      "math_efficiency": -0.086213,
      "code_efficiency": -0.158858,
      "reasoning_efficiency": -0.131082,
      "general_scores": [
        55.63,
        38.3025,
        55.02,
        39.4907143,
        56.19,
        38.275,
        55.73,
        39.2192857,
        54.94,
        39.3875,
        55.13,
        39.935
      ],
      "math_scores": [
        78.39,
        37.06,
        38.6,
        20.69,
        3.33,
        78.09,
        37.56,
        39.4,
        18.16,
        6.67,
        76.8,
        36.94,
        37.8,
        19.26,
        6.67
      ],
      "code_scores": [
        38.41,
        52.14,
        0.0,
        35.7,
        3.85,
        30.49,
        40.24,
        51.36,
        0.0,
        36.53,
        4.07,
        30.49,
        37.2,
        49.42,
        0.0,
        36.53,
        2.26,
        31.71
      ],
      "reasoning_scores": [
        25.42,
        60.38,
        28.28,
        0.31304348,
        8.61,
        24.41,
        60.26,
        28.79,
        0.3151087,
        9.2,
        25.42,
        60.7,
        27.78,
        0.31413043,
        9.35
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 38.62
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.25
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.45
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.05
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.17,
          "math_avg": -7.11,
          "code_avg": -13.1,
          "reasoning_avg": -10.81,
          "overall_avg": -8.8,
          "overall_efficiency": -0.106695,
          "general_efficiency": -0.050627,
          "math_efficiency": -0.086213,
          "code_efficiency": -0.158858,
          "reasoning_efficiency": -0.131082,
          "general_task_scores": [
            -12.72,
            3.17,
            -2.45,
            -4.69
          ],
          "math_task_scores": [
            -2.22,
            -13.93,
            -11.6,
            -6.67,
            -1.11
          ],
          "code_task_scores": [
            -38.82,
            -20.63,
            -8.24,
            35.21,
            -33.71,
            -12.39
          ],
          "reasoning_task_scores": [
            -11.52,
            -9.01,
            -6.57,
            -0.08,
            -26.86
          ]
        },
        "vs_instruct": {
          "general_avg": -18.94,
          "math_avg": -21.12,
          "code_avg": -21.68,
          "reasoning_avg": -9.81,
          "overall_avg": -17.89,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.61,
            -34.3,
            -12.15,
            -17.68
          ],
          "math_task_scores": [
            -14.51,
            -37.83,
            -39.2,
            -10.03,
            -4.02
          ],
          "code_task_scores": [
            -25.4,
            -23.74,
            -11.11,
            -6.76,
            -38.47,
            -24.59
          ],
          "reasoning_task_scores": [
            -0.34,
            -11.17,
            -5.56,
            -0.11,
            -31.9
          ]
        }
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.05,
      "math_avg": 47.46,
      "code_avg": 51.26,
      "reasoning_avg": 28.17,
      "overall_avg": 45.74,
      "overall_efficiency": 0.043021,
      "general_efficiency": 0.058874,
      "math_efficiency": 0.059482,
      "code_efficiency": 0.14659,
      "reasoning_efficiency": -0.092865,
      "general_scores": [
        62.72,
        50.47,
        61.33,
        49.0335714,
        62.71,
        51.1875,
        60.27,
        48.7007143,
        63.12,
        52.885,
        60.87,
        49.33
      ],
      "math_scores": [
        84.76,
        57.38,
        57.6,
        23.28,
        10.0,
        85.9,
        57.18,
        59.2,
        22.83,
        16.67,
        84.84,
        57.72,
        57.8,
        23.37,
        13.33
      ],
      "code_scores": [
        78.05,
        71.21,
        12.9,
        41.34,
        45.48,
        64.63,
        72.56,
        71.98,
        12.54,
        40.08,
        46.15,
        60.98,
        77.44,
        69.65,
        12.54,
        40.08,
        43.44,
        61.59
      ],
      "reasoning_scores": [
        18.31,
        68.0,
        32.83,
        0.41326087,
        18.84,
        19.32,
        67.95,
        32.83,
        0.41228261,
        21.36,
        19.66,
        67.84,
        33.84,
        0.39076087,
        20.62
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.5
              },
              {
                "metric": "lcb_test_output",
                "score": 45.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 62.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.27
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.61,
          "math_avg": 4.66,
          "code_avg": 11.47,
          "reasoning_avg": -7.27,
          "overall_avg": 3.37,
          "overall_efficiency": 0.043021,
          "general_efficiency": 0.058874,
          "math_efficiency": 0.059482,
          "code_efficiency": 0.14659,
          "reasoning_efficiency": -0.092865,
          "general_task_scores": [
            -5.46,
            16.02,
            3.08,
            4.78
          ],
          "math_task_scores": [
            5.19,
            6.31,
            8.0,
            -2.88,
            6.66
          ],
          "code_task_scores": [
            -1.42,
            -0.65,
            4.42,
            39.46,
            7.92,
            19.11
          ],
          "reasoning_task_scores": [
            -17.5,
            -1.53,
            -1.68,
            0.02,
            -15.64
          ]
        },
        "vs_instruct": {
          "general_avg": -10.16,
          "math_avg": -9.36,
          "code_avg": 2.89,
          "reasoning_avg": -6.28,
          "overall_avg": -5.72,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.35,
            -21.45,
            -6.62,
            -8.21
          ],
          "math_task_scores": [
            -7.1,
            -17.59,
            -19.6,
            -6.24,
            3.75
          ],
          "code_task_scores": [
            12.0,
            -3.76,
            1.55,
            -2.51,
            3.16,
            6.91
          ],
          "reasoning_task_scores": [
            -6.32,
            -3.69,
            -0.67,
            -0.01,
            -20.68
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.26,
      "math_avg": 41.93,
      "code_avg": 48.49,
      "reasoning_avg": 27.6,
      "overall_avg": 43.07,
      "overall_efficiency": 0.005732,
      "general_efficiency": 0.023064,
      "math_efficiency": -0.007188,
      "code_efficiency": 0.071386,
      "reasoning_efficiency": -0.064335,
      "general_scores": [
        55.69,
        52.59,
        58.7,
        49.46,
        56.2,
        53.985,
        58.44,
        49.1271429,
        55.85,
        53.6375,
        58.46,
        48.9485714
      ],
      "math_scores": [
        83.62,
        49.04,
        50.0,
        18.41,
        6.67,
        84.15,
        49.62,
        48.6,
        18.77,
        6.67,
        83.09,
        49.2,
        51.6,
        19.44,
        10.0
      ],
      "code_scores": [
        74.39,
        67.32,
        2.15,
        42.17,
        39.59,
        61.59,
        76.22,
        68.48,
        1.79,
        42.17,
        43.21,
        62.8,
        75.0,
        66.54,
        3.23,
        40.92,
        41.86,
        63.41
      ],
      "reasoning_scores": [
        24.75,
        67.06,
        32.32,
        0.38369565,
        15.28,
        20.34,
        67.12,
        32.32,
        0.3825,
        15.28,
        24.41,
        66.96,
        31.82,
        0.38565217,
        15.13
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.29
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 41.55
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 62.6
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.81,
          "math_avg": -0.88,
          "code_avg": 8.71,
          "reasoning_avg": -7.85,
          "overall_avg": 0.7,
          "overall_efficiency": 0.005732,
          "general_efficiency": 0.023064,
          "math_efficiency": -0.007188,
          "code_efficiency": 0.071386,
          "reasoning_efficiency": -0.064335,
          "general_task_scores": [
            -12.4,
            17.91,
            0.79,
            4.94
          ],
          "math_task_scores": [
            3.64,
            -1.83,
            -0.13,
            -7.17,
            1.11
          ],
          "code_task_scores": [
            -2.24,
            -4.15,
            -5.85,
            40.71,
            4.45,
            19.31
          ],
          "reasoning_task_scores": [
            -13.43,
            -2.41,
            -2.7,
            -0.01,
            -20.68
          ]
        },
        "vs_instruct": {
          "general_avg": -11.95,
          "math_avg": -14.89,
          "code_avg": 0.12,
          "reasoning_avg": -6.85,
          "overall_avg": -8.39,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.29,
            -19.56,
            -8.91,
            -8.05
          ],
          "math_task_scores": [
            -8.65,
            -25.73,
            -27.73,
            -10.53,
            -1.8
          ],
          "code_task_scores": [
            11.18,
            -7.26,
            -8.72,
            -1.26,
            -0.31,
            7.11
          ],
          "reasoning_task_scores": [
            -2.25,
            -4.57,
            -1.69,
            -0.04,
            -25.72
          ]
        }
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 57.02,
      "math_avg": 48.23,
      "code_avg": 48.5,
      "reasoning_avg": 29.28,
      "overall_avg": 45.76,
      "overall_efficiency": 0.182085,
      "general_efficiency": 0.299738,
      "math_efficiency": 0.291604,
      "code_efficiency": 0.468157,
      "reasoning_efficiency": -0.331158,
      "general_scores": [
        65.95,
        50.66,
        61.83,
        49.5642857,
        66.42,
        50.6575,
        61.69,
        49.6342857,
        65.74,
        51.18,
        61.75,
        49.2021429
      ],
      "math_scores": [
        84.69,
        57.46,
        58.2,
        24.86,
        13.33,
        84.76,
        57.54,
        59.8,
        25.29,
        13.33,
        85.52,
        57.4,
        56.4,
        24.86,
        20.0
      ],
      "code_scores": [
        76.22,
        71.6,
        0.36,
        42.17,
        34.62,
        69.51,
        74.39,
        67.32,
        0.0,
        42.38,
        35.97,
        68.9,
        74.39,
        69.26,
        0.0,
        42.38,
        36.43,
        67.07
      ],
      "reasoning_scores": [
        21.69,
        67.93,
        33.33,
        0.38195652,
        23.44,
        20.0,
        68.66,
        31.82,
        0.38565217,
        23.0,
        23.05,
        68.61,
        32.32,
        0.38586957,
        24.18
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.04
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.76
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.99
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.12
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 35.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.54
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.58,
          "math_avg": 5.43,
          "code_avg": 8.71,
          "reasoning_avg": -6.16,
          "overall_avg": 3.39,
          "overall_efficiency": 0.182085,
          "general_efficiency": 0.299738,
          "math_efficiency": 0.291604,
          "code_efficiency": 0.468157,
          "reasoning_efficiency": -0.331158,
          "general_task_scores": [
            -2.27,
            15.34,
            4.02,
            5.23
          ],
          "math_task_scores": [
            5.01,
            6.35,
            7.93,
            -1.04,
            8.88
          ],
          "code_task_scores": [
            -2.44,
            -2.21,
            -8.12,
            41.27,
            -1.43,
            25.2
          ],
          "reasoning_task_scores": [
            -15.02,
            -1.06,
            -2.36,
            -0.01,
            -12.37
          ]
        },
        "vs_instruct": {
          "general_avg": -9.19,
          "math_avg": -8.59,
          "code_avg": 0.13,
          "reasoning_avg": -5.17,
          "overall_avg": -5.7,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.16,
            -22.13,
            -5.68,
            -7.76
          ],
          "math_task_scores": [
            -7.28,
            -17.55,
            -19.67,
            -4.4,
            5.97
          ],
          "code_task_scores": [
            10.98,
            -5.32,
            -10.99,
            -0.7,
            -6.19,
            13.0
          ],
          "reasoning_task_scores": [
            -3.84,
            -3.22,
            -1.35,
            -0.04,
            -17.41
          ]
        }
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 59.34,
      "math_avg": 51.31,
      "code_avg": 47.52,
      "reasoning_avg": 29.38,
      "overall_avg": 46.89,
      "overall_efficiency": 0.199798,
      "general_efficiency": 0.349102,
      "math_efficiency": 0.37612,
      "code_efficiency": 0.342087,
      "reasoning_efficiency": -0.268114,
      "general_scores": [
        74.59,
        48.8025,
        63.94,
        48.7921429,
        76.02,
        50.9975,
        63.4,
        48.7921429,
        75.88,
        49.385,
        63.32,
        48.1242857
      ],
      "math_scores": [
        86.5,
        63.26,
        63.2,
        25.72,
        23.33,
        86.81,
        63.6,
        63.4,
        25.61,
        10.0,
        86.96,
        63.12,
        62.6,
        25.47,
        20.0
      ],
      "code_scores": [
        78.66,
        68.09,
        14.34,
        17.95,
        44.8,
        61.59,
        77.44,
        67.7,
        14.34,
        15.24,
        44.8,
        64.02,
        78.01,
        69.65,
        13.26,
        15.87,
        46.38,
        63.2
      ],
      "reasoning_scores": [
        22.71,
        67.14,
        34.34,
        0.3825,
        23.74,
        18.98,
        67.06,
        34.34,
        0.39032609,
        23.74,
        19.32,
        67.67,
        34.85,
        0.38032609,
        25.67
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.98
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.35
              },
              {
                "metric": "lcb_test_output",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 62.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.29
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.38
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.89,
          "math_avg": 8.5,
          "code_avg": 7.73,
          "reasoning_avg": -6.06,
          "overall_avg": 4.52,
          "overall_efficiency": 0.199798,
          "general_efficiency": 0.349102,
          "math_efficiency": 0.37612,
          "code_efficiency": 0.342087,
          "reasoning_efficiency": -0.268114,
          "general_task_scores": [
            7.19,
            14.24,
            5.81,
            4.33
          ],
          "math_task_scores": [
            6.78,
            12.21,
            12.87,
            -0.44,
            11.11
          ],
          "code_task_scores": [
            0.6,
            -3.12,
            5.74,
            15.31,
            8.23,
            19.65
          ],
          "reasoning_task_scores": [
            -16.26,
            -2.17,
            -0.34,
            -0.01,
            -11.53
          ]
        },
        "vs_instruct": {
          "general_avg": -6.87,
          "math_avg": -5.51,
          "code_avg": -0.85,
          "reasoning_avg": -5.07,
          "overall_avg": -4.57,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.3,
            -23.23,
            -3.89,
            -8.66
          ],
          "math_task_scores": [
            -5.51,
            -11.69,
            -14.73,
            -3.8,
            8.2
          ],
          "code_task_scores": [
            14.02,
            -6.23,
            2.87,
            -26.66,
            3.47,
            7.45
          ],
          "reasoning_task_scores": [
            -5.08,
            -4.33,
            0.67,
            -0.04,
            -16.57
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 53.67,
      "math_avg": 48.0,
      "code_avg": 34.61,
      "reasoning_avg": 28.59,
      "overall_avg": 41.22,
      "overall_efficiency": -0.003779,
      "general_efficiency": 0.007296,
      "math_efficiency": 0.01706,
      "code_efficiency": -0.016969,
      "reasoning_efficiency": -0.022505,
      "general_scores": [
        68.75,
        42.085,
        59.03,
        45.8735714,
        65.46,
        41.9775,
        59.03,
        46.21,
        68.39,
        42.1625,
        59.03,
        46.0121429
      ],
      "math_scores": [
        83.4,
        60.38,
        58.2,
        24.71,
        13.33,
        83.4,
        60.38,
        58.2,
        24.68,
        13.33,
        83.4,
        60.38,
        58.2,
        24.68,
        13.33
      ],
      "code_scores": [
        37.8,
        64.2,
        6.81,
        38.41,
        19.0,
        48.17,
        26.22,
        64.2,
        6.81,
        38.41,
        19.0,
        50.0,
        31.1,
        64.2,
        6.81,
        38.41,
        19.0,
        44.51
      ],
      "reasoning_scores": [
        26.78,
        64.43,
        32.32,
        0.36758242,
        19.29,
        26.78,
        64.43,
        30.3,
        0.36456522,
        22.4,
        26.78,
        64.43,
        30.3,
        0.36758242,
        19.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.38
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.71
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 19.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 47.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.78
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.38
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.22,
          "math_avg": 5.2,
          "code_avg": -5.17,
          "reasoning_avg": -6.86,
          "overall_avg": -1.15,
          "overall_efficiency": -0.003779,
          "general_efficiency": 0.007296,
          "math_efficiency": 0.01706,
          "code_efficiency": -0.016969,
          "reasoning_efficiency": -0.022505,
          "general_task_scores": [
            -0.78,
            6.59,
            1.29,
            1.79
          ],
          "math_task_scores": [
            3.42,
            9.26,
            8.0,
            -1.35,
            6.66
          ],
          "code_task_scores": [
            -45.73,
            -7.4,
            -1.43,
            37.37,
            -18.1,
            4.27
          ],
          "reasoning_task_scores": [
            -9.82,
            -5.03,
            -3.88,
            -0.02,
            -15.53
          ]
        },
        "vs_instruct": {
          "general_avg": -12.54,
          "math_avg": -8.81,
          "code_avg": -13.75,
          "reasoning_avg": -5.87,
          "overall_avg": -10.24,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.33,
            -30.88,
            -8.41,
            -11.2
          ],
          "math_task_scores": [
            -8.87,
            -14.64,
            -19.6,
            -4.71,
            3.75
          ],
          "code_task_scores": [
            -32.31,
            -10.51,
            -4.3,
            -4.6,
            -22.86,
            -7.93
          ],
          "reasoning_task_scores": [
            1.36,
            -7.19,
            -2.87,
            -0.05,
            -20.57
          ]
        }
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 57.9,
      "math_avg": 54.36,
      "code_avg": 50.85,
      "reasoning_avg": 30.88,
      "overall_avg": 48.5,
      "overall_efficiency": 0.016131,
      "general_efficiency": 0.01699,
      "math_efficiency": 0.030423,
      "code_efficiency": 0.029119,
      "reasoning_efficiency": -0.012006,
      "general_scores": [
        62.35,
        53.8,
        64.65,
        52.5285714,
        62.15,
        52.785,
        65.69,
        52.3235714,
        58.0,
        53.7575,
        64.56,
        52.2142857
      ],
      "math_scores": [
        87.57,
        67.26,
        66.8,
        32.36,
        20.0,
        88.93,
        69.06,
        70.8,
        33.42,
        13.33,
        87.34,
        66.58,
        65.6,
        33.06,
        13.33
      ],
      "code_scores": [
        82.32,
        73.54,
        16.85,
        37.58,
        20.59,
        76.22,
        81.71,
        73.54,
        13.98,
        35.28,
        20.81,
        75.61,
        82.32,
        73.15,
        16.85,
        37.37,
        20.14,
        77.44
      ],
      "reasoning_scores": [
        24.75,
        68.07,
        32.83,
        0.32945652,
        29.08,
        25.76,
        68.7,
        32.32,
        0.33717391,
        33.23,
        20.34,
        68.64,
        33.84,
        0.34119565,
        24.63
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.95
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 82.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.41
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 15.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.74
              },
              {
                "metric": "lcb_test_output",
                "score": 20.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 76.42
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.98
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.46,
          "math_avg": 11.56,
          "code_avg": 11.06,
          "reasoning_avg": -4.56,
          "overall_avg": 6.13,
          "overall_efficiency": 0.016131,
          "general_efficiency": 0.01699,
          "math_efficiency": 0.030423,
          "code_efficiency": 0.029119,
          "reasoning_efficiency": -0.012006,
          "general_task_scores": [
            -7.48,
            17.96,
            7.23,
            8.12
          ],
          "math_task_scores": [
            7.97,
            16.51,
            17.53,
            6.91,
            8.88
          ],
          "code_task_scores": [
            4.68,
            1.81,
            7.65,
            35.7,
            -16.59,
            33.13
          ],
          "reasoning_task_scores": [
            -12.98,
            -0.99,
            -1.85,
            -0.05,
            -6.93
          ]
        },
        "vs_instruct": {
          "general_avg": -8.31,
          "math_avg": -2.45,
          "code_avg": 2.48,
          "reasoning_avg": -3.57,
          "overall_avg": -2.96,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -6.37,
            -19.51,
            -2.47,
            -4.87
          ],
          "math_task_scores": [
            -4.32,
            -7.39,
            -10.07,
            3.55,
            5.97
          ],
          "code_task_scores": [
            18.1,
            -1.3,
            4.78,
            -6.27,
            -21.35,
            20.93
          ],
          "reasoning_task_scores": [
            -1.8,
            -3.15,
            -0.84,
            -0.08,
            -11.97
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 57.41,
      "math_avg": 52.25,
      "code_avg": 53.45,
      "reasoning_avg": 30.92,
      "overall_avg": 48.51,
      "overall_efficiency": 0.056664,
      "general_efficiency": 0.055048,
      "math_efficiency": 0.087197,
      "code_efficiency": 0.126092,
      "reasoning_efficiency": -0.041684,
      "general_scores": [
        64.77,
        52.9625,
        62.53,
        49.9814286,
        64.64,
        52.7925,
        62.76,
        49.4192857,
        64.06,
        52.8175,
        62.54,
        49.6607143
      ],
      "math_scores": [
        88.78,
        65.86,
        69.4,
        26.65,
        13.33,
        88.48,
        65.92,
        65.8,
        26.76,
        10.0,
        89.01,
        65.68,
        65.6,
        25.86,
        16.67
      ],
      "code_scores": [
        78.05,
        72.76,
        12.54,
        42.17,
        46.83,
        68.29,
        80.49,
        71.6,
        11.83,
        42.38,
        49.77,
        66.46,
        76.83,
        71.6,
        13.26,
        40.71,
        47.06,
        69.51
      ],
      "reasoning_scores": [
        20.0,
        69.88,
        37.37,
        0.45108696,
        23.74,
        26.1,
        70.41,
        36.36,
        0.44326087,
        24.33,
        26.1,
        70.29,
        36.87,
        0.4498913,
        21.07
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.61
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 47.89
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.09
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.05
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.97,
          "math_avg": 9.45,
          "code_avg": 13.67,
          "reasoning_avg": -4.52,
          "overall_avg": 6.14,
          "overall_efficiency": 0.056664,
          "general_efficiency": 0.055048,
          "math_efficiency": 0.087197,
          "code_efficiency": 0.126092,
          "reasoning_efficiency": -0.041684,
          "general_task_scores": [
            -3.82,
            17.37,
            4.87,
            5.45
          ],
          "math_task_scores": [
            8.78,
            14.7,
            16.73,
            0.38,
            6.66
          ],
          "code_task_scores": [
            1.02,
            0.39,
            4.3,
            40.71,
            10.79,
            24.8
          ],
          "reasoning_task_scores": [
            -12.53,
            0.73,
            2.02,
            0.06,
            -12.86
          ]
        },
        "vs_instruct": {
          "general_avg": -8.8,
          "math_avg": -4.56,
          "code_avg": 5.09,
          "reasoning_avg": -3.53,
          "overall_avg": -2.95,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.71,
            -20.1,
            -4.83,
            -7.54
          ],
          "math_task_scores": [
            -3.51,
            -9.2,
            -10.87,
            -2.98,
            3.75
          ],
          "code_task_scores": [
            14.44,
            -2.72,
            1.43,
            -1.26,
            6.03,
            12.6
          ],
          "reasoning_task_scores": [
            -1.35,
            -1.43,
            3.03,
            0.03,
            -17.9
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 52.21,
      "math_avg": 38.99,
      "code_avg": 32.92,
      "reasoning_avg": 27.37,
      "overall_avg": 37.87,
      "overall_efficiency": -0.00431,
      "general_efficiency": 0.000738,
      "math_efficiency": -0.003659,
      "code_efficiency": -0.006579,
      "reasoning_efficiency": -0.007742,
      "general_scores": [
        57.05,
        49.1375,
        53.53,
        48.7123077,
        56.63,
        50.0225,
        54.01,
        48.7421429,
        59.75,
        49.3175,
        50.38,
        49.2942857
      ],
      "math_scores": [
        80.82,
        38.4,
        39.0,
        29.27,
        10.0,
        81.12,
        39.42,
        42.8,
        29.27,
        3.33,
        81.2,
        37.94,
        37.6,
        27.94,
        6.67
      ],
      "code_scores": [
        56.71,
        65.37,
        0.0,
        11.9,
        1.81,
        54.88,
        57.93,
        66.54,
        0.0,
        19.21,
        2.94,
        53.66,
        62.2,
        64.98,
        0.0,
        19.0,
        1.81,
        53.66
      ],
      "reasoning_scores": [
        22.71,
        63.37,
        39.39,
        0.38391304,
        9.2,
        23.39,
        64.23,
        36.36,
        0.34673913,
        16.02,
        19.32,
        64.29,
        35.35,
        0.38543478,
        15.73
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.95
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.7
              },
              {
                "metric": "lcb_test_output",
                "score": 2.19
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 54.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.77,
          "math_avg": -3.82,
          "code_avg": -6.86,
          "reasoning_avg": -8.08,
          "overall_avg": -4.5,
          "overall_efficiency": -0.00431,
          "general_efficiency": 0.000738,
          "math_efficiency": -0.003659,
          "code_efficiency": -0.006579,
          "reasoning_efficiency": -0.007742,
          "general_task_scores": [
            -10.5,
            14.0,
            -5.1,
            4.68
          ],
          "math_task_scores": [
            1.07,
            -12.53,
            -10.4,
            2.79,
            0.0
          ],
          "code_task_scores": [
            -18.49,
            -5.97,
            -8.24,
            15.66,
            -34.91,
            10.78
          ],
          "reasoning_task_scores": [
            -14.79,
            -5.5,
            2.18,
            -0.02,
            -22.26
          ]
        },
        "vs_instruct": {
          "general_avg": -13.99,
          "math_avg": -17.83,
          "code_avg": -15.44,
          "reasoning_avg": -7.09,
          "overall_avg": -13.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.39,
            -23.47,
            -14.8,
            -8.31
          ],
          "math_task_scores": [
            -11.22,
            -36.43,
            -38.0,
            -0.57,
            -2.91
          ],
          "code_task_scores": [
            -5.07,
            -9.08,
            -11.11,
            -26.31,
            -39.67,
            -1.42
          ],
          "reasoning_task_scores": [
            -3.61,
            -7.66,
            3.19,
            -0.05,
            -27.3
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 56.87,
      "math_avg": 49.04,
      "code_avg": 49.35,
      "reasoning_avg": 30.36,
      "overall_avg": 46.4,
      "overall_efficiency": 0.040347,
      "general_efficiency": 0.05421,
      "math_efficiency": 0.062347,
      "code_efficiency": 0.095667,
      "reasoning_efficiency": -0.050835,
      "general_scores": [
        64.27,
        53.28,
        61.0,
        49.6371429,
        64.73,
        52.515,
        61.09,
        49.5992857,
        64.28,
        51.73,
        61.17,
        49.0842857
      ],
      "math_scores": [
        87.87,
        62.68,
        63.4,
        24.62,
        10.0,
        87.26,
        60.52,
        61.6,
        24.71,
        13.33,
        85.9,
        59.58,
        59.8,
        24.28,
        10.0
      ],
      "code_scores": [
        73.17,
        73.93,
        9.32,
        44.89,
        37.1,
        60.37,
        71.34,
        74.32,
        9.68,
        44.05,
        32.81,
        62.8,
        72.56,
        72.76,
        9.32,
        45.51,
        32.81,
        61.59
      ],
      "reasoning_scores": [
        23.73,
        66.92,
        33.33,
        0.42434783,
        27.0,
        21.36,
        67.3,
        35.35,
        0.41923913,
        25.82,
        24.75,
        67.3,
        37.37,
        0.41923913,
        23.89
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.82
              },
              {
                "metric": "lcb_test_output",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 61.59
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.17
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": 6.23,
          "code_avg": 9.57,
          "reasoning_avg": -5.08,
          "overall_avg": 4.03,
          "overall_efficiency": 0.040347,
          "general_efficiency": 0.05421,
          "math_efficiency": 0.062347,
          "code_efficiency": 0.095667,
          "reasoning_efficiency": -0.050835,
          "general_task_scores": [
            -3.88,
            17.02,
            3.35,
            5.2
          ],
          "math_task_scores": [
            7.03,
            9.81,
            11.4,
            -1.5,
            4.44
          ],
          "code_task_scores": [
            -5.08,
            2.07,
            1.2,
            43.78,
            -2.86,
            18.3
          ],
          "reasoning_task_scores": [
            -13.32,
            -2.29,
            0.5,
            0.03,
            -10.34
          ]
        },
        "vs_instruct": {
          "general_avg": -9.34,
          "math_avg": -7.78,
          "code_avg": 0.98,
          "reasoning_avg": -4.09,
          "overall_avg": -5.06,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.77,
            -20.45,
            -6.35,
            -7.79
          ],
          "math_task_scores": [
            -5.26,
            -14.09,
            -16.2,
            -4.86,
            1.53
          ],
          "code_task_scores": [
            8.34,
            -1.04,
            -1.67,
            1.81,
            -7.62,
            6.1
          ],
          "reasoning_task_scores": [
            -2.14,
            -4.45,
            1.51,
            -0.0,
            -15.38
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 58.57,
      "math_avg": 54.86,
      "code_avg": 49.61,
      "reasoning_avg": 32.22,
      "overall_avg": 48.81,
      "overall_efficiency": 0.286455,
      "general_efficiency": 0.316896,
      "math_efficiency": 0.535733,
      "code_efficiency": 0.436618,
      "reasoning_efficiency": -0.143426,
      "general_scores": [
        66.35,
        52.75,
        65.69,
        49.4478571,
        66.51,
        52.01,
        66.2,
        49.8314286,
        65.96,
        52.525,
        66.3,
        49.3214286
      ],
      "math_scores": [
        89.61,
        70.88,
        72.6,
        26.38,
        16.67,
        89.08,
        71.12,
        73.0,
        26.65,
        13.33,
        88.86,
        71.46,
        73.0,
        26.87,
        13.33
      ],
      "code_scores": [
        78.66,
        73.15,
        0.0,
        39.25,
        39.14,
        73.78,
        79.27,
        71.98,
        0.0,
        38.0,
        33.71,
        70.12,
        79.27,
        71.98,
        0.0,
        38.83,
        33.26,
        72.56
      ],
      "reasoning_scores": [
        17.97,
        71.07,
        35.86,
        0.44152174,
        40.36,
        21.69,
        71.12,
        35.86,
        0.44076087,
        25.37,
        16.61,
        70.92,
        36.36,
        0.4373913,
        38.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.53
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.18
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.69
              },
              {
                "metric": "lcb_test_output",
                "score": 35.37
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 72.15
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.76
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.82
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.13,
          "math_avg": 12.05,
          "code_avg": 9.82,
          "reasoning_avg": -3.23,
          "overall_avg": 6.45,
          "overall_efficiency": 0.286455,
          "general_efficiency": 0.316896,
          "math_efficiency": 0.535733,
          "code_efficiency": 0.436618,
          "reasoning_efficiency": -0.143426,
          "general_task_scores": [
            -2.04,
            16.94,
            8.32,
            5.29
          ],
          "math_task_scores": [
            9.2,
            20.03,
            22.67,
            0.59,
            7.77
          ],
          "code_task_scores": [
            1.63,
            0.77,
            -8.24,
            37.65,
            -1.73,
            28.86
          ],
          "reasoning_task_scores": [
            -17.84,
            1.58,
            1.18,
            0.05,
            -1.09
          ]
        },
        "vs_instruct": {
          "general_avg": -7.63,
          "math_avg": -1.96,
          "code_avg": 1.24,
          "reasoning_avg": -2.24,
          "overall_avg": -2.65,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.93,
            -20.53,
            -1.38,
            -7.7
          ],
          "math_task_scores": [
            -3.09,
            -3.87,
            -4.93,
            -2.77,
            4.86
          ],
          "code_task_scores": [
            15.05,
            -2.34,
            -11.11,
            -4.32,
            -6.49,
            16.66
          ],
          "reasoning_task_scores": [
            -6.66,
            -0.58,
            2.19,
            0.02,
            -6.13
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 56.78,
      "math_avg": 48.67,
      "code_avg": 52.55,
      "reasoning_avg": 30.68,
      "overall_avg": 47.17,
      "overall_efficiency": 0.061124,
      "general_efficiency": 0.067895,
      "math_efficiency": 0.074695,
      "code_efficiency": 0.162487,
      "reasoning_efficiency": -0.06058,
      "general_scores": [
        64.72,
        52.2125,
        61.42,
        48.9078571,
        65.58,
        50.24,
        61.83,
        49.095,
        65.09,
        51.9275,
        61.48,
        48.85
      ],
      "math_scores": [
        85.6,
        60.66,
        59.8,
        26.63,
        13.33,
        84.31,
        58.48,
        58.0,
        25.61,
        10.0,
        84.31,
        61.86,
        62.2,
        25.95,
        13.33
      ],
      "code_scores": [
        75.61,
        73.93,
        11.11,
        46.14,
        49.1,
        64.63,
        72.56,
        71.21,
        11.83,
        47.39,
        44.34,
        64.63,
        72.56,
        70.04,
        10.39,
        46.14,
        49.1,
        65.24
      ],
      "reasoning_scores": [
        23.39,
        69.56,
        36.87,
        0.43152174,
        23.0,
        23.39,
        68.61,
        37.88,
        0.42391304,
        22.26,
        26.78,
        68.85,
        37.88,
        0.4375,
        20.47
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.46
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.58
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.74
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.56
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.01
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.91
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.33,
          "math_avg": 5.87,
          "code_avg": 12.77,
          "reasoning_avg": -4.76,
          "overall_avg": 4.8,
          "overall_efficiency": 0.061124,
          "general_efficiency": 0.067895,
          "math_efficiency": 0.074695,
          "code_efficiency": 0.162487,
          "reasoning_efficiency": -0.06058,
          "general_task_scores": [
            -3.18,
            15.97,
            3.84,
            4.71
          ],
          "math_task_scores": [
            4.76,
            9.21,
            9.8,
            0.02,
            5.55
          ],
          "code_task_scores": [
            -3.86,
            0.13,
            2.87,
            45.52,
            10.41,
            21.54
          ],
          "reasoning_task_scores": [
            -12.08,
            -0.45,
            2.69,
            0.04,
            -14.0
          ]
        },
        "vs_instruct": {
          "general_avg": -9.43,
          "math_avg": -8.14,
          "code_avg": 4.19,
          "reasoning_avg": -3.77,
          "overall_avg": -4.29,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.07,
            -21.5,
            -5.86,
            -8.28
          ],
          "math_task_scores": [
            -7.53,
            -14.69,
            -17.8,
            -3.34,
            2.64
          ],
          "code_task_scores": [
            9.56,
            -2.98,
            0.0,
            3.55,
            5.65,
            9.34
          ],
          "reasoning_task_scores": [
            -0.9,
            -2.61,
            3.7,
            0.01,
            -19.04
          ]
        }
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 50.0,
      "math_avg": 40.79,
      "code_avg": 26.2,
      "reasoning_avg": 24.0,
      "overall_avg": 35.25,
      "overall_efficiency": -0.065082,
      "general_efficiency": -0.013202,
      "math_efficiency": -0.018409,
      "code_efficiency": -0.12417,
      "reasoning_efficiency": -0.104548,
      "general_scores": [
        49.87,
        37.7,
        59.5,
        49.1907143,
        58.46,
        35.775,
        60.32,
        48.5914286,
        53.01,
        38.29,
        60.65,
        48.6442857
      ],
      "math_scores": [
        80.74,
        47.6,
        48.4,
        19.2,
        12.89,
        83.55,
        49.48,
        52.0,
        19.11,
        10.0,
        76.57,
        39.56,
        40.8,
        18.59,
        13.33
      ],
      "code_scores": [
        52.44,
        71.6,
        12.54,
        11.23,
        4.67,
        10.37,
        48.17,
        71.6,
        12.9,
        11.9,
        5.66,
        11.59,
        38.41,
        71.6,
        12.54,
        11.06,
        1.13,
        12.2
      ],
      "reasoning_scores": [
        24.75,
        69.12,
        25.76,
        0.2912714,
        7.27,
        24.75,
        69.09,
        23.74,
        0.2898913,
        0.89,
        20.68,
        68.78,
        22.73,
        0.29869565,
        1.63
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.81
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.29
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.4
              },
              {
                "metric": "lcb_test_output",
                "score": 3.82
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 11.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.26
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.44,
          "math_avg": -2.01,
          "code_avg": -13.58,
          "reasoning_avg": -11.44,
          "overall_avg": -7.12,
          "overall_efficiency": -0.065082,
          "general_efficiency": -0.013202,
          "math_efficiency": -0.018409,
          "code_efficiency": -0.12417,
          "reasoning_efficiency": -0.104548,
          "general_task_scores": [
            -14.53,
            1.76,
            2.42,
            4.57
          ],
          "math_task_scores": [
            0.31,
            -5.57,
            -3.13,
            -7.07,
            5.4
          ],
          "code_task_scores": [
            -31.1,
            0.0,
            4.42,
            10.36,
            -33.28,
            -31.9
          ],
          "reasoning_task_scores": [
            -13.21,
            -0.46,
            -10.77,
            -0.1,
            -32.65
          ]
        },
        "vs_instruct": {
          "general_avg": -16.21,
          "math_avg": -16.03,
          "code_avg": -22.17,
          "reasoning_avg": -10.45,
          "overall_avg": -16.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -13.42,
            -35.71,
            -7.28,
            -8.42
          ],
          "math_task_scores": [
            -11.98,
            -29.47,
            -30.73,
            -10.43,
            2.49
          ],
          "code_task_scores": [
            -17.68,
            -3.11,
            1.55,
            -31.61,
            -38.04,
            -44.1
          ],
          "reasoning_task_scores": [
            -2.03,
            -2.62,
            -9.76,
            -0.13,
            -37.69
          ]
        }
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 57.59,
      "math_avg": 44.22,
      "code_avg": 48.93,
      "reasoning_avg": 28.82,
      "overall_avg": 44.89,
      "overall_efficiency": 0.008716,
      "general_efficiency": 0.021269,
      "math_efficiency": 0.004893,
      "code_efficiency": 0.031619,
      "reasoning_efficiency": -0.022919,
      "general_scores": [
        71.16,
        51.635,
        59.84,
        49.2021429,
        69.69,
        50.245,
        60.61,
        49.5564286,
        71.12,
        48.7625,
        59.94,
        49.3578571
      ],
      "math_scores": [
        82.49,
        53.1,
        51.6,
        22.15,
        16.67,
        81.5,
        52.82,
        53.4,
        22.06,
        10.0,
        81.65,
        53.72,
        53.4,
        22.02,
        6.67
      ],
      "code_scores": [
        78.66,
        68.09,
        12.19,
        16.08,
        41.86,
        68.29,
        77.44,
        68.09,
        13.26,
        24.43,
        42.99,
        72.56,
        76.22,
        68.48,
        13.98,
        27.77,
        43.21,
        67.07
      ],
      "reasoning_scores": [
        17.97,
        66.6,
        35.35,
        0.39576087,
        18.84,
        24.41,
        66.02,
        35.86,
        0.38217391,
        18.1,
        25.42,
        66.63,
        35.86,
        0.38217391,
        20.03
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.66
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.88
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 69.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.42
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.15,
          "math_avg": 1.41,
          "code_avg": 9.14,
          "reasoning_avg": -6.63,
          "overall_avg": 2.52,
          "overall_efficiency": 0.008716,
          "general_efficiency": 0.021269,
          "math_efficiency": 0.004893,
          "code_efficiency": 0.031619,
          "reasoning_efficiency": -0.022919,
          "general_task_scores": [
            2.35,
            14.72,
            2.39,
            5.13
          ],
          "math_task_scores": [
            1.9,
            2.09,
            2.6,
            -3.96,
            4.44
          ],
          "code_task_scores": [
            0.0,
            -3.38,
            4.9,
            21.72,
            5.59,
            26.02
          ],
          "reasoning_task_scores": [
            -14.0,
            -3.04,
            0.84,
            -0.0,
            -16.92
          ]
        },
        "vs_instruct": {
          "general_avg": -8.62,
          "math_avg": -12.6,
          "code_avg": 0.56,
          "reasoning_avg": -5.63,
          "overall_avg": -6.57,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.46,
            -22.75,
            -7.31,
            -7.86
          ],
          "math_task_scores": [
            -10.39,
            -21.81,
            -25.0,
            -7.32,
            1.53
          ],
          "code_task_scores": [
            13.42,
            -6.49,
            2.03,
            -20.25,
            0.83,
            13.82
          ],
          "reasoning_task_scores": [
            -2.82,
            -5.2,
            1.85,
            -0.03,
            -21.96
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 58.01,
      "math_avg": 47.43,
      "code_avg": 47.04,
      "reasoning_avg": 29.29,
      "overall_avg": 45.44,
      "overall_efficiency": 0.041573,
      "general_efficiency": 0.088745,
      "math_efficiency": 0.062607,
      "code_efficiency": 0.098143,
      "reasoning_efficiency": -0.083207,
      "general_scores": [
        69.15,
        50.8675,
        62.02,
        49.4178571,
        70.87,
        50.395,
        62.52,
        49.3135714,
        68.9,
        50.9425,
        62.29,
        49.3764286
      ],
      "math_scores": [
        83.4,
        56.8,
        59.6,
        23.94,
        19.16625,
        83.02,
        56.44,
        56.4,
        23.55,
        10.0,
        84.0,
        56.8,
        57.8,
        23.87,
        16.67
      ],
      "code_scores": [
        79.88,
        71.6,
        12.54,
        13.15,
        44.12,
        62.8,
        78.66,
        70.82,
        11.11,
        13.78,
        44.8,
        62.8,
        76.83,
        70.82,
        12.19,
        12.53,
        43.67,
        64.63
      ],
      "reasoning_scores": [
        20.68,
        67.64,
        37.37,
        0.39271739,
        21.07,
        18.31,
        66.84,
        36.36,
        0.39521739,
        21.22,
        23.05,
        67.66,
        37.37,
        0.38826087,
        20.62
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.64
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.47
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.79
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.28
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.95
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.15
              },
              {
                "metric": "lcb_test_output",
                "score": 44.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 63.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.68
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.56,
          "math_avg": 4.63,
          "code_avg": 7.26,
          "reasoning_avg": -6.15,
          "overall_avg": 3.07,
          "overall_efficiency": 0.041573,
          "general_efficiency": 0.088745,
          "math_efficiency": 0.062607,
          "code_efficiency": 0.098143,
          "reasoning_efficiency": -0.083207,
          "general_task_scores": [
            1.33,
            15.25,
            4.54,
            5.13
          ],
          "math_task_scores": [
            3.49,
            5.56,
            7.73,
            -2.25,
            8.61
          ],
          "code_task_scores": [
            1.02,
            -0.52,
            3.71,
            12.11,
            7.1,
            20.12
          ],
          "reasoning_task_scores": [
            -15.92,
            -2.08,
            2.18,
            -0.0,
            -14.94
          ]
        },
        "vs_instruct": {
          "general_avg": -8.2,
          "math_avg": -9.38,
          "code_avg": -1.33,
          "reasoning_avg": -5.16,
          "overall_avg": -6.02,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.44,
            -22.22,
            -5.16,
            -7.86
          ],
          "math_task_scores": [
            -8.8,
            -18.34,
            -19.87,
            -5.61,
            5.7
          ],
          "code_task_scores": [
            14.44,
            -3.63,
            0.84,
            -29.86,
            2.34,
            7.92
          ],
          "reasoning_task_scores": [
            -4.74,
            -4.24,
            3.19,
            -0.03,
            -19.98
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 55.02,
      "math_avg": 49.79,
      "code_avg": 46.13,
      "reasoning_avg": 30.79,
      "overall_avg": 45.43,
      "overall_efficiency": 0.067414,
      "general_efficiency": 0.078729,
      "math_efficiency": 0.153728,
      "code_efficiency": 0.139561,
      "reasoning_efficiency": -0.102366,
      "general_scores": [
        65.18,
        44.0975,
        64.11,
        48.3571429,
        63.91,
        44.235,
        63.89,
        47.945,
        64.25,
        42.895,
        63.92,
        47.4807143
      ],
      "math_scores": [
        85.9,
        62.1,
        62.2,
        23.19,
        10.0,
        85.97,
        64.5,
        66.4,
        23.53,
        16.67,
        86.05,
        63.08,
        63.6,
        23.64,
        10.0
      ],
      "code_scores": [
        68.9,
        71.6,
        12.54,
        23.17,
        41.86,
        57.32,
        68.9,
        71.98,
        14.34,
        24.22,
        41.4,
        59.15,
        69.51,
        70.43,
        13.26,
        23.38,
        41.63,
        56.71
      ],
      "reasoning_scores": [
        25.08,
        68.61,
        36.36,
        0.36380435,
        23.74,
        23.39,
        68.52,
        36.87,
        0.36141304,
        26.26,
        22.03,
        68.17,
        36.36,
        0.36597826,
        25.37
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.93
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.59
              },
              {
                "metric": "lcb_test_output",
                "score": 41.63
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.5
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.58,
          "math_avg": 6.99,
          "code_avg": 6.34,
          "reasoning_avg": -4.65,
          "overall_avg": 3.06,
          "overall_efficiency": 0.067414,
          "general_efficiency": 0.078729,
          "math_efficiency": 0.153728,
          "code_efficiency": 0.139561,
          "reasoning_efficiency": -0.102366,
          "general_task_scores": [
            -3.86,
            8.25,
            6.23,
            3.69
          ],
          "math_task_scores": [
            5.99,
            12.11,
            13.87,
            -2.59,
            5.55
          ],
          "code_task_scores": [
            -8.34,
            -0.26,
            5.14,
            22.55,
            4.53,
            14.44
          ],
          "reasoning_task_scores": [
            -13.1,
            -1.03,
            1.68,
            -0.03,
            -10.79
          ]
        },
        "vs_instruct": {
          "general_avg": -11.19,
          "math_avg": -7.03,
          "code_avg": -2.24,
          "reasoning_avg": -3.66,
          "overall_avg": -6.03,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.75,
            -29.22,
            -3.47,
            -9.3
          ],
          "math_task_scores": [
            -6.3,
            -11.79,
            -13.73,
            -5.95,
            2.64
          ],
          "code_task_scores": [
            5.08,
            -3.37,
            2.27,
            -19.42,
            -0.23,
            2.24
          ],
          "reasoning_task_scores": [
            -1.92,
            -3.19,
            2.69,
            -0.06,
            -15.83
          ]
        }
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 56.84,
      "math_avg": 50.81,
      "code_avg": 44.27,
      "reasoning_avg": 31.57,
      "overall_avg": 45.87,
      "overall_efficiency": 0.008366,
      "general_efficiency": 0.012881,
      "math_efficiency": 0.019122,
      "code_efficiency": 0.010707,
      "reasoning_efficiency": -0.009243,
      "general_scores": [
        67.42,
        47.275,
        64.11,
        48.685,
        65.42,
        46.285,
        64.6,
        48.6821429,
        69.08,
        47.0825,
        64.26,
        49.13
      ],
      "math_scores": [
        87.72,
        66.7,
        67.8,
        27.12,
        10.0,
        87.57,
        66.68,
        67.6,
        26.56,
        3.33,
        86.81,
        66.1,
        65.2,
        26.22,
        6.67
      ],
      "code_scores": [
        50.61,
        66.93,
        9.32,
        42.17,
        35.97,
        63.41,
        47.78,
        66.54,
        9.68,
        40.92,
        31.0,
        62.8,
        50.61,
        68.48,
        9.32,
        43.01,
        36.65,
        61.59
      ],
      "reasoning_scores": [
        28.47,
        67.82,
        34.85,
        0.39728261,
        30.42,
        25.76,
        67.91,
        32.83,
        0.40619565,
        30.56,
        23.39,
        67.61,
        31.31,
        0.42152174,
        31.45
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.83
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.67
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 34.54
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 62.6
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.87
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.39,
          "math_avg": 8.0,
          "code_avg": 4.48,
          "reasoning_avg": -3.87,
          "overall_avg": 3.5,
          "overall_efficiency": 0.008366,
          "general_efficiency": 0.012881,
          "math_efficiency": 0.019122,
          "code_efficiency": 0.010707,
          "reasoning_efficiency": -0.009243,
          "general_task_scores": [
            -1.0,
            11.39,
            6.58,
            4.59
          ],
          "math_task_scores": [
            7.39,
            15.37,
            16.67,
            0.59,
            0.0
          ],
          "code_task_scores": [
            -27.77,
            -4.28,
            1.2,
            40.99,
            -2.56,
            19.31
          ],
          "reasoning_task_scores": [
            -10.73,
            -1.68,
            -1.85,
            0.02,
            -5.1
          ]
        },
        "vs_instruct": {
          "general_avg": -9.37,
          "math_avg": -6.01,
          "code_avg": -4.1,
          "reasoning_avg": -2.88,
          "overall_avg": -5.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.11,
            -26.08,
            -3.12,
            -8.4
          ],
          "math_task_scores": [
            -4.9,
            -8.53,
            -10.93,
            -2.77,
            -2.91
          ],
          "code_task_scores": [
            -14.35,
            -7.39,
            -1.67,
            -0.98,
            -7.32,
            7.11
          ],
          "reasoning_task_scores": [
            0.45,
            -3.84,
            -0.84,
            -0.01,
            -10.14
          ]
        }
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "Open-Platypus",
      "domain": "reasoning",
      "general_avg": 55.37,
      "math_avg": 41.51,
      "code_avg": 50.6,
      "reasoning_avg": 26.77,
      "overall_avg": 43.56,
      "overall_efficiency": 0.047817,
      "general_efficiency": 0.157419,
      "math_efficiency": -0.051994,
      "code_efficiency": 0.433951,
      "reasoning_efficiency": -0.34811,
      "general_scores": [
        61.28,
        48.8825,
        61.22,
        50.0907143
      ],
      "math_scores": [
        83.55,
        52.64,
        49.2,
        18.81,
        3.33
      ],
      "code_scores": [
        76.22,
        72.76,
        10.04,
        35.07,
        43.67,
        65.85
      ],
      "reasoning_scores": [
        16.95,
        67.48,
        32.83,
        0.39706522,
        16.17
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.28
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.81
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.07
              },
              {
                "metric": "lcb_test_output",
                "score": 43.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 65.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.95
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.48
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.92,
          "math_avg": -1.3,
          "code_avg": 10.82,
          "reasoning_avg": -8.68,
          "overall_avg": 1.19,
          "overall_efficiency": 0.047817,
          "general_efficiency": 0.157419,
          "math_efficiency": -0.051994,
          "code_efficiency": 0.433951,
          "reasoning_efficiency": -0.34811,
          "general_task_scores": [
            -7.03,
            13.39,
            3.48,
            5.85
          ],
          "math_task_scores": [
            3.57,
            1.52,
            -1.0,
            -7.23,
            -3.34
          ],
          "code_task_scores": [
            -1.22,
            1.16,
            1.8,
            34.03,
            6.57,
            22.56
          ],
          "reasoning_task_scores": [
            -19.65,
            -1.98,
            -2.02,
            0.01,
            -19.74
          ]
        },
        "vs_instruct": {
          "general_avg": -10.84,
          "math_avg": -15.31,
          "code_avg": 2.23,
          "reasoning_avg": -7.69,
          "overall_avg": -7.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.92,
            -24.08,
            -6.22,
            -7.14
          ],
          "math_task_scores": [
            -8.72,
            -22.38,
            -28.6,
            -10.59,
            -6.25
          ],
          "code_task_scores": [
            12.2,
            -1.95,
            -1.07,
            -7.94,
            1.81,
            10.36
          ],
          "reasoning_task_scores": [
            -8.47,
            -4.14,
            -1.01,
            -0.02,
            -24.78
          ]
        }
      },
      "affiliation": "Boston University",
      "year": "2023",
      "size": "24.9k",
      "size_precise": "24926",
      "link": "https://huggingface.co/datasets/garage-bAInd/Open-Platypus",
      "paper_link": "https://arxiv.org/abs/2308.07317",
      "tag": "general,math,code,science"
    },
    {
      "id": 71,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 54.96,
      "math_avg": 61.17,
      "code_avg": 34.68,
      "reasoning_avg": 31.76,
      "overall_avg": 45.64,
      "overall_efficiency": 0.034915,
      "general_efficiency": 0.037495,
      "math_efficiency": 0.19594,
      "code_efficiency": -0.054445,
      "reasoning_efficiency": -0.039331,
      "general_scores": [
        82.73,
        48.36,
        58.93,
        29.8157143
      ],
      "math_scores": [
        93.4,
        81.32,
        82.0,
        32.45,
        16.67
      ],
      "code_scores": [
        29.27,
        74.71,
        8.24,
        27.14,
        16.29,
        52.44
      ],
      "reasoning_scores": [
        18.98,
        67.71,
        20.71,
        0.33913043,
        51.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.73
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.4
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.24
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.14
              },
              {
                "metric": "lcb_test_output",
                "score": 16.29
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 52.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.98
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.51,
          "math_avg": 18.37,
          "code_avg": -5.1,
          "reasoning_avg": -3.69,
          "overall_avg": 3.27,
          "overall_efficiency": 0.034915,
          "general_efficiency": 0.037495,
          "math_efficiency": 0.19594,
          "code_efficiency": -0.054445,
          "reasoning_efficiency": -0.039331,
          "general_task_scores": [
            14.42,
            12.87,
            1.19,
            -14.42
          ],
          "math_task_scores": [
            13.42,
            30.2,
            31.8,
            6.41,
            10.0
          ],
          "code_task_scores": [
            -48.17,
            3.11,
            0.0,
            26.1,
            -20.81,
            9.15
          ],
          "reasoning_task_scores": [
            -17.62,
            -1.75,
            -14.14,
            -0.05,
            15.13
          ]
        },
        "vs_instruct": {
          "general_avg": -11.25,
          "math_avg": 4.35,
          "code_avg": -13.69,
          "reasoning_avg": -2.7,
          "overall_avg": -5.82,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            15.53,
            -24.6,
            -8.51,
            -27.41
          ],
          "math_task_scores": [
            1.13,
            6.3,
            4.2,
            3.05,
            7.09
          ],
          "code_task_scores": [
            -34.75,
            0.0,
            -2.87,
            -15.87,
            -25.57,
            -3.05
          ],
          "reasoning_task_scores": [
            -6.44,
            -3.91,
            -13.13,
            -0.08,
            10.09
          ]
        }
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 72,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 59.77,
      "math_avg": 58.89,
      "code_avg": 43.11,
      "reasoning_avg": 37.29,
      "overall_avg": 49.76,
      "overall_efficiency": 0.055552,
      "general_efficiency": 0.062527,
      "math_efficiency": 0.120862,
      "code_efficiency": 0.024968,
      "reasoning_efficiency": 0.013853,
      "general_scores": [
        83.74,
        42.26,
        59.1,
        53.9678571
      ],
      "math_scores": [
        93.93,
        77.52,
        77.4,
        27.26,
        18.335
      ],
      "code_scores": [
        63.41,
        73.15,
        11.47,
        44.05,
        4.98,
        61.59
      ],
      "reasoning_scores": [
        34.24,
        71.11,
        26.77,
        0.34336957,
        53.9678571
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.05
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 61.59
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.32,
          "math_avg": 16.09,
          "code_avg": 3.32,
          "reasoning_avg": 1.84,
          "overall_avg": 7.39,
          "overall_efficiency": 0.055552,
          "general_efficiency": 0.062527,
          "math_efficiency": 0.120862,
          "code_efficiency": 0.024968,
          "reasoning_efficiency": 0.013853,
          "general_task_scores": [
            15.43,
            6.77,
            1.36,
            9.73
          ],
          "math_task_scores": [
            13.95,
            26.4,
            27.2,
            1.22,
            11.67
          ],
          "code_task_scores": [
            -14.03,
            1.55,
            3.23,
            43.01,
            -32.12,
            18.3
          ],
          "reasoning_task_scores": [
            -2.36,
            1.65,
            -8.08,
            -0.05,
            18.06
          ]
        },
        "vs_instruct": {
          "general_avg": -6.44,
          "math_avg": 2.07,
          "code_avg": -5.26,
          "reasoning_avg": 2.84,
          "overall_avg": -1.7,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.54,
            -30.7,
            -8.34,
            -3.26
          ],
          "math_task_scores": [
            1.66,
            2.5,
            -0.4,
            -2.14,
            8.76
          ],
          "code_task_scores": [
            -0.61,
            -1.56,
            0.36,
            1.04,
            -36.88,
            6.1
          ],
          "reasoning_task_scores": [
            8.82,
            -0.51,
            -7.07,
            -0.08,
            13.02
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 73,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 59.48,
      "math_avg": 56.8,
      "code_avg": 31.62,
      "reasoning_avg": 30.56,
      "overall_avg": 44.62,
      "overall_efficiency": 0.016282,
      "general_efficiency": 0.05821,
      "math_efficiency": 0.101471,
      "code_efficiency": -0.059203,
      "reasoning_efficiency": -0.035351,
      "general_scores": [
        83.44,
        41.2775,
        62.55,
        50.6421429
      ],
      "math_scores": [
        92.12,
        73.3,
        75.2,
        25.07,
        18.335
      ],
      "code_scores": [
        4.27,
        72.37,
        13.26,
        34.03,
        1.13,
        64.63
      ],
      "reasoning_scores": [
        23.73,
        69.8,
        19.19,
        0.3398913,
        39.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.8
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.03,
          "math_avg": 14.0,
          "code_avg": -8.17,
          "reasoning_avg": -4.88,
          "overall_avg": 2.25,
          "overall_efficiency": 0.016282,
          "general_efficiency": 0.05821,
          "math_efficiency": 0.101471,
          "code_efficiency": -0.059203,
          "reasoning_efficiency": -0.035351,
          "general_task_scores": [
            15.13,
            5.79,
            4.81,
            6.4
          ],
          "math_task_scores": [
            12.14,
            22.18,
            25.0,
            -0.97,
            11.67
          ],
          "code_task_scores": [
            -73.17,
            0.77,
            5.02,
            32.99,
            -35.97,
            21.34
          ],
          "reasoning_task_scores": [
            -12.87,
            0.34,
            -15.66,
            -0.05,
            3.85
          ]
        },
        "vs_instruct": {
          "general_avg": -6.73,
          "math_avg": -0.01,
          "code_avg": -16.75,
          "reasoning_avg": -3.89,
          "overall_avg": -6.84,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.24,
            -31.68,
            -4.89,
            -6.59
          ],
          "math_task_scores": [
            -0.15,
            -1.72,
            -2.6,
            -4.33,
            8.76
          ],
          "code_task_scores": [
            -59.75,
            -2.34,
            2.15,
            -8.98,
            -40.73,
            9.14
          ],
          "reasoning_task_scores": [
            -1.69,
            -1.82,
            -14.65,
            -0.08,
            -1.19
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 74,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 34.77,
      "math_avg": 53.61,
      "code_avg": 33.73,
      "reasoning_avg": 34.09,
      "overall_avg": 39.05,
      "overall_efficiency": -0.198577,
      "general_efficiency": -0.997627,
      "math_efficiency": 0.646858,
      "code_efficiency": -0.362558,
      "reasoning_efficiency": -0.08098,
      "general_scores": [
        62.4,
        27.34,
        29.0,
        20.3564286
      ],
      "math_scores": [
        85.44,
        71.94,
        72.0,
        25.34,
        13.335
      ],
      "code_scores": [
        57.32,
        62.26,
        13.62,
        14.82,
        0.68,
        53.66
      ],
      "reasoning_scores": [
        72.2,
        41.41,
        19.19,
        0.25608696,
        37.39
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.94
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 62.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.82
              },
              {
                "metric": "lcb_test_output",
                "score": 0.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 53.66
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.39
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -16.67,
          "math_avg": 10.81,
          "code_avg": -6.06,
          "reasoning_avg": -1.35,
          "overall_avg": -3.32,
          "overall_efficiency": -0.198577,
          "general_efficiency": -0.997627,
          "math_efficiency": 0.646858,
          "code_efficiency": -0.362558,
          "reasoning_efficiency": -0.08098,
          "general_task_scores": [
            -5.91,
            -8.15,
            -28.74,
            -23.88
          ],
          "math_task_scores": [
            5.46,
            20.82,
            21.8,
            -0.7,
            6.67
          ],
          "code_task_scores": [
            -20.12,
            -9.34,
            5.38,
            13.78,
            -36.42,
            10.37
          ],
          "reasoning_task_scores": [
            35.6,
            -28.05,
            -15.66,
            -0.13,
            1.48
          ]
        },
        "vs_instruct": {
          "general_avg": -31.43,
          "math_avg": -3.2,
          "code_avg": -14.64,
          "reasoning_avg": -0.36,
          "overall_avg": -12.41,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.8,
            -45.62,
            -38.44,
            -36.87
          ],
          "math_task_scores": [
            -6.83,
            -3.08,
            -5.8,
            -4.06,
            3.76
          ],
          "code_task_scores": [
            -6.7,
            -12.45,
            2.51,
            -28.19,
            -41.18,
            -1.83
          ],
          "reasoning_task_scores": [
            46.78,
            -30.21,
            -14.65,
            -0.16,
            -3.56
          ]
        }
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "size_precise": "16710",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k",
      "paper_link": "https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation",
      "tag": "general,code,math,science"
    },
    {
      "id": 75,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 59.08,
      "math_avg": 63.06,
      "code_avg": 46.83,
      "reasoning_avg": 35.67,
      "overall_avg": 51.16,
      "overall_efficiency": 0.077146,
      "general_efficiency": 0.067046,
      "math_efficiency": 0.177764,
      "code_efficiency": 0.061807,
      "reasoning_efficiency": 0.001964,
      "general_scores": [
        85.96,
        41.2125,
        53.6,
        55.5671429
      ],
      "math_scores": [
        92.27,
        83.94,
        83.6,
        32.99,
        22.4975
      ],
      "code_scores": [
        68.9,
        73.93,
        17.2,
        32.36,
        19.68,
        68.9
      ],
      "reasoning_scores": [
        20.0,
        72.38,
        32.32,
        0.37130435,
        53.26
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.94
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.5
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 17.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.36
              },
              {
                "metric": "lcb_test_output",
                "score": 19.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 72.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.26
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.64,
          "math_avg": 20.26,
          "code_avg": 7.04,
          "reasoning_avg": 0.22,
          "overall_avg": 8.79,
          "overall_efficiency": 0.077146,
          "general_efficiency": 0.067046,
          "math_efficiency": 0.177764,
          "code_efficiency": 0.061807,
          "reasoning_efficiency": 0.001964,
          "general_task_scores": [
            17.65,
            5.72,
            -4.14,
            11.33
          ],
          "math_task_scores": [
            12.29,
            32.82,
            33.4,
            6.95,
            15.83
          ],
          "code_task_scores": [
            -8.54,
            2.33,
            8.96,
            31.32,
            -17.42,
            25.61
          ],
          "reasoning_task_scores": [
            -16.6,
            2.92,
            -2.53,
            -0.02,
            17.35
          ]
        },
        "vs_instruct": {
          "general_avg": -7.12,
          "math_avg": 6.24,
          "code_avg": -1.54,
          "reasoning_avg": 1.22,
          "overall_avg": -0.3,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            18.76,
            -31.75,
            -13.84,
            -1.66
          ],
          "math_task_scores": [
            0.0,
            8.92,
            5.8,
            3.59,
            12.92
          ],
          "code_task_scores": [
            4.88,
            -0.78,
            6.09,
            -10.65,
            -22.18,
            13.41
          ],
          "reasoning_task_scores": [
            -5.42,
            0.76,
            -1.52,
            -0.05,
            12.31
          ]
        }
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,code,math,science"
    },
    {
      "id": 76,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 57.12,
      "math_avg": 47.85,
      "code_avg": 48.45,
      "reasoning_avg": 30.31,
      "overall_avg": 45.93,
      "overall_efficiency": 0.020759,
      "general_efficiency": 0.033046,
      "math_efficiency": 0.02942,
      "code_efficiency": 0.050481,
      "reasoning_efficiency": -0.029911,
      "general_scores": [
        68.95,
        46.615,
        63.27,
        49.6321429
      ],
      "math_scores": [
        87.41,
        61.2,
        60.0,
        23.98,
        6.67
      ],
      "code_scores": [
        65.24,
        68.48,
        11.11,
        38.41,
        43.44,
        64.02
      ],
      "reasoning_scores": [
        21.02,
        65.23,
        33.33,
        0.36141304,
        31.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.98
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 43.44
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.02
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.67,
          "math_avg": 5.05,
          "code_avg": 8.66,
          "reasoning_avg": -5.13,
          "overall_avg": 3.56,
          "overall_efficiency": 0.020759,
          "general_efficiency": 0.033046,
          "math_efficiency": 0.02942,
          "code_efficiency": 0.050481,
          "reasoning_efficiency": -0.029911,
          "general_task_scores": [
            0.64,
            11.13,
            5.53,
            5.39
          ],
          "math_task_scores": [
            7.43,
            10.08,
            9.8,
            -2.06,
            0.0
          ],
          "code_task_scores": [
            -12.2,
            -3.12,
            2.87,
            37.37,
            6.34,
            20.73
          ],
          "reasoning_task_scores": [
            -15.58,
            -4.23,
            -1.52,
            -0.03,
            -4.31
          ]
        },
        "vs_instruct": {
          "general_avg": -9.09,
          "math_avg": -8.96,
          "code_avg": 0.08,
          "reasoning_avg": -4.14,
          "overall_avg": -5.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.75,
            -26.34,
            -4.17,
            -7.6
          ],
          "math_task_scores": [
            -4.86,
            -13.82,
            -17.8,
            -5.42,
            -2.91
          ],
          "code_task_scores": [
            1.22,
            -6.23,
            0.0,
            -4.6,
            1.58,
            8.53
          ],
          "reasoning_task_scores": [
            -4.4,
            -6.39,
            -0.51,
            -0.06,
            -9.35
          ]
        }
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 77,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 56.34,
      "math_avg": 54.59,
      "code_avg": 43.18,
      "reasoning_avg": 45.69,
      "overall_avg": 49.95,
      "overall_efficiency": 0.097621,
      "general_efficiency": 0.063067,
      "math_efficiency": 0.151792,
      "code_efficiency": 0.04366,
      "reasoning_efficiency": 0.131965,
      "general_scores": [
        77.74,
        37.1225,
        60.81,
        49.7028571
      ],
      "math_scores": [
        90.45,
        73.5,
        74.2,
        24.82,
        10.0
      ],
      "code_scores": [
        67.07,
        74.71,
        12.54,
        11.27,
        25.79,
        67.68
      ],
      "reasoning_scores": [
        80.68,
        72.49,
        33.84,
        0.36054348,
        41.1
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.45
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.27
              },
              {
                "metric": "lcb_test_output",
                "score": 25.79
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.68
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 72.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.1
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.9,
          "math_avg": 11.79,
          "code_avg": 3.39,
          "reasoning_avg": 10.25,
          "overall_avg": 7.58,
          "overall_efficiency": 0.097621,
          "general_efficiency": 0.063067,
          "math_efficiency": 0.151792,
          "code_efficiency": 0.04366,
          "reasoning_efficiency": 0.131965,
          "general_task_scores": [
            9.43,
            1.63,
            3.07,
            5.46
          ],
          "math_task_scores": [
            10.47,
            22.38,
            24.0,
            -1.22,
            3.33
          ],
          "code_task_scores": [
            -10.37,
            3.11,
            4.3,
            10.23,
            -11.31,
            24.39
          ],
          "reasoning_task_scores": [
            44.08,
            3.03,
            -1.01,
            -0.03,
            5.19
          ]
        },
        "vs_instruct": {
          "general_avg": -9.86,
          "math_avg": -2.22,
          "code_avg": -5.19,
          "reasoning_avg": 11.24,
          "overall_avg": -1.51,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            10.54,
            -35.84,
            -6.63,
            -7.53
          ],
          "math_task_scores": [
            -1.82,
            -1.52,
            -3.6,
            -4.58,
            0.42
          ],
          "code_task_scores": [
            3.05,
            0.0,
            1.43,
            -31.74,
            -16.07,
            12.19
          ],
          "reasoning_task_scores": [
            55.26,
            0.87,
            0.0,
            -0.06,
            0.15
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 78,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 55.47,
      "math_avg": 53.37,
      "code_avg": 38.99,
      "reasoning_avg": 31.37,
      "overall_avg": 44.8,
      "overall_efficiency": 7.435759,
      "general_efficiency": 12.313511,
      "math_efficiency": 32.305811,
      "code_efficiency": -2.43629,
      "reasoning_efficiency": -12.439994,
      "general_scores": [
        64.66,
        49.0625,
        59.26,
        48.90143
      ],
      "math_scores": [
        86.66,
        69.4,
        69.6,
        26.17,
        15.0
      ],
      "code_scores": [
        0.0,
        72.76,
        13.26,
        44.05,
        45.93,
        57.93
      ],
      "reasoning_scores": [
        18.98,
        70.74,
        34.85,
        0.402609,
        31.9
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.66
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.4
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.17
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.05
              },
              {
                "metric": "lcb_test_output",
                "score": 45.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 57.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.98
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.74
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.9
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.03,
          "math_avg": 10.56,
          "code_avg": -0.8,
          "reasoning_avg": -4.07,
          "overall_avg": 2.43,
          "overall_efficiency": 7.435759,
          "general_efficiency": 12.313511,
          "math_efficiency": 32.305811,
          "code_efficiency": -2.43629,
          "reasoning_efficiency": -12.439994,
          "general_task_scores": [
            -3.65,
            13.57,
            1.52,
            4.66
          ],
          "math_task_scores": [
            6.68,
            18.28,
            19.4,
            0.13,
            8.33
          ],
          "code_task_scores": [
            -77.44,
            1.16,
            5.02,
            43.01,
            8.83,
            14.64
          ],
          "reasoning_task_scores": [
            -17.62,
            1.28,
            0.0,
            0.01,
            -4.01
          ]
        },
        "vs_instruct": {
          "general_avg": -10.74,
          "math_avg": -3.45,
          "code_avg": -9.38,
          "reasoning_avg": -3.08,
          "overall_avg": -6.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.54,
            -23.9,
            -8.18,
            -8.33
          ],
          "math_task_scores": [
            -5.61,
            -5.62,
            -8.2,
            -3.23,
            5.42
          ],
          "code_task_scores": [
            -64.02,
            -1.95,
            2.15,
            1.04,
            4.07,
            2.44
          ],
          "reasoning_task_scores": [
            -6.44,
            -0.88,
            1.01,
            -0.02,
            -9.05
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 79,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 58.87,
      "math_avg": 56.35,
      "code_avg": 17.99,
      "reasoning_avg": 42.43,
      "overall_avg": 43.91,
      "overall_efficiency": 0.024493,
      "general_efficiency": 0.117995,
      "math_efficiency": 0.215335,
      "code_efficiency": -0.346339,
      "reasoning_efficiency": 0.11098,
      "general_scores": [
        83.76,
        42.22,
        56.09,
        53.4071429
      ],
      "math_scores": [
        91.28,
        74.9,
        76.4,
        29.18,
        10.0
      ],
      "code_scores": [
        0.0,
        72.37,
        10.04,
        16.49,
        9.05,
        0.0
      ],
      "reasoning_scores": [
        68.81,
        74.06,
        26.77,
        0.34913043,
        42.14
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.49
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 74.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.14
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.42,
          "math_avg": 13.55,
          "code_avg": -21.79,
          "reasoning_avg": 6.98,
          "overall_avg": 1.54,
          "overall_efficiency": 0.024493,
          "general_efficiency": 0.117995,
          "math_efficiency": 0.215335,
          "code_efficiency": -0.346339,
          "reasoning_efficiency": 0.11098,
          "general_task_scores": [
            15.45,
            6.73,
            -1.65,
            9.17
          ],
          "math_task_scores": [
            11.3,
            23.78,
            26.2,
            3.14,
            3.33
          ],
          "code_task_scores": [
            -77.44,
            0.77,
            1.8,
            15.45,
            -28.05,
            -43.29
          ],
          "reasoning_task_scores": [
            32.21,
            4.6,
            -8.08,
            -0.04,
            6.23
          ]
        },
        "vs_instruct": {
          "general_avg": -7.34,
          "math_avg": -0.46,
          "code_avg": -30.38,
          "reasoning_avg": 7.97,
          "overall_avg": -7.55,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.56,
            -30.74,
            -11.35,
            -3.82
          ],
          "math_task_scores": [
            -0.99,
            -0.12,
            -1.4,
            -0.22,
            0.42
          ],
          "code_task_scores": [
            -64.02,
            -2.34,
            -1.07,
            -26.52,
            -32.81,
            -55.49
          ],
          "reasoning_task_scores": [
            43.39,
            2.44,
            -7.07,
            -0.07,
            1.19
          ]
        }
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 80,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 54.72,
      "math_avg": 55.22,
      "code_avg": 36.69,
      "reasoning_avg": 43.5,
      "overall_avg": 47.53,
      "overall_efficiency": 0.020662,
      "general_efficiency": 0.013092,
      "math_efficiency": 0.049684,
      "code_efficiency": -0.012378,
      "reasoning_efficiency": 0.032249,
      "general_scores": [
        81.75,
        34.23,
        57.49,
        45.3957143
      ],
      "math_scores": [
        88.48,
        73.58,
        72.4,
        29.97,
        11.665
      ],
      "code_scores": [
        52.44,
        70.43,
        7.53,
        36.95,
        1.58,
        51.22
      ],
      "reasoning_scores": [
        88.14,
        70.82,
        19.19,
        0.34097826,
        39.02
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.75
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.23
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.48
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.43
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.95
              },
              {
                "metric": "lcb_test_output",
                "score": 1.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 51.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.14
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.02
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.27,
          "math_avg": 12.42,
          "code_avg": -3.09,
          "reasoning_avg": 8.06,
          "overall_avg": 5.16,
          "overall_efficiency": 0.020662,
          "general_efficiency": 0.013092,
          "math_efficiency": 0.049684,
          "code_efficiency": -0.012378,
          "reasoning_efficiency": 0.032249,
          "general_task_scores": [
            13.44,
            -1.26,
            -0.25,
            1.16
          ],
          "math_task_scores": [
            8.5,
            22.46,
            22.2,
            3.93,
            4.99
          ],
          "code_task_scores": [
            -25.0,
            -1.17,
            -0.71,
            35.91,
            -35.52,
            7.93
          ],
          "reasoning_task_scores": [
            51.54,
            1.36,
            -15.66,
            -0.05,
            3.11
          ]
        },
        "vs_instruct": {
          "general_avg": -11.49,
          "math_avg": -1.6,
          "code_avg": -11.67,
          "reasoning_avg": 9.05,
          "overall_avg": -3.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            14.55,
            -38.73,
            -9.95,
            -11.83
          ],
          "math_task_scores": [
            -3.79,
            -1.44,
            -5.4,
            0.57,
            2.08
          ],
          "code_task_scores": [
            -11.58,
            -4.28,
            -3.58,
            -6.06,
            -40.28,
            -4.27
          ],
          "reasoning_task_scores": [
            62.72,
            -0.8,
            -14.65,
            -0.08,
            -1.93
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 61.08,
      "math_avg": 41.59,
      "code_avg": 49.63,
      "reasoning_avg": 34.32,
      "overall_avg": 46.65,
      "overall_efficiency": 0.028577,
      "general_efficiency": 0.064228,
      "math_efficiency": -0.008087,
      "code_efficiency": 0.065634,
      "reasoning_efficiency": -0.007467,
      "general_scores": [
        78.37,
        55.5325,
        63.03,
        47.3821429
      ],
      "math_scores": [
        90.14,
        49.88,
        47.2,
        19.06,
        1.665
      ],
      "code_scores": [
        77.44,
        74.71,
        15.41,
        24.63,
        33.03,
        72.56
      ],
      "reasoning_scores": [
        57.63,
        65.52,
        28.79,
        0.38163043,
        19.29
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 15.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.63
              },
              {
                "metric": "lcb_test_output",
                "score": 33.03
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 72.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.52
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.79
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.63,
          "math_avg": -1.21,
          "code_avg": 9.84,
          "reasoning_avg": -1.12,
          "overall_avg": 4.29,
          "overall_efficiency": 0.028577,
          "general_efficiency": 0.064228,
          "math_efficiency": -0.008087,
          "code_efficiency": 0.065634,
          "reasoning_efficiency": -0.007467,
          "general_task_scores": [
            10.06,
            20.04,
            5.29,
            3.14
          ],
          "math_task_scores": [
            10.16,
            -1.24,
            -3.0,
            -6.98,
            -5.01
          ],
          "code_task_scores": [
            0.0,
            3.11,
            7.17,
            23.59,
            -4.07,
            29.27
          ],
          "reasoning_task_scores": [
            21.03,
            -3.94,
            -6.06,
            -0.01,
            -16.62
          ]
        },
        "vs_instruct": {
          "general_avg": -5.13,
          "math_avg": -15.23,
          "code_avg": 1.26,
          "reasoning_avg": -0.13,
          "overall_avg": -4.81,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            11.17,
            -17.43,
            -4.41,
            -9.85
          ],
          "math_task_scores": [
            -2.13,
            -25.14,
            -30.6,
            -10.34,
            -7.92
          ],
          "code_task_scores": [
            13.42,
            0.0,
            4.3,
            -18.38,
            -8.83,
            17.07
          ],
          "reasoning_task_scores": [
            32.21,
            -6.1,
            -5.05,
            -0.04,
            -21.66
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 58.39,
      "math_avg": 55.09,
      "code_avg": 37.19,
      "reasoning_avg": 30.18,
      "overall_avg": 45.21,
      "overall_efficiency": 0.011364,
      "general_efficiency": 0.027773,
      "math_efficiency": 0.049152,
      "code_efficiency": -0.010397,
      "reasoning_efficiency": -0.021072,
      "general_scores": [
        88.1,
        41.1025,
        54.18,
        50.16
      ],
      "math_scores": [
        88.48,
        73.58,
        72.4,
        24.3,
        16.67
      ],
      "code_scores": [
        33.54,
        74.32,
        6.09,
        30.48,
        4.3,
        74.39
      ],
      "reasoning_scores": [
        26.44,
        69.28,
        15.66,
        0.33086957,
        39.17
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.48
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 4.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 74.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.66
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.94,
          "math_avg": 12.28,
          "code_avg": -2.6,
          "reasoning_avg": -5.27,
          "overall_avg": 2.84,
          "overall_efficiency": 0.011364,
          "general_efficiency": 0.027773,
          "math_efficiency": 0.049152,
          "code_efficiency": -0.010397,
          "reasoning_efficiency": -0.021072,
          "general_task_scores": [
            19.79,
            5.61,
            -3.56,
            5.92
          ],
          "math_task_scores": [
            8.5,
            22.46,
            22.2,
            -1.74,
            10.0
          ],
          "code_task_scores": [
            -43.9,
            2.72,
            -2.15,
            29.44,
            -32.8,
            31.1
          ],
          "reasoning_task_scores": [
            -10.16,
            -0.18,
            -19.19,
            -0.06,
            3.26
          ]
        },
        "vs_instruct": {
          "general_avg": -7.82,
          "math_avg": -1.73,
          "code_avg": -11.18,
          "reasoning_avg": -4.27,
          "overall_avg": -6.25,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            20.9,
            -31.86,
            -13.26,
            -7.07
          ],
          "math_task_scores": [
            -3.79,
            -1.44,
            -5.4,
            -5.1,
            7.09
          ],
          "code_task_scores": [
            -30.48,
            -0.39,
            -5.02,
            -12.53,
            -37.56,
            18.9
          ],
          "reasoning_task_scores": [
            1.02,
            -2.34,
            -18.18,
            -0.09,
            -1.78
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 44.14,
      "math_avg": 42.68,
      "code_avg": 35.44,
      "reasoning_avg": 28.29,
      "overall_avg": 37.64,
      "overall_efficiency": -0.237197,
      "general_efficiency": -0.366173,
      "math_efficiency": -0.005866,
      "code_efficiency": -0.218111,
      "reasoning_efficiency": -0.358638,
      "general_scores": [
        28.43,
        44.7325,
        60.51,
        42.8935714
      ],
      "math_scores": [
        85.75,
        52.66,
        52.4,
        16.78,
        5.835
      ],
      "code_scores": [
        0.0,
        71.6,
        9.68,
        27.77,
        37.1,
        66.46
      ],
      "reasoning_scores": [
        22.71,
        67.97,
        31.31,
        0.31858696,
        19.14
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.75
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.84
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.77
              },
              {
                "metric": "lcb_test_output",
                "score": 37.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.14
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -7.3,
          "math_avg": -0.12,
          "code_avg": -4.35,
          "reasoning_avg": -7.15,
          "overall_avg": -4.73,
          "overall_efficiency": -0.237197,
          "general_efficiency": -0.366173,
          "math_efficiency": -0.005866,
          "code_efficiency": -0.218111,
          "reasoning_efficiency": -0.358638,
          "general_task_scores": [
            -39.88,
            9.24,
            2.77,
            -1.35
          ],
          "math_task_scores": [
            5.77,
            1.54,
            2.2,
            -9.26,
            -0.83
          ],
          "code_task_scores": [
            -77.44,
            0.0,
            1.44,
            26.73,
            0.0,
            23.17
          ],
          "reasoning_task_scores": [
            -13.89,
            -1.49,
            -3.54,
            -0.07,
            -16.77
          ]
        },
        "vs_instruct": {
          "general_avg": -22.07,
          "math_avg": -14.13,
          "code_avg": -12.93,
          "reasoning_avg": -6.16,
          "overall_avg": -13.82,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.77,
            -28.23,
            -6.93,
            -14.34
          ],
          "math_task_scores": [
            -6.52,
            -22.36,
            -25.4,
            -12.62,
            -3.74
          ],
          "code_task_scores": [
            -64.02,
            -3.11,
            -1.43,
            -15.24,
            -4.76,
            10.97
          ],
          "reasoning_task_scores": [
            -2.71,
            -3.65,
            -2.53,
            -0.1,
            -21.81
          ]
        }
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,code,math,science"
    },
    {
      "id": 84,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 55.11,
      "math_avg": 53.73,
      "code_avg": 30.95,
      "reasoning_avg": 32.1,
      "overall_avg": 42.97,
      "overall_efficiency": 0.030533,
      "general_efficiency": 0.186053,
      "math_efficiency": 0.55438,
      "code_efficiency": -0.448471,
      "reasoning_efficiency": -0.169831,
      "general_scores": [
        62.65,
        42.0875,
        64.49,
        51.2142857
      ],
      "math_scores": [
        89.76,
        67.82,
        71.2,
        27.35,
        12.4975
      ],
      "code_scores": [
        15.24,
        72.76,
        11.47,
        44.26,
        10.86,
        31.1
      ],
      "reasoning_scores": [
        24.75,
        70.33,
        30.3,
        0.38021739,
        34.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.26
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 31.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.67,
          "math_avg": 10.92,
          "code_avg": -8.84,
          "reasoning_avg": -3.35,
          "overall_avg": 0.6,
          "overall_efficiency": 0.030533,
          "general_efficiency": 0.186053,
          "math_efficiency": 0.55438,
          "code_efficiency": -0.448471,
          "reasoning_efficiency": -0.169831,
          "general_task_scores": [
            -5.66,
            6.6,
            6.75,
            6.97
          ],
          "math_task_scores": [
            9.78,
            16.7,
            21.0,
            1.31,
            5.83
          ],
          "code_task_scores": [
            -62.2,
            1.16,
            3.23,
            43.22,
            -26.24,
            -12.19
          ],
          "reasoning_task_scores": [
            -11.85,
            0.87,
            -4.55,
            -0.01,
            -1.19
          ]
        },
        "vs_instruct": {
          "general_avg": -11.1,
          "math_avg": -3.09,
          "code_avg": -17.42,
          "reasoning_avg": -2.35,
          "overall_avg": -8.49,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.55,
            -30.87,
            -2.95,
            -6.02
          ],
          "math_task_scores": [
            -2.51,
            -7.2,
            -6.6,
            -2.05,
            2.92
          ],
          "code_task_scores": [
            -48.78,
            -1.95,
            0.36,
            1.25,
            -31.0,
            -24.39
          ],
          "reasoning_task_scores": [
            -0.67,
            -1.29,
            -3.54,
            -0.04,
            -6.23
          ]
        }
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 85,
      "name": "SCP-116K",
      "domain": "reasoning",
      "general_avg": 53.17,
      "math_avg": 61.32,
      "code_avg": 28.13,
      "reasoning_avg": 44.98,
      "overall_avg": 46.9,
      "overall_efficiency": 0.016521,
      "general_efficiency": 0.006302,
      "math_efficiency": 0.06753,
      "code_efficiency": -0.042517,
      "reasoning_efficiency": 0.034771,
      "general_scores": [
        83.48,
        33.5275,
        45.52,
        50.1614286
      ],
      "math_scores": [
        90.52,
        76.78,
        74.2,
        31.75,
        33.3325
      ],
      "code_scores": [
        42.68,
        60.7,
        10.04,
        15.66,
        2.49,
        37.2
      ],
      "reasoning_scores": [
        87.12,
        71.47,
        23.74,
        0.26728261,
        42.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 60.7
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 15.66
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 37.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.12
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.73,
          "math_avg": 18.51,
          "code_avg": -11.66,
          "reasoning_avg": 9.53,
          "overall_avg": 4.53,
          "overall_efficiency": 0.016521,
          "general_efficiency": 0.006302,
          "math_efficiency": 0.06753,
          "code_efficiency": -0.042517,
          "reasoning_efficiency": 0.034771,
          "general_task_scores": [
            15.17,
            -1.96,
            -12.22,
            5.92
          ],
          "math_task_scores": [
            10.54,
            25.66,
            24.0,
            5.71,
            26.66
          ],
          "code_task_scores": [
            -34.76,
            -10.9,
            1.8,
            14.62,
            -34.61,
            -6.09
          ],
          "reasoning_task_scores": [
            50.52,
            2.01,
            -11.11,
            -0.12,
            6.37
          ]
        },
        "vs_instruct": {
          "general_avg": -13.04,
          "math_avg": 4.5,
          "code_avg": -20.24,
          "reasoning_avg": 10.52,
          "overall_avg": -4.56,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.28,
            -39.43,
            -21.92,
            -7.07
          ],
          "math_task_scores": [
            -1.75,
            1.76,
            -3.6,
            2.35,
            23.75
          ],
          "code_task_scores": [
            -21.34,
            -14.01,
            -1.07,
            -27.35,
            -39.37,
            -18.29
          ],
          "reasoning_task_scores": [
            61.7,
            -0.15,
            -10.1,
            -0.15,
            1.33
          ]
        }
      },
      "affiliation": "Fudan University",
      "year": "2025",
      "size": "274k",
      "size_precise": "274166",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K",
      "paper_link": "https://arxiv.org/abs/2501.15587",
      "tag": "general,math,science"
    },
    {
      "id": 86,
      "name": "AM-Thinking-v1-Distilled-code",
      "domain": "reasoning",
      "general_avg": 48.09,
      "math_avg": 56.68,
      "code_avg": 50.63,
      "reasoning_avg": 38.35,
      "overall_avg": 48.44,
      "overall_efficiency": 0.01874,
      "general_efficiency": -0.010349,
      "math_efficiency": 0.042854,
      "code_efficiency": 0.033471,
      "reasoning_efficiency": 0.008983,
      "general_scores": [
        84.0,
        32.945,
        38.32,
        37.1014286
      ],
      "math_scores": [
        88.1,
        70.66,
        69.4,
        31.93,
        23.335
      ],
      "code_scores": [
        78.05,
        78.99,
        40.86,
        26.1,
        2.94,
        76.83
      ],
      "reasoning_scores": [
        82.37,
        55.88,
        11.11,
        0.26358696,
        42.14
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 78.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 40.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.1
              },
              {
                "metric": "lcb_test_output",
                "score": 2.94
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 76.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.88
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.14
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -3.35,
          "math_avg": 13.88,
          "code_avg": 10.84,
          "reasoning_avg": 2.91,
          "overall_avg": 6.07,
          "overall_efficiency": 0.01874,
          "general_efficiency": -0.010349,
          "math_efficiency": 0.042854,
          "code_efficiency": 0.033471,
          "reasoning_efficiency": 0.008983,
          "general_task_scores": [
            15.69,
            -2.55,
            -19.42,
            -7.14
          ],
          "math_task_scores": [
            8.12,
            19.54,
            19.2,
            5.89,
            16.67
          ],
          "code_task_scores": [
            0.61,
            7.39,
            32.62,
            25.06,
            -34.16,
            33.54
          ],
          "reasoning_task_scores": [
            45.77,
            -13.58,
            -23.74,
            -0.13,
            6.23
          ]
        },
        "vs_instruct": {
          "general_avg": -18.12,
          "math_avg": -0.13,
          "code_avg": 2.26,
          "reasoning_avg": 3.9,
          "overall_avg": -3.02,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.8,
            -40.02,
            -29.12,
            -20.13
          ],
          "math_task_scores": [
            -4.17,
            -4.36,
            -8.4,
            2.53,
            13.76
          ],
          "code_task_scores": [
            14.03,
            4.28,
            29.75,
            -16.91,
            -38.92,
            21.34
          ],
          "reasoning_task_scores": [
            56.95,
            -15.74,
            -22.73,
            -0.16,
            1.19
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "324k",
      "size_precise": "323965",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/code.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "code"
    },
    {
      "id": 87,
      "name": "AM-Thinking-v1-Distilled-math",
      "domain": "reasoning",
      "general_avg": 53.2,
      "math_avg": 81.19,
      "code_avg": 31.66,
      "reasoning_avg": 43.26,
      "overall_avg": 52.33,
      "overall_efficiency": 0.017843,
      "general_efficiency": 0.003137,
      "math_efficiency": 0.068787,
      "code_efficiency": -0.014561,
      "reasoning_efficiency": 0.01401,
      "general_scores": [
        90.67,
        30.2025,
        43.37,
        48.5385714
      ],
      "math_scores": [
        94.16,
        95.06,
        96.2,
        60.55,
        60.0
      ],
      "code_scores": [
        48.17,
        57.98,
        8.96,
        10.44,
        21.72,
        42.68
      ],
      "reasoning_scores": [
        53.22,
        72.14,
        17.68,
        0.2676087,
        73.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.2
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.37
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.54
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 94.16
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 95.06
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 96.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 60.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 10.44
              },
              {
                "metric": "lcb_test_output",
                "score": 21.72
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 42.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 72.14
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.75,
          "math_avg": 38.39,
          "code_avg": -8.13,
          "reasoning_avg": 7.82,
          "overall_avg": 9.96,
          "overall_efficiency": 0.017843,
          "general_efficiency": 0.003137,
          "math_efficiency": 0.068787,
          "code_efficiency": -0.014561,
          "reasoning_efficiency": 0.01401,
          "general_task_scores": [
            22.36,
            -5.29,
            -14.37,
            4.3
          ],
          "math_task_scores": [
            14.18,
            43.94,
            46.0,
            34.51,
            53.33
          ],
          "code_task_scores": [
            -29.27,
            -13.62,
            0.72,
            9.4,
            -15.38,
            -0.61
          ],
          "reasoning_task_scores": [
            16.62,
            2.68,
            -17.17,
            -0.12,
            37.09
          ]
        },
        "vs_instruct": {
          "general_avg": -13.01,
          "math_avg": 24.38,
          "code_avg": -16.71,
          "reasoning_avg": 8.81,
          "overall_avg": 0.87,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            23.47,
            -42.76,
            -24.07,
            -8.69
          ],
          "math_task_scores": [
            1.89,
            20.04,
            18.4,
            31.15,
            50.42
          ],
          "code_task_scores": [
            -15.85,
            -16.73,
            -2.15,
            -32.57,
            -20.14,
            -12.81
          ],
          "reasoning_task_scores": [
            27.8,
            0.52,
            -16.16,
            -0.15,
            32.05
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "558k",
      "size_precise": "558129",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/math.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "math"
    },
    {
      "id": 88,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 52.48,
      "math_avg": 69.95,
      "code_avg": 33.58,
      "reasoning_avg": 47.19,
      "overall_avg": 50.8,
      "overall_efficiency": 0.106139,
      "general_efficiency": 0.012994,
      "math_efficiency": 0.341759,
      "code_efficiency": -0.078132,
      "reasoning_efficiency": 0.147933,
      "general_scores": [
        79.4,
        38.4875,
        45.77,
        46.2492857
      ],
      "math_scores": [
        92.04,
        88.04,
        88.0,
        43.34,
        38.335
      ],
      "code_scores": [
        45.73,
        66.15,
        3.23,
        30.06,
        16.06,
        40.24
      ],
      "reasoning_scores": [
        77.97,
        72.73,
        24.75,
        0.28054348,
        60.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 66.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.06
              },
              {
                "metric": "lcb_test_output",
                "score": 16.06
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 40.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.03,
          "math_avg": 27.15,
          "code_avg": -6.21,
          "reasoning_avg": 11.75,
          "overall_avg": 8.43,
          "overall_efficiency": 0.106139,
          "general_efficiency": 0.012994,
          "math_efficiency": 0.341759,
          "code_efficiency": -0.078132,
          "reasoning_efficiency": 0.147933,
          "general_task_scores": [
            11.09,
            3.0,
            -11.97,
            2.01
          ],
          "math_task_scores": [
            12.06,
            36.92,
            37.8,
            17.3,
            31.67
          ],
          "code_task_scores": [
            -31.71,
            -5.45,
            -5.01,
            29.02,
            -21.04,
            -3.05
          ],
          "reasoning_task_scores": [
            41.37,
            3.27,
            -10.1,
            -0.11,
            24.33
          ]
        },
        "vs_instruct": {
          "general_avg": -13.73,
          "math_avg": 13.14,
          "code_avg": -14.79,
          "reasoning_avg": 12.74,
          "overall_avg": -0.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            12.2,
            -34.47,
            -21.67,
            -10.98
          ],
          "math_task_scores": [
            -0.23,
            13.02,
            10.2,
            13.94,
            28.76
          ],
          "code_task_scores": [
            -18.29,
            -8.56,
            -7.88,
            -12.95,
            -25.8,
            -15.25
          ],
          "reasoning_task_scores": [
            52.55,
            1.11,
            -9.09,
            -0.14,
            19.29
          ]
        }
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,code,math,science"
    },
    {
      "id": 89,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 56.82,
      "math_avg": 62.02,
      "code_avg": 31.03,
      "reasoning_avg": 31.93,
      "overall_avg": 45.45,
      "overall_efficiency": 0.390554,
      "general_efficiency": 0.680933,
      "math_efficiency": 2.433291,
      "code_efficiency": -1.108017,
      "reasoning_efficiency": -0.443991,
      "general_scores": [
        84.94,
        43.3925,
        53.87,
        45.0928571
      ],
      "math_scores": [
        90.6,
        80.44,
        80.0,
        35.75,
        23.335
      ],
      "code_scores": [
        43.29,
        74.71,
        6.09,
        7.52,
        13.12,
        41.46
      ],
      "reasoning_scores": [
        13.22,
        66.05,
        29.8,
        0.30434783,
        50.3
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.39
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.87
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.44
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.52
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 41.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "olp",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.3
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.38,
          "math_avg": 19.22,
          "code_avg": -8.75,
          "reasoning_avg": -3.51,
          "overall_avg": 3.09,
          "overall_efficiency": 0.390554,
          "general_efficiency": 0.680933,
          "math_efficiency": 2.433291,
          "code_efficiency": -1.108017,
          "reasoning_efficiency": -0.443991,
          "general_task_scores": [
            16.63,
            7.9,
            -3.87,
            0.85
          ],
          "math_task_scores": [
            10.62,
            29.32,
            29.8,
            9.71,
            16.67
          ],
          "code_task_scores": [
            -34.15,
            3.11,
            -2.15,
            6.48,
            -23.98,
            -1.83
          ],
          "reasoning_task_scores": [
            -23.38,
            -3.41,
            -5.05,
            -0.09,
            14.39
          ]
        },
        "vs_instruct": {
          "general_avg": -9.38,
          "math_avg": 5.21,
          "code_avg": -17.33,
          "reasoning_avg": -2.52,
          "overall_avg": -6.01,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            17.74,
            -29.57,
            -13.57,
            -12.14
          ],
          "math_task_scores": [
            -1.67,
            5.42,
            2.2,
            6.35,
            13.76
          ],
          "code_task_scores": [
            -20.73,
            0.0,
            -5.02,
            -35.49,
            -28.74,
            -14.03
          ],
          "reasoning_task_scores": [
            -12.2,
            -5.57,
            -4.04,
            -0.12,
            9.35
          ]
        }
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ]
}