{
  "llama": [
    {
      "id": 0,
      "name": "Llama-3.1-8B-Instruct",
      "domain": "instruct",
      "general_avg": 56.54,
      "math_avg": 39.48,
      "code_avg": 55.14,
      "reasoning_avg": 44.19,
      "overall_avg": 48.84,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        80.9,
        60.325,
        40.98,
        43.96214
      ],
      "math_task_scores": [
        85.67,
        48.2,
        12.62,
        14.39,
        8.335,
        67.68
      ],
      "code_task_scores": [
        71.21,
        9.68,
        60.37,
        79.32
      ],
      "reasoning_task_scores": [
        62.85,
        24.24,
        42.163,
        47.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.9
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 60.325
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96214
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.335
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.21
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.163
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.52
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "Llama-3.1-8B",
      "domain": "base",
      "general_avg": 39.05,
      "math_avg": 17.9,
      "code_avg": 42.37,
      "reasoning_avg": 36.41,
      "overall_avg": 33.93,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        64.37,
        20.3375,
        36.65,
        34.8271429
      ],
      "math_task_scores": [
        56.41,
        19.2,
        4.34,
        0.0,
        0.0,
        27.44
      ],
      "code_task_scores": [
        54.47,
        3.58,
        31.1,
        80.34
      ],
      "reasoning_task_scores": [
        62.56,
        29.8,
        31.2,
        22.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.3375
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.8271429
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 54.47
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.34
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          }
        ]
      }
    },
    {
      "id": 2,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 34.23,
      "math_avg": 19.22,
      "code_avg": 36.82,
      "reasoning_avg": 32.58,
      "overall_avg": 30.71,
      "overall_efficiency": -0.003982,
      "general_efficiency": -0.005959,
      "math_efficiency": 0.001635,
      "code_efficiency": -0.006869,
      "reasoning_efficiency": -0.004735,
      "general_scores": [
        44.46,
        30.76,
        31.37,
        27.4807692,
        46.54,
        34.1625,
        31.7,
        26.2907692,
        47.65,
        31.745,
        31.29,
        27.2678571
      ],
      "math_scores": [
        55.42,
        11.4,
        7.66,
        1.48,
        3.33,
        35.98,
        54.81,
        10.6,
        7.7,
        3.12,
        0.0,
        37.2,
        57.77,
        10.6,
        7.36,
        2.52,
        0.0,
        39.02
      ],
      "code_scores": [
        45.53,
        4.3,
        24.39,
        68.81,
        50.58,
        1.79,
        27.44,
        68.14,
        49.03,
        3.58,
        31.1,
        67.12
      ],
      "reasoning_scores": [
        49.22,
        23.23,
        28.815217,
        26.72,
        46.13,
        25.25,
        29.815217,
        26.8,
        48.34,
        27.27,
        31.217391,
        28.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.4
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.22
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.64
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.95
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.82,
          "math_avg": 1.32,
          "code_avg": -5.56,
          "reasoning_avg": -3.83,
          "overall_avg": -3.22,
          "overall_efficiency": -0.003982,
          "general_efficiency": -0.005959,
          "math_efficiency": 0.001635,
          "code_efficiency": -0.006869,
          "reasoning_efficiency": -0.004735,
          "general_task_scores": [
            -18.15,
            11.88,
            -5.2,
            -7.82
          ],
          "math_task_scores": [
            -0.41,
            -8.33,
            3.23,
            2.37,
            1.11,
            9.96
          ],
          "code_task_scores": [
            -6.09,
            -0.36,
            -3.46,
            -12.32
          ],
          "reasoning_task_scores": [
            -14.66,
            -4.55,
            -1.25,
            5.15
          ]
        },
        "vs_instruct": {
          "general_avg": -22.32,
          "math_avg": -20.26,
          "code_avg": -18.33,
          "reasoning_avg": -11.61,
          "overall_avg": -18.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.68,
            -28.11,
            -9.53,
            -16.95
          ],
          "math_task_scores": [
            -29.67,
            -37.33,
            -5.05,
            -12.02,
            -7.22,
            -30.28
          ],
          "code_task_scores": [
            -22.83,
            -6.46,
            -32.73,
            -11.3
          ],
          "reasoning_task_scores": [
            -14.95,
            1.01,
            -12.21,
            -20.29
          ]
        }
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 40.05,
      "math_avg": 12.28,
      "code_avg": 36.82,
      "reasoning_avg": 29.81,
      "overall_avg": 29.74,
      "overall_efficiency": -0.080572,
      "general_efficiency": 0.019319,
      "math_efficiency": -0.108062,
      "code_efficiency": -0.106694,
      "reasoning_efficiency": -0.126849,
      "general_scores": [
        53.33,
        42.8125,
        34.36,
        28.7385714,
        56.23,
        44.035,
        35.52,
        28.7321429,
        54.87,
        36.945,
        36.2,
        28.8364286
      ],
      "math_scores": [
        26.84,
        8.4,
        6.05,
        2.82,
        0.0,
        32.93,
        24.87,
        8.8,
        6.01,
        2.82,
        3.33,
        25.0,
        25.25,
        7.8,
        6.35,
        2.37,
        3.33,
        28.05
      ],
      "code_scores": [
        45.91,
        3.23,
        24.39,
        70.51,
        50.19,
        2.51,
        23.7,
        71.19,
        48.64,
        3.94,
        26.83,
        70.85
      ],
      "reasoning_scores": [
        37.81,
        25.76,
        30.26087,
        23.12,
        43.84,
        25.76,
        29.945652,
        23.12,
        38.37,
        26.26,
        30.956522,
        22.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.67
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.97
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.01
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.0,
          "math_avg": -5.62,
          "code_avg": -5.55,
          "reasoning_avg": -6.6,
          "overall_avg": -4.19,
          "overall_efficiency": -0.080572,
          "general_efficiency": 0.019319,
          "math_efficiency": -0.108062,
          "code_efficiency": -0.106694,
          "reasoning_efficiency": -0.126849,
          "general_task_scores": [
            -9.56,
            20.92,
            -1.29,
            -6.06
          ],
          "math_task_scores": [
            -30.76,
            -10.87,
            1.8,
            2.67,
            2.22,
            1.22
          ],
          "code_task_scores": [
            -6.22,
            -0.35,
            -6.13,
            -9.49
          ],
          "reasoning_task_scores": [
            -22.55,
            -3.87,
            -0.81,
            0.85
          ]
        },
        "vs_instruct": {
          "general_avg": -16.49,
          "math_avg": -27.2,
          "code_avg": -18.32,
          "reasoning_avg": -14.38,
          "overall_avg": -19.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -26.09,
            -19.07,
            -5.62,
            -15.19
          ],
          "math_task_scores": [
            -60.02,
            -39.87,
            -6.48,
            -11.72,
            -6.12,
            -39.02
          ],
          "code_task_scores": [
            -22.96,
            -6.45,
            -35.4,
            -8.47
          ],
          "reasoning_task_scores": [
            -22.84,
            1.69,
            -11.77,
            -24.59
          ]
        }
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 40.01,
      "math_avg": 17.72,
      "code_avg": 39.54,
      "reasoning_avg": 29.69,
      "overall_avg": 31.74,
      "overall_efficiency": -0.042151,
      "general_efficiency": 0.018483,
      "math_efficiency": -0.003386,
      "code_efficiency": -0.054517,
      "reasoning_efficiency": -0.129182,
      "general_scores": [
        54.48,
        41.765,
        33.5,
        28.625,
        57.61,
        41.5825,
        33.71,
        25.4623077,
        57.71,
        42.3275,
        35.82,
        27.4953846
      ],
      "math_scores": [
        48.67,
        12.8,
        9.35,
        2.08,
        0.0,
        24.39,
        50.57,
        13.2,
        9.98,
        2.23,
        0.0,
        34.15,
        54.13,
        12.4,
        8.06,
        2.23,
        0.0,
        34.76
      ],
      "code_scores": [
        52.14,
        2.51,
        27.44,
        68.14,
        53.7,
        3.58,
        28.66,
        71.53,
        56.81,
        6.09,
        32.32,
        71.53
      ],
      "reasoning_scores": [
        37.05,
        23.23,
        32.521739,
        26.88,
        29.86,
        21.21,
        35.26087,
        27.04,
        39.27,
        23.23,
        32.434783,
        28.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.06
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.47
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.39
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.96,
          "math_avg": -0.18,
          "code_avg": -2.84,
          "reasoning_avg": -6.72,
          "overall_avg": -2.19,
          "overall_efficiency": -0.042151,
          "general_efficiency": 0.018483,
          "math_efficiency": -0.003386,
          "code_efficiency": -0.054517,
          "reasoning_efficiency": -0.129182,
          "general_task_scores": [
            -7.77,
            21.55,
            -2.31,
            -7.64
          ],
          "math_task_scores": [
            -5.29,
            -6.4,
            4.79,
            2.18,
            0.0,
            3.66
          ],
          "code_task_scores": [
            -0.25,
            0.48,
            -1.63,
            -9.94
          ],
          "reasoning_task_scores": [
            -27.17,
            -7.24,
            2.21,
            5.33
          ]
        },
        "vs_instruct": {
          "general_avg": -16.53,
          "math_avg": -21.76,
          "code_avg": -15.61,
          "reasoning_avg": -14.5,
          "overall_avg": -17.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.3,
            -18.44,
            -6.64,
            -16.77
          ],
          "math_task_scores": [
            -34.55,
            -35.4,
            -3.49,
            -12.21,
            -8.34,
            -36.58
          ],
          "code_task_scores": [
            -16.99,
            -5.62,
            -30.9,
            -8.92
          ],
          "reasoning_task_scores": [
            -27.46,
            -1.68,
            -8.75,
            -20.11
          ]
        }
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 5,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 38.82,
      "math_avg": 12.82,
      "code_avg": 34.96,
      "reasoning_avg": 32.82,
      "overall_avg": 29.85,
      "overall_efficiency": -0.271759,
      "general_efficiency": -0.015068,
      "math_efficiency": -0.338641,
      "code_efficiency": -0.493915,
      "reasoning_efficiency": -0.239412,
      "general_scores": [
        65.61,
        38.7275,
        30.98,
        23.6092857,
        62.11,
        37.7775,
        25.67,
        23.5507143,
        61.78,
        42.2075,
        30.94,
        22.8771429
      ],
      "math_scores": [
        43.75,
        11.6,
        8.69,
        3.12,
        0.0,
        0.61,
        45.26,
        13.8,
        7.9,
        3.26,
        0.0,
        10.98,
        48.52,
        12.2,
        8.42,
        3.41,
        0.0,
        9.15
      ],
      "code_scores": [
        54.86,
        2.87,
        23.78,
        64.41,
        55.64,
        2.87,
        3.66,
        61.36,
        56.03,
        1.43,
        23.78,
        68.81
      ],
      "reasoning_scores": [
        52.1,
        24.24,
        33.684783,
        25.68,
        49.95,
        19.7,
        36.23913,
        26.64,
        43.9,
        23.23,
        35.630435,
        22.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.2
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.35
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.91
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.07
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.86
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.23,
          "math_avg": -5.08,
          "code_avg": -7.41,
          "reasoning_avg": -3.59,
          "overall_avg": -4.08,
          "overall_efficiency": -0.271759,
          "general_efficiency": -0.015068,
          "math_efficiency": -0.338641,
          "code_efficiency": -0.493915,
          "reasoning_efficiency": -0.239412,
          "general_task_scores": [
            -1.2,
            19.23,
            -7.45,
            -11.48
          ],
          "math_task_scores": [
            -10.57,
            -6.67,
            4.0,
            3.26,
            0.0,
            -20.53
          ],
          "code_task_scores": [
            1.04,
            -1.19,
            -14.03,
            -15.48
          ],
          "reasoning_task_scores": [
            -13.91,
            -7.41,
            3.98,
            2.96
          ]
        },
        "vs_instruct": {
          "general_avg": -17.72,
          "math_avg": -26.67,
          "code_avg": -20.19,
          "reasoning_avg": -11.38,
          "overall_avg": -18.99,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -17.73,
            -20.76,
            -11.78,
            -20.61
          ],
          "math_task_scores": [
            -39.83,
            -35.67,
            -4.28,
            -11.13,
            -8.34,
            -60.77
          ],
          "code_task_scores": [
            -15.7,
            -7.29,
            -43.3,
            -14.46
          ],
          "reasoning_task_scores": [
            -14.2,
            -1.85,
            -6.98,
            -22.48
          ]
        }
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 44.27,
      "math_avg": 20.44,
      "code_avg": 43.64,
      "reasoning_avg": 37.17,
      "overall_avg": 36.38,
      "overall_efficiency": 0.034986,
      "general_efficiency": 0.074574,
      "math_efficiency": 0.036334,
      "code_efficiency": 0.018155,
      "reasoning_efficiency": 0.010882,
      "general_scores": [
        61.71,
        49.03,
        34.23,
        33.6464286,
        56.77,
        49.6325,
        35.49,
        33.1392857,
        59.98,
        48.0125,
        35.8,
        33.755
      ],
      "math_scores": [
        52.24,
        15.0,
        8.69,
        2.37,
        0.0,
        43.9,
        55.04,
        15.2,
        7.79,
        1.93,
        0.0,
        44.51,
        55.34,
        16.2,
        7.29,
        2.82,
        0.0,
        39.63
      ],
      "code_scores": [
        57.98,
        7.89,
        39.63,
        73.56,
        53.7,
        6.45,
        39.63,
        72.54,
        55.64,
        6.09,
        38.41,
        72.2
      ],
      "reasoning_scores": [
        59.9,
        25.76,
        33.619565,
        31.68,
        57.8,
        25.25,
        33.01087,
        30.48,
        58.85,
        26.77,
        34.380435,
        28.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.51
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.92
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.81
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.22
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.77
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.22,
          "math_avg": 2.54,
          "code_avg": 1.27,
          "reasoning_avg": 0.76,
          "overall_avg": 2.45,
          "overall_efficiency": 0.034986,
          "general_efficiency": 0.074574,
          "math_efficiency": 0.036334,
          "code_efficiency": 0.018155,
          "reasoning_efficiency": 0.010882,
          "general_task_scores": [
            -4.88,
            28.55,
            -1.48,
            -1.32
          ],
          "math_task_scores": [
            -2.2,
            -3.73,
            3.58,
            2.37,
            0.0,
            15.24
          ],
          "code_task_scores": [
            1.3,
            3.23,
            8.12,
            -7.57
          ],
          "reasoning_task_scores": [
            -3.71,
            -3.87,
            2.47,
            8.16
          ]
        },
        "vs_instruct": {
          "general_avg": -12.28,
          "math_avg": -19.04,
          "code_avg": -11.5,
          "reasoning_avg": -7.02,
          "overall_avg": -12.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.41,
            -11.44,
            -5.81,
            -10.45
          ],
          "math_task_scores": [
            -31.46,
            -32.73,
            -4.7,
            -12.02,
            -8.34,
            -25.0
          ],
          "code_task_scores": [
            -15.44,
            -2.87,
            -21.15,
            -6.55
          ],
          "reasoning_task_scores": [
            -4.0,
            1.69,
            -8.49,
            -17.28
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 40.68,
      "math_avg": 19.52,
      "code_avg": 40.0,
      "reasoning_avg": 36.86,
      "overall_avg": 34.27,
      "overall_efficiency": 0.002332,
      "general_efficiency": 0.011457,
      "math_efficiency": 0.011329,
      "code_efficiency": -0.01662,
      "reasoning_efficiency": 0.003164,
      "general_scores": [
        45.63,
        47.9275,
        35.91,
        32.5321429,
        44.83,
        49.1075,
        34.16,
        31.6115385,
        50.2,
        49.3525,
        34.78,
        32.1721429
      ],
      "math_scores": [
        55.88,
        15.4,
        8.11,
        2.82,
        3.33,
        34.15,
        54.74,
        14.4,
        7.72,
        2.37,
        0.0,
        38.41,
        56.18,
        16.2,
        7.7,
        2.82,
        0.0,
        31.1
      ],
      "code_scores": [
        55.64,
        5.02,
        18.9,
        75.25,
        52.53,
        3.23,
        23.78,
        72.88,
        57.59,
        5.73,
        34.15,
        75.25
      ],
      "reasoning_scores": [
        54.41,
        27.78,
        31.336957,
        31.2,
        55.51,
        28.79,
        35.206522,
        30.96,
        56.47,
        24.24,
        33.565217,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.67
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.61
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.46
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.68
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.64,
          "math_avg": 1.62,
          "code_avg": -2.38,
          "reasoning_avg": 0.45,
          "overall_avg": 0.33,
          "overall_efficiency": 0.002332,
          "general_efficiency": 0.011457,
          "math_efficiency": 0.011329,
          "code_efficiency": -0.01662,
          "reasoning_efficiency": 0.003164,
          "general_task_scores": [
            -17.48,
            28.46,
            -1.7,
            -2.72
          ],
          "math_task_scores": [
            -0.81,
            -3.87,
            3.5,
            2.67,
            1.11,
            7.11
          ],
          "code_task_scores": [
            0.78,
            1.08,
            -5.49,
            -5.88
          ],
          "reasoning_task_scores": [
            -7.1,
            -2.86,
            2.17,
            9.6
          ]
        },
        "vs_instruct": {
          "general_avg": -15.86,
          "math_avg": -19.96,
          "code_avg": -15.15,
          "reasoning_avg": -7.33,
          "overall_avg": -14.58,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.01,
            -11.53,
            -6.03,
            -11.85
          ],
          "math_task_scores": [
            -30.07,
            -32.87,
            -4.78,
            -11.72,
            -7.22,
            -33.13
          ],
          "code_task_scores": [
            -15.96,
            -5.02,
            -34.76,
            -4.86
          ],
          "reasoning_task_scores": [
            -7.39,
            2.7,
            -8.79,
            -15.84
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 8,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 40.22,
      "math_avg": 22.83,
      "code_avg": 40.8,
      "reasoning_avg": 37.15,
      "overall_avg": 35.25,
      "overall_efficiency": 1.214292,
      "general_efficiency": 1.080517,
      "math_efficiency": 4.551558,
      "code_efficiency": -1.454489,
      "reasoning_efficiency": 0.679582,
      "general_scores": [
        62.58,
        29.2525,
        32.64,
        36.4978571,
        62.87,
        30.36,
        31.92,
        35.9578571,
        62.6,
        29.9575,
        31.62,
        36.3535714
      ],
      "math_scores": [
        67.85,
        19.0,
        10.52,
        4.9,
        0.0,
        35.37,
        66.34,
        19.6,
        10.46,
        4.75,
        0.0,
        33.54,
        67.55,
        20.8,
        10.48,
        4.45,
        0.0,
        35.37
      ],
      "code_scores": [
        59.14,
        0.36,
        28.05,
        75.25,
        58.75,
        0.36,
        32.32,
        72.2,
        58.37,
        0.36,
        30.49,
        73.9
      ],
      "reasoning_scores": [
        61.43,
        26.26,
        27.391304,
        32.56,
        61.26,
        27.27,
        27.641304,
        33.04,
        61.3,
        26.77,
        27.717391,
        33.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.7
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.76
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.78
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.58
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.91
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.17,
          "math_avg": 4.93,
          "code_avg": -1.58,
          "reasoning_avg": 0.74,
          "overall_avg": 1.32,
          "overall_efficiency": 1.214292,
          "general_efficiency": 1.080517,
          "math_efficiency": 4.551558,
          "code_efficiency": -1.454489,
          "reasoning_efficiency": 0.679582,
          "general_task_scores": [
            -1.69,
            9.52,
            -4.59,
            1.44
          ],
          "math_task_scores": [
            10.84,
            0.6,
            6.15,
            4.7,
            0.0,
            7.32
          ],
          "code_task_scores": [
            4.28,
            -3.22,
            -0.81,
            -6.56
          ],
          "reasoning_task_scores": [
            -1.23,
            -3.03,
            -3.62,
            10.83
          ]
        },
        "vs_instruct": {
          "general_avg": -16.32,
          "math_avg": -16.65,
          "code_avg": -14.35,
          "reasoning_avg": -7.05,
          "overall_avg": -13.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -18.22,
            -30.47,
            -8.92,
            -7.69
          ],
          "math_task_scores": [
            -18.42,
            -28.4,
            -2.13,
            -9.69,
            -8.34,
            -32.92
          ],
          "code_task_scores": [
            -12.46,
            -9.32,
            -30.08,
            -5.54
          ],
          "reasoning_task_scores": [
            -1.52,
            2.53,
            -14.58,
            -14.61
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 9,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 48.66,
      "math_avg": 31.97,
      "code_avg": 42.45,
      "reasoning_avg": 38.53,
      "overall_avg": 40.4,
      "overall_efficiency": 0.006509,
      "general_efficiency": 0.009668,
      "math_efficiency": 0.014154,
      "code_efficiency": 8.2e-05,
      "reasoning_efficiency": 0.002132,
      "general_scores": [
        67.99,
        54.355,
        38.47,
        34.4692857,
        65.39,
        55.04,
        40.69,
        35.2471429,
        65.32,
        54.7825,
        38.87,
        33.2485714
      ],
      "math_scores": [
        78.32,
        41.2,
        13.37,
        10.09,
        0.0,
        47.56,
        80.89,
        38.6,
        14.23,
        13.06,
        0.0,
        45.12,
        79.38,
        39.8,
        12.76,
        13.5,
        0.0,
        47.56
      ],
      "code_scores": [
        48.64,
        7.53,
        31.71,
        77.97,
        50.58,
        6.45,
        34.76,
        79.66,
        49.42,
        6.81,
        36.59,
        79.32
      ],
      "reasoning_scores": [
        62.69,
        17.68,
        34.293478,
        40.08,
        60.88,
        17.17,
        36.043478,
        40.16,
        62.07,
        17.17,
        34.51087,
        39.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.45
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.75
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.93
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.35
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.88
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.95
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.95
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.61,
          "math_avg": 14.07,
          "code_avg": 0.08,
          "reasoning_avg": 2.12,
          "overall_avg": 6.47,
          "overall_efficiency": 0.006509,
          "general_efficiency": 0.009668,
          "math_efficiency": 0.014154,
          "code_efficiency": 8.2e-05,
          "reasoning_efficiency": 0.002132,
          "general_task_scores": [
            1.86,
            34.39,
            2.69,
            -0.51
          ],
          "math_task_scores": [
            23.12,
            20.67,
            9.11,
            12.22,
            0.0,
            19.31
          ],
          "code_task_scores": [
            -4.92,
            3.35,
            3.25,
            -1.36
          ],
          "reasoning_task_scores": [
            -0.68,
            -12.46,
            3.75,
            17.87
          ]
        },
        "vs_instruct": {
          "general_avg": -7.89,
          "math_avg": -7.51,
          "code_avg": -12.69,
          "reasoning_avg": -5.66,
          "overall_avg": -8.44,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -14.67,
            -5.6,
            -1.64,
            -9.64
          ],
          "math_task_scores": [
            -6.14,
            -8.33,
            0.83,
            -2.17,
            -8.34,
            -20.93
          ],
          "code_task_scores": [
            -21.66,
            -2.75,
            -26.02,
            -0.34
          ],
          "reasoning_task_scores": [
            -0.97,
            -6.9,
            -7.21,
            -7.57
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 10,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 43.58,
      "math_avg": 27.94,
      "code_avg": 44.14,
      "reasoning_avg": 37.46,
      "overall_avg": 38.28,
      "overall_efficiency": 0.004341,
      "general_efficiency": 0.004525,
      "math_efficiency": 0.010025,
      "code_efficiency": 0.00176,
      "reasoning_efficiency": 0.001051,
      "general_scores": [
        50.63,
        50.9275,
        40.8,
        32.3721429,
        52.1,
        52.215,
        40.62,
        32.1278571,
        48.24,
        50.3825,
        40.58,
        31.9478571
      ],
      "math_scores": [
        74.53,
        32.0,
        11.72,
        4.6,
        0.0,
        46.34,
        73.62,
        27.8,
        10.28,
        4.45,
        0.0,
        47.56,
        74.53,
        30.6,
        9.64,
        3.41,
        0.0,
        51.83
      ],
      "code_scores": [
        52.53,
        6.09,
        41.46,
        76.61,
        52.53,
        3.58,
        40.24,
        79.32,
        56.42,
        4.66,
        37.2,
        78.98
      ],
      "reasoning_scores": [
        54.92,
        30.3,
        38.608696,
        29.92,
        54.08,
        26.77,
        36.76087,
        31.6,
        54.66,
        26.77,
        34.847826,
        30.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.15
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.58
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.78
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.63
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.95
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.74
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.61
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.53,
          "math_avg": 10.04,
          "code_avg": 1.76,
          "reasoning_avg": 1.05,
          "overall_avg": 4.35,
          "overall_efficiency": 0.004341,
          "general_efficiency": 0.004525,
          "math_efficiency": 0.010025,
          "code_efficiency": 0.00176,
          "reasoning_efficiency": 0.001051,
          "general_task_scores": [
            -14.05,
            30.84,
            4.02,
            -2.68
          ],
          "math_task_scores": [
            17.82,
            10.93,
            6.21,
            4.15,
            0.0,
            21.14
          ],
          "code_task_scores": [
            -0.64,
            1.2,
            8.53,
            -2.04
          ],
          "reasoning_task_scores": [
            -8.01,
            -1.85,
            5.54,
            8.53
          ]
        },
        "vs_instruct": {
          "general_avg": -12.96,
          "math_avg": -11.54,
          "code_avg": -11.01,
          "reasoning_avg": -6.73,
          "overall_avg": -10.56,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -30.58,
            -9.15,
            -0.31,
            -11.81
          ],
          "math_task_scores": [
            -11.44,
            -18.07,
            -2.07,
            -10.24,
            -8.34,
            -19.1
          ],
          "code_task_scores": [
            -17.38,
            -4.9,
            -20.74,
            -1.02
          ],
          "reasoning_task_scores": [
            -8.3,
            3.71,
            -5.42,
            -16.91
          ]
        }
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 11,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 24.51,
      "math_avg": 11.82,
      "code_avg": 17.0,
      "reasoning_avg": 18.3,
      "overall_avg": 17.91,
      "overall_efficiency": -0.194389,
      "general_efficiency": -0.176341,
      "math_efficiency": -0.073698,
      "code_efficiency": -0.307813,
      "reasoning_efficiency": -0.219706,
      "general_scores": [
        38.05,
        26.06,
        22.41,
        14.0907143,
        35.35,
        26.63,
        20.8,
        15.2178571,
        35.04,
        25.58,
        19.59,
        15.2864286
      ],
      "math_scores": [
        59.21,
        2.2,
        4.83,
        1.93,
        0.0,
        7.32,
        50.49,
        3.6,
        4.41,
        2.23,
        0.0,
        8.53,
        53.6,
        4.4,
        4.47,
        1.93,
        0.0,
        3.66
      ],
      "code_scores": [
        14.79,
        0.0,
        6.71,
        48.81,
        19.46,
        0.0,
        7.31,
        43.73,
        17.12,
        0.0,
        3.66,
        42.37
      ],
      "reasoning_scores": [
        22.01,
        14.65,
        25.108696,
        13.04,
        22.93,
        21.72,
        25.021739,
        11.28,
        16.11,
        11.11,
        23.391304,
        13.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.57
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.5
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.89
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.51
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -14.54,
          "math_avg": -6.08,
          "code_avg": -25.38,
          "reasoning_avg": -18.11,
          "overall_avg": -16.03,
          "overall_efficiency": -0.194389,
          "general_efficiency": -0.176341,
          "math_efficiency": -0.073698,
          "code_efficiency": -0.307813,
          "reasoning_efficiency": -0.219706,
          "general_task_scores": [
            -28.22,
            5.75,
            -15.72,
            -19.97
          ],
          "math_task_scores": [
            -1.98,
            -15.8,
            0.23,
            2.03,
            0.0,
            -20.94
          ],
          "code_task_scores": [
            -37.35,
            -3.58,
            -25.21,
            -35.37
          ],
          "reasoning_task_scores": [
            -42.21,
            -13.97,
            -6.69,
            -9.57
          ]
        },
        "vs_instruct": {
          "general_avg": -32.03,
          "math_avg": -27.66,
          "code_avg": -38.15,
          "reasoning_avg": -25.9,
          "overall_avg": -30.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -44.75,
            -34.24,
            -20.05,
            -29.1
          ],
          "math_task_scores": [
            -31.24,
            -44.8,
            -8.05,
            -12.36,
            -8.34,
            -61.18
          ],
          "code_task_scores": [
            -54.09,
            -9.68,
            -54.48,
            -34.35
          ],
          "reasoning_task_scores": [
            -42.5,
            -8.41,
            -17.65,
            -35.01
          ]
        }
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 12,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 48.93,
      "math_avg": 32.88,
      "code_avg": 47.15,
      "reasoning_avg": 36.85,
      "overall_avg": 41.45,
      "overall_efficiency": 0.008006,
      "general_efficiency": 0.010519,
      "math_efficiency": 0.01595,
      "code_efficiency": 0.005089,
      "reasoning_efficiency": 0.000465,
      "general_scores": [
        58.35,
        72.9025,
        38.39,
        31.1471429,
        54.94,
        71.025,
        39.44,
        30.5671429,
        55.58,
        69.4225,
        36.33,
        29.0335714
      ],
      "math_scores": [
        74.45,
        31.6,
        13.98,
        17.21,
        0.0,
        65.24,
        76.04,
        30.0,
        13.89,
        13.8,
        0.0,
        63.41,
        75.97,
        28.4,
        13.21,
        12.46,
        0.0,
        62.2
      ],
      "code_scores": [
        57.59,
        5.02,
        52.44,
        73.22,
        57.98,
        7.53,
        55.49,
        76.61,
        53.31,
        3.23,
        54.27,
        69.15
      ],
      "reasoning_scores": [
        56.83,
        23.74,
        39.130435,
        29.76,
        56.42,
        21.72,
        36.663043,
        32.64,
        57.98,
        20.71,
        35.043478,
        31.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 71.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.69
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.62
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.07
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.99
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.95
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.88,
          "math_avg": 14.98,
          "code_avg": 4.78,
          "reasoning_avg": 0.44,
          "overall_avg": 7.52,
          "overall_efficiency": 0.008006,
          "general_efficiency": 0.010519,
          "math_efficiency": 0.01595,
          "code_efficiency": 0.005089,
          "reasoning_efficiency": 0.000465,
          "general_task_scores": [
            -8.08,
            50.78,
            1.4,
            -4.58
          ],
          "math_task_scores": [
            19.08,
            10.8,
            9.35,
            14.49,
            0.0,
            36.18
          ],
          "code_task_scores": [
            1.82,
            1.68,
            22.97,
            -7.35
          ],
          "reasoning_task_scores": [
            -5.48,
            -7.74,
            5.75,
            9.23
          ]
        },
        "vs_instruct": {
          "general_avg": -7.61,
          "math_avg": -6.6,
          "code_avg": -7.99,
          "reasoning_avg": -7.35,
          "overall_avg": -7.39,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.61,
            10.8,
            -2.93,
            -13.71
          ],
          "math_task_scores": [
            -10.18,
            -18.2,
            1.07,
            0.1,
            -8.34,
            -4.06
          ],
          "code_task_scores": [
            -14.92,
            -4.42,
            -6.3,
            -6.33
          ],
          "reasoning_task_scores": [
            -5.77,
            -2.18,
            -5.21,
            -16.21
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 39.64,
      "math_avg": 19.26,
      "code_avg": 34.69,
      "reasoning_avg": 35.62,
      "overall_avg": 32.3,
      "overall_efficiency": -0.001825,
      "general_efficiency": 0.000662,
      "math_efficiency": 0.001531,
      "code_efficiency": -0.008615,
      "reasoning_efficiency": -0.000881,
      "general_scores": [
        61.91,
        36.08,
        35.59,
        27.6864286,
        58.16,
        34.5625,
        36.75,
        25.6484615,
        59.0,
        36.8025,
        36.22,
        27.2292857
      ],
      "math_scores": [
        68.16,
        14.2,
        8.36,
        1.93,
        0.0,
        25.0,
        70.28,
        12.6,
        7.68,
        3.71,
        0.0,
        20.73,
        68.08,
        11.8,
        7.61,
        2.23,
        0.0,
        24.39
      ],
      "code_scores": [
        41.63,
        2.51,
        20.12,
        75.25,
        43.58,
        2.15,
        16.46,
        72.88,
        41.25,
        2.15,
        21.34,
        76.95
      ],
      "reasoning_scores": [
        50.26,
        23.23,
        33.782609,
        31.68,
        50.79,
        23.74,
        36.076087,
        31.12,
        51.53,
        27.27,
        36.576087,
        31.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.19
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.88
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 23.37
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 19.31
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.48
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.59,
          "math_avg": 1.37,
          "code_avg": -7.68,
          "reasoning_avg": -0.79,
          "overall_avg": -1.63,
          "overall_efficiency": -0.001825,
          "general_efficiency": 0.000662,
          "math_efficiency": 0.001531,
          "code_efficiency": -0.008615,
          "reasoning_efficiency": -0.000881,
          "general_task_scores": [
            -4.68,
            15.48,
            -0.46,
            -7.98
          ],
          "math_task_scores": [
            12.43,
            -6.33,
            3.54,
            2.62,
            0.0,
            -4.07
          ],
          "code_task_scores": [
            -12.32,
            -1.31,
            -11.79,
            -5.31
          ],
          "reasoning_task_scores": [
            -11.7,
            -5.05,
            4.28,
            9.33
          ]
        },
        "vs_instruct": {
          "general_avg": -16.91,
          "math_avg": -20.22,
          "code_avg": -20.46,
          "reasoning_avg": -8.57,
          "overall_avg": -16.54,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.21,
            -24.51,
            -4.79,
            -17.11
          ],
          "math_task_scores": [
            -16.83,
            -35.33,
            -4.74,
            -11.77,
            -8.34,
            -44.31
          ],
          "code_task_scores": [
            -29.06,
            -7.41,
            -41.06,
            -4.29
          ],
          "reasoning_task_scores": [
            -11.99,
            0.51,
            -6.68,
            -16.11
          ]
        }
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 43.74,
      "math_avg": 25.49,
      "code_avg": 45.69,
      "reasoning_avg": 37.97,
      "overall_avg": 38.22,
      "overall_efficiency": 0.429069,
      "general_efficiency": 0.469274,
      "math_efficiency": 0.759,
      "code_efficiency": 0.33175,
      "reasoning_efficiency": 0.156254,
      "general_scores": [
        63.4,
        34.355,
        41.24,
        38.8053846,
        55.65,
        35.99,
        41.57,
        39.8642857,
        57.83,
        35.845,
        41.24,
        39.0771429
      ],
      "math_scores": [
        63.84,
        22.4,
        9.55,
        5.19,
        3.33,
        43.29,
        65.05,
        26.4,
        9.71,
        6.38,
        0.0,
        49.39,
        66.72,
        27.4,
        9.98,
        4.15,
        0.0,
        46.01
      ],
      "code_scores": [
        57.59,
        6.81,
        43.29,
        75.25,
        57.59,
        8.96,
        40.24,
        75.93,
        57.59,
        7.89,
        41.21,
        75.93
      ],
      "reasoning_scores": [
        59.83,
        27.78,
        26.684783,
        38.32,
        58.97,
        27.78,
        27.586957,
        39.04,
        60.5,
        24.24,
        26.858696,
        38.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.75
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.23
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 57.59
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.89
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.58
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.77
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.6
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.04
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.69,
          "math_avg": 7.59,
          "code_avg": 3.32,
          "reasoning_avg": 1.56,
          "overall_avg": 4.29,
          "overall_efficiency": 0.429069,
          "general_efficiency": 0.469274,
          "math_efficiency": 0.759,
          "code_efficiency": 0.33175,
          "reasoning_efficiency": 0.156254,
          "general_task_scores": [
            -5.41,
            15.06,
            4.7,
            4.42
          ],
          "math_task_scores": [
            8.79,
            6.2,
            5.41,
            5.24,
            1.11,
            18.79
          ],
          "code_task_scores": [
            3.12,
            4.31,
            10.48,
            -4.64
          ],
          "reasoning_task_scores": [
            -2.79,
            -3.2,
            -4.16,
            16.4
          ]
        },
        "vs_instruct": {
          "general_avg": -12.8,
          "math_avg": -13.99,
          "code_avg": -9.45,
          "reasoning_avg": -6.22,
          "overall_avg": -10.62,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -21.94,
            -24.93,
            0.37,
            -4.71
          ],
          "math_task_scores": [
            -20.47,
            -22.8,
            -2.87,
            -9.15,
            -7.22,
            -21.45
          ],
          "code_task_scores": [
            -13.62,
            -1.79,
            -18.79,
            -3.62
          ],
          "reasoning_task_scores": [
            -3.08,
            2.36,
            -15.12,
            -9.04
          ]
        }
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 32.66,
      "math_avg": 13.44,
      "code_avg": 30.59,
      "reasoning_avg": 27.97,
      "overall_avg": 26.17,
      "overall_efficiency": -0.015563,
      "general_efficiency": -0.012798,
      "math_efficiency": -0.008932,
      "code_efficiency": -0.023613,
      "reasoning_efficiency": -0.01691,
      "general_scores": [
        42.76,
        40.2925,
        31.0,
        15.9878571,
        43.88,
        40.24,
        30.84,
        19.5707143,
        39.99,
        41.0375,
        30.38,
        15.9671429
      ],
      "math_scores": [
        35.03,
        6.0,
        5.06,
        1.34,
        0.0,
        26.83,
        40.49,
        8.8,
        5.69,
        1.34,
        0.0,
        29.88,
        37.91,
        5.8,
        4.92,
        1.78,
        0.0,
        31.1
      ],
      "code_scores": [
        38.52,
        3.23,
        18.29,
        64.07,
        28.79,
        3.23,
        24.39,
        61.69,
        31.13,
        3.23,
        26.83,
        63.73
      ],
      "reasoning_scores": [
        25.85,
        31.31,
        32.641304,
        22.96,
        27.84,
        30.81,
        32.673913,
        26.24,
        22.61,
        26.26,
        32.184783,
        24.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.22
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.27
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 32.81
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.5
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.38,
          "math_avg": -4.46,
          "code_avg": -11.78,
          "reasoning_avg": -8.44,
          "overall_avg": -7.76,
          "overall_efficiency": -0.015563,
          "general_efficiency": -0.012798,
          "math_efficiency": -0.008932,
          "code_efficiency": -0.023613,
          "reasoning_efficiency": -0.01691,
          "general_task_scores": [
            -22.16,
            20.18,
            -5.91,
            -17.65
          ],
          "math_task_scores": [
            -18.6,
            -12.33,
            0.88,
            1.49,
            0.0,
            1.83
          ],
          "code_task_scores": [
            -21.66,
            -0.35,
            -7.93,
            -17.18
          ],
          "reasoning_task_scores": [
            -37.13,
            -0.34,
            1.3,
            2.43
          ]
        },
        "vs_instruct": {
          "general_avg": -23.88,
          "math_avg": -26.04,
          "code_avg": -24.55,
          "reasoning_avg": -16.22,
          "overall_avg": -22.67,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.69,
            -19.8,
            -10.24,
            -26.78
          ],
          "math_task_scores": [
            -47.86,
            -41.33,
            -7.4,
            -12.9,
            -8.34,
            -38.41
          ],
          "code_task_scores": [
            -38.4,
            -6.45,
            -37.2,
            -16.16
          ],
          "reasoning_task_scores": [
            -37.42,
            5.22,
            -9.66,
            -23.01
          ]
        }
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 33.42,
      "math_avg": 9.25,
      "code_avg": 22.97,
      "reasoning_avg": 20.94,
      "overall_avg": 21.64,
      "overall_efficiency": -0.091279,
      "general_efficiency": -0.041774,
      "math_efficiency": -0.064268,
      "code_efficiency": -0.144114,
      "reasoning_efficiency": -0.114956,
      "general_scores": [
        70.9,
        28.2,
        22.83,
        12.525,
        65.14,
        29.3725,
        24.34,
        13.25214,
        67.83,
        29.2975,
        23.73,
        13.6585714
      ],
      "math_scores": [
        21.15,
        10.6,
        6.53,
        2.37,
        0.0,
        11.59,
        22.14,
        12.0,
        6.8,
        3.56,
        0.0,
        10.37,
        22.74,
        11.6,
        6.64,
        3.12,
        0.0,
        15.24
      ],
      "code_scores": [
        38.91,
        0.0,
        4.88,
        50.51,
        32.3,
        0.0,
        5.49,
        50.85,
        29.96,
        0.0,
        11.59,
        51.19
      ],
      "reasoning_scores": [
        24.08,
        21.21,
        20.76087,
        19.44,
        23.4,
        14.14,
        20.6196,
        21.36,
        24.5,
        21.72,
        20.478261,
        19.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.96
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.66
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.4
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 33.72
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.85
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.02
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -5.62,
          "math_avg": -8.65,
          "code_avg": -19.4,
          "reasoning_avg": -15.47,
          "overall_avg": -12.29,
          "overall_efficiency": -0.091279,
          "general_efficiency": -0.041774,
          "math_efficiency": -0.064268,
          "code_efficiency": -0.144114,
          "reasoning_efficiency": -0.114956,
          "general_task_scores": [
            3.59,
            8.62,
            -13.02,
            -21.68
          ],
          "math_task_scores": [
            -34.4,
            -7.8,
            2.32,
            3.02,
            0.0,
            -15.04
          ],
          "code_task_scores": [
            -20.75,
            -3.58,
            -23.78,
            -29.49
          ],
          "reasoning_task_scores": [
            -38.57,
            -10.78,
            -10.58,
            -1.97
          ]
        },
        "vs_instruct": {
          "general_avg": -23.12,
          "math_avg": -30.24,
          "code_avg": -32.17,
          "reasoning_avg": -23.26,
          "overall_avg": -27.2,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.94,
            -31.36,
            -17.35,
            -30.81
          ],
          "math_task_scores": [
            -63.66,
            -36.8,
            -5.96,
            -11.37,
            -8.34,
            -55.28
          ],
          "code_task_scores": [
            -37.49,
            -9.68,
            -53.05,
            -28.47
          ],
          "reasoning_task_scores": [
            -38.86,
            -5.22,
            -21.54,
            -27.41
          ]
        }
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 32.75,
      "math_avg": 13.37,
      "code_avg": 20.11,
      "reasoning_avg": 25.35,
      "overall_avg": 22.89,
      "overall_efficiency": -1.476926,
      "general_efficiency": -0.842792,
      "math_efficiency": -0.605811,
      "code_efficiency": -2.978612,
      "reasoning_efficiency": -1.48049,
      "general_scores": [
        68.63,
        19.3875,
        22.56,
        21.9753846,
        69.8,
        18.1275,
        21.12,
        21.4585714,
        69.99,
        16.5275,
        21.87,
        21.5292857
      ],
      "math_scores": [
        61.87,
        9.6,
        5.67,
        1.93,
        0.0,
        0.0,
        63.31,
        10.4,
        5.94,
        1.04,
        0.0,
        0.0,
        61.71,
        11.2,
        5.49,
        2.52,
        0.0,
        0.0
      ],
      "code_scores": [
        24.51,
        0.0,
        0.0,
        59.66,
        14.79,
        0.0,
        0.0,
        55.93,
        29.18,
        0.0,
        0.0,
        57.29
      ],
      "reasoning_scores": [
        50.39,
        16.16,
        12.402174,
        23.28,
        49.48,
        13.13,
        11.141304,
        25.04,
        49.66,
        15.15,
        12.402174,
        25.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.7
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 22.83
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.81
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.98
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.3,
          "math_avg": -4.53,
          "code_avg": -22.26,
          "reasoning_avg": -11.06,
          "overall_avg": -11.04,
          "overall_efficiency": -1.476926,
          "general_efficiency": -0.842792,
          "math_efficiency": -0.605811,
          "code_efficiency": -2.978612,
          "reasoning_efficiency": -1.48049,
          "general_task_scores": [
            5.1,
            -2.33,
            -14.8,
            -13.18
          ],
          "math_task_scores": [
            5.89,
            -8.8,
            1.36,
            1.83,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -31.64,
            -3.58,
            -31.1,
            -22.71
          ],
          "reasoning_task_scores": [
            -12.72,
            -14.99,
            -19.22,
            2.67
          ]
        },
        "vs_instruct": {
          "general_avg": -23.79,
          "math_avg": -26.11,
          "code_avg": -35.03,
          "reasoning_avg": -18.85,
          "overall_avg": -25.95,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.43,
            -42.32,
            -19.13,
            -22.31
          ],
          "math_task_scores": [
            -23.37,
            -37.8,
            -6.92,
            -12.56,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -48.38,
            -9.68,
            -60.37,
            -21.69
          ],
          "reasoning_task_scores": [
            -13.01,
            -9.43,
            -30.18,
            -22.77
          ]
        }
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 39.03,
      "math_avg": 14.38,
      "code_avg": 32.49,
      "reasoning_avg": 31.35,
      "overall_avg": 29.31,
      "overall_efficiency": -0.615674,
      "general_efficiency": -0.001794,
      "math_efficiency": -0.469259,
      "code_efficiency": -1.317556,
      "reasoning_efficiency": -0.674087,
      "general_scores": [
        74.15,
        24.5925,
        31.25,
        30.4071429,
        73.27,
        24.1725,
        30.24,
        28.1321429,
        72.39,
        22.2625,
        29.98,
        27.5457143
      ],
      "math_scores": [
        53.75,
        21.0,
        9.67,
        2.82,
        0.0,
        0.0,
        53.83,
        19.0,
        8.85,
        3.71,
        0.0,
        1.22,
        51.48,
        19.4,
        8.85,
        3.41,
        0.0,
        1.83
      ],
      "code_scores": [
        54.86,
        0.72,
        0.0,
        75.25,
        52.92,
        1.43,
        1.22,
        72.54,
        56.03,
        1.43,
        0.61,
        72.88
      ],
      "reasoning_scores": [
        56.37,
        22.22,
        17.293478,
        31.52,
        52.85,
        23.23,
        16.75,
        30.56,
        54.25,
        22.73,
        15.358696,
        33.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.12
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.31
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 54.6
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.19
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.47
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.01,
          "math_avg": -3.52,
          "code_avg": -9.88,
          "reasoning_avg": -5.06,
          "overall_avg": -4.62,
          "overall_efficiency": -0.615674,
          "general_efficiency": -0.001794,
          "math_efficiency": -0.469259,
          "code_efficiency": -1.317556,
          "reasoning_efficiency": -0.674087,
          "general_task_scores": [
            8.9,
            3.34,
            -6.16,
            -6.13
          ],
          "math_task_scores": [
            -3.39,
            0.6,
            4.78,
            3.31,
            0.0,
            -26.42
          ],
          "code_task_scores": [
            0.13,
            -2.39,
            -30.49,
            -6.78
          ],
          "reasoning_task_scores": [
            -8.07,
            -7.07,
            -14.73,
            9.65
          ]
        },
        "vs_instruct": {
          "general_avg": -17.51,
          "math_avg": -25.1,
          "code_avg": -22.65,
          "reasoning_avg": -12.84,
          "overall_avg": -19.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.63,
            -36.65,
            -10.49,
            -15.26
          ],
          "math_task_scores": [
            -32.65,
            -28.4,
            -3.5,
            -11.08,
            -8.34,
            -66.66
          ],
          "code_task_scores": [
            -16.61,
            -8.49,
            -59.76,
            -5.76
          ],
          "reasoning_task_scores": [
            -8.36,
            -1.51,
            -25.69,
            -15.79
          ]
        }
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 36.91,
      "math_avg": 22.52,
      "code_avg": 28.61,
      "reasoning_avg": 25.19,
      "overall_avg": 28.31,
      "overall_efficiency": -0.005626,
      "general_efficiency": -0.002136,
      "math_efficiency": 0.004624,
      "code_efficiency": -0.013765,
      "reasoning_efficiency": -0.011223,
      "general_scores": [
        69.87,
        24.0,
        32.89,
        19.9392857,
        73.83,
        28.025,
        34.86,
        19.0228571,
        64.83,
        26.545,
        33.23,
        15.8757143
      ],
      "math_scores": [
        63.76,
        21.8,
        18.27,
        23.59,
        3.33,
        3.66,
        62.17,
        17.2,
        18.9,
        25.37,
        3.33,
        9.15,
        64.44,
        20.4,
        18.34,
        23.15,
        0.0,
        8.54
      ],
      "code_scores": [
        40.86,
        0.72,
        7.93,
        62.37,
        37.74,
        1.79,
        2.44,
        64.41,
        36.58,
        2.15,
        19.51,
        66.78
      ],
      "reasoning_scores": [
        24.09,
        21.72,
        18.771739,
        30.88,
        32.11,
        20.2,
        20.554348,
        28.32,
        32.63,
        21.72,
        20.445652,
        30.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.46
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.5
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.12
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.55
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.96
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.92
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -2.14,
          "math_avg": 4.62,
          "code_avg": -13.77,
          "reasoning_avg": -11.22,
          "overall_avg": -5.63,
          "overall_efficiency": -0.005626,
          "general_efficiency": -0.002136,
          "math_efficiency": 0.004624,
          "code_efficiency": -0.013765,
          "reasoning_efficiency": -0.011223,
          "general_task_scores": [
            5.14,
            5.85,
            -2.99,
            -16.55
          ],
          "math_task_scores": [
            7.05,
            0.6,
            14.16,
            24.04,
            2.22,
            -20.32
          ],
          "code_task_scores": [
            -16.08,
            -2.03,
            -21.14,
            -15.82
          ],
          "reasoning_task_scores": [
            -32.95,
            -8.59,
            -11.28,
            7.92
          ]
        },
        "vs_instruct": {
          "general_avg": -19.63,
          "math_avg": -16.96,
          "code_avg": -26.54,
          "reasoning_avg": -19.01,
          "overall_avg": -20.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.39,
            -34.14,
            -7.32,
            -25.68
          ],
          "math_task_scores": [
            -22.21,
            -28.4,
            5.88,
            9.65,
            -6.12,
            -60.56
          ],
          "code_task_scores": [
            -32.82,
            -8.13,
            -50.41,
            -14.8
          ],
          "reasoning_task_scores": [
            -33.24,
            -3.03,
            -22.24,
            -17.52
          ]
        }
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 40.99,
      "math_avg": 30.67,
      "code_avg": 28.57,
      "reasoning_avg": 24.81,
      "overall_avg": 31.26,
      "overall_efficiency": -0.003106,
      "general_efficiency": 0.002266,
      "math_efficiency": 0.01486,
      "code_efficiency": -0.016054,
      "reasoning_efficiency": -0.013493,
      "general_scores": [
        75.33,
        24.8925,
        40.85,
        29.3614286,
        74.09,
        25.495,
        40.3,
        26.8942857,
        67.03,
        23.0425,
        40.26,
        24.3728571
      ],
      "math_scores": [
        83.85,
        49.0,
        19.76,
        21.07,
        0.0,
        11.59,
        82.87,
        52.0,
        19.2,
        19.29,
        3.33,
        7.93,
        83.32,
        49.0,
        19.99,
        22.26,
        3.33,
        4.27
      ],
      "code_scores": [
        40.86,
        0.36,
        6.1,
        66.78,
        40.08,
        0.36,
        4.88,
        68.14,
        36.19,
        0.36,
        10.98,
        67.8
      ],
      "reasoning_scores": [
        24.77,
        18.69,
        16.913043,
        37.44,
        30.17,
        23.23,
        16.945652,
        36.48,
        20.88,
        19.19,
        16.804348,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.48
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.65
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.93
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 39.04
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.89
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.95,
          "math_avg": 12.77,
          "code_avg": -13.8,
          "reasoning_avg": -11.6,
          "overall_avg": -2.67,
          "overall_efficiency": -0.003106,
          "general_efficiency": 0.002266,
          "math_efficiency": 0.01486,
          "code_efficiency": -0.016054,
          "reasoning_efficiency": -0.013493,
          "general_task_scores": [
            7.78,
            4.14,
            3.82,
            -7.95
          ],
          "math_task_scores": [
            26.94,
            30.8,
            15.31,
            20.87,
            2.22,
            -19.51
          ],
          "code_task_scores": [
            -15.43,
            -3.22,
            -23.78,
            -12.77
          ],
          "reasoning_task_scores": [
            -37.29,
            -9.43,
            -14.31,
            14.64
          ]
        },
        "vs_instruct": {
          "general_avg": -15.55,
          "math_avg": -8.81,
          "code_avg": -26.57,
          "reasoning_avg": -19.38,
          "overall_avg": -17.58,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.75,
            -35.84,
            -0.51,
            -17.08
          ],
          "math_task_scores": [
            -2.32,
            1.8,
            7.03,
            6.48,
            -6.12,
            -59.75
          ],
          "code_task_scores": [
            -32.17,
            -9.32,
            -53.05,
            -11.75
          ],
          "reasoning_task_scores": [
            -37.58,
            -3.87,
            -25.27,
            -10.8
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 39.74,
      "math_avg": 23.44,
      "code_avg": 44.68,
      "reasoning_avg": 22.67,
      "overall_avg": 32.63,
      "overall_efficiency": -0.017916,
      "general_efficiency": 0.009575,
      "math_efficiency": 0.076483,
      "code_efficiency": 0.0319,
      "reasoning_efficiency": -0.189623,
      "general_scores": [
        61.66,
        31.35,
        36.58,
        34.4142857,
        53.16,
        31.675,
        37.53,
        35.3457143,
        51.88,
        31.505,
        37.21,
        34.5671429
      ],
      "math_scores": [
        41.39,
        25.0,
        14.86,
        9.05,
        0.0,
        48.17,
        41.47,
        26.0,
        15.15,
        9.5,
        0.0,
        48.17,
        40.86,
        26.0,
        14.81,
        9.64,
        0.0,
        51.83
      ],
      "code_scores": [
        52.53,
        7.17,
        40.85,
        78.98,
        55.25,
        5.02,
        38.41,
        77.97,
        55.25,
        6.09,
        39.02,
        79.66
      ],
      "reasoning_scores": [
        16.99,
        23.74,
        17.597826,
        30.8,
        13.46,
        27.78,
        17.934783,
        30.56,
        15.13,
        30.81,
        17.119565,
        30.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.78
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.24
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.09
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.43
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.87
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.55
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.69,
          "math_avg": 5.54,
          "code_avg": 2.31,
          "reasoning_avg": -13.74,
          "overall_avg": -1.3,
          "overall_efficiency": -0.017916,
          "general_efficiency": 0.009575,
          "math_efficiency": 0.076483,
          "code_efficiency": 0.0319,
          "reasoning_efficiency": -0.189623,
          "general_task_scores": [
            -8.8,
            11.17,
            0.46,
            -0.05
          ],
          "math_task_scores": [
            -15.17,
            6.47,
            10.6,
            9.4,
            0.0,
            21.95
          ],
          "code_task_scores": [
            -0.13,
            2.51,
            8.33,
            -1.47
          ],
          "reasoning_task_scores": [
            -47.37,
            -2.36,
            -13.65,
            8.43
          ]
        },
        "vs_instruct": {
          "general_avg": -16.8,
          "math_avg": -16.04,
          "code_avg": -10.46,
          "reasoning_avg": -21.52,
          "overall_avg": -16.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -25.33,
            -28.82,
            -3.87,
            -9.18
          ],
          "math_task_scores": [
            -44.43,
            -22.53,
            2.32,
            -4.99,
            -8.34,
            -18.29
          ],
          "code_task_scores": [
            -16.87,
            -3.59,
            -20.94,
            -0.45
          ],
          "reasoning_task_scores": [
            -47.66,
            3.2,
            -24.61,
            -17.01
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 32.05,
      "math_avg": 20.94,
      "code_avg": 18.23,
      "reasoning_avg": 23.42,
      "overall_avg": 23.66,
      "overall_efficiency": -0.026004,
      "general_efficiency": -0.017701,
      "math_efficiency": 0.007706,
      "code_efficiency": -0.061133,
      "reasoning_efficiency": -0.03289,
      "general_scores": [
        65.46,
        22.3325,
        26.7,
        13.9314286,
        67.28,
        23.03,
        27.24,
        13.8142857,
        62.9,
        22.7125,
        26.45,
        12.8
      ],
      "math_scores": [
        74.75,
        32.8,
        10.34,
        6.38,
        0.0,
        0.0,
        76.72,
        32.8,
        9.96,
        7.42,
        0.0,
        0.0,
        76.88,
        31.8,
        9.78,
        7.33,
        0.0,
        0.0
      ],
      "code_scores": [
        19.84,
        0.0,
        0.0,
        52.54,
        17.9,
        0.0,
        0.0,
        56.27,
        20.62,
        0.0,
        0.0,
        51.53
      ],
      "reasoning_scores": [
        35.66,
        22.73,
        15.347826,
        23.68,
        35.94,
        19.7,
        14.304348,
        20.64,
        35.44,
        15.66,
        16.956522,
        24.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.03
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 19.45
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.68
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.36
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.54
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -6.99,
          "math_avg": 3.04,
          "code_avg": -24.15,
          "reasoning_avg": -12.99,
          "overall_avg": -10.27,
          "overall_efficiency": -0.026004,
          "general_efficiency": -0.017701,
          "math_efficiency": 0.007706,
          "code_efficiency": -0.061133,
          "reasoning_efficiency": -0.03289,
          "general_task_scores": [
            0.84,
            2.35,
            -9.85,
            -21.31
          ],
          "math_task_scores": [
            19.71,
            13.27,
            5.69,
            7.04,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -35.02,
            -3.58,
            -31.1,
            -26.89
          ],
          "reasoning_task_scores": [
            -26.88,
            -10.44,
            -15.66,
            1.01
          ]
        },
        "vs_instruct": {
          "general_avg": -24.49,
          "math_avg": -18.54,
          "code_avg": -36.92,
          "reasoning_avg": -20.78,
          "overall_avg": -25.18,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.69,
            -37.64,
            -14.18,
            -30.44
          ],
          "math_task_scores": [
            -9.55,
            -15.73,
            -2.59,
            -7.35,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -51.76,
            -9.68,
            -60.37,
            -25.87
          ],
          "reasoning_task_scores": [
            -27.17,
            -4.88,
            -26.62,
            -24.43
          ]
        }
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 40.23,
      "math_avg": 20.63,
      "code_avg": 32.16,
      "reasoning_avg": 32.34,
      "overall_avg": 31.34,
      "overall_efficiency": -0.009886,
      "general_efficiency": 0.004532,
      "math_efficiency": 0.010423,
      "code_efficiency": -0.038983,
      "reasoning_efficiency": -0.015515,
      "general_scores": [
        63.24,
        34.2625,
        32.89,
        27.605,
        63.57,
        35.44,
        34.86,
        27.0735714,
        70.25,
        32.9325,
        33.23,
        27.45
      ],
      "math_scores": [
        63.76,
        21.8,
        9.08,
        3.71,
        0.0,
        28.05,
        62.17,
        17.2,
        8.81,
        3.41,
        0.0,
        28.66,
        64.44,
        20.4,
        8.99,
        3.41,
        0.0,
        27.44
      ],
      "code_scores": [
        43.97,
        0.72,
        17.68,
        62.37,
        43.58,
        0.0,
        18.29,
        64.41,
        42.41,
        2.51,
        23.17,
        66.78
      ],
      "reasoning_scores": [
        49.76,
        26.26,
        25.086957,
        30.88,
        50.93,
        18.69,
        25.73913,
        28.32,
        49.5,
        24.33,
        27.836957,
        30.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.46
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.05
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.08
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 19.71
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.19,
          "math_avg": 2.73,
          "code_avg": -10.22,
          "reasoning_avg": -4.07,
          "overall_avg": -2.59,
          "overall_efficiency": -0.009886,
          "general_efficiency": 0.004532,
          "math_efficiency": 0.010423,
          "code_efficiency": -0.038983,
          "reasoning_efficiency": -0.015515,
          "general_task_scores": [
            1.32,
            13.87,
            -2.99,
            -7.45
          ],
          "math_task_scores": [
            7.05,
            0.6,
            4.62,
            3.51,
            0.0,
            0.61
          ],
          "code_task_scores": [
            -11.15,
            -2.5,
            -11.39,
            -15.82
          ],
          "reasoning_task_scores": [
            -12.5,
            -6.71,
            -4.98,
            7.92
          ]
        },
        "vs_instruct": {
          "general_avg": -16.31,
          "math_avg": -18.85,
          "code_avg": -22.99,
          "reasoning_avg": -11.85,
          "overall_avg": -17.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.21,
            -26.12,
            -7.32,
            -16.58
          ],
          "math_task_scores": [
            -22.21,
            -28.4,
            -3.66,
            -10.88,
            -8.34,
            -39.63
          ],
          "code_task_scores": [
            -27.89,
            -8.6,
            -40.66,
            -14.8
          ],
          "reasoning_task_scores": [
            -12.79,
            -1.15,
            -15.94,
            -17.52
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 29.99,
      "math_avg": 19.47,
      "code_avg": 20.2,
      "reasoning_avg": 16.39,
      "overall_avg": 21.51,
      "overall_efficiency": -0.416258,
      "general_efficiency": -0.303553,
      "math_efficiency": 0.052601,
      "code_efficiency": -0.742982,
      "reasoning_efficiency": -0.671099,
      "general_scores": [
        43.68,
        26.495,
        26.7,
        24.1247,
        41.93,
        25.9,
        27.24,
        24.4721429,
        42.24,
        26.5475,
        26.45,
        24.0892857
      ],
      "math_scores": [
        74.75,
        32.8,
        5.85,
        2.23,
        0.0,
        0.0,
        76.72,
        32.8,
        6.12,
        2.67,
        0.0,
        0.0,
        76.88,
        31.8,
        6.17,
        1.63,
        0.0,
        0.0
      ],
      "code_scores": [
        28.02,
        0.0,
        0.0,
        52.54,
        26.46,
        0.0,
        0.0,
        56.27,
        27.63,
        0.0,
        0.0,
        51.53
      ],
      "reasoning_scores": [
        3.9,
        23.56,
        13.880435,
        23.68,
        3.64,
        23.74,
        16.217391,
        20.64,
        4.71,
        22.22,
        15.48913,
        24.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.05
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.08
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -9.06,
          "math_avg": 1.57,
          "code_avg": -22.17,
          "reasoning_avg": -20.02,
          "overall_avg": -12.42,
          "overall_efficiency": -0.416258,
          "general_efficiency": -0.303553,
          "math_efficiency": 0.052601,
          "code_efficiency": -0.742982,
          "reasoning_efficiency": -0.671099,
          "general_task_scores": [
            -21.75,
            5.97,
            -9.85,
            -10.6
          ],
          "math_task_scores": [
            19.71,
            13.27,
            1.71,
            2.18,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -27.1,
            -3.58,
            -31.1,
            -26.89
          ],
          "reasoning_task_scores": [
            -58.48,
            -6.63,
            -16.0,
            1.01
          ]
        },
        "vs_instruct": {
          "general_avg": -26.55,
          "math_avg": -20.01,
          "code_avg": -34.94,
          "reasoning_avg": -27.81,
          "overall_avg": -27.33,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.28,
            -34.02,
            -14.18,
            -19.73
          ],
          "math_task_scores": [
            -9.55,
            -15.73,
            -6.57,
            -12.21,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -43.84,
            -9.68,
            -60.37,
            -25.87
          ],
          "reasoning_task_scores": [
            -58.77,
            -1.07,
            -26.96,
            -24.43
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 39.26,
      "math_avg": 14.72,
      "code_avg": 32.8,
      "reasoning_avg": 30.75,
      "overall_avg": 29.38,
      "overall_efficiency": -0.090987,
      "general_efficiency": 0.004348,
      "math_efficiency": -0.063567,
      "code_efficiency": -0.1915,
      "reasoning_efficiency": -0.11323,
      "general_scores": [
        74.15,
        21.0,
        32.71,
        28.4,
        74.15,
        24.7925,
        33.03,
        28.1971429,
        72.39,
        21.8575,
        33.0,
        27.4857143
      ],
      "math_scores": [
        54.06,
        18.6,
        8.79,
        3.71,
        0.0,
        0.0,
        54.66,
        18.2,
        9.55,
        3.86,
        0.0,
        0.0,
        54.28,
        20.0,
        8.9,
        5.19,
        3.33,
        1.83
      ],
      "code_scores": [
        54.09,
        0.36,
        0.0,
        74.92,
        55.25,
        1.43,
        1.22,
        74.58,
        56.81,
        1.08,
        0.61,
        73.22
      ],
      "reasoning_scores": [
        54.08,
        22.13,
        16.869565,
        31.76,
        55.61,
        19.19,
        16.869565,
        31.68,
        52.05,
        22.22,
        15.163043,
        31.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.96
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.22,
          "math_avg": -3.18,
          "code_avg": -9.57,
          "reasoning_avg": -5.66,
          "overall_avg": -4.55,
          "overall_efficiency": -0.090987,
          "general_efficiency": 0.004348,
          "math_efficiency": -0.063567,
          "code_efficiency": -0.1915,
          "reasoning_efficiency": -0.11323,
          "general_task_scores": [
            9.19,
            2.21,
            -3.74,
            -6.8
          ],
          "math_task_scores": [
            -2.08,
            -0.27,
            4.74,
            4.25,
            1.11,
            -26.83
          ],
          "code_task_scores": [
            0.91,
            -2.62,
            -30.49,
            -6.1
          ],
          "reasoning_task_scores": [
            -8.65,
            -8.62,
            -14.9,
            9.52
          ]
        },
        "vs_instruct": {
          "general_avg": -17.28,
          "math_avg": -24.76,
          "code_avg": -22.35,
          "reasoning_avg": -13.44,
          "overall_avg": -19.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.34,
            -37.78,
            -8.07,
            -15.93
          ],
          "math_task_scores": [
            -31.34,
            -29.27,
            -3.54,
            -10.14,
            -7.22,
            -67.07
          ],
          "code_task_scores": [
            -15.83,
            -8.72,
            -59.76,
            -5.08
          ],
          "reasoning_task_scores": [
            -8.94,
            -3.06,
            -25.86,
            -15.92
          ]
        }
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 26,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 42.98,
      "math_avg": 17.46,
      "code_avg": 37.98,
      "reasoning_avg": 30.98,
      "overall_avg": 32.35,
      "overall_efficiency": -0.052924,
      "general_efficiency": 0.131764,
      "math_efficiency": -0.014514,
      "code_efficiency": -0.147034,
      "reasoning_efficiency": -0.181913,
      "general_scores": [
        62.36,
        33.68,
        39.18,
        32.2707143,
        71.51,
        32.125,
        39.24,
        32.4614286,
        67.61,
        31.85,
        40.16,
        33.31571
      ],
      "math_scores": [
        57.77,
        29.6,
        11.36,
        4.45,
        0.0,
        2.44,
        56.71,
        30.4,
        11.36,
        3.71,
        0.0,
        1.83,
        55.88,
        31.8,
        11.0,
        4.23,
        0.0,
        1.83
      ],
      "code_scores": [
        55.64,
        2.51,
        25.61,
        75.59,
        52.14,
        2.87,
        24.39,
        74.24,
        54.47,
        2.51,
        14.63,
        71.19
      ],
      "reasoning_scores": [
        39.81,
        24.75,
        28.271739,
        32.16,
        39.74,
        21.72,
        28.728261,
        31.6,
        38.89,
        25.76,
        28.793478,
        31.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.16
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.03
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.63
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.54
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.48
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.6
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.93,
          "math_avg": -0.43,
          "code_avg": -4.39,
          "reasoning_avg": -5.43,
          "overall_avg": -1.58,
          "overall_efficiency": -0.052924,
          "general_efficiency": 0.131764,
          "math_efficiency": -0.014514,
          "code_efficiency": -0.147034,
          "reasoning_efficiency": -0.181913,
          "general_task_scores": [
            2.79,
            12.21,
            2.88,
            -2.15
          ],
          "math_task_scores": [
            0.38,
            11.4,
            6.9,
            4.13,
            0.0,
            -25.41
          ],
          "code_task_scores": [
            -0.39,
            -0.95,
            -9.56,
            -6.67
          ],
          "reasoning_task_scores": [
            -23.08,
            -5.72,
            -2.6,
            9.68
          ]
        },
        "vs_instruct": {
          "general_avg": -13.56,
          "math_avg": -22.02,
          "code_avg": -17.16,
          "reasoning_avg": -13.21,
          "overall_avg": -16.49,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -13.74,
            -27.78,
            -1.45,
            -11.28
          ],
          "math_task_scores": [
            -28.88,
            -17.6,
            -1.38,
            -10.26,
            -8.34,
            -65.65
          ],
          "code_task_scores": [
            -17.13,
            -7.05,
            -38.83,
            -5.65
          ],
          "reasoning_task_scores": [
            -23.37,
            -0.16,
            -13.56,
            -15.76
          ]
        }
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 33.93,
      "math_avg": 30.81,
      "code_avg": 41.09,
      "reasoning_avg": 29.57,
      "overall_avg": 33.85,
      "overall_efficiency": -0.000538,
      "general_efficiency": -0.034092,
      "math_efficiency": 0.086093,
      "code_efficiency": -0.008558,
      "reasoning_efficiency": -0.045596,
      "general_scores": [
        42.23,
        31.32,
        36.01,
        18.6878571,
        47.94,
        31.6875,
        36.92,
        26.4385714,
        41.87,
        33.075,
        37.22,
        23.805
      ],
      "math_scores": [
        79.38,
        39.2,
        14.52,
        17.06,
        3.33,
        31.71,
        79.08,
        40.2,
        14.21,
        14.39,
        0.0,
        33.54,
        80.44,
        39.6,
        15.15,
        16.77,
        0.0,
        35.98
      ],
      "code_scores": [
        55.64,
        6.45,
        28.66,
        68.14,
        56.81,
        7.53,
        30.49,
        73.22,
        54.86,
        8.24,
        30.49,
        72.54
      ],
      "reasoning_scores": [
        34.29,
        21.21,
        27.228261,
        39.2,
        31.44,
        17.68,
        25.282609,
        39.36,
        32.99,
        19.7,
        26.967391,
        39.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.74
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.41
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.88
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.49
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -5.11,
          "math_avg": 12.91,
          "code_avg": -1.28,
          "reasoning_avg": -6.84,
          "overall_avg": -0.08,
          "overall_efficiency": -0.000538,
          "general_efficiency": -0.034092,
          "math_efficiency": 0.086093,
          "code_efficiency": -0.008558,
          "reasoning_efficiency": -0.045596,
          "general_task_scores": [
            -20.36,
            11.69,
            0.07,
            -11.85
          ],
          "math_task_scores": [
            23.22,
            20.47,
            10.29,
            16.07,
            1.11,
            6.3
          ],
          "code_task_scores": [
            1.3,
            3.83,
            -1.22,
            -9.04
          ],
          "reasoning_task_scores": [
            -29.65,
            -10.27,
            -4.71,
            17.28
          ]
        },
        "vs_instruct": {
          "general_avg": -22.61,
          "math_avg": -8.67,
          "code_avg": -14.06,
          "reasoning_avg": -14.62,
          "overall_avg": -14.99,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.89,
            -28.3,
            -4.26,
            -20.98
          ],
          "math_task_scores": [
            -6.04,
            -8.53,
            2.01,
            1.68,
            -7.22,
            -33.94
          ],
          "code_task_scores": [
            -15.44,
            -2.27,
            -30.49,
            -8.02
          ],
          "reasoning_task_scores": [
            -29.94,
            -4.71,
            -15.67,
            -8.16
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 34.52,
      "math_avg": 20.17,
      "code_avg": 26.74,
      "reasoning_avg": 27.81,
      "overall_avg": 27.31,
      "overall_efficiency": -0.331028,
      "general_efficiency": -0.226198,
      "math_efficiency": 0.113472,
      "code_efficiency": -0.781542,
      "reasoning_efficiency": -0.429848,
      "general_scores": [
        52.17,
        26.94,
        33.7,
        31.1328571,
        43.26,
        25.765,
        33.23,
        26.4592857,
        53.1,
        25.1,
        34.15,
        29.2592857
      ],
      "math_scores": [
        74.53,
        26.0,
        11.5,
        7.86,
        0.0,
        0.0,
        75.36,
        26.6,
        10.84,
        8.31,
        0.0,
        0.0,
        75.51,
        27.6,
        10.9,
        8.01,
        0.0,
        0.0
      ],
      "code_scores": [
        48.64,
        0.0,
        0.0,
        70.85,
        35.8,
        0.0,
        0.0,
        67.46,
        33.07,
        0.0,
        0.0,
        65.08
      ],
      "reasoning_scores": [
        34.07,
        14.14,
        24.25,
        35.6,
        38.59,
        16.67,
        23.369565,
        36.08,
        33.76,
        17.17,
        23.336957,
        36.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.13
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.08
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.8
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.65
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.52,
          "math_avg": 2.27,
          "code_avg": -15.63,
          "reasoning_avg": -8.6,
          "overall_avg": -6.62,
          "overall_efficiency": -0.331028,
          "general_efficiency": -0.226198,
          "math_efficiency": 0.113472,
          "code_efficiency": -0.781542,
          "reasoning_efficiency": -0.429848,
          "general_task_scores": [
            -14.86,
            5.6,
            -2.96,
            -5.88
          ],
          "math_task_scores": [
            18.72,
            7.53,
            6.74,
            8.06,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -15.3,
            -3.58,
            -31.1,
            -12.54
          ],
          "reasoning_task_scores": [
            -27.09,
            -13.81,
            -7.55,
            14.05
          ]
        },
        "vs_instruct": {
          "general_avg": -22.02,
          "math_avg": -19.31,
          "code_avg": -28.4,
          "reasoning_avg": -16.38,
          "overall_avg": -21.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -31.39,
            -34.39,
            -7.29,
            -15.01
          ],
          "math_task_scores": [
            -10.54,
            -21.47,
            -1.54,
            -6.33,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -32.04,
            -9.68,
            -60.37,
            -11.52
          ],
          "reasoning_task_scores": [
            -27.38,
            -8.25,
            -18.51,
            -11.39
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 41.19,
      "math_avg": 38.09,
      "code_avg": 28.36,
      "reasoning_avg": 24.82,
      "overall_avg": 33.11,
      "overall_efficiency": -0.000817,
      "general_efficiency": 0.002132,
      "math_efficiency": 0.02012,
      "code_efficiency": -0.013968,
      "reasoning_efficiency": -0.011549,
      "general_scores": [
        81.82,
        23.6725,
        38.11,
        20.105,
        83.71,
        24.2225,
        38.43,
        20.1028571,
        83.04,
        23.88,
        38.65,
        18.4838462
      ],
      "math_scores": [
        88.48,
        64.4,
        21.0,
        28.34,
        6.67,
        19.51,
        90.67,
        64.0,
        20.57,
        28.49,
        0.0,
        23.17,
        88.55,
        67.2,
        20.89,
        27.45,
        6.67,
        19.51
      ],
      "code_scores": [
        30.74,
        2.51,
        16.46,
        62.71,
        35.02,
        2.15,
        20.73,
        62.71,
        31.91,
        0.36,
        14.63,
        60.34
      ],
      "reasoning_scores": [
        20.33,
        28.79,
        18.402174,
        33.36,
        22.91,
        22.73,
        18.641304,
        35.92,
        20.61,
        23.23,
        18.130435,
        34.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.73
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.67
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.27
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.92
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.92
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.14,
          "math_avg": 20.19,
          "code_avg": -14.02,
          "reasoning_avg": -11.59,
          "overall_avg": -0.82,
          "overall_efficiency": -0.000817,
          "general_efficiency": 0.002132,
          "math_efficiency": 0.02012,
          "code_efficiency": -0.013968,
          "reasoning_efficiency": -0.011549,
          "general_task_scores": [
            18.49,
            3.58,
            1.75,
            -15.27
          ],
          "math_task_scores": [
            32.82,
            46.0,
            16.48,
            28.09,
            4.45,
            -6.71
          ],
          "code_task_scores": [
            -21.91,
            -1.91,
            -13.83,
            -18.42
          ],
          "reasoning_task_scores": [
            -41.28,
            -4.88,
            -12.81,
            12.61
          ]
        },
        "vs_instruct": {
          "general_avg": -15.36,
          "math_avg": -1.4,
          "code_avg": -26.79,
          "reasoning_avg": -19.37,
          "overall_avg": -15.73,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.96,
            -36.4,
            -2.58,
            -24.4
          ],
          "math_task_scores": [
            3.56,
            17.0,
            8.2,
            13.7,
            -3.89,
            -46.95
          ],
          "code_task_scores": [
            -38.65,
            -8.01,
            -43.1,
            -17.4
          ],
          "reasoning_task_scores": [
            -41.57,
            0.68,
            -23.77,
            -12.83
          ]
        }
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 49.16,
      "math_avg": 49.79,
      "code_avg": 22.84,
      "reasoning_avg": 29.66,
      "overall_avg": 37.86,
      "overall_efficiency": 0.012718,
      "general_efficiency": 0.032718,
      "math_efficiency": 0.103187,
      "code_efficiency": -0.063207,
      "reasoning_efficiency": -0.021826,
      "general_scores": [
        78.91,
        29.9,
        47.26,
        40.5628571
      ],
      "math_scores": [
        88.86,
        84.8,
        40.67,
        58.31,
        20.0,
        6.1
      ],
      "code_scores": [
        1.95,
        0.72,
        10.37,
        78.31
      ],
      "reasoning_scores": [
        19.22,
        20.71,
        33.847826,
        44.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.67
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.31
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.1
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 1.95
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.72
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.37
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.85
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 10.11,
          "math_avg": 31.89,
          "code_avg": -19.54,
          "reasoning_avg": -6.75,
          "overall_avg": 3.93,
          "overall_efficiency": 0.012718,
          "general_efficiency": 0.032718,
          "math_efficiency": 0.103187,
          "code_efficiency": -0.063207,
          "reasoning_efficiency": -0.021826,
          "general_task_scores": [
            14.54,
            9.56,
            10.61,
            5.73
          ],
          "math_task_scores": [
            32.45,
            65.6,
            36.33,
            58.31,
            20.0,
            -21.34
          ],
          "code_task_scores": [
            -52.52,
            -2.86,
            -20.73,
            -2.03
          ],
          "reasoning_task_scores": [
            -43.34,
            -9.09,
            2.65,
            22.8
          ]
        },
        "vs_instruct": {
          "general_avg": -7.38,
          "math_avg": 10.31,
          "code_avg": -32.31,
          "reasoning_avg": -14.53,
          "overall_avg": -10.98,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.99,
            -30.43,
            6.28,
            -3.4
          ],
          "math_task_scores": [
            3.19,
            36.6,
            28.05,
            43.92,
            11.66,
            -61.58
          ],
          "code_task_scores": [
            -69.26,
            -8.96,
            -50.0,
            -1.01
          ],
          "reasoning_task_scores": [
            -43.63,
            -3.53,
            -8.31,
            -2.64
          ]
        }
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 38.09,
      "math_avg": 25.55,
      "code_avg": 28.93,
      "reasoning_avg": 25.84,
      "overall_avg": 29.6,
      "overall_efficiency": -0.025535,
      "general_efficiency": -0.00565,
      "math_efficiency": 0.045149,
      "code_efficiency": -0.079274,
      "reasoning_efficiency": -0.062364,
      "general_scores": [
        69.02,
        23.81,
        34.83,
        26.6914286,
        62.72,
        24.03,
        36.17,
        21.8414286,
        72.35,
        25.4725,
        35.77,
        24.355
      ],
      "math_scores": [
        73.69,
        47.4,
        16.24,
        12.17,
        3.33,
        0.0,
        74.22,
        49.8,
        16.46,
        13.65,
        3.33,
        0.61,
        72.86,
        46.4,
        15.83,
        13.95,
        0.0,
        0.0
      ],
      "code_scores": [
        43.19,
        0.0,
        0.0,
        70.17,
        45.91,
        0.0,
        0.0,
        72.2,
        43.19,
        0.0,
        0.0,
        72.54
      ],
      "reasoning_scores": [
        34.49,
        15.66,
        20.608696,
        29.76,
        34.21,
        17.17,
        21.119565,
        31.52,
        37.13,
        15.15,
        20.913043,
        32.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.44
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.18
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.64
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.88
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.96,
          "math_avg": 7.65,
          "code_avg": -13.44,
          "reasoning_avg": -10.57,
          "overall_avg": -4.33,
          "overall_efficiency": -0.025535,
          "general_efficiency": -0.00565,
          "math_efficiency": 0.045149,
          "code_efficiency": -0.079274,
          "reasoning_efficiency": -0.062364,
          "general_task_scores": [
            3.66,
            4.1,
            -1.06,
            -10.53
          ],
          "math_task_scores": [
            17.18,
            28.67,
            11.84,
            13.26,
            2.22,
            -27.24
          ],
          "code_task_scores": [
            -10.37,
            -3.58,
            -31.1,
            -8.7
          ],
          "reasoning_task_scores": [
            -27.28,
            -13.81,
            -10.32,
            9.12
          ]
        },
        "vs_instruct": {
          "general_avg": -18.45,
          "math_avg": -13.93,
          "code_avg": -26.21,
          "reasoning_avg": -18.36,
          "overall_avg": -19.24,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.87,
            -35.89,
            -5.39,
            -19.66
          ],
          "math_task_scores": [
            -12.08,
            -0.33,
            3.56,
            -1.13,
            -6.12,
            -67.48
          ],
          "code_task_scores": [
            -27.11,
            -9.68,
            -60.37,
            -7.68
          ],
          "reasoning_task_scores": [
            -27.57,
            -8.25,
            -21.28,
            -16.32
          ]
        }
      },
      "affiliation": "Ruben Roy",
      "year": "2025",
      "size": "170k",
      "size_precise": "169527",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 40.6,
      "math_avg": 25.02,
      "code_avg": 25.68,
      "reasoning_avg": 29.6,
      "overall_avg": 30.23,
      "overall_efficiency": -0.01853,
      "general_efficiency": 0.007777,
      "math_efficiency": 0.035591,
      "code_efficiency": -0.083422,
      "reasoning_efficiency": -0.034066,
      "general_scores": [
        73.45,
        29.815,
        33.55,
        24.0585714,
        73.13,
        29.4225,
        34.41,
        25.3292857,
        75.09,
        31.1675,
        32.41,
        25.39
      ],
      "math_scores": [
        85.75,
        27.6,
        11.18,
        7.42,
        0.0,
        20.12,
        85.97,
        28.6,
        10.98,
        7.12,
        0.0,
        17.07,
        86.05,
        28.2,
        11.11,
        6.08,
        0.0,
        17.07
      ],
      "code_scores": [
        36.19,
        1.08,
        4.88,
        58.64,
        40.86,
        1.79,
        3.05,
        61.69,
        39.3,
        1.08,
        6.1,
        53.56
      ],
      "reasoning_scores": [
        43.53,
        18.69,
        22.565217,
        33.6,
        44.5,
        20.71,
        21.793478,
        30.88,
        44.49,
        19.19,
        22.23913,
        32.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.93
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.09
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.09
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 38.78
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.32
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.68
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.56,
          "math_avg": 7.12,
          "code_avg": -16.69,
          "reasoning_avg": -6.81,
          "overall_avg": -3.71,
          "overall_efficiency": -0.01853,
          "general_efficiency": 0.007777,
          "math_efficiency": 0.035591,
          "code_efficiency": -0.083422,
          "reasoning_efficiency": -0.034066,
          "general_task_scores": [
            9.52,
            9.8,
            -3.19,
            -9.9
          ],
          "math_task_scores": [
            29.51,
            8.93,
            6.75,
            6.87,
            0.0,
            -9.35
          ],
          "code_task_scores": [
            -15.69,
            -2.26,
            -26.42,
            -22.38
          ],
          "reasoning_task_scores": [
            -18.39,
            -10.27,
            -9.0,
            10.4
          ]
        },
        "vs_instruct": {
          "general_avg": -15.94,
          "math_avg": -14.46,
          "code_avg": -29.46,
          "reasoning_avg": -14.6,
          "overall_avg": -18.62,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.01,
            -30.18,
            -7.52,
            -19.03
          ],
          "math_task_scores": [
            0.25,
            -20.07,
            -1.53,
            -7.52,
            -8.34,
            -49.59
          ],
          "code_task_scores": [
            -32.43,
            -8.36,
            -55.69,
            -21.36
          ],
          "reasoning_task_scores": [
            -18.68,
            -4.71,
            -19.96,
            -15.04
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 40.26,
      "math_avg": 30.76,
      "code_avg": 32.15,
      "reasoning_avg": 28.62,
      "overall_avg": 32.95,
      "overall_efficiency": -0.00168,
      "general_efficiency": 0.002079,
      "math_efficiency": 0.021979,
      "code_efficiency": -0.017468,
      "reasoning_efficiency": -0.01331,
      "general_scores": [
        75.63,
        24.375,
        36.13,
        25.4164286,
        74.56,
        25.16,
        36.13,
        25.97,
        74.58,
        23.74,
        35.7,
        25.7685714
      ],
      "math_scores": [
        83.7,
        48.0,
        14.61,
        15.73,
        3.33,
        18.9,
        83.7,
        48.0,
        14.84,
        16.02,
        0.0,
        19.51,
        84.31,
        47.2,
        14.59,
        15.43,
        3.33,
        22.56
      ],
      "code_scores": [
        40.86,
        0.36,
        18.29,
        62.37,
        47.86,
        1.08,
        22.56,
        62.37,
        47.47,
        2.15,
        17.68,
        62.71
      ],
      "reasoning_scores": [
        44.06,
        19.19,
        19.586957,
        32.32,
        42.44,
        18.18,
        21.054348,
        32.32,
        43.34,
        18.18,
        19.315217,
        33.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.68
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.32
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 45.4
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 19.51
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.99
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.22,
          "math_avg": 12.87,
          "code_avg": -10.23,
          "reasoning_avg": -7.79,
          "overall_avg": -0.98,
          "overall_efficiency": -0.00168,
          "general_efficiency": 0.002079,
          "math_efficiency": 0.021979,
          "code_efficiency": -0.017468,
          "reasoning_efficiency": -0.01331,
          "general_task_scores": [
            10.55,
            4.08,
            -0.66,
            -9.11
          ],
          "math_task_scores": [
            27.49,
            28.53,
            10.34,
            15.73,
            2.22,
            -7.12
          ],
          "code_task_scores": [
            -9.07,
            -2.38,
            -11.59,
            -17.86
          ],
          "reasoning_task_scores": [
            -19.28,
            -11.28,
            -11.21,
            10.61
          ]
        },
        "vs_instruct": {
          "general_avg": -16.28,
          "math_avg": -8.72,
          "code_avg": -23.0,
          "reasoning_avg": -15.57,
          "overall_avg": -15.89,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.98,
            -35.9,
            -4.99,
            -18.24
          ],
          "math_task_scores": [
            -1.77,
            -0.47,
            2.06,
            1.34,
            -6.12,
            -47.36
          ],
          "code_task_scores": [
            -25.81,
            -8.48,
            -40.86,
            -16.84
          ],
          "reasoning_task_scores": [
            -19.57,
            -5.72,
            -22.17,
            -14.83
          ]
        }
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 36.29,
      "math_avg": 17.27,
      "code_avg": 28.83,
      "reasoning_avg": 22.96,
      "overall_avg": 26.34,
      "overall_efficiency": -0.322162,
      "general_efficiency": -0.116958,
      "math_efficiency": -0.02672,
      "code_efficiency": -0.57437,
      "reasoning_efficiency": -0.570599,
      "general_scores": [
        76.87,
        22.8025,
        29.56,
        17.1028571,
        76.48,
        21.865,
        31.14,
        17.2092857,
        75.01,
        21.5775,
        30.15,
        15.695
      ],
      "math_scores": [
        74.0,
        16.4,
        7.48,
        4.01,
        0.0,
        0.0,
        73.01,
        18.6,
        7.32,
        4.75,
        0.0,
        0.0,
        72.25,
        18.2,
        7.77,
        3.71,
        3.33,
        0.0
      ],
      "code_scores": [
        41.63,
        0.0,
        0.0,
        71.53,
        47.08,
        0.0,
        0.0,
        72.54,
        47.08,
        0.0,
        0.0,
        66.1
      ],
      "reasoning_scores": [
        25.66,
        14.65,
        18.434783,
        30.56,
        29.84,
        14.14,
        17.978261,
        32.24,
        27.13,
        16.67,
        18.173913,
        30.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.52
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 45.26
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.06
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.54
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -2.76,
          "math_avg": -0.63,
          "code_avg": -13.54,
          "reasoning_avg": -13.45,
          "overall_avg": -7.6,
          "overall_efficiency": -0.322162,
          "general_efficiency": -0.116958,
          "math_efficiency": -0.02672,
          "code_efficiency": -0.57437,
          "reasoning_efficiency": -0.570599,
          "general_task_scores": [
            11.75,
            1.74,
            -6.37,
            -18.16
          ],
          "math_task_scores": [
            16.68,
            -1.47,
            3.18,
            4.16,
            1.11,
            -27.44
          ],
          "code_task_scores": [
            -9.21,
            -3.58,
            -31.1,
            -10.28
          ],
          "reasoning_task_scores": [
            -35.02,
            -14.65,
            -13.0,
            8.85
          ]
        },
        "vs_instruct": {
          "general_avg": -20.25,
          "math_avg": -22.21,
          "code_avg": -26.31,
          "reasoning_avg": -21.24,
          "overall_avg": -22.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.78,
            -38.25,
            -10.7,
            -27.29
          ],
          "math_task_scores": [
            -12.58,
            -30.47,
            -5.1,
            -10.23,
            -7.22,
            -67.68
          ],
          "code_task_scores": [
            -25.95,
            -9.68,
            -60.37,
            -9.26
          ],
          "reasoning_task_scores": [
            -35.31,
            -9.09,
            -23.96,
            -16.59
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 45.58,
      "math_avg": 26.37,
      "code_avg": 51.89,
      "reasoning_avg": 39.38,
      "overall_avg": 40.8,
      "overall_efficiency": 0.007086,
      "general_efficiency": 0.006734,
      "math_efficiency": 0.008735,
      "code_efficiency": 0.009812,
      "reasoning_efficiency": 0.003061,
      "general_scores": [
        68.29,
        42.195,
        37.6,
        32.3742857,
        69.3,
        42.1725,
        37.03,
        32.5442857,
        72.57,
        41.9,
        37.91,
        33.0542857
      ],
      "math_scores": [
        56.33,
        21.6,
        10.89,
        3.86,
        3.33,
        64.02,
        52.62,
        20.8,
        11.11,
        3.86,
        3.33,
        64.02,
        54.66,
        20.4,
        11.7,
        3.86,
        0.0,
        68.29
      ],
      "code_scores": [
        66.93,
        6.81,
        56.1,
        78.98,
        64.98,
        5.02,
        57.32,
        77.97,
        64.2,
        5.02,
        60.37,
        78.98
      ],
      "reasoning_scores": [
        58.95,
        30.81,
        33.01087,
        35.92,
        58.69,
        28.79,
        34.315217,
        35.6,
        60.49,
        27.78,
        32.434783,
        35.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.66
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.23
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.86
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.62
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.64
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.13
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.53,
          "math_avg": 8.47,
          "code_avg": 9.52,
          "reasoning_avg": 2.97,
          "overall_avg": 6.87,
          "overall_efficiency": 0.007086,
          "general_efficiency": 0.006734,
          "math_efficiency": 0.008735,
          "code_efficiency": 0.009812,
          "reasoning_efficiency": 0.003061,
          "general_task_scores": [
            5.68,
            21.75,
            0.86,
            -2.17
          ],
          "math_task_scores": [
            -1.87,
            1.73,
            6.89,
            3.86,
            2.22,
            38.0
          ],
          "code_task_scores": [
            10.9,
            2.04,
            26.83,
            -1.7
          ],
          "reasoning_task_scores": [
            -3.18,
            -0.67,
            2.05,
            13.68
          ]
        },
        "vs_instruct": {
          "general_avg": -10.96,
          "math_avg": -13.11,
          "code_avg": -3.26,
          "reasoning_avg": -4.81,
          "overall_avg": -8.04,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.85,
            -18.24,
            -3.47,
            -11.3
          ],
          "math_task_scores": [
            -31.13,
            -27.27,
            -1.39,
            -10.53,
            -6.12,
            -2.24
          ],
          "code_task_scores": [
            -5.84,
            -4.06,
            -2.44,
            -0.68
          ],
          "reasoning_task_scores": [
            -3.47,
            4.89,
            -8.91,
            -11.76
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 30.56,
      "math_avg": 18.14,
      "code_avg": 34.17,
      "reasoning_avg": 22.51,
      "overall_avg": 26.34,
      "overall_efficiency": -0.007744,
      "general_efficiency": -0.00866,
      "math_efficiency": 0.000246,
      "code_efficiency": -0.008374,
      "reasoning_efficiency": -0.014189,
      "general_scores": [
        49.43,
        32.3675,
        25.81,
        14.0,
        35.65,
        28.535,
        29.6,
        13.3678571,
        57.92,
        28.1425,
        29.6,
        22.2878571
      ],
      "math_scores": [
        58.45,
        20.8,
        9.85,
        5.04,
        0.0,
        9.76,
        63.61,
        16.6,
        10.34,
        4.01,
        0.0,
        19.51,
        63.61,
        16.6,
        11.52,
        4.01,
        0.0,
        12.8
      ],
      "code_scores": [
        45.53,
        2.87,
        26.83,
        63.39,
        45.53,
        3.23,
        23.78,
        62.03,
        46.3,
        2.87,
        25.61,
        62.03
      ],
      "reasoning_scores": [
        18.38,
        21.21,
        22.717391,
        32.32,
        15.63,
        14.65,
        21.304348,
        32.16,
        19.87,
        17.68,
        21.98913,
        32.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.57
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.99
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.41
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.49,
          "math_avg": 0.24,
          "code_avg": -8.21,
          "reasoning_avg": -13.9,
          "overall_avg": -7.59,
          "overall_efficiency": -0.007744,
          "general_efficiency": -0.00866,
          "math_efficiency": 0.000246,
          "code_efficiency": -0.008374,
          "reasoning_efficiency": -0.014189,
          "general_task_scores": [
            -16.7,
            9.34,
            -8.31,
            -18.28
          ],
          "math_task_scores": [
            5.48,
            -1.2,
            6.23,
            4.35,
            0.0,
            -13.42
          ],
          "code_task_scores": [
            -8.68,
            -0.59,
            -5.69,
            -17.86
          ],
          "reasoning_task_scores": [
            -44.6,
            -11.95,
            -9.2,
            10.13
          ]
        },
        "vs_instruct": {
          "general_avg": -25.98,
          "math_avg": -21.34,
          "code_avg": -20.98,
          "reasoning_avg": -21.69,
          "overall_avg": -22.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -33.23,
            -30.65,
            -12.64,
            -27.41
          ],
          "math_task_scores": [
            -23.78,
            -30.2,
            -2.05,
            -10.04,
            -8.34,
            -53.66
          ],
          "code_task_scores": [
            -25.42,
            -6.69,
            -34.96,
            -16.84
          ],
          "reasoning_task_scores": [
            -44.89,
            -6.39,
            -20.16,
            -15.31
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 37,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 38.38,
      "math_avg": 27.59,
      "code_avg": 41.17,
      "reasoning_avg": 26.98,
      "overall_avg": 33.53,
      "overall_efficiency": -0.008045,
      "general_efficiency": -0.013237,
      "math_efficiency": 0.193766,
      "code_efficiency": -0.02405,
      "reasoning_efficiency": -0.188659,
      "general_scores": [
        48.18,
        27.6075,
        37.34,
        35.5157143,
        60.17,
        28.75,
        38.25,
        36.5564286,
        47.39,
        28.1825,
        37.67,
        34.9992857
      ],
      "math_scores": [
        66.64,
        31.8,
        16.78,
        10.53,
        3.33,
        35.37,
        66.41,
        35.0,
        17.1,
        9.79,
        0.0,
        40.85,
        67.17,
        34.4,
        16.26,
        10.98,
        0.0,
        34.15
      ],
      "code_scores": [
        60.7,
        3.58,
        25.61,
        75.59,
        61.87,
        2.87,
        18.9,
        75.59,
        61.48,
        3.94,
        28.66,
        75.25
      ],
      "reasoning_scores": [
        24.23,
        21.21,
        23.5,
        35.52,
        22.58,
        30.81,
        24.684783,
        35.84,
        26.43,
        20.71,
        24.369565,
        33.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.43
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.79
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.46
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.66,
          "math_avg": 9.69,
          "code_avg": -1.2,
          "reasoning_avg": -9.43,
          "overall_avg": -0.4,
          "overall_efficiency": -0.008045,
          "general_efficiency": -0.013237,
          "math_efficiency": 0.193766,
          "code_efficiency": -0.02405,
          "reasoning_efficiency": -0.188659,
          "general_task_scores": [
            -12.46,
            7.84,
            1.1,
            0.86
          ],
          "math_task_scores": [
            10.33,
            14.53,
            12.37,
            10.43,
            1.11,
            9.35
          ],
          "code_task_scores": [
            6.88,
            -0.12,
            -6.71,
            -4.86
          ],
          "reasoning_task_scores": [
            -38.15,
            -5.56,
            -7.02,
            12.99
          ]
        },
        "vs_instruct": {
          "general_avg": -18.16,
          "math_avg": -11.9,
          "code_avg": -13.97,
          "reasoning_avg": -17.22,
          "overall_avg": -15.31,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -28.99,
            -32.15,
            -3.23,
            -8.27
          ],
          "math_task_scores": [
            -18.93,
            -14.47,
            4.09,
            -3.96,
            -7.22,
            -30.89
          ],
          "code_task_scores": [
            -9.86,
            -6.22,
            -35.98,
            -3.84
          ],
          "reasoning_task_scores": [
            -38.44,
            0.0,
            -17.98,
            -12.45
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 38,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 44.78,
      "math_avg": 31.78,
      "code_avg": 43.65,
      "reasoning_avg": 35.17,
      "overall_avg": 38.85,
      "overall_efficiency": 0.005497,
      "general_efficiency": 0.006415,
      "math_efficiency": 0.015528,
      "code_efficiency": 0.001432,
      "reasoning_efficiency": -0.001388,
      "general_scores": [
        77.43,
        33.9675,
        38.04,
        32.0078571,
        78.58,
        31.5525,
        36.28,
        31.13,
        78.3,
        34.055,
        36.53,
        29.4914286
      ],
      "math_scores": [
        87.41,
        36.6,
        13.14,
        11.28,
        0.0,
        44.51,
        86.13,
        40.2,
        13.71,
        13.06,
        0.0,
        40.24,
        85.97,
        39.8,
        13.48,
        11.13,
        0.0,
        35.37
      ],
      "code_scores": [
        60.7,
        3.58,
        33.54,
        76.61,
        56.03,
        4.3,
        32.32,
        77.63,
        60.31,
        3.58,
        36.59,
        78.64
      ],
      "reasoning_scores": [
        55.0,
        21.21,
        28.184783,
        40.8,
        51.36,
        21.21,
        27.869565,
        39.2,
        50.53,
        20.2,
        28.467391,
        38.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.44
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.04
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 59.01
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.82
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.15
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.73,
          "math_avg": 13.88,
          "code_avg": 1.28,
          "reasoning_avg": -1.24,
          "overall_avg": 4.91,
          "overall_efficiency": 0.005497,
          "general_efficiency": 0.006415,
          "math_efficiency": 0.015528,
          "code_efficiency": 0.001432,
          "reasoning_efficiency": -0.001388,
          "general_task_scores": [
            13.73,
            12.85,
            0.3,
            -3.95
          ],
          "math_task_scores": [
            30.09,
            19.67,
            9.1,
            11.82,
            0.0,
            12.6
          ],
          "code_task_scores": [
            4.54,
            0.24,
            3.05,
            -2.71
          ],
          "reasoning_task_scores": [
            -10.26,
            -8.93,
            -3.03,
            17.25
          ]
        },
        "vs_instruct": {
          "general_avg": -11.76,
          "math_avg": -7.7,
          "code_avg": -11.49,
          "reasoning_avg": -9.02,
          "overall_avg": -10.0,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.8,
            -27.14,
            -4.03,
            -13.08
          ],
          "math_task_scores": [
            0.83,
            -9.33,
            0.82,
            -2.57,
            -8.34,
            -27.64
          ],
          "code_task_scores": [
            -12.2,
            -5.86,
            -26.22,
            -1.69
          ],
          "reasoning_task_scores": [
            -10.55,
            -3.37,
            -13.99,
            -8.19
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 44.01,
      "math_avg": 35.7,
      "code_avg": 45.05,
      "reasoning_avg": 27.47,
      "overall_avg": 38.06,
      "overall_efficiency": 0.020626,
      "general_efficiency": 0.024813,
      "math_efficiency": 0.089001,
      "code_efficiency": 0.013365,
      "reasoning_efficiency": -0.044676,
      "general_scores": [
        78.62,
        33.4775,
        42.71,
        22.755,
        78.94,
        33.3025,
        42.78,
        20.2385714,
        78.89,
        33.2825,
        42.44,
        20.68
      ],
      "math_scores": [
        88.7,
        42.4,
        14.68,
        15.58,
        6.67,
        44.51,
        88.78,
        42.0,
        15.2,
        16.62,
        3.33,
        46.95,
        89.01,
        41.6,
        14.93,
        14.39,
        6.67,
        50.61
      ],
      "code_scores": [
        55.25,
        5.38,
        49.39,
        74.24,
        55.64,
        6.81,
        40.24,
        73.22,
        55.64,
        7.89,
        46.34,
        70.51
      ],
      "reasoning_scores": [
        21.41,
        20.2,
        27.847826,
        42.4,
        20.92,
        16.16,
        27.195652,
        40.08,
        21.02,
        23.23,
        26.173913,
        43.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.82
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.36
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.66
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.12
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.07
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.96,
          "math_avg": 17.8,
          "code_avg": 2.67,
          "reasoning_avg": -8.94,
          "overall_avg": 4.13,
          "overall_efficiency": 0.020626,
          "general_efficiency": 0.024813,
          "math_efficiency": 0.089001,
          "code_efficiency": 0.013365,
          "reasoning_efficiency": -0.044676,
          "general_task_scores": [
            14.45,
            13.01,
            5.99,
            -13.61
          ],
          "math_task_scores": [
            32.42,
            22.8,
            10.6,
            15.53,
            5.56,
            19.92
          ],
          "code_task_scores": [
            1.04,
            3.11,
            14.22,
            -7.68
          ],
          "reasoning_task_scores": [
            -41.44,
            -9.94,
            -4.13,
            19.76
          ]
        },
        "vs_instruct": {
          "general_avg": -12.53,
          "math_avg": -3.78,
          "code_avg": -10.1,
          "reasoning_avg": -16.72,
          "overall_avg": -10.78,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.08,
            -26.98,
            1.66,
            -22.74
          ],
          "math_task_scores": [
            3.16,
            -6.2,
            2.32,
            1.14,
            -2.78,
            -20.32
          ],
          "code_task_scores": [
            -15.7,
            -2.99,
            -15.05,
            -6.66
          ],
          "reasoning_task_scores": [
            -41.73,
            -4.38,
            -15.09,
            -5.68
          ]
        }
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 40.28,
      "math_avg": 31.84,
      "code_avg": 27.35,
      "reasoning_avg": 29.35,
      "overall_avg": 32.21,
      "overall_efficiency": -0.001925,
      "general_efficiency": 0.001372,
      "math_efficiency": 0.015557,
      "code_efficiency": -0.016758,
      "reasoning_efficiency": -0.007872,
      "general_scores": [
        74.77,
        25.6175,
        36.82,
        25.775,
        71.43,
        25.19,
        35.96,
        26.4435714,
        75.07,
        26.26,
        35.46,
        24.51
      ],
      "math_scores": [
        83.47,
        54.8,
        19.49,
        25.07,
        10.0,
        6.71,
        82.56,
        53.0,
        19.83,
        24.63,
        0.0,
        7.93,
        82.64,
        49.6,
        19.58,
        21.96,
        3.33,
        8.54
      ],
      "code_scores": [
        36.96,
        0.36,
        6.71,
        62.37,
        33.46,
        1.43,
        9.15,
        62.03,
        40.86,
        1.08,
        6.71,
        67.12
      ],
      "reasoning_scores": [
        39.04,
        21.21,
        21.836957,
        37.68,
        32.48,
        19.19,
        20.858696,
        39.2,
        35.3,
        25.76,
        21.782609,
        37.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.63
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.89
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 37.09
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.96
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.52
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.49
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.27
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.23,
          "math_avg": 13.94,
          "code_avg": -15.02,
          "reasoning_avg": -7.06,
          "overall_avg": -1.73,
          "overall_efficiency": -0.001925,
          "general_efficiency": 0.001372,
          "math_efficiency": 0.015557,
          "code_efficiency": -0.016758,
          "reasoning_efficiency": -0.007872,
          "general_task_scores": [
            9.39,
            5.35,
            -0.57,
            -9.25
          ],
          "math_task_scores": [
            26.48,
            33.27,
            15.29,
            23.89,
            4.44,
            -19.71
          ],
          "code_task_scores": [
            -17.38,
            -2.62,
            -23.58,
            -16.5
          ],
          "reasoning_task_scores": [
            -26.95,
            -7.75,
            -9.71,
            16.19
          ]
        },
        "vs_instruct": {
          "general_avg": -16.27,
          "math_avg": -7.64,
          "code_avg": -27.79,
          "reasoning_avg": -14.84,
          "overall_avg": -16.63,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.14,
            -34.64,
            -4.9,
            -18.38
          ],
          "math_task_scores": [
            -2.78,
            4.27,
            7.01,
            9.5,
            -3.9,
            -59.95
          ],
          "code_task_scores": [
            -34.12,
            -8.72,
            -52.85,
            -15.48
          ],
          "reasoning_task_scores": [
            -27.24,
            -2.19,
            -20.67,
            -9.25
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 44.06,
      "math_avg": 27.42,
      "code_avg": 33.79,
      "reasoning_avg": 34.69,
      "overall_avg": 34.99,
      "overall_efficiency": 0.01766,
      "general_efficiency": 0.083716,
      "math_efficiency": 0.158925,
      "code_efficiency": -0.14323,
      "reasoning_efficiency": -0.028771,
      "general_scores": [
        69.86,
        35.1975,
        37.65,
        31.24071,
        72.88,
        34.855,
        37.24,
        31.8835714,
        73.24,
        34.1825,
        38.54,
        31.9521429
      ],
      "math_scores": [
        82.71,
        36.6,
        11.04,
        18.1,
        0.0,
        12.8,
        79.91,
        34.0,
        10.75,
        17.66,
        3.33,
        14.02,
        81.43,
        39.2,
        10.86,
        18.25,
        3.33,
        19.51
      ],
      "code_scores": [
        49.03,
        2.51,
        7.93,
        66.1,
        51.36,
        2.51,
        17.68,
        67.12,
        53.31,
        1.08,
        17.07,
        69.83
      ],
      "reasoning_scores": [
        44.42,
        23.74,
        28.3913,
        37.76,
        51.66,
        22.73,
        27.717391,
        40.4,
        54.27,
        17.17,
        28.543478,
        39.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.88
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 51.23
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.23
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.12
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.01,
          "math_avg": 9.52,
          "code_avg": -8.58,
          "reasoning_avg": -1.72,
          "overall_avg": 1.06,
          "overall_efficiency": 0.01766,
          "general_efficiency": 0.083716,
          "math_efficiency": 0.158925,
          "code_efficiency": -0.14323,
          "reasoning_efficiency": -0.028771,
          "general_task_scores": [
            7.62,
            14.4,
            1.16,
            -3.14
          ],
          "math_task_scores": [
            24.94,
            17.4,
            6.54,
            18.0,
            2.22,
            -12.0
          ],
          "code_task_scores": [
            -3.24,
            -1.55,
            -16.87,
            -12.66
          ],
          "reasoning_task_scores": [
            -12.44,
            -8.59,
            -2.98,
            17.12
          ]
        },
        "vs_instruct": {
          "general_avg": -12.48,
          "math_avg": -12.07,
          "code_avg": -21.35,
          "reasoning_avg": -9.51,
          "overall_avg": -13.85,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.91,
            -25.58,
            -3.17,
            -12.27
          ],
          "math_task_scores": [
            -4.32,
            -11.6,
            -1.74,
            3.61,
            -6.12,
            -52.24
          ],
          "code_task_scores": [
            -19.98,
            -7.65,
            -46.14,
            -11.64
          ],
          "reasoning_task_scores": [
            -12.73,
            -3.03,
            -13.94,
            -8.32
          ]
        }
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 42,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 46.22,
      "math_avg": 30.67,
      "code_avg": 51.96,
      "reasoning_avg": 40.58,
      "overall_avg": 42.36,
      "overall_efficiency": 0.018199,
      "general_efficiency": 0.0155,
      "math_efficiency": 0.027581,
      "code_efficiency": 0.020703,
      "reasoning_efficiency": 0.009011,
      "general_scores": [
        53.26,
        54.035,
        40.41,
        35.9864286,
        56.15,
        53.8975,
        39.84,
        35.0885714,
        55.79,
        54.4775,
        40.3,
        35.4385714
      ],
      "math_scores": [
        70.43,
        28.0,
        11.99,
        6.38,
        3.33,
        68.29,
        72.86,
        25.0,
        11.09,
        5.64,
        0.0,
        68.9,
        71.65,
        25.0,
        11.2,
        4.6,
        0.0,
        67.68
      ],
      "code_scores": [
        65.37,
        7.17,
        56.1,
        78.64,
        65.37,
        7.53,
        56.1,
        80.0,
        66.93,
        7.17,
        55.49,
        77.63
      ],
      "reasoning_scores": [
        64.32,
        26.26,
        34.152174,
        34.88,
        62.75,
        25.76,
        38.271739,
        38.0,
        62.72,
        24.24,
        37.793478,
        37.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.07
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.43
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.29
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.89
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.29
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.9
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.26
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.74
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.91
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.18,
          "math_avg": 12.77,
          "code_avg": 9.59,
          "reasoning_avg": 4.17,
          "overall_avg": 8.43,
          "overall_efficiency": 0.018199,
          "general_efficiency": 0.0155,
          "math_efficiency": 0.027581,
          "code_efficiency": 0.020703,
          "reasoning_efficiency": 0.009011,
          "general_task_scores": [
            -9.3,
            33.8,
            3.53,
            0.67
          ],
          "math_task_scores": [
            15.24,
            6.8,
            7.09,
            5.54,
            1.11,
            40.85
          ],
          "code_task_scores": [
            11.42,
            3.71,
            24.8,
            -1.58
          ],
          "reasoning_task_scores": [
            0.7,
            -4.38,
            5.54,
            14.83
          ]
        },
        "vs_instruct": {
          "general_avg": -10.32,
          "math_avg": -8.81,
          "code_avg": -3.19,
          "reasoning_avg": -3.61,
          "overall_avg": -6.48,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -25.83,
            -6.19,
            -0.8,
            -8.46
          ],
          "math_task_scores": [
            -14.02,
            -22.2,
            -1.19,
            -8.85,
            -7.22,
            0.61
          ],
          "code_task_scores": [
            -5.32,
            -2.39,
            -4.47,
            -0.56
          ],
          "reasoning_task_scores": [
            0.41,
            1.18,
            -5.42,
            -10.61
          ]
        }
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 43,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 39.95,
      "math_avg": 12.79,
      "code_avg": 34.99,
      "reasoning_avg": 30.58,
      "overall_avg": 29.58,
      "overall_efficiency": -0.217384,
      "general_efficiency": 0.045249,
      "math_efficiency": -0.254969,
      "code_efficiency": -0.368803,
      "reasoning_efficiency": -0.291014,
      "general_scores": [
        55.04,
        45.455,
        35.44,
        19.9164286,
        55.95,
        44.5575,
        36.77,
        25.7914286,
        57.15,
        44.5625,
        35.3,
        23.4928571
      ],
      "math_scores": [
        24.94,
        8.2,
        6.39,
        3.41,
        0.0,
        34.74,
        27.98,
        8.4,
        6.28,
        2.08,
        3.33,
        29.27,
        26.46,
        7.6,
        6.23,
        2.37,
        3.33,
        29.27
      ],
      "code_scores": [
        46.69,
        0.0,
        23.17,
        69.83,
        42.41,
        0.0,
        26.83,
        72.2,
        47.08,
        0.0,
        20.12,
        71.53
      ],
      "reasoning_scores": [
        45.78,
        25.76,
        29.0,
        19.12,
        49.66,
        25.76,
        29.076087,
        23.36,
        45.28,
        24.24,
        30.923913,
        19.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.46
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.09
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 23.37
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.67
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.91,
          "math_avg": -5.1,
          "code_avg": -7.38,
          "reasoning_avg": -5.83,
          "overall_avg": -4.35,
          "overall_efficiency": -0.217384,
          "general_efficiency": 0.045249,
          "math_efficiency": -0.254969,
          "code_efficiency": -0.368803,
          "reasoning_efficiency": -0.291014,
          "general_task_scores": [
            -8.32,
            24.52,
            -0.81,
            -11.76
          ],
          "math_task_scores": [
            -29.95,
            -11.13,
            1.96,
            2.62,
            2.22,
            3.65
          ],
          "code_task_scores": [
            -9.08,
            -3.58,
            -7.73,
            -9.15
          ],
          "reasoning_task_scores": [
            -15.65,
            -4.55,
            -1.53,
            -1.57
          ]
        },
        "vs_instruct": {
          "general_avg": -16.59,
          "math_avg": -26.69,
          "code_avg": -20.16,
          "reasoning_avg": -13.61,
          "overall_avg": -19.26,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.85,
            -15.47,
            -5.14,
            -20.89
          ],
          "math_task_scores": [
            -59.21,
            -40.13,
            -6.32,
            -11.77,
            -6.12,
            -36.59
          ],
          "code_task_scores": [
            -25.82,
            -9.68,
            -37.0,
            -8.13
          ],
          "reasoning_task_scores": [
            -15.94,
            1.01,
            -12.49,
            -27.01
          ]
        }
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 35.21,
      "math_avg": 26.2,
      "code_avg": 50.45,
      "reasoning_avg": 33.09,
      "overall_avg": 36.24,
      "overall_efficiency": 0.005288,
      "general_efficiency": -0.008785,
      "math_efficiency": 0.019024,
      "code_efficiency": 0.018518,
      "reasoning_efficiency": -0.007603,
      "general_scores": [
        36.87,
        37.875,
        29.04,
        28.7871429,
        43.8,
        41.305,
        33.12,
        26.9528571,
        42.1,
        39.6725,
        33.77,
        29.2584615
      ],
      "math_scores": [
        48.07,
        15.0,
        14.77,
        2.97,
        0.0,
        78.66,
        47.23,
        11.8,
        15.45,
        2.82,
        6.67,
        76.3,
        43.97,
        14.0,
        15.02,
        3.26,
        0.0,
        75.6
      ],
      "code_scores": [
        63.81,
        7.53,
        66.46,
        68.81,
        67.7,
        7.53,
        53.66,
        71.53,
        65.76,
        6.81,
        54.3,
        71.53
      ],
      "reasoning_scores": [
        48.99,
        26.26,
        27.619565,
        30.32,
        48.14,
        25.25,
        30.434783,
        30.8,
        46.37,
        23.23,
        28.576087,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.33
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.85
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.29
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.14
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.62
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.83
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.88
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.75
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -3.83,
          "math_avg": 8.3,
          "code_avg": 8.08,
          "reasoning_avg": -3.32,
          "overall_avg": 2.31,
          "overall_efficiency": 0.005288,
          "general_efficiency": -0.008785,
          "math_efficiency": 0.019024,
          "code_efficiency": 0.018518,
          "reasoning_efficiency": -0.007603,
          "general_task_scores": [
            -23.45,
            19.28,
            -4.67,
            -6.5
          ],
          "math_task_scores": [
            -9.99,
            -5.6,
            10.74,
            3.02,
            2.22,
            49.41
          ],
          "code_task_scores": [
            11.29,
            3.71,
            27.04,
            -9.72
          ],
          "reasoning_task_scores": [
            -14.73,
            -4.89,
            -2.32,
            8.67
          ]
        },
        "vs_instruct": {
          "general_avg": -21.33,
          "math_avg": -13.28,
          "code_avg": -4.69,
          "reasoning_avg": -11.1,
          "overall_avg": -12.6,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -39.98,
            -20.71,
            -9.0,
            -15.63
          ],
          "math_task_scores": [
            -39.25,
            -34.6,
            2.46,
            -11.37,
            -6.12,
            9.17
          ],
          "code_task_scores": [
            -5.45,
            -2.39,
            -2.23,
            -8.7
          ],
          "reasoning_task_scores": [
            -15.02,
            0.67,
            -13.28,
            -16.77
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 38.78,
      "math_avg": 24.07,
      "code_avg": 48.59,
      "reasoning_avg": 33.82,
      "overall_avg": 36.32,
      "overall_efficiency": 0.015236,
      "general_efficiency": -0.001718,
      "math_efficiency": 0.039447,
      "code_efficiency": 0.039743,
      "reasoning_efficiency": -0.01653,
      "general_scores": [
        41.45,
        45.0025,
        35.87,
        32.2007143,
        47.87,
        45.9125,
        33.26,
        31.435,
        43.18,
        44.7925,
        33.85,
        30.505
      ],
      "math_scores": [
        49.81,
        12.6,
        11.4,
        2.82,
        0.0,
        64.63,
        47.23,
        14.2,
        10.23,
        2.23,
        3.33,
        67.78,
        50.57,
        14.8,
        11.16,
        2.23,
        0.0,
        68.29
      ],
      "code_scores": [
        57.59,
        6.45,
        56.71,
        74.58,
        53.31,
        7.53,
        56.1,
        72.88,
        57.59,
        5.38,
        62.8,
        72.2
      ],
      "reasoning_scores": [
        54.55,
        24.75,
        26.608696,
        31.2,
        53.7,
        27.27,
        24.445652,
        32.96,
        50.86,
        24.24,
        25.206522,
        30.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.33
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.43
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.9
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.45
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.54
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.27,
          "math_avg": 6.17,
          "code_avg": 6.22,
          "reasoning_avg": -2.59,
          "overall_avg": 2.38,
          "overall_efficiency": 0.015236,
          "general_efficiency": -0.001718,
          "math_efficiency": 0.039447,
          "code_efficiency": 0.039743,
          "reasoning_efficiency": -0.01653,
          "general_task_scores": [
            -20.2,
            24.9,
            -2.32,
            -3.45
          ],
          "math_task_scores": [
            -7.21,
            -5.33,
            6.59,
            2.43,
            1.11,
            39.46
          ],
          "code_task_scores": [
            1.69,
            2.87,
            27.44,
            -7.12
          ],
          "reasoning_task_scores": [
            -9.52,
            -4.38,
            -5.78,
            9.33
          ]
        },
        "vs_instruct": {
          "general_avg": -17.76,
          "math_avg": -15.41,
          "code_avg": -6.55,
          "reasoning_avg": -10.37,
          "overall_avg": -12.52,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.73,
            -15.08,
            -6.65,
            -12.58
          ],
          "math_task_scores": [
            -36.47,
            -34.33,
            -1.69,
            -11.96,
            -7.22,
            -0.78
          ],
          "code_task_scores": [
            -15.05,
            -3.23,
            -1.83,
            -6.1
          ],
          "reasoning_task_scores": [
            -9.81,
            1.18,
            -16.74,
            -16.11
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 38.34,
      "math_avg": 26.38,
      "code_avg": 50.24,
      "reasoning_avg": 33.38,
      "overall_avg": 37.09,
      "overall_efficiency": 0.028344,
      "general_efficiency": -0.006352,
      "math_efficiency": 0.07626,
      "code_efficiency": 0.070675,
      "reasoning_efficiency": -0.027206,
      "general_scores": [
        46.68,
        39.87,
        34.55,
        31.7035714,
        43.66,
        41.98,
        35.5,
        31.7228571,
        44.31,
        42.24,
        35.17,
        32.685
      ],
      "math_scores": [
        52.69,
        16.6,
        16.06,
        3.26,
        0.0,
        68.29,
        54.89,
        11.4,
        15.29,
        3.12,
        6.67,
        73.17,
        54.59,
        14.0,
        14.84,
        2.97,
        0.0,
        67.07
      ],
      "code_scores": [
        58.37,
        5.02,
        61.59,
        74.58,
        59.14,
        5.38,
        59.15,
        77.29,
        59.14,
        6.81,
        59.76,
        76.61
      ],
      "reasoning_scores": [
        53.03,
        19.19,
        25.380435,
        31.28,
        53.36,
        23.23,
        27.521739,
        33.28,
        54.76,
        19.7,
        27.380435,
        32.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.06
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.74
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.17
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.16
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.72
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.76
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.35
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.71,
          "math_avg": 8.49,
          "code_avg": 7.86,
          "reasoning_avg": -3.03,
          "overall_avg": 3.15,
          "overall_efficiency": 0.028344,
          "general_efficiency": -0.006352,
          "math_efficiency": 0.07626,
          "code_efficiency": 0.070675,
          "reasoning_efficiency": -0.027206,
          "general_task_scores": [
            -19.49,
            21.02,
            -1.58,
            -2.79
          ],
          "math_task_scores": [
            -2.35,
            -5.2,
            11.06,
            3.12,
            2.22,
            42.07
          ],
          "code_task_scores": [
            4.41,
            2.16,
            29.07,
            -4.18
          ],
          "reasoning_task_scores": [
            -8.84,
            -9.09,
            -4.44,
            10.27
          ]
        },
        "vs_instruct": {
          "general_avg": -18.2,
          "math_avg": -13.1,
          "code_avg": -4.91,
          "reasoning_avg": -10.81,
          "overall_avg": -11.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.02,
            -18.97,
            -5.91,
            -11.92
          ],
          "math_task_scores": [
            -31.61,
            -34.2,
            2.78,
            -11.27,
            -6.12,
            1.83
          ],
          "code_task_scores": [
            -12.33,
            -3.94,
            -0.2,
            -3.16
          ],
          "reasoning_task_scores": [
            -9.13,
            -3.53,
            -15.4,
            -15.17
          ]
        }
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 37.97,
      "math_avg": 20.51,
      "code_avg": 44.32,
      "reasoning_avg": 26.51,
      "overall_avg": 32.33,
      "overall_efficiency": -0.021343,
      "general_efficiency": -0.014371,
      "math_efficiency": 0.034746,
      "code_efficiency": 0.025909,
      "reasoning_efficiency": -0.131659,
      "general_scores": [
        51.47,
        35.24,
        36.01,
        31.8564286,
        44.26,
        35.8975,
        35.28,
        32.9446154,
        51.16,
        34.9925,
        34.57,
        31.9057143
      ],
      "math_scores": [
        51.4,
        13.8,
        8.15,
        3.56,
        0.0,
        48.17,
        50.87,
        15.8,
        8.58,
        2.52,
        0.0,
        43.9,
        49.28,
        14.8,
        8.45,
        2.97,
        0.0,
        46.95
      ],
      "code_scores": [
        52.53,
        2.51,
        44.51,
        77.29,
        52.53,
        3.94,
        45.12,
        76.61,
        53.7,
        2.87,
        40.24,
        80.0
      ],
      "reasoning_scores": [
        27.74,
        22.73,
        22.836957,
        28.0,
        28.16,
        24.24,
        23.73913,
        27.92,
        37.2,
        24.75,
        23.119565,
        27.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.03
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.08,
          "math_avg": 2.61,
          "code_avg": 1.95,
          "reasoning_avg": -9.9,
          "overall_avg": -1.6,
          "overall_efficiency": -0.021343,
          "general_efficiency": -0.014371,
          "math_efficiency": 0.034746,
          "code_efficiency": 0.025909,
          "reasoning_efficiency": -0.131659,
          "general_task_scores": [
            -15.41,
            15.04,
            -1.36,
            -2.59
          ],
          "math_task_scores": [
            -5.89,
            -4.4,
            4.05,
            3.02,
            0.0,
            18.9
          ],
          "code_task_scores": [
            -1.55,
            -0.47,
            12.19,
            -2.37
          ],
          "reasoning_task_scores": [
            -31.53,
            -5.89,
            -7.97,
            5.79
          ]
        },
        "vs_instruct": {
          "general_avg": -18.58,
          "math_avg": -18.97,
          "code_avg": -10.82,
          "reasoning_avg": -17.68,
          "overall_avg": -16.51,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -31.94,
            -24.94,
            -5.69,
            -11.72
          ],
          "math_task_scores": [
            -35.15,
            -33.4,
            -4.23,
            -11.37,
            -8.34,
            -21.34
          ],
          "code_task_scores": [
            -18.29,
            -6.57,
            -17.08,
            -1.35
          ],
          "reasoning_task_scores": [
            -31.82,
            -0.33,
            -18.93,
            -19.65
          ]
        }
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 34.49,
      "math_avg": 24.73,
      "code_avg": 47.51,
      "reasoning_avg": 25.9,
      "overall_avg": 33.16,
      "overall_efficiency": -0.011671,
      "general_efficiency": -0.068673,
      "math_efficiency": 0.102947,
      "code_efficiency": 0.07743,
      "reasoning_efficiency": -0.158388,
      "general_scores": [
        35.06,
        42.0775,
        32.84,
        27.8492857,
        41.69,
        37.0,
        33.39,
        25.5164286,
        38.37,
        37.8975,
        32.65,
        29.5085714
      ],
      "math_scores": [
        62.4,
        10.6,
        17.48,
        1.78,
        6.67,
        48.78,
        59.21,
        11.0,
        17.66,
        2.82,
        0.0,
        59.76,
        58.68,
        9.2,
        17.34,
        2.37,
        3.33,
        56.1
      ],
      "code_scores": [
        59.14,
        8.96,
        47.56,
        76.95,
        58.37,
        6.81,
        49.39,
        74.24,
        56.42,
        7.53,
        54.27,
        70.51
      ],
      "reasoning_scores": [
        29.31,
        30.81,
        22.0,
        20.96,
        32.06,
        28.28,
        23.684783,
        19.36,
        32.29,
        26.77,
        22.423913,
        22.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.88
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.77
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 50.41
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.7
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.04
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.56,
          "math_avg": 6.83,
          "code_avg": 5.14,
          "reasoning_avg": -10.51,
          "overall_avg": -0.77,
          "overall_efficiency": -0.011671,
          "general_efficiency": -0.068673,
          "math_efficiency": 0.102947,
          "code_efficiency": 0.07743,
          "reasoning_efficiency": -0.158388,
          "general_task_scores": [
            -26.0,
            18.65,
            -3.69,
            -7.21
          ],
          "math_task_scores": [
            3.69,
            -8.93,
            13.15,
            2.32,
            3.33,
            27.44
          ],
          "code_task_scores": [
            3.51,
            4.19,
            19.31,
            -6.44
          ],
          "reasoning_task_scores": [
            -31.34,
            -1.18,
            -8.5,
            -1.04
          ]
        },
        "vs_instruct": {
          "general_avg": -22.05,
          "math_avg": -14.75,
          "code_avg": -7.63,
          "reasoning_avg": -18.3,
          "overall_avg": -15.68,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -42.53,
            -21.34,
            -8.02,
            -16.34
          ],
          "math_task_scores": [
            -25.57,
            -37.93,
            4.87,
            -12.07,
            -5.01,
            -12.8
          ],
          "code_task_scores": [
            -13.23,
            -1.91,
            -9.96,
            -5.42
          ],
          "reasoning_task_scores": [
            -31.63,
            4.38,
            -19.46,
            -26.48
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 23.59,
      "math_avg": 7.82,
      "code_avg": 26.12,
      "reasoning_avg": 23.45,
      "overall_avg": 20.25,
      "overall_efficiency": -0.248301,
      "general_efficiency": -0.280435,
      "math_efficiency": -0.182803,
      "code_efficiency": -0.294888,
      "reasoning_efficiency": -0.235079,
      "general_scores": [
        34.19,
        24.61,
        23.52,
        16.5578571,
        30.04,
        23.915,
        21.53,
        17.355,
        28.96,
        23.7825,
        21.33,
        17.2828571
      ],
      "math_scores": [
        22.29,
        6.6,
        2.76,
        1.04,
        0.0,
        18.9,
        22.74,
        4.2,
        3.21,
        0.59,
        0.0,
        15.24,
        22.14,
        5.8,
        2.51,
        0.59,
        0.0,
        12.2
      ],
      "code_scores": [
        47.47,
        0.36,
        12.8,
        46.44,
        46.3,
        0.72,
        13.41,
        39.32,
        46.69,
        1.08,
        10.37,
        48.47
      ],
      "reasoning_scores": [
        48.76,
        18.18,
        15.815217,
        19.2,
        37.61,
        18.69,
        14.782609,
        18.0,
        40.61,
        15.66,
        14.130435,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.06
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.72
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.19
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.91
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -15.46,
          "math_avg": -10.08,
          "code_avg": -16.25,
          "reasoning_avg": -12.96,
          "overall_avg": -13.69,
          "overall_efficiency": -0.248301,
          "general_efficiency": -0.280435,
          "math_efficiency": -0.182803,
          "code_efficiency": -0.294888,
          "reasoning_efficiency": -0.235079,
          "general_task_scores": [
            -33.31,
            3.76,
            -14.52,
            -17.76
          ],
          "math_task_scores": [
            -34.02,
            -13.67,
            -1.51,
            0.74,
            0.0,
            -11.99
          ],
          "code_task_scores": [
            -7.65,
            -2.86,
            -18.91,
            -35.6
          ],
          "reasoning_task_scores": [
            -20.23,
            -12.29,
            -16.29,
            -3.01
          ]
        },
        "vs_instruct": {
          "general_avg": -32.95,
          "math_avg": -31.66,
          "code_avg": -29.03,
          "reasoning_avg": -20.74,
          "overall_avg": -28.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -49.84,
            -36.22,
            -18.85,
            -26.89
          ],
          "math_task_scores": [
            -63.28,
            -42.67,
            -9.79,
            -13.65,
            -8.34,
            -52.23
          ],
          "code_task_scores": [
            -24.39,
            -8.96,
            -48.18,
            -34.58
          ],
          "reasoning_task_scores": [
            -20.52,
            -6.73,
            -27.25,
            -28.45
          ]
        }
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 25.81,
      "math_avg": 20.09,
      "code_avg": 36.93,
      "reasoning_avg": 20.37,
      "overall_avg": 25.8,
      "overall_efficiency": -0.160563,
      "general_efficiency": -0.261324,
      "math_efficiency": 0.043163,
      "code_efficiency": -0.107479,
      "reasoning_efficiency": -0.316614,
      "general_scores": [
        27.07,
        30.26,
        20.75,
        15.8507143,
        52.11,
        34.065,
        22.92,
        16.8221429,
        18.22,
        29.7375,
        22.11,
        19.7714286
      ],
      "math_scores": [
        48.75,
        10.0,
        15.65,
        1.04,
        0.0,
        50.61,
        42.15,
        8.6,
        16.46,
        1.93,
        0.0,
        47.56,
        47.31,
        8.8,
        15.74,
        0.59,
        0.0,
        46.34
      ],
      "code_scores": [
        55.64,
        3.58,
        25.61,
        60.68,
        55.25,
        3.94,
        25.0,
        64.75,
        57.2,
        3.23,
        29.27,
        58.98
      ],
      "reasoning_scores": [
        26.58,
        22.22,
        14.956522,
        14.8,
        33.66,
        19.19,
        15.380435,
        18.16,
        29.33,
        18.69,
        15.793478,
        15.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.48
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.19
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.17
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 56.03
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -13.24,
          "math_avg": 2.19,
          "code_avg": -5.44,
          "reasoning_avg": -16.04,
          "overall_avg": -8.13,
          "overall_efficiency": -0.160563,
          "general_efficiency": -0.261324,
          "math_efficiency": 0.043163,
          "code_efficiency": -0.107479,
          "reasoning_efficiency": -0.316614,
          "general_task_scores": [
            -31.9,
            11.01,
            -14.72,
            -17.35
          ],
          "math_task_scores": [
            -10.34,
            -10.07,
            11.61,
            1.19,
            0.0,
            20.73
          ],
          "code_task_scores": [
            1.56,
            0.0,
            -4.47,
            -18.87
          ],
          "reasoning_task_scores": [
            -32.7,
            -9.77,
            -15.82,
            -5.87
          ]
        },
        "vs_instruct": {
          "general_avg": -30.73,
          "math_avg": -19.4,
          "code_avg": -18.22,
          "reasoning_avg": -23.82,
          "overall_avg": -23.04,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -48.43,
            -28.98,
            -19.05,
            -26.48
          ],
          "math_task_scores": [
            -39.6,
            -39.07,
            3.33,
            -13.2,
            -8.34,
            -19.51
          ],
          "code_task_scores": [
            -15.18,
            -6.1,
            -33.74,
            -17.85
          ],
          "reasoning_task_scores": [
            -32.99,
            -4.21,
            -26.78,
            -31.31
          ]
        }
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 37.29,
      "math_avg": 23.6,
      "code_avg": 43.06,
      "reasoning_avg": 36.23,
      "overall_avg": 35.04,
      "overall_efficiency": 0.001083,
      "general_efficiency": -0.001713,
      "math_efficiency": 0.005552,
      "code_efficiency": 0.000667,
      "reasoning_efficiency": -0.000174,
      "general_scores": [
        39.63,
        46.4425,
        31.6,
        27.8085714,
        43.81,
        47.25,
        32.2,
        28.68,
        42.1,
        46.66,
        33.35,
        27.9064286
      ],
      "math_scores": [
        55.12,
        14.2,
        7.0,
        2.82,
        0.0,
        57.93,
        56.86,
        14.4,
        7.45,
        2.82,
        0.0,
        64.63,
        55.12,
        14.0,
        7.59,
        2.08,
        0.0,
        62.8
      ],
      "code_scores": [
        32.3,
        3.23,
        55.49,
        67.12,
        59.92,
        2.87,
        53.66,
        63.05,
        54.47,
        2.51,
        54.27,
        67.8
      ],
      "reasoning_scores": [
        51.65,
        25.76,
        39.184783,
        27.12,
        52.43,
        25.76,
        40.597826,
        30.48,
        50.81,
        24.75,
        35.032609,
        31.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.79
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.87
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.47
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.99
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.76,
          "math_avg": 5.7,
          "code_avg": 0.68,
          "reasoning_avg": -0.18,
          "overall_avg": 1.11,
          "overall_efficiency": 0.001083,
          "general_efficiency": -0.001713,
          "math_efficiency": 0.005552,
          "code_efficiency": 0.000667,
          "reasoning_efficiency": -0.000174,
          "general_task_scores": [
            -22.52,
            26.44,
            -4.27,
            -6.7
          ],
          "math_task_scores": [
            -0.71,
            -5.0,
            3.01,
            2.57,
            0.0,
            34.35
          ],
          "code_task_scores": [
            -5.57,
            -0.71,
            23.37,
            -14.35
          ],
          "reasoning_task_scores": [
            -10.93,
            -4.38,
            7.07,
            7.52
          ]
        },
        "vs_instruct": {
          "general_avg": -19.26,
          "math_avg": -15.88,
          "code_avg": -12.09,
          "reasoning_avg": -7.96,
          "overall_avg": -13.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -39.05,
            -13.55,
            -8.6,
            -15.83
          ],
          "math_task_scores": [
            -29.97,
            -34.0,
            -5.27,
            -11.82,
            -8.34,
            -5.89
          ],
          "code_task_scores": [
            -22.31,
            -6.81,
            -5.9,
            -13.33
          ],
          "reasoning_task_scores": [
            -11.22,
            1.18,
            -3.89,
            -17.92
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 28.85,
      "math_avg": 21.61,
      "code_avg": 32.55,
      "reasoning_avg": 21.5,
      "overall_avg": 26.13,
      "overall_efficiency": -0.222982,
      "general_efficiency": -0.291447,
      "math_efficiency": 0.106145,
      "code_efficiency": -0.280627,
      "reasoning_efficiency": -0.425999,
      "general_scores": [
        33.99,
        37.9425,
        20.11,
        20.1121429,
        37.28,
        35.685,
        20.67,
        14.6814286,
        40.29,
        39.87,
        23.66,
        21.8585714
      ],
      "math_scores": [
        70.51,
        2.4,
        14.54,
        0.59,
        0.0,
        40.85,
        68.08,
        2.0,
        16.62,
        1.34,
        0.0,
        40.24,
        72.63,
        3.8,
        12.33,
        1.04,
        0.0,
        42.07
      ],
      "code_scores": [
        50.97,
        1.43,
        32.93,
        46.1,
        47.47,
        1.43,
        37.2,
        40.34,
        47.86,
        0.36,
        35.37,
        49.15
      ],
      "reasoning_scores": [
        46.75,
        10.61,
        10.391304,
        14.0,
        44.4,
        12.63,
        10.847826,
        14.72,
        47.29,
        16.16,
        9.326087,
        20.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.48
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.05
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.07
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.17
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.13
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -10.2,
          "math_avg": 3.72,
          "code_avg": -9.82,
          "reasoning_avg": -14.91,
          "overall_avg": -7.8,
          "overall_efficiency": -0.222982,
          "general_efficiency": -0.291447,
          "math_efficiency": 0.106145,
          "code_efficiency": -0.280627,
          "reasoning_efficiency": -0.425999,
          "general_task_scores": [
            -27.18,
            17.49,
            -15.17,
            -15.95
          ],
          "math_task_scores": [
            14.0,
            -16.47,
            10.16,
            0.99,
            0.0,
            13.61
          ],
          "code_task_scores": [
            -5.7,
            -2.51,
            4.07,
            -35.14
          ],
          "reasoning_task_scores": [
            -16.41,
            -16.67,
            -21.01,
            -5.55
          ]
        },
        "vs_instruct": {
          "general_avg": -27.7,
          "math_avg": -17.87,
          "code_avg": -22.59,
          "reasoning_avg": -22.69,
          "overall_avg": -22.71,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -43.71,
            -22.5,
            -19.5,
            -25.08
          ],
          "math_task_scores": [
            -15.26,
            -45.47,
            1.88,
            -13.4,
            -8.34,
            -26.63
          ],
          "code_task_scores": [
            -22.44,
            -8.61,
            -25.2,
            -34.12
          ],
          "reasoning_task_scores": [
            -16.7,
            -11.11,
            -31.97,
            -30.99
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 18.64,
      "math_avg": 26.8,
      "code_avg": 41.1,
      "reasoning_avg": 12.13,
      "overall_avg": 24.67,
      "overall_efficiency": -0.019141,
      "general_efficiency": -0.042159,
      "math_efficiency": 0.018384,
      "code_efficiency": -0.002625,
      "reasoning_efficiency": -0.050164,
      "general_scores": [
        21.96,
        30.5625,
        22.17,
        3.50333333,
        25.95,
        29.95,
        22.18,
        3.58285714,
        13.6,
        31.2475,
        16.33,
        2.61214286
      ],
      "math_scores": [
        72.4,
        3.0,
        16.73,
        1.19,
        0.0,
        63.41,
        76.57,
        2.2,
        18.34,
        0.15,
        0.0,
        68.29,
        74.45,
        2.4,
        15.85,
        0.15,
        0.0,
        67.24
      ],
      "code_scores": [
        60.7,
        0.0,
        60.37,
        43.05,
        57.2,
        0.0,
        62.8,
        42.03,
        61.48,
        0.0,
        62.2,
        43.39
      ],
      "reasoning_scores": [
        3.87,
        18.69,
        14.717391,
        15.28,
        3.95,
        11.62,
        14.663043,
        13.6,
        4.39,
        14.14,
        15.869565,
        14.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.97
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.31
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.79
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -20.41,
          "math_avg": 8.9,
          "code_avg": -1.27,
          "reasoning_avg": -24.28,
          "overall_avg": -9.27,
          "overall_efficiency": -0.019141,
          "general_efficiency": -0.042159,
          "math_efficiency": 0.018384,
          "code_efficiency": -0.002625,
          "reasoning_efficiency": -0.050164,
          "general_task_scores": [
            -43.87,
            10.25,
            -16.42,
            -31.6
          ],
          "math_task_scores": [
            18.06,
            -16.67,
            12.63,
            0.5,
            0.0,
            38.87
          ],
          "code_task_scores": [
            5.32,
            -3.58,
            30.69,
            -37.52
          ],
          "reasoning_task_scores": [
            -58.49,
            -14.98,
            -16.12,
            -7.55
          ]
        },
        "vs_instruct": {
          "general_avg": -37.9,
          "math_avg": -12.68,
          "code_avg": -14.04,
          "reasoning_avg": -32.07,
          "overall_avg": -24.17,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -60.4,
            -29.74,
            -20.75,
            -40.73
          ],
          "math_task_scores": [
            -11.2,
            -45.67,
            4.35,
            -13.89,
            -8.34,
            -1.37
          ],
          "code_task_scores": [
            -11.42,
            -9.68,
            1.42,
            -36.5
          ],
          "reasoning_task_scores": [
            -58.78,
            -9.42,
            -27.08,
            -32.99
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 31.46,
      "math_avg": 15.29,
      "code_avg": 24.48,
      "reasoning_avg": 28.91,
      "overall_avg": 25.04,
      "overall_efficiency": -0.107889,
      "general_efficiency": -0.091999,
      "math_efficiency": -0.031586,
      "code_efficiency": -0.216989,
      "reasoning_efficiency": -0.090986,
      "general_scores": [
        44.28,
        37.3625,
        22.41,
        14.2771429,
        44.74,
        39.5775,
        20.8,
        45.48,
        38.46,
        19.59,
        19.1035714
      ],
      "math_scores": [
        59.21,
        2.2,
        13.53,
        1.93,
        0.0,
        23.78,
        50.49,
        3.6,
        8.42,
        1.93,
        0.0,
        20.73,
        53.6,
        4.4,
        10.21,
        2.37,
        0.0,
        18.9
      ],
      "code_scores": [
        37.74,
        0.0,
        15.85,
        48.81,
        39.69,
        0.0,
        16.46,
        43.73,
        36.96,
        0.0,
        12.2,
        42.37
      ],
      "reasoning_scores": [
        50.86,
        24.75,
        23.043478,
        13.04,
        49.59,
        26.793478,
        11.28,
        51.84,
        29.8,
        23.804348,
        13.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.14
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.84
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.76
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.55
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -7.58,
          "math_avg": -2.6,
          "code_avg": -17.89,
          "reasoning_avg": -7.5,
          "overall_avg": -8.89,
          "overall_efficiency": -0.107889,
          "general_efficiency": -0.091999,
          "math_efficiency": -0.031586,
          "code_efficiency": -0.216989,
          "reasoning_efficiency": -0.090986,
          "general_task_scores": [
            -19.54,
            18.13,
            -15.72,
            -18.14
          ],
          "math_task_scores": [
            -1.98,
            -15.8,
            6.38,
            2.08,
            0.0,
            -6.3
          ],
          "code_task_scores": [
            -16.34,
            -3.58,
            -16.26,
            -35.37
          ],
          "reasoning_task_scores": [
            -11.8,
            -2.52,
            -6.65,
            -9.57
          ]
        },
        "vs_instruct": {
          "general_avg": -25.08,
          "math_avg": -24.19,
          "code_avg": -30.66,
          "reasoning_avg": -15.28,
          "overall_avg": -23.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -36.07,
            -21.86,
            -20.05,
            -27.27
          ],
          "math_task_scores": [
            -31.24,
            -44.8,
            -1.9,
            -12.31,
            -8.34,
            -46.54
          ],
          "code_task_scores": [
            -33.08,
            -9.68,
            -45.53,
            -34.35
          ],
          "reasoning_task_scores": [
            -12.09,
            3.04,
            -17.61,
            -35.01
          ]
        }
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 41.29,
      "math_avg": 19.28,
      "code_avg": 44.29,
      "reasoning_avg": 35.79,
      "overall_avg": 35.16,
      "overall_efficiency": 0.015734,
      "general_efficiency": 0.028661,
      "math_efficiency": 0.017611,
      "code_efficiency": 0.024553,
      "reasoning_efficiency": -0.007888,
      "general_scores": [
        47.25,
        48.8325,
        37.03,
        30.0128571,
        51.56,
        47.2175,
        36.46,
        29.7723077,
        51.69,
        47.5275,
        36.54,
        31.5785714
      ],
      "math_scores": [
        50.87,
        10.6,
        7.0,
        2.08,
        0.0,
        48.17,
        47.31,
        10.8,
        6.71,
        2.52,
        0.0,
        45.73,
        51.33,
        11.8,
        7.45,
        1.93,
        0.0,
        42.68
      ],
      "code_scores": [
        53.7,
        1.08,
        43.29,
        77.63,
        53.7,
        2.51,
        45.12,
        77.63,
        53.31,
        1.08,
        44.51,
        77.97
      ],
      "reasoning_scores": [
        54.67,
        26.77,
        35.402174,
        26.32,
        53.19,
        26.26,
        36.195652,
        25.92,
        55.03,
        26.77,
        35.543478,
        27.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.53
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.56
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.31
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.6
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.24,
          "math_avg": 1.38,
          "code_avg": 1.92,
          "reasoning_avg": -0.62,
          "overall_avg": 1.23,
          "overall_efficiency": 0.015734,
          "general_efficiency": 0.028661,
          "math_efficiency": 0.017611,
          "code_efficiency": 0.024553,
          "reasoning_efficiency": -0.007888,
          "general_task_scores": [
            -14.2,
            27.52,
            0.03,
            -4.38
          ],
          "math_task_scores": [
            -6.57,
            -8.13,
            2.71,
            2.18,
            0.0,
            18.09
          ],
          "code_task_scores": [
            -0.9,
            -2.02,
            13.21,
            -2.6
          ],
          "reasoning_task_scores": [
            -8.26,
            -3.2,
            4.51,
            4.48
          ]
        },
        "vs_instruct": {
          "general_avg": -15.25,
          "math_avg": -20.21,
          "code_avg": -10.85,
          "reasoning_avg": -8.4,
          "overall_avg": -13.68,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -30.73,
            -12.47,
            -4.3,
            -13.51
          ],
          "math_task_scores": [
            -35.83,
            -37.13,
            -5.57,
            -12.21,
            -8.34,
            -22.15
          ],
          "code_task_scores": [
            -17.64,
            -8.12,
            -16.06,
            -1.58
          ],
          "reasoning_task_scores": [
            -8.55,
            2.36,
            -6.45,
            -20.96
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 38.18,
      "math_avg": 15.62,
      "code_avg": 38.32,
      "reasoning_avg": 31.35,
      "overall_avg": 30.87,
      "overall_efficiency": -0.025126,
      "general_efficiency": -0.007142,
      "math_efficiency": -0.018695,
      "code_efficiency": -0.033187,
      "reasoning_efficiency": -0.04148,
      "general_scores": [
        48.07,
        42.0,
        33.69,
        28.0976923,
        49.7,
        44.71,
        31.14,
        27.8978571,
        46.98,
        43.045,
        33.47,
        29.3007143
      ],
      "math_scores": [
        38.51,
        8.6,
        6.59,
        2.82,
        0.0,
        45.12,
        30.02,
        8.6,
        6.66,
        1.93,
        0.0,
        39.02,
        34.8,
        9.4,
        6.46,
        2.97,
        0.0,
        39.63
      ],
      "code_scores": [
        50.58,
        0.0,
        32.32,
        68.47,
        54.09,
        0.0,
        32.93,
        68.47,
        45.53,
        0.0,
        35.98,
        71.53
      ],
      "reasoning_scores": [
        45.04,
        29.8,
        28.347826,
        25.52,
        46.26,
        25.76,
        27.5,
        22.88,
        46.6,
        26.26,
        27.206522,
        25.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.44
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.26
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.74
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.68
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.87,
          "math_avg": -2.28,
          "code_avg": -4.05,
          "reasoning_avg": -5.06,
          "overall_avg": -3.06,
          "overall_efficiency": -0.025126,
          "general_efficiency": -0.007142,
          "math_efficiency": -0.018695,
          "code_efficiency": -0.033187,
          "reasoning_efficiency": -0.04148,
          "general_task_scores": [
            -16.12,
            22.91,
            -3.88,
            -6.4
          ],
          "math_task_scores": [
            -21.97,
            -10.33,
            2.23,
            2.57,
            0.0,
            13.82
          ],
          "code_task_scores": [
            -4.4,
            -3.58,
            2.64,
            -10.85
          ],
          "reasoning_task_scores": [
            -16.59,
            -2.53,
            -3.52,
            2.4
          ]
        },
        "vs_instruct": {
          "general_avg": -18.37,
          "math_avg": -23.86,
          "code_avg": -16.82,
          "reasoning_avg": -12.84,
          "overall_avg": -17.97,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -32.65,
            -17.08,
            -8.21,
            -15.53
          ],
          "math_task_scores": [
            -51.23,
            -39.33,
            -6.05,
            -11.82,
            -8.34,
            -26.42
          ],
          "code_task_scores": [
            -21.14,
            -9.68,
            -26.63,
            -9.83
          ],
          "reasoning_task_scores": [
            -16.88,
            3.03,
            -14.48,
            -23.04
          ]
        }
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 40.96,
      "math_avg": 16.1,
      "code_avg": 39.9,
      "reasoning_avg": 33.89,
      "overall_avg": 32.71,
      "overall_efficiency": -0.065483,
      "general_efficiency": 0.102904,
      "math_efficiency": -0.096742,
      "code_efficiency": -0.132666,
      "reasoning_efficiency": -0.135432,
      "general_scores": [
        43.48,
        46.8975,
        37.88,
        33.3314286,
        48.21,
        48.0325,
        37.1,
        31.7071429,
        47.06,
        47.0875,
        37.92,
        32.8307143
      ],
      "math_scores": [
        35.56,
        14.4,
        7.36,
        3.26,
        0.0,
        36.59,
        35.18,
        11.2,
        7.38,
        2.97,
        3.33,
        37.2,
        33.66,
        11.4,
        6.91,
        2.23,
        3.33,
        37.8
      ],
      "code_scores": [
        54.47,
        0.0,
        31.1,
        73.22,
        52.53,
        0.0,
        32.93,
        73.22,
        54.47,
        0.0,
        32.32,
        74.58
      ],
      "reasoning_scores": [
        53.47,
        25.76,
        30.804348,
        26.4,
        53.88,
        24.75,
        32.304348,
        25.68,
        53.76,
        23.74,
        30.043478,
        26.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.2
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 32.12
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.7
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.05
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.05
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.92,
          "math_avg": -1.8,
          "code_avg": -2.47,
          "reasoning_avg": -2.52,
          "overall_avg": -1.22,
          "overall_efficiency": -0.065483,
          "general_efficiency": 0.102904,
          "math_efficiency": -0.096742,
          "code_efficiency": -0.132666,
          "reasoning_efficiency": -0.135432,
          "general_task_scores": [
            -18.12,
            27.0,
            0.98,
            -2.21
          ],
          "math_task_scores": [
            -21.61,
            -6.87,
            2.88,
            2.82,
            2.22,
            9.76
          ],
          "code_task_scores": [
            -0.65,
            -3.58,
            1.02,
            -6.67
          ],
          "reasoning_task_scores": [
            -8.86,
            -5.05,
            -0.15,
            3.97
          ]
        },
        "vs_instruct": {
          "general_avg": -15.58,
          "math_avg": -23.38,
          "code_avg": -15.24,
          "reasoning_avg": -10.3,
          "overall_avg": -16.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -34.65,
            -12.98,
            -3.35,
            -11.34
          ],
          "math_task_scores": [
            -50.87,
            -35.87,
            -5.4,
            -11.57,
            -6.12,
            -30.48
          ],
          "code_task_scores": [
            -17.39,
            -9.68,
            -28.25,
            -5.65
          ],
          "reasoning_task_scores": [
            -9.15,
            0.51,
            -11.11,
            -21.47
          ]
        }
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.78,
      "math_avg": 19.54,
      "code_avg": 45.17,
      "reasoning_avg": 35.69,
      "overall_avg": 35.8,
      "overall_efficiency": 0.082461,
      "general_efficiency": 0.16532,
      "math_efficiency": 0.072797,
      "code_efficiency": 0.123776,
      "reasoning_efficiency": -0.032046,
      "general_scores": [
        59.33,
        45.2575,
        36.23,
        32.4157143,
        56.05,
        44.4375,
        37.2,
        33.3942857,
        55.872,
        44.0875,
        35.85,
        33.28
      ],
      "math_scores": [
        47.92,
        13.8,
        8.2,
        2.23,
        0.0,
        54.27,
        51.71,
        14.2,
        8.31,
        2.23,
        0.0,
        51.83,
        50.11,
        16.8,
        8.27,
        2.37,
        0.0
      ],
      "code_scores": [
        56.42,
        7.89,
        46.95,
        73.56,
        55.64,
        8.24,
        40.85,
        73.9,
        54.09,
        6.45,
        44.5,
        73.56
      ],
      "reasoning_scores": [
        57.22,
        25.25,
        29.184783,
        31.92,
        57.96,
        22.73,
        29.641304,
        31.92,
        57.88,
        22.73,
        30.75,
        31.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.43
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.91
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.28
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.05
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.53
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.57
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.63
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.74,
          "math_avg": 1.65,
          "code_avg": 2.8,
          "reasoning_avg": -0.72,
          "overall_avg": 1.86,
          "overall_efficiency": 0.082461,
          "general_efficiency": 0.16532,
          "math_efficiency": 0.072797,
          "code_efficiency": 0.123776,
          "reasoning_efficiency": -0.032046,
          "general_task_scores": [
            -7.29,
            24.25,
            -0.22,
            -1.8
          ],
          "math_task_scores": [
            -6.5,
            -4.27,
            3.92,
            2.28,
            0.0,
            25.61
          ],
          "code_task_scores": [
            0.91,
            3.95,
            13.0,
            -6.67
          ],
          "reasoning_task_scores": [
            -4.87,
            -6.23,
            -1.34,
            9.55
          ]
        },
        "vs_instruct": {
          "general_avg": -13.76,
          "math_avg": -19.94,
          "code_avg": -9.97,
          "reasoning_avg": -8.51,
          "overall_avg": -13.04,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.82,
            -15.74,
            -4.55,
            -10.93
          ],
          "math_task_scores": [
            -35.76,
            -33.27,
            -4.36,
            -12.11,
            -8.34,
            -14.63
          ],
          "code_task_scores": [
            -15.83,
            -2.15,
            -16.27,
            -5.65
          ],
          "reasoning_task_scores": [
            -5.16,
            -0.67,
            -12.3,
            -15.89
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 22.8,
      "math_avg": 15.09,
      "code_avg": 25.08,
      "reasoning_avg": 21.83,
      "overall_avg": 21.2,
      "overall_efficiency": -0.041781,
      "general_efficiency": -0.053316,
      "math_efficiency": -0.009206,
      "code_efficiency": -0.056754,
      "reasoning_efficiency": -0.047847,
      "general_scores": [
        41.82,
        23.7175,
        17.69,
        5.67,
        42.42,
        24.685,
        18.65,
        5.56214286,
        42.4,
        27.2425,
        18.09,
        5.66785714
      ],
      "math_scores": [
        66.72,
        9.8,
        6.35,
        2.67,
        3.33,
        3.66,
        61.79,
        10.6,
        6.75,
        2.67,
        3.33,
        3.66,
        62.32,
        10.6,
        6.39,
        1.93,
        6.67,
        2.44
      ],
      "code_scores": [
        39.3,
        0.0,
        7.32,
        50.51,
        39.3,
        0.0,
        6.1,
        56.27,
        42.02,
        0.0,
        1.83,
        58.31
      ],
      "reasoning_scores": [
        29.34,
        20.933333,
        20.0,
        28.57,
        12.12,
        22.6,
        20.64,
        32.73,
        13.64,
        20.533333,
        19.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.61
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.5
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.25
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 40.21
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.08
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.21
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.88
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.89
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -16.24,
          "math_avg": -2.8,
          "code_avg": -17.29,
          "reasoning_avg": -14.58,
          "overall_avg": -12.73,
          "overall_efficiency": -0.041781,
          "general_efficiency": -0.053316,
          "math_efficiency": -0.009206,
          "code_efficiency": -0.056754,
          "reasoning_efficiency": -0.047847,
          "general_task_scores": [
            -22.16,
            4.88,
            -18.51,
            -29.2
          ],
          "math_task_scores": [
            7.2,
            -8.87,
            2.16,
            2.42,
            4.44,
            -24.19
          ],
          "code_task_scores": [
            -14.26,
            -3.58,
            -26.02,
            -25.31
          ],
          "reasoning_task_scores": [
            -32.35,
            -16.92,
            -9.84,
            -2.19
          ]
        },
        "vs_instruct": {
          "general_avg": -33.74,
          "math_avg": -24.39,
          "code_avg": -30.06,
          "reasoning_avg": -22.36,
          "overall_avg": -27.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.69,
            -35.11,
            -22.84,
            -38.33
          ],
          "math_task_scores": [
            -22.06,
            -37.87,
            -6.12,
            -11.97,
            -3.9,
            -64.43
          ],
          "code_task_scores": [
            -31.0,
            -9.68,
            -55.29,
            -24.29
          ],
          "reasoning_task_scores": [
            -32.64,
            -11.36,
            -20.8,
            -27.63
          ]
        }
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 16.07,
      "math_avg": 23.62,
      "code_avg": 37.92,
      "reasoning_avg": 9.49,
      "overall_avg": 21.77,
      "overall_efficiency": -0.031996,
      "general_efficiency": -0.060474,
      "math_efficiency": 0.015058,
      "code_efficiency": -0.011724,
      "reasoning_efficiency": -0.070846,
      "general_scores": [
        7.37,
        31.015,
        20.31,
        7.53642857,
        3.54,
        31.8075,
        19.98,
        6.67461538,
        6.37,
        30.205,
        19.7,
        8.28214286
      ],
      "math_scores": [
        42.99,
        7.8,
        31.82,
        0.59,
        0.0,
        60.37,
        42.91,
        8.0,
        30.76,
        0.74,
        0.0,
        57.93,
        41.7,
        4.6,
        32.48,
        0.89,
        0.0,
        61.59
      ],
      "code_scores": [
        21.4,
        8.6,
        54.27,
        40.34,
        54.47,
        10.39,
        56.1,
        47.12,
        54.09,
        8.6,
        54.88,
        44.75
      ],
      "reasoning_scores": [
        6.76,
        7.07,
        13.891304,
        9.36,
        7.43,
        7.07,
        12.956522,
        11.92,
        6.15,
        7.58,
        12.315217,
        11.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.69
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.96
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.08
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.05
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -22.98,
          "math_avg": 5.72,
          "code_avg": -4.46,
          "reasoning_avg": -26.92,
          "overall_avg": -12.16,
          "overall_efficiency": -0.031996,
          "general_efficiency": -0.060474,
          "math_efficiency": 0.015058,
          "code_efficiency": -0.011724,
          "reasoning_efficiency": -0.070846,
          "general_task_scores": [
            -58.61,
            10.67,
            -16.65,
            -27.33
          ],
          "math_task_scores": [
            -13.88,
            -12.4,
            27.35,
            0.74,
            0.0,
            32.52
          ],
          "code_task_scores": [
            -11.15,
            5.62,
            23.98,
            -36.27
          ],
          "reasoning_task_scores": [
            -55.78,
            -22.56,
            -18.15,
            -11.2
          ]
        },
        "vs_instruct": {
          "general_avg": -40.48,
          "math_avg": -15.86,
          "code_avg": -17.23,
          "reasoning_avg": -34.7,
          "overall_avg": -27.07,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -75.14,
            -29.32,
            -20.98,
            -36.46
          ],
          "math_task_scores": [
            -43.14,
            -41.4,
            19.07,
            -13.65,
            -8.34,
            -7.72
          ],
          "code_task_scores": [
            -27.89,
            -0.48,
            -5.29,
            -35.25
          ],
          "reasoning_task_scores": [
            -56.07,
            -17.0,
            -29.11,
            -36.64
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 42.92,
      "math_avg": 25.29,
      "code_avg": 44.33,
      "reasoning_avg": 32.53,
      "overall_avg": 36.26,
      "overall_efficiency": 0.021525,
      "general_efficiency": 0.035702,
      "math_efficiency": 0.068185,
      "code_efficiency": 0.018052,
      "reasoning_efficiency": -0.035835,
      "general_scores": [
        66.6,
        45.01,
        33.82,
        27.085,
        63.37,
        44.8675,
        32.8,
        29.2685714,
        67.32,
        43.4825,
        33.89,
        27.4778571
      ],
      "math_scores": [
        63.0,
        20.2,
        12.08,
        4.3,
        0.0,
        52.4,
        63.15,
        20.2,
        11.34,
        4.01,
        3.33,
        50.61,
        60.96,
        19.2,
        11.88,
        5.49,
        0.0,
        53.05
      ],
      "code_scores": [
        57.59,
        1.08,
        39.63,
        74.92,
        61.09,
        3.94,
        41.46,
        75.93,
        59.53,
        1.08,
        41.46,
        74.24
      ],
      "reasoning_scores": [
        55.67,
        23.74,
        17.163043,
        32.16,
        56.42,
        25.25,
        18.043478,
        31.52,
        57.16,
        24.75,
        17.152174,
        31.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 59.4
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.85
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.58
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.87,
          "math_avg": 7.39,
          "code_avg": 1.96,
          "reasoning_avg": -3.88,
          "overall_avg": 2.33,
          "overall_efficiency": 0.021525,
          "general_efficiency": 0.035702,
          "math_efficiency": 0.068185,
          "code_efficiency": 0.018052,
          "reasoning_efficiency": -0.035835,
          "general_task_scores": [
            1.39,
            24.11,
            -3.15,
            -6.89
          ],
          "math_task_scores": [
            5.96,
            0.67,
            7.43,
            4.6,
            1.11,
            24.58
          ],
          "code_task_scores": [
            4.93,
            -1.55,
            9.75,
            -5.31
          ],
          "reasoning_task_scores": [
            -6.14,
            -5.22,
            -13.75,
            9.57
          ]
        },
        "vs_instruct": {
          "general_avg": -13.63,
          "math_avg": -14.19,
          "code_avg": -10.82,
          "reasoning_avg": -11.67,
          "overall_avg": -12.58,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -15.14,
            -15.88,
            -7.48,
            -16.02
          ],
          "math_task_scores": [
            -23.3,
            -28.33,
            -0.85,
            -9.79,
            -7.22,
            -15.66
          ],
          "code_task_scores": [
            -11.81,
            -7.65,
            -19.52,
            -4.29
          ],
          "reasoning_task_scores": [
            -6.43,
            0.34,
            -24.71,
            -15.87
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 21.12,
      "math_avg": 22.95,
      "code_avg": 37.17,
      "reasoning_avg": 8.0,
      "overall_avg": 22.31,
      "overall_efficiency": -0.011139,
      "general_efficiency": -0.017178,
      "math_efficiency": 0.004841,
      "code_efficiency": -0.00499,
      "reasoning_efficiency": -0.027231,
      "general_scores": [
        28.92,
        29.495,
        24.82,
        13.8064286,
        24.57,
        30.8075,
        10.19,
        7.05785714,
        17.23,
        33.365,
        17.7,
        15.5371429
      ],
      "math_scores": [
        80.67,
        1.4,
        20.82,
        0.0,
        0.0,
        36.59,
        80.21,
        1.4,
        21.54,
        0.59,
        0.0,
        35.37,
        73.92,
        2.0,
        19.24,
        0.3,
        0.0,
        39.02
      ],
      "code_scores": [
        47.08,
        0.0,
        34.15,
        64.41,
        55.25,
        0.0,
        29.88,
        60.34,
        53.7,
        0.0,
        37.8,
        63.39
      ],
      "reasoning_scores": [
        5.96,
        22.73,
        3.206522,
        10.4,
        4.53,
        15.15,
        1.586957,
        5.84,
        2.91,
        11.62,
        1.293478,
        10.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.27
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.53
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.99
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 52.01
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.94
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.01
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -17.92,
          "math_avg": 5.05,
          "code_avg": -5.21,
          "reasoning_avg": -28.41,
          "overall_avg": -11.62,
          "overall_efficiency": -0.011139,
          "general_efficiency": -0.017178,
          "math_efficiency": 0.004841,
          "code_efficiency": -0.00499,
          "reasoning_efficiency": -0.027231,
          "general_task_scores": [
            -40.8,
            10.88,
            -19.08,
            -22.7
          ],
          "math_task_scores": [
            21.86,
            -17.6,
            16.19,
            0.3,
            0.0,
            9.55
          ],
          "code_task_scores": [
            -2.46,
            -3.58,
            2.84,
            -17.63
          ],
          "reasoning_task_scores": [
            -58.09,
            -13.3,
            -29.17,
            -13.07
          ]
        },
        "vs_instruct": {
          "general_avg": -35.42,
          "math_avg": -16.53,
          "code_avg": -17.98,
          "reasoning_avg": -36.19,
          "overall_avg": -26.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -57.33,
            -29.11,
            -23.41,
            -31.83
          ],
          "math_task_scores": [
            -7.4,
            -46.6,
            7.91,
            -14.09,
            -8.34,
            -30.69
          ],
          "code_task_scores": [
            -19.2,
            -9.68,
            -26.43,
            -16.61
          ],
          "reasoning_task_scores": [
            -58.38,
            -7.74,
            -40.13,
            -38.51
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 15.44,
      "math_avg": 12.28,
      "code_avg": 5.07,
      "reasoning_avg": 4.03,
      "overall_avg": 9.2,
      "overall_efficiency": -0.247281,
      "general_efficiency": -0.236023,
      "math_efficiency": -0.056227,
      "code_efficiency": -0.373075,
      "reasoning_efficiency": -0.323799,
      "general_scores": [
        24.26,
        24.49,
        5.83,
        2.715714,
        28.25,
        25.09,
        7.58,
        1.91357143,
        29.72,
        24.2225,
        9.21,
        2.045
      ],
      "math_scores": [
        66.63,
        0.4,
        2.62,
        0.0,
        0.0,
        0.0,
        74.15,
        0.6,
        3.0,
        0.0,
        0.0,
        0.0,
        69.6,
        0.8,
        3.16,
        0.0,
        0.0,
        0.0
      ],
      "code_scores": [
        0.0,
        0.0,
        0.0,
        18.31,
        0.39,
        0.0,
        0.0,
        21.69,
        0.39,
        0.0,
        0.0,
        20.0
      ],
      "reasoning_scores": [
        6.18,
        1.52,
        2.0,
        4.56,
        6.22,
        5.56,
        4.01087,
        2.08,
        7.57,
        4.04,
        2.380435,
        2.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.6
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.54
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.13
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.93
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.66
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.8
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -23.6,
          "math_avg": -5.62,
          "code_avg": -37.31,
          "reasoning_avg": -32.38,
          "overall_avg": -24.73,
          "overall_efficiency": -0.247281,
          "general_efficiency": -0.236023,
          "math_efficiency": -0.056227,
          "code_efficiency": -0.373075,
          "reasoning_efficiency": -0.323799,
          "general_task_scores": [
            -36.96,
            4.26,
            -29.11,
            -32.61
          ],
          "math_task_scores": [
            13.72,
            -18.6,
            -1.41,
            0.0,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -54.21,
            -3.58,
            -31.1,
            -60.34
          ],
          "reasoning_task_scores": [
            -55.9,
            -26.09,
            -28.4,
            -19.12
          ]
        },
        "vs_instruct": {
          "general_avg": -41.1,
          "math_avg": -27.21,
          "code_avg": -50.08,
          "reasoning_avg": -40.16,
          "overall_avg": -39.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -53.49,
            -35.72,
            -33.44,
            -41.74
          ],
          "math_task_scores": [
            -15.54,
            -47.6,
            -9.69,
            -14.39,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -70.95,
            -9.68,
            -60.37,
            -59.32
          ],
          "reasoning_task_scores": [
            -56.19,
            -20.53,
            -39.36,
            -44.56
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 30.24,
      "math_avg": 16.62,
      "code_avg": 31.21,
      "reasoning_avg": 14.43,
      "overall_avg": 23.12,
      "overall_efficiency": -0.480306,
      "general_efficiency": -0.391398,
      "math_efficiency": -0.05679,
      "code_efficiency": -0.496296,
      "reasoning_efficiency": -0.976737,
      "general_scores": [
        35.39,
        41.3275,
        26.68,
        16.3792857,
        45.86,
        45.1325,
        31.03,
        14.3221429,
        38.58,
        40.27,
        21.11,
        6.795
      ],
      "math_scores": [
        46.85,
        7.2,
        17.66,
        2.08,
        3.33,
        32.32,
        45.49,
        9.0,
        13.98,
        2.82,
        0.0,
        28.66,
        41.62,
        5.6,
        16.98,
        1.19,
        0.0,
        24.39
      ],
      "code_scores": [
        42.41,
        0.0,
        22.56,
        68.81,
        35.02,
        0.0,
        18.9,
        63.05,
        44.75,
        0.0,
        15.24,
        63.73
      ],
      "reasoning_scores": [
        29.31,
        13.13,
        8.576087,
        11.28,
        31.61,
        10.61,
        8.815217,
        13.2,
        20.22,
        11.11,
        6.619565,
        8.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.21
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 40.73
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.81,
          "math_avg": -1.28,
          "code_avg": -11.17,
          "reasoning_avg": -21.98,
          "overall_avg": -10.81,
          "overall_efficiency": -0.480306,
          "general_efficiency": -0.391398,
          "math_efficiency": -0.05679,
          "code_efficiency": -0.496296,
          "reasoning_efficiency": -0.976737,
          "general_task_scores": [
            -24.43,
            21.9,
            -10.38,
            -22.33
          ],
          "math_task_scores": [
            -11.76,
            -11.93,
            11.87,
            2.03,
            1.11,
            1.02
          ],
          "code_task_scores": [
            -13.74,
            -3.58,
            -12.2,
            -15.14
          ],
          "reasoning_task_scores": [
            -35.51,
            -18.18,
            -23.2,
            -11.01
          ]
        },
        "vs_instruct": {
          "general_avg": -26.3,
          "math_avg": -22.86,
          "code_avg": -23.94,
          "reasoning_avg": -29.76,
          "overall_avg": -25.72,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -40.96,
            -18.08,
            -14.71,
            -31.46
          ],
          "math_task_scores": [
            -41.02,
            -40.93,
            3.59,
            -12.36,
            -7.22,
            -39.22
          ],
          "code_task_scores": [
            -30.48,
            -9.68,
            -41.47,
            -14.12
          ],
          "reasoning_task_scores": [
            -35.8,
            -12.62,
            -34.16,
            -36.45
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 15.16,
      "math_avg": 15.26,
      "code_avg": 17.26,
      "reasoning_avg": 6.59,
      "overall_avg": 13.57,
      "overall_efficiency": -0.259173,
      "general_efficiency": -0.304029,
      "math_efficiency": -0.033619,
      "code_efficiency": -0.319527,
      "reasoning_efficiency": -0.379519,
      "general_scores": [
        20.94,
        26.68,
        11.57,
        0.75785714,
        23.43,
        23.56,
        10.83,
        7.24928571,
        25.96,
        21.7775,
        7.57,
        1.55357143
      ],
      "math_scores": [
        88.55,
        2.6,
        4.04,
        0.15,
        0.0,
        0.0,
        79.98,
        4.8,
        3.84,
        0.3,
        0.0,
        0.0,
        85.82,
        2.4,
        1.99,
        0.15,
        0.0,
        0.0
      ],
      "code_scores": [
        42.41,
        0.0,
        0.0,
        22.71,
        46.3,
        0.0,
        0.0,
        26.44,
        48.64,
        0.0,
        0.0,
        20.68
      ],
      "reasoning_scores": [
        0.09,
        5.05,
        11.967391,
        7.04,
        2.62,
        8.59,
        14.521739,
        6.56,
        0.02,
        3.03,
        13.413043,
        6.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.29
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 45.78
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -23.89,
          "math_avg": -2.64,
          "code_avg": -25.11,
          "reasoning_avg": -29.82,
          "overall_avg": -20.37,
          "overall_efficiency": -0.259173,
          "general_efficiency": -0.304029,
          "math_efficiency": -0.033619,
          "code_efficiency": -0.319527,
          "reasoning_efficiency": -0.379519,
          "general_task_scores": [
            -40.93,
            3.67,
            -26.66,
            -31.64
          ],
          "math_task_scores": [
            28.37,
            -15.93,
            -1.05,
            0.2,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -8.69,
            -3.58,
            -31.1,
            -57.06
          ],
          "reasoning_task_scores": [
            -61.65,
            -24.24,
            -17.9,
            -15.49
          ]
        },
        "vs_instruct": {
          "general_avg": -41.39,
          "math_avg": -24.23,
          "code_avg": -37.88,
          "reasoning_avg": -37.6,
          "overall_avg": -35.27,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -57.46,
            -36.32,
            -30.99,
            -40.77
          ],
          "math_task_scores": [
            -0.89,
            -44.93,
            -9.33,
            -14.19,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -25.43,
            -9.68,
            -60.37,
            -56.04
          ],
          "reasoning_task_scores": [
            -61.94,
            -18.68,
            -28.86,
            -40.93
          ]
        }
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 6.52,
      "math_avg": 8.42,
      "code_avg": 15.86,
      "reasoning_avg": 0.02,
      "overall_avg": 7.71,
      "overall_efficiency": -0.239712,
      "general_efficiency": -0.297304,
      "math_efficiency": -0.086598,
      "code_efficiency": -0.24231,
      "reasoning_efficiency": -0.332635,
      "general_scores": [
        0.39,
        16.575,
        1.19,
        7.99214286,
        0.36,
        16.575,
        1.25,
        7.864286,
        0.36,
        16.575,
        1.25,
        7.864286
      ],
      "math_scores": [
        50.49,
        0.0,
        0.09,
        0.0,
        0.0,
        0.0,
        50.42,
        0.0,
        0.11,
        0.0,
        0.0,
        0.0,
        50.42,
        0.0,
        0.11,
        0.0,
        0.0,
        0.0
      ],
      "code_scores": [
        36.58,
        0.0,
        0.0,
        28.14,
        35.02,
        0.0,
        0.0,
        27.8,
        35.02,
        0.0,
        0.0,
        27.8
      ],
      "reasoning_scores": [
        0.0,
        0.0,
        0.076087,
        0.0,
        0.0,
        0.0,
        0.0761,
        0.0,
        0.0,
        0.0,
        0.0761,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.58
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.44
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 35.54
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -32.53,
          "math_avg": -9.47,
          "code_avg": -26.51,
          "reasoning_avg": -36.39,
          "overall_avg": -26.22,
          "overall_efficiency": -0.239712,
          "general_efficiency": -0.297304,
          "math_efficiency": -0.086598,
          "code_efficiency": -0.24231,
          "reasoning_efficiency": -0.332635,
          "general_task_scores": [
            -64.0,
            -3.76,
            -35.42,
            -26.92
          ],
          "math_task_scores": [
            -5.97,
            -19.2,
            -4.24,
            0.0,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -18.93,
            -3.58,
            -31.1,
            -52.43
          ],
          "reasoning_task_scores": [
            -62.56,
            -29.8,
            -31.12,
            -22.08
          ]
        },
        "vs_instruct": {
          "general_avg": -50.02,
          "math_avg": -31.06,
          "code_avg": -39.28,
          "reasoning_avg": -44.17,
          "overall_avg": -41.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -80.53,
            -43.74,
            -39.75,
            -36.05
          ],
          "math_task_scores": [
            -35.23,
            -48.2,
            -12.52,
            -14.39,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -35.67,
            -9.68,
            -60.37,
            -51.41
          ],
          "reasoning_task_scores": [
            -62.85,
            -24.24,
            -42.08,
            -47.52
          ]
        }
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 45.01,
      "math_avg": 27.28,
      "code_avg": 51.36,
      "reasoning_avg": 39.02,
      "overall_avg": 40.67,
      "overall_efficiency": 0.023306,
      "general_efficiency": 0.020639,
      "math_efficiency": 0.032456,
      "code_efficiency": 0.0311,
      "reasoning_efficiency": 0.009031,
      "general_scores": [
        66.51,
        41.69,
        36.43,
        32.37,
        67.97,
        43.42,
        37.33,
        31.4957143,
        70.04,
        41.3525,
        37.77,
        33.7771429
      ],
      "math_scores": [
        55.34,
        23.0,
        11.56,
        4.01,
        6.67,
        67.68,
        52.92,
        22.8,
        11.34,
        5.34,
        3.33,
        66.46,
        57.16,
        20.8,
        11.74,
        4.45,
        0.0,
        66.46
      ],
      "code_scores": [
        65.37,
        7.17,
        54.88,
        79.32,
        64.98,
        3.58,
        55.49,
        79.32,
        65.37,
        6.81,
        56.1,
        77.97
      ],
      "reasoning_scores": [
        60.15,
        27.78,
        32.402174,
        34.64,
        59.12,
        27.78,
        33.869565,
        36.16,
        61.17,
        26.77,
        31.847826,
        36.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.17
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.87
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.85
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.87
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.71
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.97,
          "math_avg": 9.38,
          "code_avg": 8.99,
          "reasoning_avg": 2.61,
          "overall_avg": 6.74,
          "overall_efficiency": 0.023306,
          "general_efficiency": 0.020639,
          "math_efficiency": 0.032456,
          "code_efficiency": 0.0311,
          "reasoning_efficiency": 0.009031,
          "general_task_scores": [
            3.8,
            21.81,
            0.53,
            -2.28
          ],
          "math_task_scores": [
            -1.27,
            3.0,
            7.21,
            4.6,
            3.33,
            39.43
          ],
          "code_task_scores": [
            10.77,
            2.27,
            24.39,
            -1.47
          ],
          "reasoning_task_scores": [
            -2.41,
            -2.36,
            1.51,
            13.71
          ]
        },
        "vs_instruct": {
          "general_avg": -11.53,
          "math_avg": -12.2,
          "code_avg": -3.78,
          "reasoning_avg": -5.17,
          "overall_avg": -8.17,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -12.73,
            -18.18,
            -3.8,
            -11.41
          ],
          "math_task_scores": [
            -30.53,
            -26.0,
            -1.07,
            -9.79,
            -5.01,
            -0.81
          ],
          "code_task_scores": [
            -5.97,
            -3.83,
            -4.88,
            -0.45
          ],
          "reasoning_task_scores": [
            -2.7,
            3.2,
            -9.45,
            -11.73
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 46.64,
      "math_avg": 25.5,
      "code_avg": 50.21,
      "reasoning_avg": 36.13,
      "overall_avg": 39.62,
      "overall_efficiency": 0.076972,
      "general_efficiency": 0.102776,
      "math_efficiency": 0.102855,
      "code_efficiency": 0.10606,
      "reasoning_efficiency": -0.003802,
      "general_scores": [
        70.78,
        49.31,
        37.89,
        24.9842857,
        71.56,
        48.2975,
        36.72,
        32.6592857,
        71.3,
        47.68,
        36.33,
        32.2192857
      ],
      "math_scores": [
        59.74,
        19.0,
        11.33,
        3.12,
        0.0,
        50.61,
        55.72,
        22.2,
        11.02,
        4.15,
        0.0,
        62.8,
        55.65,
        22.6,
        11.56,
        4.3,
        0.0,
        65.24
      ],
      "code_scores": [
        59.92,
        3.94,
        43.9,
        75.93,
        65.37,
        7.89,
        56.71,
        79.32,
        62.26,
        8.24,
        59.76,
        79.32
      ],
      "reasoning_scores": [
        43.83,
        23.74,
        21.619565,
        32.96,
        62.48,
        28.28,
        31.043478,
        36.4,
        62.85,
        26.26,
        29.043478,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.3
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.86
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 62.52
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.46
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.39
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.6,
          "math_avg": 7.6,
          "code_avg": 7.84,
          "reasoning_avg": -0.28,
          "overall_avg": 5.69,
          "overall_efficiency": 0.076972,
          "general_efficiency": 0.102776,
          "math_efficiency": 0.102855,
          "code_efficiency": 0.10606,
          "reasoning_efficiency": -0.003802,
          "general_task_scores": [
            6.84,
            28.09,
            0.33,
            -4.88
          ],
          "math_task_scores": [
            0.63,
            2.07,
            6.96,
            3.86,
            0.0,
            32.11
          ],
          "code_task_scores": [
            8.05,
            3.11,
            22.36,
            -2.15
          ],
          "reasoning_task_scores": [
            -6.17,
            -3.71,
            -3.96,
            12.72
          ]
        },
        "vs_instruct": {
          "general_avg": -9.9,
          "math_avg": -13.98,
          "code_avg": -4.93,
          "reasoning_avg": -8.06,
          "overall_avg": -9.22,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.69,
            -11.9,
            -4.0,
            -14.01
          ],
          "math_task_scores": [
            -28.63,
            -26.93,
            -1.32,
            -10.53,
            -8.34,
            -8.13
          ],
          "code_task_scores": [
            -8.69,
            -2.99,
            -6.91,
            -1.13
          ],
          "reasoning_task_scores": [
            -6.46,
            1.85,
            -14.92,
            -12.72
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 20.96,
      "math_avg": 17.01,
      "code_avg": 35.7,
      "reasoning_avg": 14.15,
      "overall_avg": 21.95,
      "overall_efficiency": -0.263544,
      "general_efficiency": -0.397976,
      "math_efficiency": -0.019522,
      "code_efficiency": -0.146816,
      "reasoning_efficiency": -0.489864,
      "general_scores": [
        26.05,
        30.395,
        17.23,
        10.1828571,
        19.66,
        28.015,
        22.5,
        12.6657143,
        29.38,
        26.9925,
        20.71,
        7.72642857
      ],
      "math_scores": [
        33.74,
        6.8,
        20.3,
        1.19,
        3.33,
        37.8,
        35.48,
        9.0,
        19.47,
        1.34,
        3.33,
        35.37,
        35.41,
        6.6,
        21.7,
        1.19,
        0.0,
        34.15
      ],
      "code_scores": [
        48.64,
        5.02,
        29.88,
        46.1,
        54.09,
        5.38,
        31.71,
        69.49,
        50.58,
        5.02,
        29.27,
        53.22
      ],
      "reasoning_scores": [
        9.88,
        15.66,
        17.923913,
        13.92,
        14.92,
        12.63,
        18.532609,
        12.96,
        9.84,
        14.14,
        16.793478,
        12.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 51.1
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.14
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.75
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -18.09,
          "math_avg": -0.89,
          "code_avg": -6.67,
          "reasoning_avg": -22.26,
          "overall_avg": -11.98,
          "overall_efficiency": -0.263544,
          "general_efficiency": -0.397976,
          "math_efficiency": -0.019522,
          "code_efficiency": -0.146816,
          "reasoning_efficiency": -0.489864,
          "general_task_scores": [
            -39.34,
            8.13,
            -16.5,
            -24.64
          ],
          "math_task_scores": [
            -21.53,
            -11.73,
            16.15,
            1.24,
            2.22,
            8.33
          ],
          "code_task_scores": [
            -3.37,
            1.56,
            -0.81,
            -24.07
          ],
          "reasoning_task_scores": [
            -51.01,
            -15.66,
            -13.45,
            -8.93
          ]
        },
        "vs_instruct": {
          "general_avg": -35.58,
          "math_avg": -22.47,
          "code_avg": -19.45,
          "reasoning_avg": -30.05,
          "overall_avg": -26.89,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -55.87,
            -31.86,
            -20.83,
            -33.77
          ],
          "math_task_scores": [
            -50.79,
            -40.73,
            7.87,
            -13.15,
            -6.12,
            -31.91
          ],
          "code_task_scores": [
            -20.11,
            -4.54,
            -30.08,
            -23.05
          ],
          "reasoning_task_scores": [
            -51.3,
            -10.1,
            -24.41,
            -34.37
          ]
        }
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 13.79,
      "math_avg": 10.96,
      "code_avg": 18.7,
      "reasoning_avg": 13.52,
      "overall_avg": 14.24,
      "overall_efficiency": -0.047044,
      "general_efficiency": -0.060334,
      "math_efficiency": -0.016581,
      "code_efficiency": -0.056567,
      "reasoning_efficiency": -0.054693,
      "general_scores": [
        1.18,
        31.1675,
        10.81,
        16.3792857,
        0.04,
        31.3975,
        11.81,
        14.3221429,
        0.07,
        30.2225,
        11.33,
        6.795
      ],
      "math_scores": [
        48.29,
        3.4,
        6.32,
        2.08,
        0.0,
        0.0,
        45.49,
        5.8,
        9.28,
        2.82,
        0.0,
        0.0,
        53.75,
        2.2,
        13.3,
        1.19,
        3.33,
        0.0
      ],
      "code_scores": [
        38.13,
        0.0,
        0.0,
        37.97,
        35.41,
        0.0,
        0.0,
        39.32,
        36.58,
        0.0,
        0.0,
        36.95
      ],
      "reasoning_scores": [
        22.24,
        13.13,
        18.326087,
        3.12,
        23.48,
        10.61,
        19.141304,
        4.16,
        14.76,
        11.11,
        20.543478,
        1.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.93
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.18
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.63
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 36.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.16
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -25.25,
          "math_avg": -6.94,
          "code_avg": -23.68,
          "reasoning_avg": -22.89,
          "overall_avg": -19.69,
          "overall_efficiency": -0.047044,
          "general_efficiency": -0.060334,
          "math_efficiency": -0.016581,
          "code_efficiency": -0.056567,
          "reasoning_efficiency": -0.054693,
          "general_task_scores": [
            -63.94,
            10.59,
            -25.33,
            -22.33
          ],
          "math_task_scores": [
            -7.23,
            -15.4,
            5.29,
            2.03,
            1.11,
            -27.44
          ],
          "code_task_scores": [
            -17.76,
            -3.58,
            -31.1,
            -42.26
          ],
          "reasoning_task_scores": [
            -42.4,
            -18.18,
            -11.86,
            -19.12
          ]
        },
        "vs_instruct": {
          "general_avg": -42.75,
          "math_avg": -28.52,
          "code_avg": -36.45,
          "reasoning_avg": -30.67,
          "overall_avg": -34.6,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -80.47,
            -29.4,
            -29.66,
            -31.46
          ],
          "math_task_scores": [
            -36.49,
            -44.4,
            -2.99,
            -12.36,
            -7.22,
            -67.68
          ],
          "code_task_scores": [
            -34.5,
            -9.68,
            -60.37,
            -41.24
          ],
          "reasoning_task_scores": [
            -42.69,
            -12.62,
            -22.82,
            -44.56
          ]
        }
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 71,
      "name": "Open-Platypus",
      "domain": "reasoning",
      "general_avg": 43.86,
      "math_avg": 21.2,
      "code_avg": 30.46,
      "reasoning_avg": 33.79,
      "overall_avg": 32.33,
      "overall_efficiency": -0.064408,
      "general_efficiency": 0.192975,
      "math_efficiency": 0.132492,
      "code_efficiency": -0.478015,
      "reasoning_efficiency": -0.105085,
      "general_scores": [
        61.32,
        41.805,
        41.28,
        31.02
      ],
      "math_scores": [
        60.05,
        32.2,
        10.66,
        3.12,
        1.665,
        19.51
      ],
      "code_scores": [
        26.07,
        2.15,
        20.73,
        72.88
      ],
      "reasoning_scores": [
        51.2,
        23.23,
        27.532609,
        33.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.66
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 19.51
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 26.07
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.15
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.73
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.88
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.2
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.53
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.81,
          "math_avg": 3.3,
          "code_avg": -11.92,
          "reasoning_avg": -2.62,
          "overall_avg": -1.61,
          "overall_efficiency": -0.064408,
          "general_efficiency": 0.192975,
          "math_efficiency": 0.132492,
          "code_efficiency": -0.478015,
          "reasoning_efficiency": -0.105085,
          "general_task_scores": [
            -3.05,
            21.46,
            4.63,
            -3.81
          ],
          "math_task_scores": [
            3.64,
            13.0,
            6.32,
            3.12,
            1.66,
            -7.93
          ],
          "code_task_scores": [
            -28.4,
            -1.43,
            -10.37,
            -7.46
          ],
          "reasoning_task_scores": [
            -11.36,
            -6.57,
            -3.67,
            11.12
          ]
        },
        "vs_instruct": {
          "general_avg": -12.69,
          "math_avg": -18.28,
          "code_avg": -24.69,
          "reasoning_avg": -10.4,
          "overall_avg": -16.51,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -19.58,
            -18.53,
            0.3,
            -12.94
          ],
          "math_task_scores": [
            -25.62,
            -16.0,
            -1.96,
            -11.27,
            -6.68,
            -48.17
          ],
          "code_task_scores": [
            -45.14,
            -7.53,
            -39.64,
            -6.44
          ],
          "reasoning_task_scores": [
            -11.65,
            -1.01,
            -14.63,
            -14.32
          ]
        }
      },
      "affiliation": "Boston University",
      "year": "2023",
      "size": "24.9k",
      "size_precise": "24926",
      "link": "https://huggingface.co/datasets/garage-bAInd/Open-Platypus",
      "paper_link": "https://arxiv.org/abs/2308.07317",
      "tag": "general,math,code,science"
    },
    {
      "id": 72,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 40.56,
      "math_avg": 45.15,
      "code_avg": 25.04,
      "reasoning_avg": 24.08,
      "overall_avg": 33.71,
      "overall_efficiency": -0.002375,
      "general_efficiency": 0.016119,
      "math_efficiency": 0.290791,
      "code_efficiency": -0.184913,
      "reasoning_efficiency": -0.131498,
      "general_scores": [
        72.19,
        28.8825,
        34.04,
        27.1158333
      ],
      "math_scores": [
        88.02,
        78.2,
        35.73,
        51.48,
        17.5,
        0.0
      ],
      "code_scores": [
        28.02,
        0.0,
        0.61,
        71.53
      ],
      "reasoning_scores": [
        19.23,
        11.62,
        23.967391,
        41.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.04
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.73
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.48
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.5
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 28.02
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.97
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.51,
          "math_avg": 27.26,
          "code_avg": -17.33,
          "reasoning_avg": -12.33,
          "overall_avg": -0.22,
          "overall_efficiency": -0.002375,
          "general_efficiency": 0.016119,
          "math_efficiency": 0.290791,
          "code_efficiency": -0.184913,
          "reasoning_efficiency": -0.131498,
          "general_task_scores": [
            7.82,
            8.54,
            -2.61,
            -7.71
          ],
          "math_task_scores": [
            31.61,
            59.0,
            31.39,
            51.48,
            17.5,
            -27.44
          ],
          "code_task_scores": [
            -26.45,
            -3.58,
            -30.49,
            -8.81
          ],
          "reasoning_task_scores": [
            -43.33,
            -18.18,
            -7.23,
            19.44
          ]
        },
        "vs_instruct": {
          "general_avg": -15.98,
          "math_avg": 5.67,
          "code_avg": -30.1,
          "reasoning_avg": -20.11,
          "overall_avg": -15.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.71,
            -31.45,
            -6.94,
            -16.84
          ],
          "math_task_scores": [
            2.35,
            30.0,
            23.11,
            37.09,
            9.16,
            -67.68
          ],
          "code_task_scores": [
            -43.19,
            -9.68,
            -59.76,
            -7.79
          ],
          "reasoning_task_scores": [
            -43.62,
            -12.62,
            -18.19,
            -6.0
          ]
        }
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 73,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 39.95,
      "math_avg": 39.38,
      "code_avg": 35.92,
      "reasoning_avg": 24.91,
      "overall_avg": 35.04,
      "overall_efficiency": 0.008339,
      "general_efficiency": 0.0068,
      "math_efficiency": 0.161417,
      "code_efficiency": -0.048497,
      "reasoning_efficiency": -0.086367,
      "general_scores": [
        57.36,
        29.295,
        40.87,
        32.28
      ],
      "math_scores": [
        88.7,
        64.0,
        21.34,
        30.27,
        3.33,
        28.66
      ],
      "code_scores": [
        44.36,
        5.02,
        17.68,
        76.61
      ],
      "reasoning_scores": [
        24.07,
        12.12,
        26.347826,
        37.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.87
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 44.36
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.07
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.91,
          "math_avg": 21.48,
          "code_avg": -6.45,
          "reasoning_avg": -11.5,
          "overall_avg": 1.11,
          "overall_efficiency": 0.008339,
          "general_efficiency": 0.0068,
          "math_efficiency": 0.161417,
          "code_efficiency": -0.048497,
          "reasoning_efficiency": -0.086367,
          "general_task_scores": [
            -7.01,
            8.96,
            4.22,
            -2.55
          ],
          "math_task_scores": [
            32.29,
            44.8,
            17.0,
            30.27,
            3.33,
            1.22
          ],
          "code_task_scores": [
            -10.11,
            1.44,
            -13.42,
            -3.73
          ],
          "reasoning_task_scores": [
            -38.49,
            -17.68,
            -4.85,
            15.04
          ]
        },
        "vs_instruct": {
          "general_avg": -16.59,
          "math_avg": -0.1,
          "code_avg": -19.23,
          "reasoning_avg": -19.28,
          "overall_avg": -13.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.54,
            -31.02,
            -0.11,
            -11.68
          ],
          "math_task_scores": [
            3.03,
            15.8,
            8.72,
            15.88,
            -5.01,
            -39.02
          ],
          "code_task_scores": [
            -26.85,
            -4.66,
            -42.69,
            -2.71
          ],
          "reasoning_task_scores": [
            -38.78,
            -12.12,
            -15.81,
            -10.4
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 74,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 38.02,
      "math_avg": 26.53,
      "code_avg": 38.47,
      "reasoning_avg": 22.97,
      "overall_avg": 31.5,
      "overall_efficiency": -0.017632,
      "general_efficiency": -0.00746,
      "math_efficiency": 0.062548,
      "code_efficiency": -0.028243,
      "reasoning_efficiency": -0.097376,
      "general_scores": [
        71.2,
        28.945,
        30.02,
        21.9018182
      ],
      "math_scores": [
        78.7,
        31.2,
        11.18,
        10.09,
        6.67,
        21.34
      ],
      "code_scores": [
        46.3,
        4.3,
        28.05,
        75.25
      ],
      "reasoning_scores": [
        29.51,
        8.08,
        24.858696,
        29.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 46.3
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.05
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.25
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.86
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.03,
          "math_avg": 8.63,
          "code_avg": -3.9,
          "reasoning_avg": -13.44,
          "overall_avg": -2.43,
          "overall_efficiency": -0.017632,
          "general_efficiency": -0.00746,
          "math_efficiency": 0.062548,
          "code_efficiency": -0.028243,
          "reasoning_efficiency": -0.097376,
          "general_task_scores": [
            6.83,
            8.6,
            -6.63,
            -12.93
          ],
          "math_task_scores": [
            22.29,
            12.0,
            6.84,
            10.09,
            6.67,
            -6.1
          ],
          "code_task_scores": [
            -8.17,
            0.72,
            -3.05,
            -5.09
          ],
          "reasoning_task_scores": [
            -33.05,
            -21.72,
            -6.34,
            7.36
          ]
        },
        "vs_instruct": {
          "general_avg": -18.53,
          "math_avg": -12.95,
          "code_avg": -16.67,
          "reasoning_avg": -21.22,
          "overall_avg": -17.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.7,
            -31.38,
            -10.96,
            -22.06
          ],
          "math_task_scores": [
            -6.97,
            -17.0,
            -1.44,
            -4.3,
            -1.67,
            -46.34
          ],
          "code_task_scores": [
            -24.91,
            -5.38,
            -32.32,
            -4.07
          ],
          "reasoning_task_scores": [
            -33.34,
            -16.16,
            -17.3,
            -18.08
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 75,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 24.92,
      "math_avg": 31.7,
      "code_avg": 22.09,
      "reasoning_avg": 19.0,
      "overall_avg": 24.43,
      "overall_efficiency": -0.568649,
      "general_efficiency": -0.845174,
      "math_efficiency": 0.826052,
      "code_efficiency": -1.213794,
      "reasoning_efficiency": -1.041683,
      "general_scores": [
        36.57,
        27.0825,
        25.31,
        10.7307143
      ],
      "math_scores": [
        85.22,
        46.2,
        14.41,
        14.99,
        5.0,
        24.39
      ],
      "code_scores": [
        15.95,
        5.38,
        21.95,
        45.08
      ],
      "reasoning_scores": [
        20.34,
        9.09,
        22.423913,
        24.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.31
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.41
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 5.38
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.95
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.09
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -14.12,
          "math_avg": 13.8,
          "code_avg": -20.28,
          "reasoning_avg": -17.41,
          "overall_avg": -9.5,
          "overall_efficiency": -0.568649,
          "general_efficiency": -0.845174,
          "math_efficiency": 0.826052,
          "code_efficiency": -1.213794,
          "reasoning_efficiency": -1.041683,
          "general_task_scores": [
            -27.8,
            6.74,
            -11.34,
            -24.1
          ],
          "math_task_scores": [
            28.81,
            27.0,
            10.07,
            14.99,
            5.0,
            -3.05
          ],
          "code_task_scores": [
            -38.52,
            1.8,
            -9.15,
            -35.26
          ],
          "reasoning_task_scores": [
            -42.22,
            -20.71,
            -8.78,
            2.08
          ]
        },
        "vs_instruct": {
          "general_avg": -31.62,
          "math_avg": -7.78,
          "code_avg": -33.05,
          "reasoning_avg": -25.19,
          "overall_avg": -24.41,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -44.33,
            -33.25,
            -15.67,
            -33.23
          ],
          "math_task_scores": [
            -0.45,
            -2.0,
            1.79,
            0.6,
            -3.34,
            -43.29
          ],
          "code_task_scores": [
            -55.26,
            -4.3,
            -38.42,
            -34.24
          ],
          "reasoning_task_scores": [
            -42.51,
            -15.15,
            -19.74,
            -23.36
          ]
        }
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "size_precise": "16710",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k",
      "paper_link": "https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation",
      "tag": "general,math,code,science"
    },
    {
      "id": 76,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 38.37,
      "math_avg": 56.36,
      "code_avg": 44.5,
      "reasoning_avg": 30.47,
      "overall_avg": 42.43,
      "overall_efficiency": 0.074554,
      "general_efficiency": -0.005918,
      "math_efficiency": 0.337525,
      "code_efficiency": 0.018713,
      "reasoning_efficiency": -0.052105,
      "general_scores": [
        57.92,
        31.1125,
        48.86,
        15.5942857
      ],
      "math_scores": [
        88.86,
        84.8,
        37.74,
        55.64,
        33.33,
        37.8
      ],
      "code_scores": [
        49.42,
        10.39,
        40.24,
        77.97
      ],
      "reasoning_scores": [
        21.26,
        17.68,
        35.108696,
        47.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.74
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.64
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.8
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.24
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.26
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.11
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.67,
          "math_avg": 38.46,
          "code_avg": 2.13,
          "reasoning_avg": -5.94,
          "overall_avg": 8.5,
          "overall_efficiency": 0.074554,
          "general_efficiency": -0.005918,
          "math_efficiency": 0.337525,
          "code_efficiency": 0.018713,
          "reasoning_efficiency": -0.052105,
          "general_task_scores": [
            -6.45,
            10.77,
            12.21,
            -19.24
          ],
          "math_task_scores": [
            32.45,
            65.6,
            33.4,
            55.64,
            33.33,
            10.36
          ],
          "code_task_scores": [
            -5.05,
            6.81,
            9.14,
            -2.37
          ],
          "reasoning_task_scores": [
            -41.3,
            -12.12,
            3.91,
            25.76
          ]
        },
        "vs_instruct": {
          "general_avg": -18.17,
          "math_avg": 16.88,
          "code_avg": -10.64,
          "reasoning_avg": -13.72,
          "overall_avg": -6.41,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -22.98,
            -29.22,
            7.88,
            -28.37
          ],
          "math_task_scores": [
            3.19,
            36.6,
            25.12,
            41.25,
            24.99,
            -29.88
          ],
          "code_task_scores": [
            -21.79,
            0.71,
            -20.13,
            -1.35
          ],
          "reasoning_task_scores": [
            -41.59,
            -6.56,
            -7.05,
            0.32
          ]
        }
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,math,code,science"
    },
    {
      "id": 77,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 30.35,
      "math_avg": 20.33,
      "code_avg": 15.98,
      "reasoning_avg": 22.68,
      "overall_avg": 22.33,
      "overall_efficiency": -0.067562,
      "general_efficiency": -0.050644,
      "math_efficiency": 0.01415,
      "code_efficiency": -0.153789,
      "reasoning_efficiency": -0.079964,
      "general_scores": [
        51.28,
        22.535,
        29.59,
        18.0078571
      ],
      "math_scores": [
        67.85,
        31.4,
        12.38,
        9.5,
        0.8325,
        0.0
      ],
      "code_scores": [
        17.12,
        0.0,
        0.0,
        46.78
      ],
      "reasoning_scores": [
        22.55,
        22.22,
        17.967391,
        28.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.28
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.59
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.38
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.83
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.78
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.97
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -8.69,
          "math_avg": 2.43,
          "code_avg": -26.4,
          "reasoning_avg": -13.73,
          "overall_avg": -11.6,
          "overall_efficiency": -0.067562,
          "general_efficiency": -0.050644,
          "math_efficiency": 0.01415,
          "code_efficiency": -0.153789,
          "reasoning_efficiency": -0.079964,
          "general_task_scores": [
            -13.09,
            2.2,
            -7.06,
            -16.82
          ],
          "math_task_scores": [
            11.44,
            12.2,
            8.04,
            9.5,
            0.83,
            -27.44
          ],
          "code_task_scores": [
            -37.35,
            -3.58,
            -31.1,
            -33.56
          ],
          "reasoning_task_scores": [
            -40.01,
            -7.58,
            -13.23,
            5.92
          ]
        },
        "vs_instruct": {
          "general_avg": -26.19,
          "math_avg": -19.16,
          "code_avg": -39.17,
          "reasoning_avg": -21.51,
          "overall_avg": -26.51,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -29.62,
            -37.79,
            -11.39,
            -25.95
          ],
          "math_task_scores": [
            -17.82,
            -16.8,
            -0.24,
            -4.89,
            -7.51,
            -67.68
          ],
          "code_task_scores": [
            -54.09,
            -9.68,
            -60.37,
            -32.54
          ],
          "reasoning_task_scores": [
            -40.3,
            -2.02,
            -24.19,
            -19.52
          ]
        }
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 78,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 44.3,
      "math_avg": 32.41,
      "code_avg": 42.69,
      "reasoning_avg": 27.65,
      "overall_avg": 36.76,
      "overall_efficiency": 0.03641,
      "general_efficiency": 0.067599,
      "math_efficiency": 0.18678,
      "code_efficiency": 0.004055,
      "reasoning_efficiency": -0.112796,
      "general_scores": [
        77.46,
        34.5225,
        38.87,
        26.3378571
      ],
      "math_scores": [
        77.48,
        40.8,
        12.62,
        11.72,
        0.0,
        51.83
      ],
      "code_scores": [
        51.36,
        4.3,
        46.95,
        68.14
      ],
      "reasoning_scores": [
        20.78,
        23.23,
        32.5,
        34.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.46
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.48
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.83
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 51.36
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.95
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.5
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.25,
          "math_avg": 14.51,
          "code_avg": 0.31,
          "reasoning_avg": -8.76,
          "overall_avg": 2.83,
          "overall_efficiency": 0.03641,
          "general_efficiency": 0.067599,
          "math_efficiency": 0.18678,
          "code_efficiency": 0.004055,
          "reasoning_efficiency": -0.112796,
          "general_task_scores": [
            13.09,
            14.18,
            2.22,
            -8.49
          ],
          "math_task_scores": [
            21.07,
            21.6,
            8.28,
            11.72,
            0.0,
            24.39
          ],
          "code_task_scores": [
            -3.11,
            0.72,
            15.85,
            -12.2
          ],
          "reasoning_task_scores": [
            -41.78,
            -6.57,
            1.3,
            12.0
          ]
        },
        "vs_instruct": {
          "general_avg": -12.24,
          "math_avg": -7.07,
          "code_avg": -12.46,
          "reasoning_avg": -16.55,
          "overall_avg": -12.08,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.44,
            -25.8,
            -2.11,
            -17.62
          ],
          "math_task_scores": [
            -8.19,
            -7.4,
            0.0,
            -2.67,
            -8.34,
            -15.85
          ],
          "code_task_scores": [
            -19.85,
            -5.38,
            -13.42,
            -11.18
          ],
          "reasoning_task_scores": [
            -42.07,
            -1.01,
            -9.66,
            -13.44
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 79,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 19.22,
      "math_avg": 17.66,
      "code_avg": 17.97,
      "reasoning_avg": 9.81,
      "overall_avg": 16.16,
      "overall_efficiency": -54.335279,
      "general_efficiency": -60.638106,
      "math_efficiency": -0.739041,
      "code_efficiency": -74.625383,
      "reasoning_efficiency": -81.338586,
      "general_scores": [
        39.47,
        15.86,
        9.96,
        11.58
      ],
      "math_scores": [
        93.1,
        1.2,
        11.34,
        0.3,
        0.0,
        0.0
      ],
      "code_scores": [
        47.47,
        0.0,
        0.0,
        24.41
      ],
      "reasoning_scores": [
        17.89,
        6.57,
        1.98913,
        12.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.89
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.99
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.8
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -19.83,
          "math_avg": -0.24,
          "code_avg": -24.4,
          "reasoning_avg": -26.6,
          "overall_avg": -17.77,
          "overall_efficiency": -54.335279,
          "general_efficiency": -60.638106,
          "math_efficiency": -0.739041,
          "code_efficiency": -74.625383,
          "reasoning_efficiency": -81.338586,
          "general_task_scores": [
            -24.9,
            -4.48,
            -26.69,
            -23.25
          ],
          "math_task_scores": [
            36.69,
            -18.0,
            7.0,
            0.3,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -7.0,
            -3.58,
            -31.1,
            -55.93
          ],
          "reasoning_task_scores": [
            -44.67,
            -23.23,
            -29.21,
            -9.28
          ]
        },
        "vs_instruct": {
          "general_avg": -37.32,
          "math_avg": -21.83,
          "code_avg": -37.17,
          "reasoning_avg": -34.38,
          "overall_avg": -32.68,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -41.43,
            -44.46,
            -31.02,
            -32.38
          ],
          "math_task_scores": [
            7.43,
            -47.0,
            -1.28,
            -14.09,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -23.74,
            -9.68,
            -60.37,
            -54.91
          ],
          "reasoning_task_scores": [
            -44.96,
            -17.67,
            -40.17,
            -34.72
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 80,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 50.94,
      "math_avg": 21.78,
      "code_avg": 36.88,
      "reasoning_avg": 42.04,
      "overall_avg": 37.91,
      "overall_efficiency": 0.063246,
      "general_efficiency": 0.189009,
      "math_efficiency": 0.061757,
      "code_efficiency": -0.087286,
      "reasoning_efficiency": 0.089506,
      "general_scores": [
        78.91,
        35.2975,
        45.49,
        44.0607143
      ],
      "math_scores": [
        78.62,
        30.2,
        13.01,
        8.46,
        0.41625,
        0.0
      ],
      "code_scores": [
        56.42,
        4.66,
        0.0,
        86.44
      ],
      "reasoning_scores": [
        69.15,
        19.19,
        37.108696,
        42.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.01
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.46
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.11
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 11.89,
          "math_avg": 3.89,
          "code_avg": -5.49,
          "reasoning_avg": 5.63,
          "overall_avg": 3.98,
          "overall_efficiency": 0.063246,
          "general_efficiency": 0.189009,
          "math_efficiency": 0.061757,
          "code_efficiency": -0.087286,
          "reasoning_efficiency": 0.089506,
          "general_task_scores": [
            14.54,
            14.96,
            8.84,
            9.23
          ],
          "math_task_scores": [
            22.21,
            11.0,
            8.67,
            8.46,
            0.42,
            -27.44
          ],
          "code_task_scores": [
            1.95,
            1.08,
            -31.1,
            6.1
          ],
          "reasoning_task_scores": [
            6.59,
            -10.61,
            5.91,
            20.64
          ]
        },
        "vs_instruct": {
          "general_avg": -5.6,
          "math_avg": -17.7,
          "code_avg": -18.26,
          "reasoning_avg": -2.15,
          "overall_avg": -10.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.99,
            -25.03,
            4.51,
            0.1
          ],
          "math_task_scores": [
            -7.05,
            -18.0,
            0.39,
            -5.93,
            -7.92,
            -67.68
          ],
          "code_task_scores": [
            -14.79,
            -5.02,
            -60.37,
            7.12
          ],
          "reasoning_task_scores": [
            6.3,
            -5.05,
            -5.05,
            -4.8
          ]
        }
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 50.18,
      "math_avg": 40.37,
      "code_avg": 35.98,
      "reasoning_avg": 27.02,
      "overall_avg": 38.39,
      "overall_efficiency": 0.017831,
      "general_efficiency": 0.044546,
      "math_efficiency": 0.089914,
      "code_efficiency": -0.025578,
      "reasoning_efficiency": -0.037559,
      "general_scores": [
        84.91,
        30.11,
        45.1,
        40.5964286
      ],
      "math_scores": [
        83.93,
        60.4,
        20.46,
        31.45,
        10.0,
        35.98
      ],
      "code_scores": [
        24.9,
        4.66,
        26.22,
        88.14
      ],
      "reasoning_scores": [
        14.65,
        17.17,
        31.152174,
        45.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.6
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.46
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.98
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 24.9
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 26.22
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 11.13,
          "math_avg": 22.47,
          "code_avg": -6.39,
          "reasoning_avg": -9.39,
          "overall_avg": 4.46,
          "overall_efficiency": 0.017831,
          "general_efficiency": 0.044546,
          "math_efficiency": 0.089914,
          "code_efficiency": -0.025578,
          "reasoning_efficiency": -0.037559,
          "general_task_scores": [
            20.54,
            9.77,
            8.45,
            5.77
          ],
          "math_task_scores": [
            27.52,
            41.2,
            16.12,
            31.45,
            10.0,
            8.54
          ],
          "code_task_scores": [
            -29.57,
            1.08,
            -4.88,
            7.8
          ],
          "reasoning_task_scores": [
            -47.91,
            -12.63,
            -0.05,
            23.04
          ]
        },
        "vs_instruct": {
          "general_avg": -6.36,
          "math_avg": 0.89,
          "code_avg": -19.16,
          "reasoning_avg": -17.17,
          "overall_avg": -10.45,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.01,
            -30.22,
            4.12,
            -3.36
          ],
          "math_task_scores": [
            -1.74,
            12.2,
            7.84,
            17.06,
            1.66,
            -31.7
          ],
          "code_task_scores": [
            -46.31,
            -5.02,
            -34.15,
            8.82
          ],
          "reasoning_task_scores": [
            -48.2,
            -7.07,
            -11.01,
            -2.4
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 47.38,
      "math_avg": 31.27,
      "code_avg": 52.61,
      "reasoning_avg": 36.42,
      "overall_avg": 41.92,
      "overall_efficiency": 0.053258,
      "general_efficiency": 0.055569,
      "math_efficiency": 0.089134,
      "code_efficiency": 0.068267,
      "reasoning_efficiency": 6.3e-05,
      "general_scores": [
        56.44,
        54.23,
        43.13,
        35.7264286
      ],
      "math_scores": [
        79.76,
        29.4,
        11.77,
        7.86,
        3.33,
        55.49
      ],
      "code_scores": [
        64.98,
        10.39,
        53.05,
        82.03
      ],
      "reasoning_scores": [
        46.23,
        26.26,
        34.467391,
        38.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.23
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.86
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 64.98
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.26
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.47
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.34,
          "math_avg": 13.37,
          "code_avg": 10.24,
          "reasoning_avg": 0.01,
          "overall_avg": 7.99,
          "overall_efficiency": 0.053258,
          "general_efficiency": 0.055569,
          "math_efficiency": 0.089134,
          "code_efficiency": 0.068267,
          "reasoning_efficiency": 6.3e-05,
          "general_task_scores": [
            -7.93,
            33.89,
            6.48,
            0.9
          ],
          "math_task_scores": [
            23.35,
            10.2,
            7.43,
            7.86,
            3.33,
            28.05
          ],
          "code_task_scores": [
            10.51,
            6.81,
            21.95,
            1.69
          ],
          "reasoning_task_scores": [
            -16.33,
            -3.54,
            3.27,
            16.64
          ]
        },
        "vs_instruct": {
          "general_avg": -9.16,
          "math_avg": -8.21,
          "code_avg": -2.53,
          "reasoning_avg": -7.77,
          "overall_avg": -6.92,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -24.46,
            -6.1,
            2.15,
            -8.23
          ],
          "math_task_scores": [
            -5.91,
            -18.8,
            -0.85,
            -6.53,
            -5.01,
            -12.19
          ],
          "code_task_scores": [
            -6.23,
            0.71,
            -7.32,
            2.71
          ],
          "reasoning_task_scores": [
            -16.62,
            2.02,
            -7.69,
            -8.8
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 49.05,
      "math_avg": 38.78,
      "code_avg": 40.78,
      "reasoning_avg": 25.33,
      "overall_avg": 38.49,
      "overall_efficiency": 0.018226,
      "general_efficiency": 0.040032,
      "math_efficiency": 0.083546,
      "code_efficiency": -0.006352,
      "reasoning_efficiency": -0.044324,
      "general_scores": [
        82.95,
        32.085,
        40.33,
        40.8385714
      ],
      "math_scores": [
        90.07,
        58.0,
        18.38,
        28.78,
        10.0,
        27.44
      ],
      "code_scores": [
        55.25,
        3.23,
        21.95,
        82.71
      ],
      "reasoning_scores": [
        21.8,
        14.14,
        28.98913,
        36.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.33
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.38
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.95
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.8
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.99
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 10.0,
          "math_avg": 20.88,
          "code_avg": -1.59,
          "reasoning_avg": -11.08,
          "overall_avg": 4.55,
          "overall_efficiency": 0.018226,
          "general_efficiency": 0.040032,
          "math_efficiency": 0.083546,
          "code_efficiency": -0.006352,
          "reasoning_efficiency": -0.044324,
          "general_task_scores": [
            18.58,
            11.74,
            3.68,
            6.01
          ],
          "math_task_scores": [
            33.66,
            38.8,
            14.04,
            28.78,
            10.0,
            0.0
          ],
          "code_task_scores": [
            0.78,
            -0.35,
            -9.15,
            2.37
          ],
          "reasoning_task_scores": [
            -40.76,
            -15.66,
            -2.21,
            14.32
          ]
        },
        "vs_instruct": {
          "general_avg": -7.49,
          "math_avg": -0.7,
          "code_avg": -14.36,
          "reasoning_avg": -18.86,
          "overall_avg": -10.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.05,
            -28.25,
            -0.65,
            -3.12
          ],
          "math_task_scores": [
            4.4,
            9.8,
            5.76,
            14.39,
            1.66,
            -40.24
          ],
          "code_task_scores": [
            -15.96,
            -6.45,
            -38.42,
            3.39
          ],
          "reasoning_task_scores": [
            -41.05,
            -10.1,
            -13.17,
            -11.12
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 84,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 39.2,
      "math_avg": 20.32,
      "code_avg": 42.56,
      "reasoning_avg": 23.58,
      "overall_avg": 31.41,
      "overall_efficiency": -0.126211,
      "general_efficiency": 0.007642,
      "math_efficiency": 0.121424,
      "code_efficiency": 0.009276,
      "reasoning_efficiency": -0.643187,
      "general_scores": [
        61.6,
        28.35,
        37.54,
        29.3042857
      ],
      "math_scores": [
        65.73,
        22.6,
        9.94,
        4.75,
        0.0,
        18.9
      ],
      "code_scores": [
        59.92,
        2.87,
        27.44,
        80.0
      ],
      "reasoning_scores": [
        12.22,
        27.78,
        24.48913,
        29.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.54
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.94
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.9
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 59.92
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.87
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.49
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.15,
          "math_avg": 2.42,
          "code_avg": 0.19,
          "reasoning_avg": -12.83,
          "overall_avg": -2.52,
          "overall_efficiency": -0.126211,
          "general_efficiency": 0.007642,
          "math_efficiency": 0.121424,
          "code_efficiency": 0.009276,
          "reasoning_efficiency": -0.643187,
          "general_task_scores": [
            -2.77,
            8.01,
            0.89,
            -5.53
          ],
          "math_task_scores": [
            9.32,
            3.4,
            5.6,
            4.75,
            0.0,
            -8.54
          ],
          "code_task_scores": [
            5.45,
            -0.71,
            -3.66,
            -0.34
          ],
          "reasoning_task_scores": [
            -50.34,
            -2.02,
            -6.71,
            7.76
          ]
        },
        "vs_instruct": {
          "general_avg": -17.34,
          "math_avg": -19.16,
          "code_avg": -12.59,
          "reasoning_avg": -20.61,
          "overall_avg": -17.43,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -19.3,
            -31.98,
            -3.44,
            -14.66
          ],
          "math_task_scores": [
            -19.94,
            -25.6,
            -2.68,
            -9.64,
            -8.34,
            -48.78
          ],
          "code_task_scores": [
            -11.29,
            -6.81,
            -32.93,
            0.68
          ],
          "reasoning_task_scores": [
            -50.63,
            3.54,
            -17.67,
            -17.68
          ]
        }
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 85,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 5.3,
      "math_avg": 3.91,
      "code_avg": 8.8,
      "reasoning_avg": 4.99,
      "overall_avg": 5.75,
      "overall_efficiency": -1.430157,
      "general_efficiency": -1.712583,
      "math_efficiency": -0.709923,
      "code_efficiency": -1.703715,
      "reasoning_efficiency": -1.594407,
      "general_scores": [
        9.69,
        11.15,
        0.0,
        0.36571429
      ],
      "math_scores": [
        22.29,
        0.2,
        0.97,
        0.0,
        0.0,
        0.0
      ],
      "code_scores": [
        33.85,
        0.0,
        0.0,
        1.36
      ],
      "reasoning_scores": [
        0.16,
        0.0,
        19.815217,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.29
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.97
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 33.85
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.82
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -33.74,
          "math_avg": -13.99,
          "code_avg": -33.57,
          "reasoning_avg": -31.42,
          "overall_avg": -28.18,
          "overall_efficiency": -1.430157,
          "general_efficiency": -1.712583,
          "math_efficiency": -0.709923,
          "code_efficiency": -1.703715,
          "reasoning_efficiency": -1.594407,
          "general_task_scores": [
            -54.68,
            -9.19,
            -36.65,
            -34.46
          ],
          "math_task_scores": [
            -34.12,
            -19.0,
            -3.37,
            0.0,
            0.0,
            -27.44
          ],
          "code_task_scores": [
            -20.62,
            -3.58,
            -31.1,
            -78.98
          ],
          "reasoning_task_scores": [
            -62.4,
            -29.8,
            -11.38,
            -22.08
          ]
        },
        "vs_instruct": {
          "general_avg": -51.24,
          "math_avg": -35.57,
          "code_avg": -46.34,
          "reasoning_avg": -39.2,
          "overall_avg": -43.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -71.21,
            -49.18,
            -40.98,
            -43.59
          ],
          "math_task_scores": [
            -63.38,
            -48.0,
            -11.65,
            -14.39,
            -8.34,
            -67.68
          ],
          "code_task_scores": [
            -37.36,
            -9.68,
            -60.37,
            -77.96
          ],
          "reasoning_task_scores": [
            -62.69,
            -24.24,
            -22.34,
            -47.52
          ]
        }
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 86,
      "name": "SCP-116K",
      "domain": "reasoning",
      "general_avg": 39.18,
      "math_avg": 43.48,
      "code_avg": 31.6,
      "reasoning_avg": 41.57,
      "overall_avg": 38.96,
      "overall_efficiency": 0.018337,
      "general_efficiency": 0.000473,
      "math_efficiency": 0.093322,
      "code_efficiency": -0.039274,
      "reasoning_efficiency": 0.018827,
      "general_scores": [
        82.4,
        32.8325,
        33.65,
        7.82142857
      ],
      "math_scores": [
        87.11,
        68.4,
        27.85,
        35.76,
        21.665,
        20.12
      ],
      "code_scores": [
        23.74,
        2.15,
        13.41,
        87.12
      ],
      "reasoning_scores": [
        69.28,
        18.69,
        30.717391,
        47.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.85
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.76
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.66
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 23.74
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.15
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.41
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.72
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.13,
          "math_avg": 25.59,
          "code_avg": -10.77,
          "reasoning_avg": 5.16,
          "overall_avg": 5.03,
          "overall_efficiency": 0.018337,
          "general_efficiency": 0.000473,
          "math_efficiency": 0.093322,
          "code_efficiency": -0.039274,
          "reasoning_efficiency": 0.018827,
          "general_task_scores": [
            18.03,
            12.49,
            -3.0,
            -27.01
          ],
          "math_task_scores": [
            30.7,
            49.2,
            23.51,
            35.76,
            21.66,
            -7.32
          ],
          "code_task_scores": [
            -30.73,
            -1.43,
            -17.69,
            6.78
          ],
          "reasoning_task_scores": [
            6.72,
            -11.11,
            -0.48,
            25.52
          ]
        },
        "vs_instruct": {
          "general_avg": -17.37,
          "math_avg": 4.0,
          "code_avg": -23.54,
          "reasoning_avg": -2.62,
          "overall_avg": -9.88,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.5,
            -27.5,
            -7.33,
            -36.14
          ],
          "math_task_scores": [
            1.44,
            20.2,
            15.23,
            21.37,
            13.32,
            -47.56
          ],
          "code_task_scores": [
            -47.47,
            -7.53,
            -46.96,
            7.8
          ],
          "reasoning_task_scores": [
            6.43,
            -5.55,
            -11.44,
            0.08
          ]
        }
      },
      "affiliation": "Fudan University",
      "year": "2025",
      "size": "274k",
      "size_precise": "274166",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K",
      "paper_link": "https://arxiv.org/abs/2501.15587",
      "tag": "general,math,science"
    },
    {
      "id": 87,
      "name": "AM-Thinking-v1-Distilled-code",
      "domain": "reasoning",
      "general_avg": 42.05,
      "math_avg": 43.24,
      "code_avg": 62.1,
      "reasoning_avg": 28.88,
      "overall_avg": 44.07,
      "overall_efficiency": 0.03129,
      "general_efficiency": 0.009259,
      "math_efficiency": 0.078228,
      "code_efficiency": 0.06091,
      "reasoning_efficiency": -0.023238,
      "general_scores": [
        71.98,
        31.85,
        35.21,
        29.1428571
      ],
      "math_scores": [
        78.32,
        45.0,
        20.93,
        21.66,
        10.0,
        83.54
      ],
      "code_scores": [
        62.26,
        35.84,
        74.39,
        75.93
      ],
      "reasoning_scores": [
        43.87,
        7.07,
        29.467391,
        35.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.21
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.54
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 62.26
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.84
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.87
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.07
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.47
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.0,
          "math_avg": 25.34,
          "code_avg": 19.73,
          "reasoning_avg": -7.53,
          "overall_avg": 10.14,
          "overall_efficiency": 0.03129,
          "general_efficiency": 0.009259,
          "math_efficiency": 0.078228,
          "code_efficiency": 0.06091,
          "reasoning_efficiency": -0.023238,
          "general_task_scores": [
            7.61,
            11.51,
            -1.44,
            -5.69
          ],
          "math_task_scores": [
            21.91,
            25.8,
            16.59,
            21.66,
            10.0,
            56.1
          ],
          "code_task_scores": [
            7.79,
            32.26,
            43.29,
            -4.41
          ],
          "reasoning_task_scores": [
            -18.69,
            -22.73,
            -1.73,
            13.04
          ]
        },
        "vs_instruct": {
          "general_avg": -14.5,
          "math_avg": 3.76,
          "code_avg": 6.96,
          "reasoning_avg": -15.31,
          "overall_avg": -4.77,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.92,
            -28.48,
            -5.77,
            -14.82
          ],
          "math_task_scores": [
            -7.35,
            -3.2,
            8.31,
            7.27,
            1.66,
            15.86
          ],
          "code_task_scores": [
            -8.95,
            26.16,
            14.02,
            -3.39
          ],
          "reasoning_task_scores": [
            -18.98,
            -17.17,
            -12.69,
            -12.4
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "324k",
      "size_precise": "323965",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/code.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "code"
    },
    {
      "id": 88,
      "name": "AM-Thinking-v1-Distilled-math",
      "domain": "reasoning",
      "general_avg": 54.99,
      "math_avg": 66.15,
      "code_avg": 39.41,
      "reasoning_avg": 41.83,
      "overall_avg": 50.59,
      "overall_efficiency": 0.029854,
      "general_efficiency": 0.02857,
      "math_efficiency": 0.086445,
      "code_efficiency": -0.005308,
      "reasoning_efficiency": 0.009711,
      "general_scores": [
        90.6,
        31.54,
        55.54,
        42.2878571
      ],
      "math_scores": [
        93.78,
        94.6,
        59.28,
        68.99,
        55.8325,
        24.39
      ],
      "code_scores": [
        48.25,
        2.15,
        23.17,
        84.07
      ],
      "reasoning_scores": [
        70.95,
        17.17,
        30.641304,
        48.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.54
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.29
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 94.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.28
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.83
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.15
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 23.17
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.64
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 15.95,
          "math_avg": 48.25,
          "code_avg": -2.96,
          "reasoning_avg": 5.42,
          "overall_avg": 16.66,
          "overall_efficiency": 0.029854,
          "general_efficiency": 0.02857,
          "math_efficiency": 0.086445,
          "code_efficiency": -0.005308,
          "reasoning_efficiency": 0.009711,
          "general_task_scores": [
            26.23,
            11.2,
            18.89,
            7.46
          ],
          "math_task_scores": [
            37.37,
            75.4,
            54.94,
            68.99,
            55.83,
            -3.05
          ],
          "code_task_scores": [
            -6.22,
            -1.43,
            -7.93,
            3.73
          ],
          "reasoning_task_scores": [
            8.39,
            -12.63,
            -0.56,
            26.48
          ]
        },
        "vs_instruct": {
          "general_avg": -1.55,
          "math_avg": 26.66,
          "code_avg": -15.74,
          "reasoning_avg": -2.36,
          "overall_avg": 1.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.7,
            -28.79,
            14.56,
            -1.67
          ],
          "math_task_scores": [
            8.11,
            46.4,
            46.66,
            54.6,
            47.5,
            -43.29
          ],
          "code_task_scores": [
            -22.96,
            -7.53,
            -37.2,
            4.75
          ],
          "reasoning_task_scores": [
            8.1,
            -7.07,
            -11.52,
            1.04
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "558k",
      "size_precise": "558129",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/math.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "math"
    },
    {
      "id": 89,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 49.4,
      "math_avg": 46.19,
      "code_avg": 27.41,
      "reasoning_avg": 27.83,
      "overall_avg": 37.71,
      "overall_efficiency": 0.047523,
      "general_efficiency": 0.130372,
      "math_efficiency": 0.356112,
      "code_efficiency": -0.188384,
      "reasoning_efficiency": -0.10801,
      "general_scores": [
        83.4,
        29.915,
        43.28,
        41.0161538
      ],
      "math_scores": [
        88.02,
        76.4,
        36.34,
        51.04,
        21.665,
        3.66
      ],
      "code_scores": [
        20.62,
        0.0,
        7.32,
        81.69
      ],
      "reasoning_scores": [
        18.06,
        19.7,
        27.23913,
        46.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.66
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 10.36,
          "math_avg": 28.29,
          "code_avg": -14.97,
          "reasoning_avg": -8.58,
          "overall_avg": 3.78,
          "overall_efficiency": 0.047523,
          "general_efficiency": 0.130372,
          "math_efficiency": 0.356112,
          "code_efficiency": -0.188384,
          "reasoning_efficiency": -0.10801,
          "general_task_scores": [
            19.03,
            9.58,
            6.63,
            6.19
          ],
          "math_task_scores": [
            31.61,
            57.2,
            32.0,
            51.04,
            21.66,
            -23.78
          ],
          "code_task_scores": [
            -33.85,
            -3.58,
            -23.78,
            1.35
          ],
          "reasoning_task_scores": [
            -44.5,
            -10.1,
            -3.96,
            24.24
          ]
        },
        "vs_instruct": {
          "general_avg": -7.14,
          "math_avg": 6.71,
          "code_avg": -27.74,
          "reasoning_avg": -16.36,
          "overall_avg": -11.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.5,
            -30.4,
            2.3,
            -2.94
          ],
          "math_task_scores": [
            2.35,
            28.2,
            23.72,
            36.65,
            13.32,
            -64.02
          ],
          "code_task_scores": [
            -50.59,
            -9.68,
            -53.05,
            2.37
          ],
          "reasoning_task_scores": [
            -44.79,
            -4.54,
            -14.92,
            -1.2
          ]
        }
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,math,code,science"
    },
    {
      "id": 90,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 43.17,
      "math_avg": 32.22,
      "code_avg": 25.28,
      "reasoning_avg": 22.48,
      "overall_avg": 30.79,
      "overall_efficiency": -0.39806,
      "general_efficiency": 0.522005,
      "math_efficiency": 1.81308,
      "code_efficiency": -2.164241,
      "reasoning_efficiency": -1.763085,
      "general_scores": [
        84.6,
        26.995,
        30.25,
        30.835
      ],
      "math_scores": [
        82.03,
        47.8,
        21.23,
        24.18,
        8.33,
        9.76
      ],
      "code_scores": [
        22.18,
        1.43,
        7.32,
        70.17
      ],
      "reasoning_scores": [
        16.93,
        14.14,
        24.456522,
        34.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.6
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.23
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 22.18
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.43
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.14
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.46
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.12,
          "math_avg": 14.32,
          "code_avg": -17.1,
          "reasoning_avg": -13.93,
          "overall_avg": -3.14,
          "overall_efficiency": -0.39806,
          "general_efficiency": 0.522005,
          "math_efficiency": 1.81308,
          "code_efficiency": -2.164241,
          "reasoning_efficiency": -1.763085,
          "general_task_scores": [
            20.23,
            6.66,
            -6.4,
            -3.99
          ],
          "math_task_scores": [
            25.62,
            28.6,
            16.89,
            24.18,
            8.33,
            -17.68
          ],
          "code_task_scores": [
            -32.29,
            -2.15,
            -23.78,
            -10.17
          ],
          "reasoning_task_scores": [
            -45.63,
            -15.66,
            -6.74,
            12.32
          ]
        },
        "vs_instruct": {
          "general_avg": -13.37,
          "math_avg": -7.26,
          "code_avg": -29.87,
          "reasoning_avg": -21.71,
          "overall_avg": -18.05,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.7,
            -33.33,
            -10.73,
            -13.12
          ],
          "math_task_scores": [
            -3.64,
            -0.4,
            8.61,
            9.79,
            -0.01,
            -57.92
          ],
          "code_task_scores": [
            -49.03,
            -8.25,
            -53.05,
            -9.15
          ],
          "reasoning_task_scores": [
            -45.92,
            -10.1,
            -17.7,
            -13.12
          ]
        }
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ],
  "qwen": [
    {
      "id": 0,
      "name": "Qwen2.5-7B-Instruct",
      "domain": "instruct",
      "general_avg": 66.21,
      "math_avg": 52.34,
      "code_avg": 41.68,
      "reasoning_avg": 47.32,
      "overall_avg": 51.89,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        67.2,
        72.96,
        67.44,
        57.2342857
      ],
      "math_task_scores": [
        92.27,
        77.8,
        29.4,
        40.95,
        9.58375,
        64.02
      ],
      "code_task_scores": [
        74.71,
        11.11,
        55.49,
        25.42
      ],
      "reasoning_task_scores": [
        71.62,
        33.84,
        42.445652,
        41.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.96
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.44
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 57.2342857
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.27
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.58375
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.62
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.445652
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.36
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "Qwen2.5-7B",
      "domain": "base",
      "general_avg": 51.44,
      "math_avg": 46.04,
      "code_avg": 39.93,
      "reasoning_avg": 44.2,
      "overall_avg": 45.4,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        68.31,
        35.49,
        57.74,
        44.2378571
      ],
      "math_task_scores": [
        79.98,
        50.2,
        26.04,
        35.91,
        6.67,
        77.44
      ],
      "code_task_scores": [
        71.6,
        8.24,
        43.29,
        36.6
      ],
      "reasoning_task_scores": [
        69.46,
        34.85,
        39.2,
        33.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.2378571
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.04
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.91
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 8.24
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.6
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.46
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.28
              }
            ]
          }
        ]
      }
    },
    {
      "id": 2,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 54.23,
      "math_avg": 40.74,
      "code_avg": 41.63,
      "reasoning_avg": 47.84,
      "overall_avg": 46.11,
      "overall_efficiency": 0.000874,
      "general_efficiency": 0.003439,
      "math_efficiency": -0.006549,
      "code_efficiency": 0.002103,
      "reasoning_efficiency": 0.004501,
      "general_scores": [
        65.09,
        43.28,
        59.22,
        47.3492857,
        65.36,
        45.345,
        59.49,
        47.2757143,
        65.71,
        46.2225,
        59.54,
        46.8328571
      ],
      "math_scores": [
        82.79,
        51.6,
        19.04,
        17.51,
        3.33,
        75.61,
        83.09,
        48.2,
        18.9,
        17.66,
        0.0,
        75.0,
        81.5,
        49.8,
        20.03,
        17.36,
        0.0,
        71.95
      ],
      "code_scores": [
        73.93,
        12.9,
        66.46,
        17.63,
        74.71,
        12.54,
        50.61,
        18.64,
        73.93,
        12.54,
        64.02,
        21.69
      ],
      "reasoning_scores": [
        66.9,
        36.36,
        46.956522,
        40.48,
        67.1,
        36.87,
        44.793478,
        40.0,
        67.55,
        38.89,
        46.315217,
        41.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.46
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.36
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.18
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.02
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.78,
          "math_avg": -5.3,
          "code_avg": 1.7,
          "reasoning_avg": 3.64,
          "overall_avg": 0.71,
          "overall_efficiency": 0.000874,
          "general_efficiency": 0.003439,
          "math_efficiency": -0.006549,
          "code_efficiency": 0.002103,
          "reasoning_efficiency": 0.004501,
          "general_task_scores": [
            -2.92,
            9.46,
            1.68,
            2.91
          ],
          "math_task_scores": [
            2.48,
            -0.33,
            -6.72,
            -18.4,
            -5.56,
            -3.25
          ],
          "code_task_scores": [
            2.59,
            4.42,
            17.07,
            -17.28
          ],
          "reasoning_task_scores": [
            -2.28,
            2.52,
            6.82,
            7.49
          ]
        },
        "vs_instruct": {
          "general_avg": -11.98,
          "math_avg": -11.59,
          "code_avg": -0.05,
          "reasoning_avg": 0.52,
          "overall_avg": -5.78,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.81,
            -28.01,
            -8.02,
            -10.08
          ],
          "math_task_scores": [
            -9.81,
            -27.93,
            -10.08,
            -23.44,
            -8.47,
            10.17
          ],
          "code_task_scores": [
            -0.52,
            1.55,
            4.87,
            -6.1
          ],
          "reasoning_task_scores": [
            -4.44,
            3.53,
            3.57,
            -0.59
          ]
        }
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.37,
      "math_avg": 38.37,
      "code_avg": 41.43,
      "reasoning_avg": 44.07,
      "overall_avg": 44.81,
      "overall_efficiency": -0.011389,
      "general_efficiency": 0.075531,
      "math_efficiency": -0.14742,
      "code_efficiency": 0.028829,
      "reasoning_efficiency": -0.002495,
      "general_scores": [
        58.52,
        54.5275,
        60.09,
        46.6914286,
        60.11,
        54.805,
        60.45,
        46.3707143,
        60.47,
        55.52,
        60.57,
        46.3421429
      ],
      "math_scores": [
        81.5,
        43.6,
        13.28,
        13.65,
        3.33,
        72.56,
        81.73,
        44.2,
        13.62,
        13.8,
        6.67,
        71.34,
        80.89,
        45.0,
        13.62,
        14.54,
        6.67,
        70.73
      ],
      "code_scores": [
        71.98,
        10.75,
        64.63,
        14.92,
        72.76,
        11.11,
        67.07,
        15.59,
        72.37,
        9.68,
        67.68,
        18.64
      ],
      "reasoning_scores": [
        68.37,
        31.82,
        38.934783,
        35.52,
        68.65,
        34.85,
        39.0,
        35.68,
        68.45,
        32.83,
        39.347826,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.7
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.54
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.46
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.09
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.93,
          "math_avg": -7.67,
          "code_avg": 1.5,
          "reasoning_avg": -0.13,
          "overall_avg": -0.59,
          "overall_efficiency": -0.011389,
          "general_efficiency": 0.075531,
          "math_efficiency": -0.14742,
          "code_efficiency": 0.028829,
          "reasoning_efficiency": -0.002495,
          "general_task_scores": [
            -8.61,
            19.46,
            2.63,
            2.23
          ],
          "math_task_scores": [
            1.39,
            -5.93,
            -12.53,
            -21.91,
            -1.11,
            -5.9
          ],
          "code_task_scores": [
            0.77,
            2.27,
            23.17,
            -20.22
          ],
          "reasoning_task_scores": [
            -0.97,
            -1.68,
            -0.11,
            2.24
          ]
        },
        "vs_instruct": {
          "general_avg": -10.84,
          "math_avg": -13.96,
          "code_avg": -0.25,
          "reasoning_avg": -3.25,
          "overall_avg": -7.07,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -7.5,
            -18.01,
            -7.07,
            -10.76
          ],
          "math_task_scores": [
            -10.9,
            -33.53,
            -15.89,
            -26.95,
            -4.02,
            7.52
          ],
          "code_task_scores": [
            -2.34,
            -0.6,
            10.97,
            -9.04
          ],
          "reasoning_task_scores": [
            -3.13,
            -0.67,
            -3.36,
            -5.84
          ]
        }
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 56.85,
      "math_avg": 45.92,
      "code_avg": 42.87,
      "reasoning_avg": 46.83,
      "overall_avg": 48.12,
      "overall_efficiency": 0.052184,
      "general_efficiency": 0.104036,
      "math_efficiency": -0.002383,
      "code_efficiency": 0.056552,
      "reasoning_efficiency": 0.050532,
      "general_scores": [
        62.8,
        51.095,
        63.73,
        49.105,
        64.71,
        50.86,
        63.63,
        48.9664286,
        62.89,
        51.565,
        63.75,
        49.1535714
      ],
      "math_scores": [
        90.07,
        59.0,
        21.3,
        22.26,
        6.67,
        78.05,
        89.16,
        57.0,
        21.48,
        21.36,
        6.67,
        80.49,
        88.48,
        56.0,
        21.48,
        20.47,
        6.67,
        79.88
      ],
      "code_scores": [
        72.76,
        12.54,
        66.46,
        21.36,
        75.1,
        11.47,
        64.63,
        16.95,
        73.15,
        13.26,
        66.46,
        20.34
      ],
      "reasoning_scores": [
        66.0,
        32.83,
        45.934783,
        42.32,
        65.93,
        33.33,
        45.108696,
        42.64,
        66.24,
        34.85,
        45.119565,
        41.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.24
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.42
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.42
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.85
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.41,
          "math_avg": -0.12,
          "code_avg": 2.94,
          "reasoning_avg": 2.63,
          "overall_avg": 2.71,
          "overall_efficiency": 0.052184,
          "general_efficiency": 0.104036,
          "math_efficiency": -0.002383,
          "code_efficiency": 0.056552,
          "reasoning_efficiency": 0.050532,
          "general_task_scores": [
            -4.84,
            15.68,
            5.96,
            4.84
          ],
          "math_task_scores": [
            9.26,
            7.13,
            -4.62,
            -14.55,
            0.0,
            2.03
          ],
          "code_task_scores": [
            2.07,
            4.18,
            22.56,
            -17.05
          ],
          "reasoning_task_scores": [
            -3.4,
            -1.18,
            6.19,
            8.91
          ]
        },
        "vs_instruct": {
          "general_avg": -9.35,
          "math_avg": -6.42,
          "code_avg": 1.19,
          "reasoning_avg": -0.49,
          "overall_avg": -3.77,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.73,
            -21.79,
            -3.74,
            -8.15
          ],
          "math_task_scores": [
            -3.03,
            -20.47,
            -7.98,
            -19.59,
            -2.91,
            15.45
          ],
          "code_task_scores": [
            -1.04,
            1.31,
            10.36,
            -5.87
          ],
          "reasoning_task_scores": [
            -5.56,
            -0.17,
            2.94,
            0.83
          ]
        }
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 5,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 54.72,
      "math_avg": 39.64,
      "code_avg": 43.73,
      "reasoning_avg": 46.05,
      "overall_avg": 46.03,
      "overall_efficiency": 0.041817,
      "general_efficiency": 0.217905,
      "math_efficiency": -0.426687,
      "code_efficiency": 0.252925,
      "reasoning_efficiency": 0.123124,
      "general_scores": [
        64.79,
        45.8825,
        61.43,
        47.5907143,
        66.29,
        44.0925,
        61.58,
        47.0185714,
        65.39,
        43.6075,
        61.02,
        47.8935714
      ],
      "math_scores": [
        82.56,
        49.8,
        20.91,
        18.55,
        10.0,
        54.88,
        81.88,
        53.2,
        20.75,
        17.95,
        13.33,
        47.56,
        82.18,
        49.6,
        20.19,
        20.33,
        10.0,
        59.76
      ],
      "code_scores": [
        73.15,
        11.11,
        65.85,
        22.71,
        71.21,
        10.04,
        65.85,
        22.03,
        72.37,
        11.83,
        67.07,
        31.53
      ],
      "reasoning_scores": [
        65.45,
        33.33,
        45.48913,
        42.24,
        65.32,
        30.81,
        44.652174,
        42.8,
        65.98,
        28.79,
        44.967391,
        42.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.99
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.26
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.58
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.98
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.04
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.27,
          "math_avg": -6.41,
          "code_avg": 3.8,
          "reasoning_avg": 1.85,
          "overall_avg": 0.63,
          "overall_efficiency": 0.041817,
          "general_efficiency": 0.217905,
          "math_efficiency": -0.426687,
          "code_efficiency": 0.252925,
          "reasoning_efficiency": 0.123124,
          "general_task_scores": [
            -2.82,
            9.04,
            3.6,
            3.26
          ],
          "math_task_scores": [
            2.23,
            0.67,
            -5.42,
            -16.97,
            4.44,
            -23.37
          ],
          "code_task_scores": [
            0.64,
            2.75,
            22.97,
            -11.18
          ],
          "reasoning_task_scores": [
            -3.88,
            -3.87,
            5.84,
            9.31
          ]
        },
        "vs_instruct": {
          "general_avg": -11.49,
          "math_avg": -12.7,
          "code_avg": 2.05,
          "reasoning_avg": -1.27,
          "overall_avg": -5.85,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.71,
            -28.43,
            -6.1,
            -9.73
          ],
          "math_task_scores": [
            -10.06,
            -26.93,
            -8.78,
            -22.01,
            1.53,
            -9.95
          ],
          "code_task_scores": [
            -2.47,
            -0.12,
            10.77,
            0.0
          ],
          "reasoning_task_scores": [
            -6.04,
            -2.86,
            2.59,
            1.23
          ]
        }
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 54.86,
      "math_avg": 44.78,
      "code_avg": 42.79,
      "reasoning_avg": 46.64,
      "overall_avg": 47.27,
      "overall_efficiency": 0.02664,
      "general_efficiency": 0.048812,
      "math_efficiency": -0.01796,
      "code_efficiency": 0.040798,
      "reasoning_efficiency": 0.034911,
      "general_scores": [
        56.3,
        53.35,
        59.68,
        49.1671429,
        57.25,
        52.2825,
        59.95,
        48.9921429,
        56.77,
        55.37,
        60.51,
        48.7142857
      ],
      "math_scores": [
        84.91,
        49.8,
        18.54,
        17.51,
        20.0,
        77.44,
        84.91,
        51.6,
        19.11,
        16.91,
        20.0,
        79.27,
        83.78,
        52.4,
        18.56,
        16.02,
        16.67,
        78.66
      ],
      "code_scores": [
        73.54,
        10.75,
        68.29,
        20.34,
        73.15,
        12.19,
        67.68,
        18.64,
        73.93,
        11.47,
        65.85,
        17.63
      ],
      "reasoning_scores": [
        66.88,
        35.35,
        45.163043,
        41.04,
        67.34,
        31.82,
        44.543478,
        41.44,
        67.74,
        32.32,
        44.858696,
        41.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.77
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.67
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.96
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.74
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.81
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.89
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.42,
          "math_avg": -1.26,
          "code_avg": 2.86,
          "reasoning_avg": 2.44,
          "overall_avg": 1.86,
          "overall_efficiency": 0.02664,
          "general_efficiency": 0.048812,
          "math_efficiency": -0.01796,
          "code_efficiency": 0.040798,
          "reasoning_efficiency": 0.034911,
          "general_task_scores": [
            -11.54,
            18.18,
            2.31,
            4.72
          ],
          "math_task_scores": [
            4.55,
            1.07,
            -7.3,
            -19.1,
            12.22,
            1.02
          ],
          "code_task_scores": [
            1.94,
            3.23,
            23.98,
            -17.73
          ],
          "reasoning_task_scores": [
            -2.14,
            -1.69,
            5.66,
            7.95
          ]
        },
        "vs_instruct": {
          "general_avg": -11.35,
          "math_avg": -7.55,
          "code_avg": 1.11,
          "reasoning_avg": -0.68,
          "overall_avg": -4.62,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.43,
            -19.29,
            -7.39,
            -8.27
          ],
          "math_task_scores": [
            -7.74,
            -26.53,
            -10.66,
            -24.14,
            9.31,
            14.44
          ],
          "code_task_scores": [
            -1.17,
            0.36,
            11.78,
            -6.55
          ],
          "reasoning_task_scores": [
            -4.3,
            -0.68,
            2.41,
            -0.13
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 55.44,
      "math_avg": 40.6,
      "code_avg": 42.37,
      "reasoning_avg": 46.13,
      "overall_avg": 46.14,
      "overall_efficiency": 0.00513,
      "general_efficiency": 0.027959,
      "math_efficiency": -0.038019,
      "code_efficiency": 0.017046,
      "reasoning_efficiency": 0.013535,
      "general_scores": [
        60.49,
        52.5275,
        61.55,
        47.5457143,
        61.62,
        53.2075,
        61.59,
        47.8192857,
        59.34,
        51.6175,
        61.45,
        46.5530769
      ],
      "math_scores": [
        83.24,
        48.0,
        19.26,
        16.02,
        0.0,
        78.66,
        81.65,
        49.8,
        19.31,
        16.02,
        0.0,
        78.66,
        81.73,
        47.4,
        19.13,
        14.54,
        0.0,
        77.44
      ],
      "code_scores": [
        74.71,
        11.11,
        62.8,
        20.0,
        74.32,
        10.75,
        60.98,
        25.42,
        72.76,
        11.83,
        60.37,
        23.39
      ],
      "reasoning_scores": [
        67.14,
        33.33,
        41.554348,
        42.4,
        66.9,
        33.33,
        41.228261,
        43.36,
        67.14,
        33.33,
        41.163043,
        42.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.25
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.38
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.0,
          "math_avg": -5.44,
          "code_avg": 2.44,
          "reasoning_avg": 1.94,
          "overall_avg": 0.73,
          "overall_efficiency": 0.00513,
          "general_efficiency": 0.027959,
          "math_efficiency": -0.038019,
          "code_efficiency": 0.017046,
          "reasoning_efficiency": 0.013535,
          "general_task_scores": [
            -7.83,
            16.96,
            3.79,
            3.07
          ],
          "math_task_scores": [
            2.23,
            -1.8,
            -6.81,
            -20.38,
            -6.67,
            0.81
          ],
          "code_task_scores": [
            2.33,
            2.99,
            18.09,
            -13.66
          ],
          "reasoning_task_scores": [
            -2.4,
            -1.52,
            2.12,
            9.55
          ]
        },
        "vs_instruct": {
          "general_avg": -10.77,
          "math_avg": -11.73,
          "code_avg": 0.69,
          "reasoning_avg": -1.18,
          "overall_avg": -5.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -6.72,
            -20.51,
            -5.91,
            -9.92
          ],
          "math_task_scores": [
            -10.06,
            -29.4,
            -10.17,
            -25.42,
            -9.58,
            14.23
          ],
          "code_task_scores": [
            -0.78,
            0.12,
            5.89,
            -2.48
          ],
          "reasoning_task_scores": [
            -4.56,
            -0.51,
            -1.13,
            1.47
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 8,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 56.86,
      "math_avg": 43.9,
      "code_avg": 41.68,
      "reasoning_avg": 46.08,
      "overall_avg": 47.13,
      "overall_efficiency": 1.593314,
      "general_efficiency": 4.998764,
      "math_efficiency": -1.97622,
      "code_efficiency": 1.614391,
      "reasoning_efficiency": 1.736323,
      "general_scores": [
        72.32,
        43.3625,
        62.55,
        48.2692857,
        73.1,
        41.78,
        63.13,
        48.0121429,
        74.5,
        43.195,
        63.01,
        49.1285714
      ],
      "math_scores": [
        83.62,
        55.4,
        22.99,
        15.88,
        10.0,
        73.17,
        81.8,
        54.6,
        22.9,
        17.8,
        16.67,
        73.17,
        83.09,
        54.0,
        23.24,
        18.99,
        13.33,
        69.51
      ],
      "code_scores": [
        70.82,
        11.47,
        64.63,
        21.02,
        69.65,
        10.75,
        64.02,
        20.68,
        69.65,
        12.19,
        64.63,
        20.68
      ],
      "reasoning_scores": [
        67.42,
        32.32,
        40.869565,
        43.6,
        67.46,
        31.31,
        39.793478,
        45.12,
        66.94,
        32.83,
        40.413043,
        44.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.95
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.79
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": -2.14,
          "code_avg": 1.75,
          "reasoning_avg": 1.88,
          "overall_avg": 1.73,
          "overall_efficiency": 1.593314,
          "general_efficiency": 4.998764,
          "math_efficiency": -1.97622,
          "code_efficiency": 1.614391,
          "reasoning_efficiency": 1.736323,
          "general_task_scores": [
            5.0,
            7.29,
            5.16,
            4.23
          ],
          "math_task_scores": [
            2.86,
            4.47,
            -3.0,
            -18.35,
            6.66,
            -5.49
          ],
          "code_task_scores": [
            -1.56,
            3.23,
            21.14,
            -15.81
          ],
          "reasoning_task_scores": [
            -2.19,
            -2.7,
            1.16,
            11.25
          ]
        },
        "vs_instruct": {
          "general_avg": -9.35,
          "math_avg": -8.44,
          "code_avg": -0.0,
          "reasoning_avg": -1.24,
          "overall_avg": -4.76,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            6.11,
            -30.18,
            -4.54,
            -8.76
          ],
          "math_task_scores": [
            -9.43,
            -23.13,
            -6.36,
            -23.39,
            3.75,
            7.93
          ],
          "code_task_scores": [
            -4.67,
            0.36,
            8.94,
            -4.63
          ],
          "reasoning_task_scores": [
            -4.35,
            -1.69,
            -2.09,
            3.17
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 9,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.26,
      "math_avg": 46.94,
      "code_avg": 58.79,
      "reasoning_avg": 47.33,
      "overall_avg": 52.83,
      "overall_efficiency": 0.007473,
      "general_efficiency": 0.006859,
      "math_efficiency": 0.000908,
      "code_efficiency": 0.018972,
      "reasoning_efficiency": 0.003154,
      "general_scores": [
        62.02,
        57.77,
        61.42,
        48.4685714,
        65.53,
        58.145,
        60.78,
        48.4246154,
        68.06,
        60.0675,
        60.56,
        47.91
      ],
      "math_scores": [
        89.92,
        61.6,
        21.27,
        20.92,
        10.0,
        75.61,
        89.84,
        61.2,
        21.18,
        23.74,
        13.33,
        78.05,
        89.84,
        62.0,
        21.36,
        23.44,
        6.67,
        75.0
      ],
      "code_scores": [
        71.98,
        12.54,
        59.15,
        89.49,
        74.32,
        12.9,
        60.37,
        88.47,
        74.32,
        14.34,
        59.15,
        88.47
      ],
      "reasoning_scores": [
        69.34,
        32.32,
        37.869565,
        48.72,
        70.42,
        32.32,
        37.706522,
        49.84,
        69.39,
        33.84,
        37.978261,
        48.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.92
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.7
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.56
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.72
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.85
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.82,
          "math_avg": 0.9,
          "code_avg": 18.86,
          "reasoning_avg": 3.13,
          "overall_avg": 7.43,
          "overall_efficiency": 0.007473,
          "general_efficiency": 0.006859,
          "math_efficiency": 0.000908,
          "code_efficiency": 0.018972,
          "reasoning_efficiency": 0.003154,
          "general_task_scores": [
            -3.11,
            23.17,
            3.18,
            4.03
          ],
          "math_task_scores": [
            9.89,
            11.4,
            -4.77,
            -13.21,
            3.33,
            -1.22
          ],
          "code_task_scores": [
            1.94,
            5.02,
            16.27,
            52.21
          ],
          "reasoning_task_scores": [
            0.26,
            -2.02,
            -1.35,
            15.65
          ]
        },
        "vs_instruct": {
          "general_avg": -7.95,
          "math_avg": -5.39,
          "code_avg": 17.11,
          "reasoning_avg": 0.02,
          "overall_avg": 0.95,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.0,
            -14.3,
            -6.52,
            -8.96
          ],
          "math_task_scores": [
            -2.4,
            -16.2,
            -8.13,
            -18.25,
            0.42,
            12.2
          ],
          "code_task_scores": [
            -1.17,
            2.15,
            4.07,
            63.39
          ],
          "reasoning_task_scores": [
            -1.9,
            -1.01,
            -4.6,
            7.57
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 10,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 57.1,
      "math_avg": 41.71,
      "code_avg": 42.3,
      "reasoning_avg": 46.07,
      "overall_avg": 46.8,
      "overall_efficiency": 0.00139,
      "general_efficiency": 0.005646,
      "math_efficiency": -0.004321,
      "code_efficiency": 0.002362,
      "reasoning_efficiency": 0.001872,
      "general_scores": [
        64.42,
        56.3125,
        60.51,
        47.53,
        63.72,
        55.5125,
        61.32,
        47.2142857,
        64.95,
        54.6775,
        61.66,
        47.3614286
      ],
      "math_scores": [
        86.43,
        54.8,
        19.76,
        17.95,
        3.33,
        66.46,
        85.9,
        50.0,
        19.67,
        18.1,
        0.0,
        78.05,
        86.96,
        52.4,
        19.33,
        18.84,
        3.33,
        69.51
      ],
      "code_scores": [
        70.82,
        12.9,
        61.59,
        21.02,
        71.21,
        12.19,
        63.41,
        24.75,
        69.65,
        12.9,
        63.41,
        23.73
      ],
      "reasoning_scores": [
        67.31,
        28.28,
        41.141304,
        44.24,
        67.85,
        32.83,
        41.532609,
        41.92,
        66.4,
        34.34,
        43.98913,
        43.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.8
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.65,
          "math_avg": -4.33,
          "code_avg": 2.37,
          "reasoning_avg": 1.88,
          "overall_avg": 1.39,
          "overall_efficiency": 0.00139,
          "general_efficiency": 0.005646,
          "math_efficiency": -0.004321,
          "code_efficiency": 0.002362,
          "reasoning_efficiency": 0.001872,
          "general_task_scores": [
            -3.95,
            20.01,
            3.42,
            3.13
          ],
          "math_task_scores": [
            6.45,
            2.2,
            -6.45,
            -17.61,
            -4.45,
            -6.1
          ],
          "code_task_scores": [
            -1.04,
            4.42,
            19.51,
            -13.43
          ],
          "reasoning_task_scores": [
            -2.27,
            -3.03,
            3.02,
            9.79
          ]
        },
        "vs_instruct": {
          "general_avg": -9.11,
          "math_avg": -10.63,
          "code_avg": 0.62,
          "reasoning_avg": -1.24,
          "overall_avg": -5.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.84,
            -17.46,
            -6.28,
            -9.86
          ],
          "math_task_scores": [
            -5.84,
            -25.4,
            -9.81,
            -22.65,
            -7.36,
            7.32
          ],
          "code_task_scores": [
            -4.15,
            1.55,
            7.31,
            -2.25
          ],
          "reasoning_task_scores": [
            -4.43,
            -2.02,
            -0.23,
            1.71
          ]
        }
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 11,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.63,
      "math_avg": 19.48,
      "code_avg": 15.31,
      "reasoning_avg": 35.92,
      "overall_avg": 27.58,
      "overall_efficiency": -0.216147,
      "general_efficiency": -0.143343,
      "math_efficiency": -0.322185,
      "code_efficiency": -0.298665,
      "reasoning_efficiency": -0.100397,
      "general_scores": [
        43.73,
        34.8875,
        48.77,
        30.055,
        43.54,
        34.605,
        48.93,
        32.25,
        42.39,
        36.355,
        47.24,
        32.7764286
      ],
      "math_scores": [
        65.13,
        20.0,
        8.72,
        2.97,
        6.67,
        17.07,
        65.05,
        19.8,
        8.47,
        2.67,
        6.67,
        15.85,
        61.18,
        18.0,
        7.95,
        2.52,
        6.67,
        15.24
      ],
      "code_scores": [
        28.4,
        0.0,
        12.8,
        19.32,
        28.4,
        0.0,
        12.2,
        19.66,
        27.63,
        0.0,
        11.59,
        23.73
      ],
      "reasoning_scores": [
        54.05,
        26.77,
        33.826087,
        27.28,
        54.92,
        30.3,
        33.630435,
        26.88,
        54.57,
        28.79,
        33.554348,
        26.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.31
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.38
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.05
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.2
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -11.82,
          "math_avg": -26.56,
          "code_avg": -24.62,
          "reasoning_avg": -8.28,
          "overall_avg": -17.82,
          "overall_efficiency": -0.216147,
          "general_efficiency": -0.143343,
          "math_efficiency": -0.322185,
          "code_efficiency": -0.298665,
          "reasoning_efficiency": -0.100397,
          "general_task_scores": [
            -25.09,
            -0.21,
            -9.43,
            -12.55
          ],
          "math_task_scores": [
            -16.19,
            -30.93,
            -17.66,
            -33.19,
            0.0,
            -61.39
          ],
          "code_task_scores": [
            -43.46,
            -8.24,
            -31.09,
            -15.7
          ],
          "reasoning_task_scores": [
            -14.95,
            -6.23,
            -5.53,
            -6.4
          ]
        },
        "vs_instruct": {
          "general_avg": -26.58,
          "math_avg": -32.86,
          "code_avg": -26.37,
          "reasoning_avg": -11.4,
          "overall_avg": -24.3,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -23.98,
            -37.68,
            -19.13,
            -25.54
          ],
          "math_task_scores": [
            -28.48,
            -58.53,
            -21.02,
            -38.23,
            -2.91,
            -47.97
          ],
          "code_task_scores": [
            -46.57,
            -11.11,
            -43.29,
            -4.52
          ],
          "reasoning_task_scores": [
            -17.11,
            -5.22,
            -8.78,
            -14.48
          ]
        }
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 12,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 60.54,
      "math_avg": 45.74,
      "code_avg": 57.88,
      "reasoning_avg": 47.08,
      "overall_avg": 52.81,
      "overall_efficiency": 0.007883,
      "general_efficiency": 0.009678,
      "math_efficiency": -0.000314,
      "code_efficiency": 0.019102,
      "reasoning_efficiency": 0.003067,
      "general_scores": [
        61.67,
        71.4675,
        62.15,
        46.0707143,
        62.37,
        72.1525,
        61.15,
        46.0535714,
        62.84,
        72.685,
        62.09,
        45.7214286
      ],
      "math_scores": [
        88.1,
        58.0,
        19.49,
        27.89,
        3.33,
        80.49,
        87.11,
        55.4,
        19.99,
        26.85,
        6.67,
        78.05,
        85.82,
        53.2,
        18.83,
        32.2,
        3.33,
        78.66
      ],
      "code_scores": [
        71.21,
        11.83,
        68.9,
        82.03,
        72.76,
        11.47,
        66.46,
        83.73,
        71.6,
        13.62,
        64.63,
        76.27
      ],
      "reasoning_scores": [
        62.09,
        35.86,
        44.315217,
        48.64,
        61.85,
        33.33,
        44.576087,
        48.16,
        61.82,
        31.82,
        43.684783,
        48.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.98
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.86
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.66
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.09,
          "math_avg": -0.3,
          "code_avg": 17.94,
          "reasoning_avg": 2.88,
          "overall_avg": 7.41,
          "overall_efficiency": 0.007883,
          "general_efficiency": 0.009678,
          "math_efficiency": -0.000314,
          "code_efficiency": 0.019102,
          "reasoning_efficiency": 0.003067,
          "general_task_scores": [
            -6.02,
            36.61,
            4.06,
            1.71
          ],
          "math_task_scores": [
            7.03,
            5.33,
            -6.6,
            -6.93,
            -2.23,
            1.63
          ],
          "code_task_scores": [
            0.26,
            4.07,
            23.37,
            44.08
          ],
          "reasoning_task_scores": [
            -7.54,
            -1.18,
            4.99,
            15.25
          ]
        },
        "vs_instruct": {
          "general_avg": -5.67,
          "math_avg": -6.59,
          "code_avg": 16.19,
          "reasoning_avg": -0.24,
          "overall_avg": 0.92,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.91,
            -0.86,
            -5.64,
            -11.28
          ],
          "math_task_scores": [
            -5.26,
            -22.27,
            -9.96,
            -11.97,
            -5.14,
            15.05
          ],
          "code_task_scores": [
            -2.85,
            1.2,
            11.17,
            55.26
          ],
          "reasoning_task_scores": [
            -9.7,
            -0.17,
            1.74,
            7.17
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 57.92,
      "math_avg": 40.57,
      "code_avg": 32.81,
      "reasoning_avg": 47.56,
      "overall_avg": 44.71,
      "overall_efficiency": -0.000774,
      "general_efficiency": 0.007258,
      "math_efficiency": -0.006129,
      "code_efficiency": -0.00799,
      "reasoning_efficiency": 0.003765,
      "general_scores": [
        69.25,
        49.3625,
        63.18,
        48.7314286,
        70.53,
        49.5325,
        62.99,
        48.8142857,
        70.21,
        50.65,
        62.86,
        48.9
      ],
      "math_scores": [
        88.25,
        58.0,
        22.0,
        25.82,
        3.33,
        39.63,
        89.16,
        56.6,
        22.36,
        25.96,
        3.33,
        46.95,
        89.31,
        59.2,
        22.27,
        25.96,
        6.67,
        45.54
      ],
      "code_scores": [
        74.71,
        7.17,
        14.63,
        40.34,
        75.1,
        7.17,
        7.93,
        41.36,
        75.1,
        11.47,
        14.63,
        24.07
      ],
      "reasoning_scores": [
        69.87,
        31.31,
        42.967391,
        45.04,
        69.82,
        34.34,
        43.793478,
        43.36,
        69.0,
        33.92,
        44.043478,
        43.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.91
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.21
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.91
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.04
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 8.6
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.4
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.26
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.6
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.47,
          "math_avg": -5.47,
          "code_avg": -7.13,
          "reasoning_avg": 3.36,
          "overall_avg": -0.69,
          "overall_efficiency": -0.000774,
          "general_efficiency": 0.007258,
          "math_efficiency": -0.006129,
          "code_efficiency": -0.00799,
          "reasoning_efficiency": 0.003765,
          "general_task_scores": [
            1.69,
            14.36,
            5.27,
            4.58
          ],
          "math_task_scores": [
            8.93,
            7.73,
            -3.83,
            -10.0,
            -2.23,
            -33.4
          ],
          "code_task_scores": [
            3.37,
            0.36,
            -30.89,
            -1.34
          ],
          "reasoning_task_scores": [
            0.1,
            -1.66,
            4.4,
            10.59
          ]
        },
        "vs_instruct": {
          "general_avg": -8.29,
          "math_avg": -11.76,
          "code_avg": -8.88,
          "reasoning_avg": 0.24,
          "overall_avg": -7.17,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.8,
            -23.11,
            -4.43,
            -8.41
          ],
          "math_task_scores": [
            -3.36,
            -19.87,
            -7.19,
            -15.04,
            -5.14,
            -19.98
          ],
          "code_task_scores": [
            0.26,
            -2.51,
            -43.09,
            9.84
          ],
          "reasoning_task_scores": [
            -2.06,
            -0.65,
            1.15,
            2.51
          ]
        }
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 59.76,
      "math_avg": 55.88,
      "code_avg": 42.04,
      "reasoning_avg": 49.56,
      "overall_avg": 51.81,
      "overall_efficiency": 0.640633,
      "general_efficiency": 0.831649,
      "math_efficiency": 0.984389,
      "code_efficiency": 0.210333,
      "reasoning_efficiency": 0.536163,
      "general_scores": [
        64.52,
        51.6575,
        68.62,
        54.6642857,
        63.1,
        51.7175,
        68.88,
        54.3228571,
        63.88,
        52.435,
        68.67,
        54.6642857
      ],
      "math_scores": [
        91.81,
        75.8,
        28.16,
        38.13,
        16.67,
        82.32,
        91.58,
        74.4,
        28.68,
        39.61,
        16.67,
        83.54,
        92.27,
        75.8,
        28.14,
        38.87,
        20.0,
        83.46
      ],
      "code_scores": [
        72.76,
        12.54,
        60.98,
        18.64,
        73.15,
        12.9,
        64.63,
        21.69,
        73.54,
        12.54,
        64.11,
        16.95
      ],
      "reasoning_scores": [
        70.64,
        33.84,
        44.032609,
        50.08,
        70.41,
        33.33,
        43.695652,
        49.76,
        71.03,
        33.84,
        43.891304,
        50.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.33
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.24
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.09
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.69
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.87
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.32,
          "math_avg": 9.84,
          "code_avg": 2.1,
          "reasoning_avg": 5.36,
          "overall_avg": 6.41,
          "overall_efficiency": 0.640633,
          "general_efficiency": 0.831649,
          "math_efficiency": 0.984389,
          "code_efficiency": 0.210333,
          "reasoning_efficiency": 0.536163,
          "general_task_scores": [
            -4.48,
            16.45,
            10.98,
            10.31
          ],
          "math_task_scores": [
            11.91,
            25.13,
            2.29,
            2.96,
            11.11,
            5.67
          ],
          "code_task_scores": [
            1.55,
            4.42,
            19.95,
            -17.51
          ],
          "reasoning_task_scores": [
            1.23,
            -1.18,
            4.67,
            16.72
          ]
        },
        "vs_instruct": {
          "general_avg": -6.45,
          "math_avg": 3.55,
          "code_avg": 0.35,
          "reasoning_avg": 2.24,
          "overall_avg": -0.08,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.37,
            -21.02,
            1.28,
            -2.68
          ],
          "math_task_scores": [
            -0.38,
            -2.47,
            -1.07,
            -2.08,
            8.2,
            19.09
          ],
          "code_task_scores": [
            -1.56,
            1.55,
            7.75,
            -6.33
          ],
          "reasoning_task_scores": [
            -0.93,
            -0.17,
            1.42,
            8.64
          ]
        }
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 52.62,
      "math_avg": 38.34,
      "code_avg": 41.63,
      "reasoning_avg": 43.94,
      "overall_avg": 44.14,
      "overall_efficiency": -0.002542,
      "general_efficiency": 0.002365,
      "math_efficiency": -0.015429,
      "code_efficiency": 0.003403,
      "reasoning_efficiency": -0.000508,
      "general_scores": [
        53.25,
        50.49,
        57.52,
        45.4307143,
        59.84,
        51.0625,
        57.37,
        45.8185714,
        56.45,
        51.04,
        58.92,
        44.30214
      ],
      "math_scores": [
        80.59,
        47.2,
        17.39,
        17.06,
        3.33,
        62.2,
        77.86,
        45.0,
        16.31,
        16.77,
        3.33,
        69.51,
        80.29,
        49.8,
        17.62,
        17.36,
        3.33,
        65.24
      ],
      "code_scores": [
        70.43,
        11.83,
        58.54,
        24.41,
        71.21,
        10.04,
        57.32,
        28.47,
        66.93,
        10.04,
        56.1,
        34.24
      ],
      "reasoning_scores": [
        65.5,
        30.81,
        40.26087,
        38.48,
        64.31,
        32.83,
        41.565217,
        39.76,
        64.13,
        32.83,
        41.0109,
        35.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.11
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.65
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 69.52
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.64
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.04
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.95
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.18,
          "math_avg": -7.7,
          "code_avg": 1.7,
          "reasoning_avg": -0.25,
          "overall_avg": -1.27,
          "overall_efficiency": -0.002542,
          "general_efficiency": 0.002365,
          "math_efficiency": -0.015429,
          "code_efficiency": 0.003403,
          "reasoning_efficiency": -0.000508,
          "general_task_scores": [
            -11.8,
            15.37,
            0.2,
            0.94
          ],
          "math_task_scores": [
            -0.4,
            -2.87,
            -8.93,
            -18.85,
            -3.34,
            -11.79
          ],
          "code_task_scores": [
            -2.08,
            2.4,
            14.03,
            -7.56
          ],
          "reasoning_task_scores": [
            -4.81,
            -2.69,
            1.75,
            4.75
          ]
        },
        "vs_instruct": {
          "general_avg": -13.58,
          "math_avg": -13.99,
          "code_avg": -0.05,
          "reasoning_avg": -3.37,
          "overall_avg": -7.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -10.69,
            -22.1,
            -9.5,
            -12.05
          ],
          "math_task_scores": [
            -12.69,
            -30.47,
            -12.29,
            -23.89,
            -6.25,
            1.63
          ],
          "code_task_scores": [
            -5.19,
            -0.47,
            1.83,
            3.62
          ],
          "reasoning_task_scores": [
            -6.97,
            -1.68,
            -1.5,
            -3.33
          ]
        }
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 50.58,
      "math_avg": 22.71,
      "code_avg": 38.24,
      "reasoning_avg": 39.31,
      "overall_avg": 37.71,
      "overall_efficiency": -0.057153,
      "general_efficiency": -0.006394,
      "math_efficiency": -0.173315,
      "code_efficiency": -0.012573,
      "reasoning_efficiency": -0.03633,
      "general_scores": [
        56.13,
        47.1375,
        49.86,
        43.3035714,
        63.44,
        47.0325,
        49.48,
        43.5214286,
        65.54,
        46.9625,
        50.09,
        44.5071429
      ],
      "math_scores": [
        38.67,
        25.6,
        10.93,
        5.79,
        0.0,
        55.67,
        38.44,
        25.2,
        10.95,
        5.34,
        0.0,
        53.66,
        37.3,
        25.4,
        10.61,
        5.79,
        3.33,
        56.1
      ],
      "code_scores": [
        66.54,
        7.17,
        53.11,
        26.44,
        64.98,
        7.17,
        53.66,
        27.12,
        65.37,
        5.73,
        52.44,
        29.15
      ],
      "reasoning_scores": [
        63.46,
        29.29,
        32.25,
        33.6,
        62.6,
        28.28,
        31.532609,
        32.8,
        62.2,
        29.8,
        32.271739,
        33.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.7
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.04
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.78
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.83
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.64
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.14
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.07
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.75
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.02
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.86,
          "math_avg": -23.33,
          "code_avg": -1.69,
          "reasoning_avg": -4.89,
          "overall_avg": -7.69,
          "overall_efficiency": -0.057153,
          "general_efficiency": -0.006394,
          "math_efficiency": -0.173315,
          "code_efficiency": -0.012573,
          "reasoning_efficiency": -0.03633,
          "general_task_scores": [
            -6.61,
            11.55,
            -7.93,
            -0.46
          ],
          "math_task_scores": [
            -41.84,
            -24.8,
            -15.21,
            -30.27,
            -5.56,
            -22.3
          ],
          "code_task_scores": [
            -5.97,
            -1.55,
            9.78,
            -9.03
          ],
          "reasoning_task_scores": [
            -6.71,
            -5.73,
            -7.18,
            0.05
          ]
        },
        "vs_instruct": {
          "general_avg": -15.62,
          "math_avg": -29.63,
          "code_avg": -3.44,
          "reasoning_avg": -8.01,
          "overall_avg": -14.18,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.5,
            -25.92,
            -17.63,
            -13.45
          ],
          "math_task_scores": [
            -54.13,
            -52.4,
            -18.57,
            -35.31,
            -8.47,
            -8.88
          ],
          "code_task_scores": [
            -9.08,
            -4.42,
            -2.42,
            2.15
          ],
          "reasoning_task_scores": [
            -8.87,
            -4.72,
            -10.43,
            -8.03
          ]
        }
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 54.64,
      "math_avg": 39.53,
      "code_avg": 43.61,
      "reasoning_avg": 47.21,
      "overall_avg": 46.25,
      "overall_efficiency": 0.113093,
      "general_efficiency": 0.42744,
      "math_efficiency": -0.870839,
      "code_efficiency": 0.491994,
      "reasoning_efficiency": 0.403778,
      "general_scores": [
        63.41,
        47.84,
        59.24,
        48.5021429,
        63.5,
        48.6625,
        59.61,
        48.1521429,
        62.63,
        46.19,
        59.51,
        48.4178571
      ],
      "math_scores": [
        75.89,
        47.6,
        18.72,
        14.09,
        13.3,
        75.0,
        77.41,
        46.6,
        18.25,
        14.84,
        3.33,
        73.78,
        75.97,
        45.8,
        17.39,
        15.28,
        3.33,
        75.0
      ],
      "code_scores": [
        71.6,
        11.83,
        68.29,
        25.76,
        70.43,
        10.75,
        66.46,
        24.07,
        72.37,
        11.83,
        68.9,
        21.02
      ],
      "reasoning_scores": [
        68.77,
        35.35,
        42.119565,
        43.2,
        68.88,
        35.86,
        40.652174,
        44.16,
        68.92,
        34.85,
        40.217391,
        43.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.12
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.65
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.59
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.88
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.62
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.86
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.19,
          "math_avg": -6.51,
          "code_avg": 3.68,
          "reasoning_avg": 3.02,
          "overall_avg": 0.85,
          "overall_efficiency": 0.113093,
          "general_efficiency": 0.42744,
          "math_efficiency": -0.870839,
          "code_efficiency": 0.491994,
          "reasoning_efficiency": 0.403778,
          "general_task_scores": [
            -5.13,
            12.07,
            1.71,
            4.12
          ],
          "math_task_scores": [
            -3.56,
            -3.53,
            -7.92,
            -21.17,
            -0.02,
            -2.85
          ],
          "code_task_scores": [
            -0.13,
            3.23,
            24.59,
            -12.98
          ],
          "reasoning_task_scores": [
            -0.6,
            0.5,
            1.8,
            10.37
          ]
        },
        "vs_instruct": {
          "general_avg": -11.57,
          "math_avg": -12.81,
          "code_avg": 1.93,
          "reasoning_avg": -0.1,
          "overall_avg": -5.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.02,
            -25.4,
            -7.99,
            -8.87
          ],
          "math_task_scores": [
            -15.85,
            -31.13,
            -11.28,
            -26.21,
            -2.93,
            10.57
          ],
          "code_task_scores": [
            -3.24,
            0.36,
            12.39,
            -1.8
          ],
          "reasoning_task_scores": [
            -2.76,
            1.51,
            -1.45,
            2.29
          ]
        }
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 54.25,
      "math_avg": 39.84,
      "code_avg": 40.82,
      "reasoning_avg": 46.5,
      "overall_avg": 45.35,
      "overall_efficiency": -0.006645,
      "general_efficiency": 0.37436,
      "math_efficiency": -0.82726,
      "code_efficiency": 0.119,
      "reasoning_efficiency": 0.307319,
      "general_scores": [
        67.69,
        45.2625,
        58.99,
        47.8485714,
        63.07,
        44.47,
        58.47,
        46.4884615,
        66.42,
        45.575,
        59.11,
        47.6314286
      ],
      "math_scores": [
        83.62,
        45.0,
        18.04,
        13.95,
        6.67,
        76.22,
        84.53,
        46.6,
        18.07,
        14.68,
        0.0,
        74.39,
        83.55,
        44.4,
        18.11,
        15.43,
        0.0,
        73.78
      ],
      "code_scores": [
        69.65,
        11.11,
        61.59,
        20.34,
        71.6,
        9.68,
        64.63,
        17.97,
        69.65,
        9.32,
        64.02,
        20.34
      ],
      "reasoning_scores": [
        68.47,
        33.84,
        42.847826,
        42.0,
        68.23,
        32.83,
        42.815217,
        41.76,
        68.2,
        32.83,
        42.445652,
        41.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.07
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.8
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.3
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.3
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.7
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.81,
          "math_avg": -6.2,
          "code_avg": 0.89,
          "reasoning_avg": 2.3,
          "overall_avg": -0.05,
          "overall_efficiency": -0.006645,
          "general_efficiency": 0.37436,
          "math_efficiency": -0.82726,
          "code_efficiency": 0.119,
          "reasoning_efficiency": 0.307319,
          "general_task_scores": [
            -2.58,
            9.61,
            1.12,
            3.08
          ],
          "math_task_scores": [
            3.92,
            -4.87,
            -7.97,
            -21.22,
            -4.45,
            -2.64
          ],
          "code_task_scores": [
            -1.3,
            1.8,
            20.12,
            -17.05
          ],
          "reasoning_task_scores": [
            -1.16,
            -1.68,
            3.5,
            8.56
          ]
        },
        "vs_instruct": {
          "general_avg": -11.96,
          "math_avg": -12.5,
          "code_avg": -0.86,
          "reasoning_avg": -0.81,
          "overall_avg": -6.53,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.47,
            -27.86,
            -8.58,
            -9.91
          ],
          "math_task_scores": [
            -8.37,
            -32.47,
            -11.33,
            -26.26,
            -7.36,
            10.78
          ],
          "code_task_scores": [
            -4.41,
            -1.07,
            7.92,
            -5.87
          ],
          "reasoning_task_scores": [
            -3.32,
            -0.67,
            0.25,
            0.48
          ]
        }
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 56.72,
      "math_avg": 48.13,
      "code_avg": 42.7,
      "reasoning_avg": 44.37,
      "overall_avg": 47.98,
      "overall_efficiency": 0.002576,
      "general_efficiency": 0.005278,
      "math_efficiency": 0.002087,
      "code_efficiency": 0.002772,
      "reasoning_efficiency": 0.000169,
      "general_scores": [
        70.22,
        45.74,
        61.7,
        50.2571429,
        69.21,
        45.3225,
        61.01,
        50.6442857,
        69.21,
        46.115,
        60.59,
        50.6442857
      ],
      "math_scores": [
        91.36,
        65.4,
        22.56,
        31.6,
        6.67,
        67.68,
        91.81,
        66.4,
        22.47,
        30.27,
        6.67,
        73.17,
        91.58,
        66.0,
        22.54,
        30.27,
        6.67,
        73.17
      ],
      "code_scores": [
        73.15,
        14.7,
        69.51,
        26.1,
        73.54,
        13.26,
        54.88,
        21.69,
        72.37,
        13.62,
        54.88,
        24.75
      ],
      "reasoning_scores": [
        65.67,
        31.31,
        32.23913,
        48.72,
        66.25,
        29.8,
        32.119565,
        48.0,
        67.55,
        29.8,
        31.902174,
        49.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.55
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.52
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.71
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.76
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.09
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.28,
          "math_avg": 2.09,
          "code_avg": 2.77,
          "reasoning_avg": 0.17,
          "overall_avg": 2.58,
          "overall_efficiency": 0.002576,
          "general_efficiency": 0.005278,
          "math_efficiency": 0.002087,
          "code_efficiency": 0.002772,
          "reasoning_efficiency": 0.000169,
          "general_task_scores": [
            1.24,
            10.24,
            3.36,
            6.28
          ],
          "math_task_scores": [
            11.6,
            15.73,
            -3.52,
            -5.2,
            0.0,
            -6.1
          ],
          "code_task_scores": [
            1.42,
            5.62,
            16.47,
            -12.42
          ],
          "reasoning_task_scores": [
            -2.97,
            -4.55,
            -7.11,
            15.31
          ]
        },
        "vs_instruct": {
          "general_avg": -9.49,
          "math_avg": -4.21,
          "code_avg": 1.02,
          "reasoning_avg": -2.95,
          "overall_avg": -3.91,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.35,
            -27.23,
            -6.34,
            -6.71
          ],
          "math_task_scores": [
            -0.69,
            -11.87,
            -6.88,
            -10.24,
            -2.91,
            7.32
          ],
          "code_task_scores": [
            -1.69,
            2.75,
            4.27,
            -1.24
          ],
          "reasoning_task_scores": [
            -5.13,
            -3.54,
            -10.36,
            7.23
          ]
        }
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 58.47,
      "math_avg": 44.41,
      "code_avg": 43.78,
      "reasoning_avg": 45.26,
      "overall_avg": 47.98,
      "overall_efficiency": 0.002999,
      "general_efficiency": 0.008171,
      "math_efficiency": -0.001892,
      "code_efficiency": 0.004481,
      "reasoning_efficiency": 0.001236,
      "general_scores": [
        69.15,
        46.485,
        64.45,
        52.1621429,
        68.99,
        47.7125,
        65.79,
        52.36,
        69.68,
        47.54,
        65.46,
        51.8207143
      ],
      "math_scores": [
        87.87,
        62.6,
        26.22,
        32.49,
        10.0,
        53.05,
        88.25,
        63.8,
        25.86,
        32.2,
        13.33,
        47.56,
        87.41,
        61.2,
        26.78,
        32.05,
        0.0,
        48.78
      ],
      "code_scores": [
        68.09,
        10.04,
        60.37,
        32.54,
        68.48,
        5.73,
        54.88,
        49.49,
        67.7,
        4.66,
        60.37,
        43.05
      ],
      "reasoning_scores": [
        59.51,
        30.81,
        37.402174,
        48.16,
        61.59,
        34.34,
        38.934783,
        47.36,
        59.03,
        38.89,
        39.336957,
        47.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.29
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.8
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.81
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.54
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.56
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.02,
          "math_avg": -1.63,
          "code_avg": 3.85,
          "reasoning_avg": 1.06,
          "overall_avg": 2.58,
          "overall_efficiency": 0.002999,
          "general_efficiency": 0.008171,
          "math_efficiency": -0.001892,
          "code_efficiency": 0.004481,
          "reasoning_efficiency": 0.001236,
          "general_task_scores": [
            0.96,
            11.76,
            7.49,
            7.87
          ],
          "math_task_scores": [
            7.86,
            12.33,
            0.25,
            -3.66,
            1.11,
            -27.64
          ],
          "code_task_scores": [
            -3.51,
            -1.43,
            15.25,
            5.09
          ],
          "reasoning_task_scores": [
            -9.42,
            -0.17,
            -0.64,
            14.48
          ]
        },
        "vs_instruct": {
          "general_avg": -7.74,
          "math_avg": -7.92,
          "code_avg": 2.1,
          "reasoning_avg": -2.06,
          "overall_avg": -3.91,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.07,
            -25.71,
            -2.21,
            -5.12
          ],
          "math_task_scores": [
            -4.43,
            -15.27,
            -3.11,
            -8.7,
            -1.8,
            -14.22
          ],
          "code_task_scores": [
            -6.62,
            -4.3,
            3.05,
            16.27
          ],
          "reasoning_task_scores": [
            -11.58,
            0.84,
            -3.89,
            6.4
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 60.45,
      "math_avg": 42.75,
      "code_avg": 43.41,
      "reasoning_avg": 47.1,
      "overall_avg": 48.43,
      "overall_efficiency": 0.041768,
      "general_efficiency": 0.124378,
      "math_efficiency": -0.045378,
      "code_efficiency": 0.047981,
      "reasoning_efficiency": 0.04009,
      "general_scores": [
        76.92,
        52.5125,
        59.41,
        51.2678571,
        76.8,
        53.2775,
        61.24,
        51.7264286,
        76.79,
        53.16,
        60.4,
        51.9507143
      ],
      "math_scores": [
        75.21,
        48.6,
        18.95,
        34.27,
        0.0,
        78.66,
        75.36,
        48.4,
        19.63,
        31.01,
        6.67,
        79.27,
        74.0,
        48.2,
        19.08,
        31.75,
        0.0,
        80.49
      ],
      "code_scores": [
        72.76,
        14.34,
        66.46,
        18.64,
        73.54,
        13.62,
        65.85,
        21.02,
        72.76,
        13.62,
        65.24,
        23.05
      ],
      "reasoning_scores": [
        68.96,
        30.3,
        37.423913,
        49.6,
        68.44,
        34.34,
        37.815217,
        48.48,
        68.52,
        34.34,
        37.880435,
        49.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.85
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.64
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.71
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.01,
          "math_avg": -3.29,
          "code_avg": 3.48,
          "reasoning_avg": 2.9,
          "overall_avg": 3.03,
          "overall_efficiency": 0.041768,
          "general_efficiency": 0.124378,
          "math_efficiency": -0.045378,
          "code_efficiency": 0.047981,
          "reasoning_efficiency": 0.04009,
          "general_task_scores": [
            8.53,
            17.49,
            2.61,
            7.41
          ],
          "math_task_scores": [
            -5.12,
            -1.8,
            -6.82,
            -3.57,
            -4.45,
            2.03
          ],
          "code_task_scores": [
            1.42,
            5.62,
            22.56,
            -15.7
          ],
          "reasoning_task_scores": [
            -0.82,
            -1.86,
            -1.49,
            15.79
          ]
        },
        "vs_instruct": {
          "general_avg": -5.75,
          "math_avg": -9.58,
          "code_avg": 1.73,
          "reasoning_avg": -0.21,
          "overall_avg": -3.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.64,
            -19.98,
            -7.09,
            -5.58
          ],
          "math_task_scores": [
            -17.41,
            -29.4,
            -10.18,
            -8.61,
            -7.36,
            15.45
          ],
          "code_task_scores": [
            -1.69,
            2.75,
            10.36,
            -4.52
          ],
          "reasoning_task_scores": [
            -2.98,
            -0.85,
            -4.74,
            7.71
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 55.69,
      "math_avg": 41.38,
      "code_avg": 37.98,
      "reasoning_avg": 47.73,
      "overall_avg": 45.69,
      "overall_efficiency": 0.000735,
      "general_efficiency": 0.010742,
      "math_efficiency": -0.011807,
      "code_efficiency": -0.004935,
      "reasoning_efficiency": 0.008942,
      "general_scores": [
        72.14,
        47.59,
        54.62,
        47.3307143,
        70.88,
        48.8025,
        55.29,
        47.7228571,
        72.7,
        47.615,
        55.97,
        47.5878571
      ],
      "math_scores": [
        84.61,
        51.4,
        19.44,
        16.32,
        0.0,
        75.61,
        84.84,
        50.2,
        18.2,
        17.95,
        3.33,
        75.61,
        84.76,
        50.0,
        18.9,
        16.77,
        0.0,
        76.83
      ],
      "code_scores": [
        71.98,
        9.32,
        46.95,
        31.53,
        69.65,
        9.68,
        46.34,
        27.12,
        68.48,
        10.04,
        42.68,
        22.03
      ],
      "reasoning_scores": [
        67.34,
        35.86,
        43.5,
        44.8,
        68.13,
        34.85,
        43.043478,
        44.72,
        67.16,
        35.35,
        43.597826,
        44.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.01
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.89
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.54
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.24,
          "math_avg": -4.66,
          "code_avg": -1.95,
          "reasoning_avg": 3.53,
          "overall_avg": 0.29,
          "overall_efficiency": 0.000735,
          "general_efficiency": 0.010742,
          "math_efficiency": -0.011807,
          "code_efficiency": -0.004935,
          "reasoning_efficiency": 0.008942,
          "general_task_scores": [
            3.6,
            12.51,
            -2.45,
            3.31
          ],
          "math_task_scores": [
            4.76,
            0.33,
            -7.19,
            -18.9,
            -5.56,
            -1.42
          ],
          "code_task_scores": [
            -1.56,
            1.44,
            2.03,
            -9.71
          ],
          "reasoning_task_scores": [
            -1.92,
            0.5,
            4.18,
            11.36
          ]
        },
        "vs_instruct": {
          "general_avg": -10.52,
          "math_avg": -10.96,
          "code_avg": -3.7,
          "reasoning_avg": 0.41,
          "overall_avg": -6.19,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.71,
            -24.96,
            -12.15,
            -9.68
          ],
          "math_task_scores": [
            -7.53,
            -27.27,
            -10.55,
            -23.94,
            -8.47,
            12.0
          ],
          "code_task_scores": [
            -4.67,
            -1.43,
            -10.17,
            1.47
          ],
          "reasoning_task_scores": [
            -4.08,
            1.51,
            0.93,
            3.28
          ]
        }
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 58.34,
      "math_avg": 39.92,
      "code_avg": 40.45,
      "reasoning_avg": 47.63,
      "overall_avg": 46.58,
      "overall_efficiency": 0.004508,
      "general_efficiency": 0.026332,
      "math_efficiency": -0.023364,
      "code_efficiency": 0.001966,
      "reasoning_efficiency": 0.013096,
      "general_scores": [
        73.18,
        50.54,
        61.04,
        50.1792857,
        70.186,
        50.905,
        60.9,
        49.7235714,
        70.71,
        51.535,
        61.38,
        49.8564286
      ],
      "math_scores": [
        80.74,
        44.8,
        18.61,
        17.95,
        0.0,
        75.0,
        81.65,
        46.4,
        19.4,
        18.25,
        0.0,
        77.439,
        78.17,
        46.2,
        19.29,
        18.4,
        0.0,
        76.22
      ],
      "code_scores": [
        71.21,
        13.26,
        57.93,
        20.0,
        73.93,
        11.11,
        63.4,
        18.31,
        72.37,
        10.39,
        55.49,
        17.97
      ],
      "reasoning_scores": [
        67.91,
        34.85,
        47.076087,
        43.52,
        67.21,
        35.86,
        44.173913,
        42.88,
        68.22,
        35.86,
        42.152174,
        41.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.59
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.94
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.47
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.75
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.9,
          "math_avg": -6.12,
          "code_avg": 0.52,
          "reasoning_avg": 3.43,
          "overall_avg": 1.18,
          "overall_efficiency": 0.004508,
          "general_efficiency": 0.026332,
          "math_efficiency": -0.023364,
          "code_efficiency": 0.001966,
          "reasoning_efficiency": 0.013096,
          "general_task_scores": [
            3.05,
            15.5,
            3.37,
            5.68
          ],
          "math_task_scores": [
            0.21,
            -4.4,
            -6.94,
            -17.71,
            -6.67,
            -1.22
          ],
          "code_task_scores": [
            0.9,
            3.35,
            15.65,
            -17.84
          ],
          "reasoning_task_scores": [
            -1.68,
            0.67,
            5.27,
            9.47
          ]
        },
        "vs_instruct": {
          "general_avg": -7.86,
          "math_avg": -12.42,
          "code_avg": -1.24,
          "reasoning_avg": 0.31,
          "overall_avg": -5.3,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.16,
            -21.97,
            -6.33,
            -7.31
          ],
          "math_task_scores": [
            -12.08,
            -32.0,
            -10.3,
            -22.75,
            -9.58,
            12.2
          ],
          "code_task_scores": [
            -2.21,
            0.48,
            3.45,
            -6.66
          ],
          "reasoning_task_scores": [
            -3.84,
            1.68,
            2.02,
            1.39
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 55.61,
      "math_avg": 40.28,
      "code_avg": 40.65,
      "reasoning_avg": 45.45,
      "overall_avg": 45.5,
      "overall_efficiency": 0.003126,
      "general_efficiency": 0.139468,
      "math_efficiency": -0.192919,
      "code_efficiency": 0.024019,
      "reasoning_efficiency": 0.041936,
      "general_scores": [
        63.76,
        50.355,
        61.44,
        45.5242857,
        63.74,
        52.5325,
        61.16,
        46.0421429,
        63.81,
        51.1575,
        61.46,
        46.2878571
      ],
      "math_scores": [
        75.97,
        43.4,
        18.93,
        16.91,
        13.33,
        75.61,
        75.82,
        44.0,
        20.01,
        16.02,
        10.0,
        78.05,
        75.66,
        45.8,
        19.06,
        13.65,
        6.67,
        76.22
      ],
      "code_scores": [
        69.26,
        11.83,
        61.59,
        23.73,
        69.65,
        10.04,
        60.37,
        18.64,
        68.09,
        12.19,
        60.37,
        22.03
      ],
      "reasoning_scores": [
        66.98,
        31.31,
        42.391304,
        40.64,
        66.58,
        32.32,
        42.51087,
        41.2,
        66.67,
        31.31,
        42.032609,
        41.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.77
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.33
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.63
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.35
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.78
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.16,
          "math_avg": -5.76,
          "code_avg": 0.72,
          "reasoning_avg": 1.25,
          "overall_avg": 0.09,
          "overall_efficiency": 0.003126,
          "general_efficiency": 0.139468,
          "math_efficiency": -0.192919,
          "code_efficiency": 0.024019,
          "reasoning_efficiency": 0.041936,
          "general_task_scores": [
            -4.54,
            15.86,
            3.61,
            1.71
          ],
          "math_task_scores": [
            -4.16,
            -5.8,
            -6.71,
            -20.38,
            3.33,
            -0.81
          ],
          "code_task_scores": [
            -2.6,
            3.11,
            17.49,
            -15.13
          ],
          "reasoning_task_scores": [
            -2.72,
            -3.2,
            3.11,
            7.81
          ]
        },
        "vs_instruct": {
          "general_avg": -10.6,
          "math_avg": -12.05,
          "code_avg": -1.03,
          "reasoning_avg": -1.87,
          "overall_avg": -6.39,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.43,
            -21.61,
            -6.09,
            -11.28
          ],
          "math_task_scores": [
            -16.45,
            -33.4,
            -10.07,
            -25.42,
            0.42,
            12.61
          ],
          "code_task_scores": [
            -5.71,
            0.24,
            5.29,
            -3.95
          ],
          "reasoning_task_scores": [
            -4.88,
            -2.19,
            -0.14,
            -0.27
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 53.99,
      "math_avg": 39.81,
      "code_avg": 42.52,
      "reasoning_avg": 46.84,
      "overall_avg": 45.79,
      "overall_efficiency": 0.007703,
      "general_efficiency": 0.050832,
      "math_efficiency": -0.1246,
      "code_efficiency": 0.051733,
      "reasoning_efficiency": 0.052846,
      "general_scores": [
        63.64,
        46.9175,
        58.25,
        49.3621429,
        63.07,
        46.815,
        57.68,
        46.4884615,
        63.31,
        45.7325,
        57.58,
        48.9871429
      ],
      "math_scores": [
        82.94,
        44.8,
        18.68,
        13.2,
        6.67,
        76.22,
        84.31,
        46.4,
        18.97,
        14.39,
        0.0,
        74.39,
        83.09,
        45.4,
        18.34,
        14.39,
        0.0,
        74.39
      ],
      "code_scores": [
        71.6,
        12.19,
        66.46,
        20.68,
        73.15,
        11.83,
        64.63,
        21.36,
        70.43,
        12.19,
        64.02,
        21.69
      ],
      "reasoning_scores": [
        67.88,
        33.84,
        42.25,
        42.96,
        68.62,
        32.83,
        42.228261,
        42.8,
        68.98,
        35.35,
        41.619565,
        42.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.07
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.04
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.03
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.54,
          "math_avg": -6.23,
          "code_avg": 2.59,
          "reasoning_avg": 2.64,
          "overall_avg": 0.39,
          "overall_efficiency": 0.007703,
          "general_efficiency": 0.050832,
          "math_efficiency": -0.1246,
          "code_efficiency": 0.051733,
          "reasoning_efficiency": 0.052846,
          "general_task_scores": [
            -4.97,
            11.0,
            0.1,
            4.04
          ],
          "math_task_scores": [
            3.47,
            -4.67,
            -7.38,
            -21.92,
            -4.45,
            -2.44
          ],
          "code_task_scores": [
            0.13,
            3.83,
            21.75,
            -15.36
          ],
          "reasoning_task_scores": [
            -0.97,
            -0.84,
            2.83,
            9.55
          ]
        },
        "vs_instruct": {
          "general_avg": -12.22,
          "math_avg": -12.53,
          "code_avg": 0.84,
          "reasoning_avg": -0.48,
          "overall_avg": -6.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.86,
            -26.47,
            -9.6,
            -8.95
          ],
          "math_task_scores": [
            -8.82,
            -32.27,
            -10.74,
            -26.96,
            -7.36,
            10.98
          ],
          "code_task_scores": [
            -2.98,
            0.96,
            9.55,
            -4.18
          ],
          "reasoning_task_scores": [
            -3.13,
            0.17,
            -0.42,
            1.47
          ]
        }
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 26,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 54.13,
      "math_avg": 30.87,
      "code_avg": 33.6,
      "reasoning_avg": 41.84,
      "overall_avg": 40.11,
      "overall_efficiency": -0.177272,
      "general_efficiency": 0.089991,
      "math_efficiency": -0.508089,
      "code_efficiency": -0.211927,
      "reasoning_efficiency": -0.079062,
      "general_scores": [
        73.1,
        37.445,
        58.32,
        48.265,
        72.98,
        37.19,
        57.61,
        48.6414286,
        71.47,
        38.0825,
        58.44,
        48.0321429
      ],
      "math_scores": [
        88.25,
        53.4,
        19.29,
        17.06,
        0.0,
        6.1,
        86.81,
        54.2,
        19.38,
        17.66,
        0.0,
        4.88,
        88.63,
        55.0,
        20.01,
        17.06,
        0.0,
        7.93
      ],
      "code_scores": [
        72.76,
        8.6,
        33.54,
        18.64,
        71.6,
        6.45,
        25.0,
        24.75,
        71.6,
        8.24,
        32.93,
        29.15
      ],
      "reasoning_scores": [
        61.45,
        30.3,
        30.858696,
        44.0,
        60.35,
        32.83,
        30.521739,
        44.24,
        61.07,
        30.81,
        30.413043,
        45.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.52
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.12
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.3
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.76
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.69,
          "math_avg": -15.17,
          "code_avg": -6.33,
          "reasoning_avg": -2.36,
          "overall_avg": -5.29,
          "overall_efficiency": -0.177272,
          "general_efficiency": 0.089991,
          "math_efficiency": -0.508089,
          "code_efficiency": -0.211927,
          "reasoning_efficiency": -0.079062,
          "general_task_scores": [
            4.21,
            2.08,
            0.38,
            4.07
          ],
          "math_task_scores": [
            7.92,
            4.0,
            -6.48,
            -18.65,
            -6.67,
            -71.14
          ],
          "code_task_scores": [
            0.39,
            -0.48,
            -12.8,
            -12.42
          ],
          "reasoning_task_scores": [
            -8.5,
            -3.54,
            -8.6,
            11.2
          ]
        },
        "vs_instruct": {
          "general_avg": -12.08,
          "math_avg": -21.47,
          "code_avg": -8.08,
          "reasoning_avg": -5.48,
          "overall_avg": -11.78,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            5.32,
            -35.39,
            -9.32,
            -8.92
          ],
          "math_task_scores": [
            -4.37,
            -23.6,
            -9.84,
            -23.69,
            -9.58,
            -57.72
          ],
          "code_task_scores": [
            -2.72,
            -3.35,
            -25.0,
            -1.24
          ],
          "reasoning_task_scores": [
            -10.66,
            -2.53,
            -11.85,
            3.12
          ]
        }
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 59.4,
      "math_avg": 49.96,
      "code_avg": 43.94,
      "reasoning_avg": 47.95,
      "overall_avg": 50.31,
      "overall_efficiency": 0.03272,
      "general_efficiency": 0.053045,
      "math_efficiency": 0.026115,
      "code_efficiency": 0.026701,
      "reasoning_efficiency": 0.025022,
      "general_scores": [
        72.29,
        47.58,
        64.9,
        51.665,
        72.84,
        48.9925,
        63.95,
        51.5114286,
        68.12,
        49.7,
        66.93,
        54.3107692
      ],
      "math_scores": [
        91.21,
        71.2,
        26.45,
        39.76,
        6.67,
        60.98,
        91.43,
        72.6,
        27.35,
        38.43,
        16.67,
        52.44,
        92.27,
        74.2,
        26.51,
        39.17,
        13.33,
        58.54
      ],
      "code_scores": [
        70.04,
        12.9,
        67.68,
        25.76,
        71.98,
        12.19,
        66.46,
        22.71,
        73.15,
        13.98,
        64.63,
        25.76
      ],
      "reasoning_scores": [
        68.7,
        35.35,
        35.880435,
        50.48,
        68.11,
        34.85,
        36.271739,
        50.8,
        70.3,
        35.35,
        37.706522,
        51.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.26
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.62
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.95,
          "math_avg": 3.92,
          "code_avg": 4.0,
          "reasoning_avg": 3.75,
          "overall_avg": 4.91,
          "overall_efficiency": 0.03272,
          "general_efficiency": 0.053045,
          "math_efficiency": 0.026115,
          "code_efficiency": 0.026701,
          "reasoning_efficiency": 0.025022,
          "general_task_scores": [
            2.77,
            13.27,
            7.52,
            8.26
          ],
          "math_task_scores": [
            11.66,
            22.47,
            0.73,
            3.21,
            5.55,
            -20.12
          ],
          "code_task_scores": [
            0.12,
            4.78,
            22.97,
            -11.86
          ],
          "reasoning_task_scores": [
            -0.42,
            0.33,
            -2.58,
            17.68
          ]
        },
        "vs_instruct": {
          "general_avg": -6.81,
          "math_avg": -2.38,
          "code_avg": 2.25,
          "reasoning_avg": 0.63,
          "overall_avg": -1.58,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.88,
            -24.2,
            -2.18,
            -4.73
          ],
          "math_task_scores": [
            -0.63,
            -5.13,
            -2.63,
            -1.83,
            2.64,
            -6.7
          ],
          "code_task_scores": [
            -2.99,
            1.91,
            10.77,
            -0.68
          ],
          "reasoning_task_scores": [
            -2.58,
            1.34,
            -5.83,
            9.6
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 61.09,
      "math_avg": 53.81,
      "code_avg": 43.35,
      "reasoning_avg": 47.73,
      "overall_avg": 51.49,
      "overall_efficiency": 0.304529,
      "general_efficiency": 0.482189,
      "math_efficiency": 0.388278,
      "code_efficiency": 0.170917,
      "reasoning_efficiency": 0.176734,
      "general_scores": [
        76.84,
        49.64,
        65.94,
        52.6142857,
        76.89,
        48.8,
        65.33,
        53.0192857,
        77.1,
        48.3725,
        65.49,
        53.0228571
      ],
      "math_scores": [
        91.51,
        72.8,
        27.46,
        38.28,
        13.33,
        81.71,
        91.74,
        73.2,
        27.35,
        39.91,
        13.33,
        80.49,
        91.74,
        72.2,
        26.94,
        36.35,
        13.33,
        76.83
      ],
      "code_scores": [
        73.93,
        12.9,
        64.63,
        25.42,
        73.93,
        12.19,
        65.85,
        19.66,
        73.93,
        12.19,
        65.24,
        20.34
      ],
      "reasoning_scores": [
        67.6,
        35.86,
        37.391304,
        50.64,
        69.85,
        34.34,
        37.108696,
        49.84,
        68.1,
        34.34,
        37.076087,
        50.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.59
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.25
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.68
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.43
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.52
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.64,
          "math_avg": 7.77,
          "code_avg": 3.42,
          "reasoning_avg": 3.53,
          "overall_avg": 6.09,
          "overall_efficiency": 0.304529,
          "general_efficiency": 0.482189,
          "math_efficiency": 0.388278,
          "code_efficiency": 0.170917,
          "reasoning_efficiency": 0.176734,
          "general_task_scores": [
            8.63,
            13.45,
            7.85,
            8.65
          ],
          "math_task_scores": [
            11.68,
            22.53,
            1.21,
            2.27,
            6.66,
            2.24
          ],
          "code_task_scores": [
            2.33,
            4.19,
            21.95,
            -14.79
          ],
          "reasoning_task_scores": [
            -0.94,
            0.0,
            -2.01,
            17.09
          ]
        },
        "vs_instruct": {
          "general_avg": -5.12,
          "math_avg": 1.47,
          "code_avg": 1.67,
          "reasoning_avg": 0.42,
          "overall_avg": -0.39,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.74,
            -24.02,
            -1.85,
            -4.34
          ],
          "math_task_scores": [
            -0.61,
            -5.07,
            -2.15,
            -2.77,
            3.75,
            15.66
          ],
          "code_task_scores": [
            -0.78,
            1.32,
            9.75,
            -3.61
          ],
          "reasoning_task_scores": [
            -3.1,
            1.01,
            -5.26,
            9.01
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 57.96,
      "math_avg": 51.2,
      "code_avg": 42.46,
      "reasoning_avg": 41.37,
      "overall_avg": 48.25,
      "overall_efficiency": 0.002835,
      "general_efficiency": 0.006497,
      "math_efficiency": 0.005141,
      "code_efficiency": 0.002516,
      "reasoning_efficiency": -0.002816,
      "general_scores": [
        87.79,
        35.41,
        60.46,
        48.2128571,
        87.38,
        36.31,
        60.17,
        47.69,
        87.88,
        37.18,
        59.84,
        47.2535714
      ],
      "math_scores": [
        90.75,
        74.0,
        26.78,
        38.13,
        13.33,
        65.85,
        91.05,
        74.0,
        27.12,
        38.58,
        10.0,
        65.24,
        91.13,
        73.6,
        27.87,
        37.69,
        10.0,
        66.46
      ],
      "code_scores": [
        73.54,
        9.68,
        62.2,
        25.76,
        72.76,
        11.47,
        64.02,
        22.71,
        73.54,
        9.32,
        62.8,
        21.69
      ],
      "reasoning_scores": [
        63.97,
        31.82,
        22.065217,
        47.76,
        64.9,
        32.32,
        22.01087,
        46.88,
        64.72,
        31.31,
        21.913043,
        46.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.85
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.16
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.01
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.52,
          "math_avg": 5.16,
          "code_avg": 2.53,
          "reasoning_avg": -2.83,
          "overall_avg": 2.84,
          "overall_efficiency": 0.002835,
          "general_efficiency": 0.006497,
          "math_efficiency": 0.005141,
          "code_efficiency": 0.002516,
          "reasoning_efficiency": -0.002816,
          "general_task_scores": [
            19.37,
            0.81,
            2.42,
            3.48
          ],
          "math_task_scores": [
            11.0,
            23.67,
            1.22,
            2.22,
            4.44,
            -11.59
          ],
          "code_task_scores": [
            1.68,
            1.92,
            19.72,
            -13.21
          ],
          "reasoning_task_scores": [
            -4.93,
            -3.03,
            -17.2,
            13.87
          ]
        },
        "vs_instruct": {
          "general_avg": -8.24,
          "math_avg": -1.14,
          "code_avg": 0.77,
          "reasoning_avg": -5.94,
          "overall_avg": -3.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            20.48,
            -36.66,
            -7.28,
            -9.51
          ],
          "math_task_scores": [
            -1.29,
            -3.93,
            -2.14,
            -2.82,
            1.53,
            1.83
          ],
          "code_task_scores": [
            -1.43,
            -0.95,
            7.52,
            -2.03
          ],
          "reasoning_task_scores": [
            -7.09,
            -2.02,
            -20.45,
            5.79
          ]
        }
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 53.39,
      "math_avg": 59.27,
      "code_avg": 41.2,
      "reasoning_avg": 41.72,
      "overall_avg": 48.9,
      "overall_efficiency": 0.011296,
      "general_efficiency": 0.006293,
      "math_efficiency": 0.0428,
      "code_efficiency": 0.004109,
      "reasoning_efficiency": -0.008015,
      "general_scores": [
        82.23,
        28.82,
        55.87,
        46.6371429
      ],
      "math_scores": [
        92.12,
        92.0,
        45.37,
        60.24,
        34.1675,
        31.71
      ],
      "code_scores": [
        49.42,
        2.87,
        30.49,
        82.03
      ],
      "reasoning_scores": [
        62.11,
        24.75,
        35.141304,
        44.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.87
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.37
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.71
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.87
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.14
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 1.94,
          "math_avg": 13.23,
          "code_avg": 1.27,
          "reasoning_avg": -2.48,
          "overall_avg": 3.49,
          "overall_efficiency": 0.011296,
          "general_efficiency": 0.006293,
          "math_efficiency": 0.0428,
          "code_efficiency": 0.004109,
          "reasoning_efficiency": -0.008015,
          "general_task_scores": [
            13.92,
            -6.67,
            -1.87,
            2.4
          ],
          "math_task_scores": [
            12.14,
            41.8,
            19.33,
            24.33,
            27.5,
            -45.73
          ],
          "code_task_scores": [
            -22.18,
            -5.37,
            -12.8,
            45.43
          ],
          "reasoning_task_scores": [
            -7.35,
            -10.1,
            -4.06,
            11.6
          ]
        },
        "vs_instruct": {
          "general_avg": -12.82,
          "math_avg": 6.93,
          "code_avg": -0.48,
          "reasoning_avg": -5.6,
          "overall_avg": -2.99,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            15.03,
            -44.14,
            -11.57,
            -10.59
          ],
          "math_task_scores": [
            -0.15,
            14.2,
            15.97,
            19.29,
            24.59,
            -32.31
          ],
          "code_task_scores": [
            -25.29,
            -8.24,
            -25.0,
            56.61
          ],
          "reasoning_task_scores": [
            -9.51,
            -9.09,
            -7.31,
            3.52
          ]
        }
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 56.3,
      "math_avg": 42.06,
      "code_avg": 41.92,
      "reasoning_avg": 43.66,
      "overall_avg": 45.98,
      "overall_efficiency": 0.003428,
      "general_efficiency": 0.028651,
      "math_efficiency": -0.023477,
      "code_efficiency": 0.011719,
      "reasoning_efficiency": -0.003182,
      "general_scores": [
        76.22,
        44.9875,
        56.38,
        50.0785714,
        74.61,
        42.615,
        56.6,
        48.8342857,
        77.1,
        44.3375,
        55.22,
        48.63714
      ],
      "math_scores": [
        88.1,
        59.2,
        21.3,
        25.07,
        0.0,
        56.1,
        88.1,
        60.4,
        22.52,
        21.36,
        3.33,
        55.37,
        85.6,
        60.6,
        21.25,
        23.74,
        8.33,
        56.71
      ],
      "code_scores": [
        71.98,
        10.75,
        60.98,
        30.51,
        73.54,
        8.96,
        57.93,
        25.42,
        71.98,
        11.83,
        53.05,
        26.1
      ],
      "reasoning_scores": [
        65.72,
        34.34,
        31.782609,
        43.44,
        66.5,
        29.29,
        34.163043,
        42.88,
        66.8,
        31.31,
        33.5109,
        44.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.27
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.69
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.89
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 56.06
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.51
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.34
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.86,
          "math_avg": -3.98,
          "code_avg": 1.99,
          "reasoning_avg": -0.54,
          "overall_avg": 0.58,
          "overall_efficiency": 0.003428,
          "general_efficiency": 0.028651,
          "math_efficiency": -0.023477,
          "code_efficiency": 0.011719,
          "reasoning_efficiency": -0.003182,
          "general_task_scores": [
            7.67,
            8.49,
            -1.67,
            4.94
          ],
          "math_task_scores": [
            7.29,
            9.87,
            -4.35,
            -12.52,
            -2.78,
            -21.38
          ],
          "code_task_scores": [
            0.9,
            2.27,
            14.03,
            -9.26
          ],
          "reasoning_task_scores": [
            -3.12,
            -3.2,
            -6.05,
            10.21
          ]
        },
        "vs_instruct": {
          "general_avg": -9.91,
          "math_avg": -10.28,
          "code_avg": 0.24,
          "reasoning_avg": -3.66,
          "overall_avg": -5.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.78,
            -28.98,
            -11.37,
            -8.05
          ],
          "math_task_scores": [
            -5.0,
            -17.73,
            -7.71,
            -17.56,
            -5.69,
            -7.96
          ],
          "code_task_scores": [
            -2.21,
            -0.6,
            1.83,
            1.92
          ],
          "reasoning_task_scores": [
            -5.28,
            -2.19,
            -9.3,
            2.13
          ]
        }
      },
      "affiliation": "Ruben Roy",
      "year": "2025",
      "size": "170k",
      "size_precise": "169527",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 60.81,
      "math_avg": 47.54,
      "code_avg": 41.4,
      "reasoning_avg": 48.02,
      "overall_avg": 49.44,
      "overall_efficiency": 0.020187,
      "general_efficiency": 0.046802,
      "math_efficiency": 0.007518,
      "code_efficiency": 0.007319,
      "reasoning_efficiency": 0.019106,
      "general_scores": [
        78.55,
        52.4675,
        64.68,
        50.4071429,
        76.01,
        53.2375,
        64.97,
        49.8278571,
        75.77,
        52.4825,
        65.15,
        46.1257143
      ],
      "math_scores": [
        89.76,
        64.8,
        23.4,
        31.31,
        10.0,
        66.46,
        89.39,
        63.2,
        24.77,
        30.71,
        6.67,
        70.12,
        89.61,
        63.0,
        24.59,
        27.6,
        13.33,
        67.07
      ],
      "code_scores": [
        73.54,
        13.62,
        60.98,
        20.68,
        75.88,
        13.62,
        54.88,
        20.0,
        73.54,
        12.9,
        56.1,
        21.02
      ],
      "reasoning_scores": [
        68.52,
        35.86,
        41.097826,
        48.88,
        67.72,
        33.84,
        40.532609,
        48.56,
        67.55,
        33.33,
        40.902174,
        49.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.79
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.88
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.38
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.84
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.36,
          "math_avg": 1.5,
          "code_avg": 1.46,
          "reasoning_avg": 3.82,
          "overall_avg": 4.04,
          "overall_efficiency": 0.020187,
          "general_efficiency": 0.046802,
          "math_efficiency": 0.007518,
          "code_efficiency": 0.007319,
          "reasoning_efficiency": 0.019106,
          "general_task_scores": [
            8.47,
            17.24,
            7.19,
            4.55
          ],
          "math_task_scores": [
            9.61,
            13.47,
            -1.79,
            -6.04,
            3.33,
            -9.56
          ],
          "code_task_scores": [
            2.72,
            5.14,
            14.03,
            -16.03
          ],
          "reasoning_task_scores": [
            -1.53,
            -0.51,
            1.64,
            15.68
          ]
        },
        "vs_instruct": {
          "general_avg": -5.4,
          "math_avg": -4.79,
          "code_avg": -0.29,
          "reasoning_avg": 0.7,
          "overall_avg": -2.44,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            9.58,
            -20.23,
            -2.51,
            -8.44
          ],
          "math_task_scores": [
            -2.68,
            -14.13,
            -5.15,
            -11.08,
            0.42,
            3.86
          ],
          "code_task_scores": [
            -0.39,
            2.27,
            1.83,
            -4.85
          ],
          "reasoning_task_scores": [
            -3.69,
            0.5,
            -1.61,
            7.6
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 57.37,
      "math_avg": 44.89,
      "code_avg": 41.37,
      "reasoning_avg": 40.3,
      "overall_avg": 45.98,
      "overall_efficiency": 0.000987,
      "general_efficiency": 0.010116,
      "math_efficiency": -0.001958,
      "code_efficiency": 0.002451,
      "reasoning_efficiency": -0.006661,
      "general_scores": [
        76.01,
        43.0725,
        62.67,
        47.535,
        76.01,
        43.3675,
        63.54,
        47.3207143,
        75.77,
        42.605,
        63.07,
        47.4235714
      ],
      "math_scores": [
        89.46,
        60.8,
        20.51,
        25.96,
        6.67,
        70.12,
        90.45,
        58.2,
        20.17,
        24.18,
        0.0,
        70.12,
        90.6,
        61.0,
        20.44,
        25.67,
        6.67,
        67.07
      ],
      "code_scores": [
        69.26,
        14.34,
        54.88,
        25.76,
        69.26,
        14.7,
        54.88,
        26.78,
        67.32,
        13.98,
        56.1,
        29.15
      ],
      "reasoning_scores": [
        40.46,
        30.81,
        44.23913,
        46.08,
        40.35,
        29.8,
        43.956522,
        46.64,
        40.22,
        30.81,
        44.532609,
        45.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.93
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.17
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.1
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.61
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.34
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.92,
          "math_avg": -1.15,
          "code_avg": 1.44,
          "reasoning_avg": -3.9,
          "overall_avg": 0.58,
          "overall_efficiency": 0.000987,
          "general_efficiency": 0.010116,
          "math_efficiency": -0.001958,
          "code_efficiency": 0.002451,
          "reasoning_efficiency": -0.006661,
          "general_task_scores": [
            7.62,
            7.52,
            5.35,
            3.19
          ],
          "math_task_scores": [
            10.19,
            9.8,
            -5.67,
            -10.64,
            -2.22,
            -8.34
          ],
          "code_task_scores": [
            -2.99,
            6.1,
            12.0,
            -9.37
          ],
          "reasoning_task_scores": [
            -29.12,
            -4.38,
            5.04,
            12.85
          ]
        },
        "vs_instruct": {
          "general_avg": -8.84,
          "math_avg": -7.44,
          "code_avg": -0.32,
          "reasoning_avg": -7.02,
          "overall_avg": -5.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.73,
            -29.95,
            -4.35,
            -9.8
          ],
          "math_task_scores": [
            -2.1,
            -17.8,
            -9.03,
            -15.68,
            -5.13,
            5.08
          ],
          "code_task_scores": [
            -6.1,
            3.23,
            -0.2,
            1.81
          ],
          "reasoning_task_scores": [
            -31.28,
            -3.37,
            1.79,
            4.77
          ]
        }
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 57.18,
      "math_avg": 40.61,
      "code_avg": 43.24,
      "reasoning_avg": 43.51,
      "overall_avg": 46.14,
      "overall_efficiency": 0.031115,
      "general_efficiency": 0.243162,
      "math_efficiency": -0.230205,
      "code_efficiency": 0.140456,
      "reasoning_efficiency": -0.028954,
      "general_scores": [
        75.66,
        48.0325,
        60.95,
        43.2571429,
        75.61,
        47.8875,
        60.81,
        43.1,
        74.95,
        48.885,
        61.11,
        45.8807143
      ],
      "math_scores": [
        88.86,
        59.6,
        19.67,
        27.0,
        6.67,
        53.66,
        89.99,
        60.2,
        18.99,
        25.37,
        6.67,
        50.0,
        89.54,
        60.0,
        19.24,
        24.18,
        3.33,
        28.05
      ],
      "code_scores": [
        72.76,
        13.98,
        64.63,
        29.15,
        73.15,
        12.9,
        58.54,
        25.08,
        73.15,
        11.83,
        54.27,
        29.49
      ],
      "reasoning_scores": [
        70.71,
        26.26,
        31.369565,
        46.4,
        70.27,
        22.73,
        31.293478,
        47.28,
        70.68,
        26.77,
        31.934783,
        46.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.46
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.3
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.9
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.15
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.53
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.73,
          "math_avg": -5.43,
          "code_avg": 3.31,
          "reasoning_avg": -0.68,
          "overall_avg": 0.73,
          "overall_efficiency": 0.031115,
          "general_efficiency": 0.243162,
          "math_efficiency": -0.230205,
          "code_efficiency": 0.140456,
          "reasoning_efficiency": -0.028954,
          "general_task_scores": [
            7.1,
            12.78,
            3.22,
            -0.16
          ],
          "math_task_scores": [
            9.48,
            9.73,
            -6.74,
            -10.39,
            -1.11,
            -33.54
          ],
          "code_task_scores": [
            1.42,
            4.66,
            15.86,
            -8.69
          ],
          "reasoning_task_scores": [
            1.09,
            -9.6,
            -7.67,
            13.44
          ]
        },
        "vs_instruct": {
          "general_avg": -9.03,
          "math_avg": -11.73,
          "code_avg": 1.56,
          "reasoning_avg": -3.8,
          "overall_avg": -5.75,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.21,
            -24.69,
            -6.48,
            -13.15
          ],
          "math_task_scores": [
            -2.81,
            -17.87,
            -10.1,
            -15.43,
            -4.02,
            -20.12
          ],
          "code_task_scores": [
            -1.69,
            1.79,
            3.66,
            2.49
          ],
          "reasoning_task_scores": [
            -1.07,
            -8.59,
            -10.92,
            5.36
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 57.61,
      "math_avg": 44.54,
      "code_avg": 42.88,
      "reasoning_avg": 46.42,
      "overall_avg": 47.86,
      "overall_efficiency": 0.002536,
      "general_efficiency": 0.006361,
      "math_efficiency": -0.001546,
      "code_efficiency": 0.003041,
      "reasoning_efficiency": 0.002288,
      "general_scores": [
        72.66,
        49.795,
        59.75,
        49.3178571,
        70.17,
        50.0625,
        61.25,
        49.2214286,
        70.97,
        48.6925,
        59.75,
        49.7342857
      ],
      "math_scores": [
        83.4,
        52.8,
        21.66,
        20.33,
        13.33,
        78.66,
        80.82,
        54.4,
        22.06,
        19.29,
        10.0,
        78.05,
        81.5,
        51.2,
        21.0,
        19.73,
        16.67,
        76.83
      ],
      "code_scores": [
        68.48,
        12.19,
        67.68,
        25.08,
        67.7,
        13.26,
        68.9,
        21.69,
        69.26,
        12.19,
        69.51,
        18.64
      ],
      "reasoning_scores": [
        66.36,
        34.85,
        39.804348,
        45.44,
        65.98,
        35.35,
        37.967391,
        45.84,
        66.31,
        35.35,
        37.902174,
        45.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.42
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.91
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.57
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.85
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.55
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.7
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.8
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.56
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.17,
          "math_avg": -1.5,
          "code_avg": 2.95,
          "reasoning_avg": 2.22,
          "overall_avg": 2.46,
          "overall_efficiency": 0.002536,
          "general_efficiency": 0.006361,
          "math_efficiency": -0.001546,
          "code_efficiency": 0.003041,
          "reasoning_efficiency": 0.002288,
          "general_task_scores": [
            2.96,
            14.03,
            2.51,
            5.18
          ],
          "math_task_scores": [
            1.93,
            2.6,
            -4.47,
            -16.13,
            6.66,
            0.41
          ],
          "code_task_scores": [
            -3.12,
            4.31,
            25.41,
            -14.8
          ],
          "reasoning_task_scores": [
            -3.24,
            0.33,
            -0.64,
            12.43
          ]
        },
        "vs_instruct": {
          "general_avg": -8.59,
          "math_avg": -7.8,
          "code_avg": 1.2,
          "reasoning_avg": -0.9,
          "overall_avg": -4.02,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            4.07,
            -23.44,
            -7.19,
            -7.81
          ],
          "math_task_scores": [
            -10.36,
            -25.0,
            -7.83,
            -21.17,
            3.75,
            13.83
          ],
          "code_task_scores": [
            -6.23,
            1.44,
            13.21,
            -3.62
          ],
          "reasoning_task_scores": [
            -5.4,
            1.34,
            -3.89,
            4.35
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 54.29,
      "math_avg": 29.34,
      "code_avg": 33.65,
      "reasoning_avg": 47.16,
      "overall_avg": 41.11,
      "overall_efficiency": -0.004379,
      "general_efficiency": 0.002905,
      "math_efficiency": -0.017041,
      "code_efficiency": -0.00641,
      "reasoning_efficiency": 0.003028,
      "general_scores": [
        67.37,
        46.36,
        58.91,
        44.1564286,
        68.11,
        46.2175,
        58.86,
        44.3985714,
        68.24,
        44.9275,
        59.13,
        44.8185714
      ],
      "math_scores": [
        81.12,
        45.6,
        17.73,
        13.5,
        0.0,
        16.46,
        81.2,
        45.8,
        16.76,
        12.76,
        3.33,
        17.07,
        81.73,
        45.0,
        17.71,
        13.2,
        3.33,
        15.85
      ],
      "code_scores": [
        66.54,
        9.32,
        33.54,
        23.05,
        67.7,
        11.11,
        38.41,
        24.07,
        66.54,
        10.75,
        31.1,
        21.69
      ],
      "reasoning_scores": [
        60.83,
        33.33,
        48.076087,
        46.0,
        60.9,
        32.32,
        48.51087,
        45.52,
        62.62,
        33.33,
        48.771739,
        45.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.46
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.4
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.15
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.35
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.45
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.85,
          "math_avg": -16.7,
          "code_avg": -6.28,
          "reasoning_avg": 2.97,
          "overall_avg": -4.29,
          "overall_efficiency": -0.004379,
          "general_efficiency": 0.002905,
          "math_efficiency": -0.017041,
          "code_efficiency": -0.00641,
          "reasoning_efficiency": 0.003028,
          "general_task_scores": [
            -0.4,
            10.35,
            1.23,
            0.22
          ],
          "math_task_scores": [
            1.37,
            -4.73,
            -8.64,
            -22.76,
            -4.45,
            -60.98
          ],
          "code_task_scores": [
            -4.67,
            2.15,
            -8.94,
            -13.66
          ],
          "reasoning_task_scores": [
            -8.01,
            -1.86,
            9.25,
            12.48
          ]
        },
        "vs_instruct": {
          "general_avg": -11.92,
          "math_avg": -23.0,
          "code_avg": -8.03,
          "reasoning_avg": -0.15,
          "overall_avg": -10.77,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.71,
            -27.12,
            -8.47,
            -12.77
          ],
          "math_task_scores": [
            -10.92,
            -32.33,
            -12.0,
            -27.8,
            -7.36,
            -47.56
          ],
          "code_task_scores": [
            -7.78,
            -0.72,
            -21.14,
            -2.48
          ],
          "reasoning_task_scores": [
            -10.17,
            -0.85,
            6.0,
            4.4
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 37,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 60.23,
      "math_avg": 43.16,
      "code_avg": 42.29,
      "reasoning_avg": 49.59,
      "overall_avg": 48.82,
      "overall_efficiency": 0.068282,
      "general_efficiency": 0.175629,
      "math_efficiency": -0.057611,
      "code_efficiency": 0.047167,
      "reasoning_efficiency": 0.107942,
      "general_scores": [
        66.54,
        51.715,
        67.69,
        53.7535714,
        66.66,
        53.0675,
        67.8,
        53.9378571,
        66.48,
        53.955,
        67.49,
        53.6221429
      ],
      "math_scores": [
        89.84,
        73.4,
        26.74,
        38.58,
        20.0,
        14.63,
        89.99,
        72.6,
        27.01,
        38.43,
        13.33,
        15.24,
        90.75,
        72.2,
        26.51,
        38.72,
        10.0,
        18.9
      ],
      "code_scores": [
        73.15,
        14.34,
        57.32,
        24.07,
        73.54,
        15.05,
        60.37,
        21.02,
        72.76,
        13.26,
        58.54,
        24.07
      ],
      "reasoning_scores": [
        69.49,
        37.88,
        42.576087,
        48.96,
        69.24,
        38.89,
        41.304348,
        48.4,
        69.3,
        37.88,
        41.934783,
        49.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.91
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.75
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.58
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 16.26
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.22
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.74
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.94
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.78,
          "math_avg": -2.88,
          "code_avg": 2.36,
          "reasoning_avg": 5.4,
          "overall_avg": 3.41,
          "overall_efficiency": 0.068282,
          "general_efficiency": 0.175629,
          "math_efficiency": -0.057611,
          "code_efficiency": 0.047167,
          "reasoning_efficiency": 0.107942,
          "general_task_scores": [
            -1.75,
            17.42,
            9.92,
            9.53
          ],
          "math_task_scores": [
            10.21,
            22.53,
            0.71,
            2.67,
            7.77,
            -61.18
          ],
          "code_task_scores": [
            1.55,
            5.98,
            15.45,
            -13.55
          ],
          "reasoning_task_scores": [
            -0.12,
            3.37,
            2.74,
            15.6
          ]
        },
        "vs_instruct": {
          "general_avg": -5.98,
          "math_avg": -9.18,
          "code_avg": 0.61,
          "reasoning_avg": 2.28,
          "overall_avg": -3.07,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.64,
            -20.05,
            0.22,
            -3.46
          ],
          "math_task_scores": [
            -2.08,
            -5.07,
            -2.65,
            -2.37,
            4.86,
            -47.76
          ],
          "code_task_scores": [
            -1.56,
            3.11,
            3.25,
            -2.37
          ],
          "reasoning_task_scores": [
            -2.28,
            4.38,
            -0.51,
            7.52
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 38,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 55.57,
      "math_avg": 40.24,
      "code_avg": 41.53,
      "reasoning_avg": 47.54,
      "overall_avg": 46.22,
      "overall_efficiency": 0.000912,
      "general_efficiency": 0.004614,
      "math_efficiency": -0.006486,
      "code_efficiency": 0.001785,
      "reasoning_efficiency": 0.003734,
      "general_scores": [
        66.39,
        49.585,
        57.63,
        49.16,
        66.78,
        49.68,
        57.88,
        49.4557143,
        65.86,
        49.875,
        55.96,
        48.5714286
      ],
      "math_scores": [
        87.57,
        51.0,
        18.81,
        18.1,
        0.0,
        55.49,
        87.57,
        50.6,
        18.41,
        20.77,
        3.33,
        65.24,
        88.48,
        52.8,
        18.13,
        18.55,
        0.0,
        69.51
      ],
      "code_scores": [
        71.21,
        7.53,
        55.49,
        23.39,
        68.87,
        12.9,
        67.07,
        21.36,
        74.71,
        12.19,
        58.54,
        25.08
      ],
      "reasoning_scores": [
        67.4,
        34.34,
        41.0109,
        46.88,
        65.8,
        37.37,
        42.076087,
        47.92,
        66.39,
        32.83,
        41.130435,
        47.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.87
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.12,
          "math_avg": -5.8,
          "code_avg": 1.6,
          "reasoning_avg": 3.34,
          "overall_avg": 0.82,
          "overall_efficiency": 0.000912,
          "general_efficiency": 0.004614,
          "math_efficiency": -0.006486,
          "code_efficiency": 0.001785,
          "reasoning_efficiency": 0.003734,
          "general_task_scores": [
            -1.97,
            14.22,
            -0.58,
            4.82
          ],
          "math_task_scores": [
            7.89,
            1.27,
            -7.59,
            -16.77,
            -5.56,
            -14.03
          ],
          "code_task_scores": [
            0.0,
            2.63,
            17.08,
            -13.32
          ],
          "reasoning_task_scores": [
            -2.93,
            0.0,
            2.21,
            14.08
          ]
        },
        "vs_instruct": {
          "general_avg": -10.64,
          "math_avg": -12.1,
          "code_avg": -0.15,
          "reasoning_avg": 0.22,
          "overall_avg": -5.67,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.86,
            -23.25,
            -10.28,
            -8.17
          ],
          "math_task_scores": [
            -4.4,
            -26.33,
            -10.95,
            -21.81,
            -8.47,
            -0.61
          ],
          "code_task_scores": [
            -3.11,
            -0.24,
            4.88,
            -2.14
          ],
          "reasoning_task_scores": [
            -5.09,
            1.01,
            -1.04,
            6.0
          ]
        }
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 62.48,
      "math_avg": 54.15,
      "code_avg": 44.73,
      "reasoning_avg": 50.54,
      "overall_avg": 52.98,
      "overall_efficiency": 0.037851,
      "general_efficiency": 0.055176,
      "math_efficiency": 0.040559,
      "code_efficiency": 0.02397,
      "reasoning_efficiency": 0.031695,
      "general_scores": [
        74.58,
        51.7625,
        68.76,
        53.3578571,
        74.64,
        52.635,
        68.91,
        54.1521429,
        75.31,
        52.88,
        68.79,
        54.0014286
      ],
      "math_scores": [
        92.42,
        74.6,
        28.27,
        39.17,
        13.33,
        79.27,
        91.36,
        77.0,
        27.46,
        38.58,
        6.67,
        79.27,
        91.51,
        77.4,
        28.18,
        38.28,
        13.33,
        78.66
      ],
      "code_scores": [
        75.1,
        13.98,
        67.68,
        23.73,
        75.1,
        14.34,
        69.51,
        17.63,
        75.1,
        15.05,
        69.51,
        20.0
      ],
      "reasoning_scores": [
        69.07,
        39.9,
        42.130435,
        50.4,
        69.06,
        38.89,
        42.51087,
        51.52,
        68.98,
        40.4,
        42.630435,
        50.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.97
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.68
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 75.1
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.46
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.9
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 11.04,
          "math_avg": 8.11,
          "code_avg": 4.8,
          "reasoning_avg": 6.34,
          "overall_avg": 7.57,
          "overall_efficiency": 0.037851,
          "general_efficiency": 0.055176,
          "math_efficiency": 0.040559,
          "code_efficiency": 0.02397,
          "reasoning_efficiency": 0.031695,
          "general_task_scores": [
            6.53,
            16.94,
            11.08,
            9.6
          ],
          "math_task_scores": [
            11.78,
            26.13,
            1.93,
            2.77,
            4.44,
            1.63
          ],
          "code_task_scores": [
            3.5,
            6.22,
            25.61,
            -16.15
          ],
          "reasoning_task_scores": [
            -0.42,
            4.88,
            3.22,
            17.68
          ]
        },
        "vs_instruct": {
          "general_avg": -3.73,
          "math_avg": 1.82,
          "code_avg": 3.04,
          "reasoning_avg": 3.22,
          "overall_avg": 1.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            7.64,
            -20.53,
            1.38,
            -3.39
          ],
          "math_task_scores": [
            -0.51,
            -1.47,
            -1.43,
            -2.27,
            1.53,
            15.05
          ],
          "code_task_scores": [
            0.39,
            3.35,
            13.41,
            -4.97
          ],
          "reasoning_task_scores": [
            -2.58,
            5.89,
            -0.03,
            9.6
          ]
        }
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 56.41,
      "math_avg": 44.85,
      "code_avg": 40.79,
      "reasoning_avg": 44.64,
      "overall_avg": 46.67,
      "overall_efficiency": 0.001415,
      "general_efficiency": 0.00554,
      "math_efficiency": -0.001331,
      "code_efficiency": 0.000956,
      "reasoning_efficiency": 0.000497,
      "general_scores": [
        64.79,
        48.38,
        61.54,
        52.225,
        65.65,
        46.18,
        61.32,
        52.2571429,
        63.72,
        47.225,
        61.18,
        52.4428571
      ],
      "math_scores": [
        89.46,
        69.4,
        24.16,
        24.63,
        10.0,
        56.1,
        88.25,
        69.2,
        23.26,
        27.15,
        6.67,
        53.66,
        89.23,
        66.8,
        25.23,
        25.52,
        0.0,
        58.54
      ],
      "code_scores": [
        66.54,
        7.89,
        50.0,
        18.98,
        67.32,
        11.83,
        52.44,
        32.2,
        69.26,
        9.32,
        60.98,
        42.71
      ],
      "reasoning_scores": [
        61.3,
        35.35,
        38.054348,
        41.28,
        62.08,
        39.9,
        37.902174,
        41.36,
        59.93,
        34.85,
        38.913043,
        44.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.72
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 56.1
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.47
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.96,
          "math_avg": -1.19,
          "code_avg": 0.86,
          "reasoning_avg": 0.45,
          "overall_avg": 1.27,
          "overall_efficiency": 0.001415,
          "general_efficiency": 0.00554,
          "math_efficiency": -0.001331,
          "code_efficiency": 0.000956,
          "reasoning_efficiency": 0.000497,
          "general_task_scores": [
            -3.59,
            11.77,
            3.61,
            8.07
          ],
          "math_task_scores": [
            9.0,
            18.27,
            -1.82,
            -10.14,
            -1.11,
            -21.34
          ],
          "code_task_scores": [
            -3.89,
            1.44,
            11.18,
            -5.3
          ],
          "reasoning_task_scores": [
            -8.36,
            1.85,
            -0.91,
            9.2
          ]
        },
        "vs_instruct": {
          "general_avg": -9.8,
          "math_avg": -7.49,
          "code_avg": -0.89,
          "reasoning_avg": -2.67,
          "overall_avg": -5.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.48,
            -25.7,
            -6.09,
            -4.92
          ],
          "math_task_scores": [
            -3.29,
            -9.33,
            -5.18,
            -15.18,
            -4.02,
            -7.92
          ],
          "code_task_scores": [
            -7.0,
            -1.43,
            -1.02,
            5.88
          ],
          "reasoning_task_scores": [
            -10.52,
            2.86,
            -4.16,
            1.12
          ]
        }
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 60.62,
      "math_avg": 51.61,
      "code_avg": 40.8,
      "reasoning_avg": 48.54,
      "overall_avg": 50.4,
      "overall_efficiency": 0.083348,
      "general_efficiency": 0.153262,
      "math_efficiency": 0.093066,
      "code_efficiency": 0.014512,
      "reasoning_efficiency": 0.072549,
      "general_scores": [
        70.54,
        52.4775,
        67.57,
        52.005,
        70.99,
        52.055,
        67.62,
        51.8535714,
        69.88,
        52.395,
        67.7,
        52.3978571
      ],
      "math_scores": [
        89.99,
        64.4,
        24.12,
        39.17,
        13.33,
        79.27,
        89.76,
        65.8,
        24.46,
        39.32,
        10.0,
        78.66,
        89.99,
        67.4,
        23.89,
        39.61,
        10.0,
        79.88
      ],
      "code_scores": [
        72.37,
        14.7,
        59.15,
        14.92,
        71.98,
        12.19,
        60.37,
        18.64,
        73.93,
        13.98,
        56.71,
        20.68
      ],
      "reasoning_scores": [
        70.05,
        31.82,
        43.01087,
        48.72,
        69.93,
        30.81,
        42.826087,
        49.28,
        69.87,
        32.83,
        43.923913,
        49.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.91
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.16
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.27
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.62
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.74
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.95
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.18,
          "math_avg": 5.57,
          "code_avg": 0.87,
          "reasoning_avg": 4.35,
          "overall_avg": 4.99,
          "overall_efficiency": 0.083348,
          "general_efficiency": 0.153262,
          "math_efficiency": 0.093066,
          "code_efficiency": 0.014512,
          "reasoning_efficiency": 0.072549,
          "general_task_scores": [
            2.16,
            16.82,
            9.89,
            7.85
          ],
          "math_task_scores": [
            9.93,
            15.67,
            -1.88,
            3.46,
            4.44,
            1.83
          ],
          "code_task_scores": [
            1.16,
            5.38,
            15.45,
            -18.52
          ],
          "reasoning_task_scores": [
            0.49,
            -3.03,
            4.05,
            15.87
          ]
        },
        "vs_instruct": {
          "general_avg": -5.58,
          "math_avg": -0.72,
          "code_avg": -0.88,
          "reasoning_avg": 1.23,
          "overall_avg": -1.49,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.27,
            -20.65,
            0.19,
            -5.14
          ],
          "math_task_scores": [
            -2.36,
            -11.93,
            -5.24,
            -1.58,
            1.53,
            15.25
          ],
          "code_task_scores": [
            -1.95,
            2.51,
            3.25,
            -7.34
          ],
          "reasoning_task_scores": [
            -1.67,
            -2.02,
            0.8,
            7.79
          ]
        }
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 42,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 57.27,
      "math_avg": 42.79,
      "code_avg": 43.8,
      "reasoning_avg": 48.15,
      "overall_avg": 48.0,
      "overall_efficiency": 0.005614,
      "general_efficiency": 0.012588,
      "math_efficiency": -0.007016,
      "code_efficiency": 0.008346,
      "reasoning_efficiency": 0.008539,
      "general_scores": [
        66.11,
        53.1125,
        61.95,
        46.8838462,
        66.31,
        52.2825,
        61.93,
        48.51286,
        65.68,
        53.95,
        61.78,
        48.7771429
      ],
      "math_scores": [
        86.35,
        50.6,
        18.88,
        17.21,
        3.33,
        80.49,
        87.26,
        51.8,
        20.1,
        16.02,
        1.665,
        78.05,
        86.35,
        48.4,
        19.67,
        16.91,
        6.67,
        80.49
      ],
      "code_scores": [
        72.76,
        11.83,
        67.68,
        21.69,
        73.54,
        10.75,
        67.07,
        22.03,
        73.54,
        10.75,
        69.51,
        24.41
      ],
      "reasoning_scores": [
        69.31,
        33.33,
        43.934783,
        44.88,
        68.91,
        34.34,
        44.0435,
        45.36,
        68.6,
        34.85,
        44.413043,
        45.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.89
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.89
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.68
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.94
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.13
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.83,
          "math_avg": -3.25,
          "code_avg": 3.86,
          "reasoning_avg": 3.95,
          "overall_avg": 2.6,
          "overall_efficiency": 0.005614,
          "general_efficiency": 0.012588,
          "math_efficiency": -0.007016,
          "code_efficiency": 0.008346,
          "reasoning_efficiency": 0.008539,
          "general_task_scores": [
            -2.28,
            17.63,
            4.15,
            3.82
          ],
          "math_task_scores": [
            6.67,
            0.07,
            -6.49,
            -19.2,
            -2.78,
            2.24
          ],
          "code_task_scores": [
            1.68,
            2.87,
            24.8,
            -13.89
          ],
          "reasoning_task_scores": [
            -0.52,
            -0.68,
            4.93,
            12.08
          ]
        },
        "vs_instruct": {
          "general_avg": -8.94,
          "math_avg": -9.55,
          "code_avg": 2.11,
          "reasoning_avg": 0.83,
          "overall_avg": -3.88,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.17,
            -19.84,
            -5.55,
            -9.17
          ],
          "math_task_scores": [
            -5.62,
            -27.53,
            -9.85,
            -24.24,
            -5.69,
            15.66
          ],
          "code_task_scores": [
            -1.43,
            0.0,
            12.6,
            -2.71
          ],
          "reasoning_task_scores": [
            -2.68,
            0.33,
            1.68,
            4.0
          ]
        }
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 43,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 55.26,
      "math_avg": 43.3,
      "code_avg": 40.84,
      "reasoning_avg": 43.63,
      "overall_avg": 45.76,
      "overall_efficiency": 0.017603,
      "general_efficiency": 0.190409,
      "math_efficiency": -0.13685,
      "code_efficiency": 0.045325,
      "reasoning_efficiency": -0.028474,
      "general_scores": [
        58.5,
        52.305,
        59.65,
        49.2307143,
        57.64,
        53.4925,
        59.79,
        50.0714286,
        59.56,
        53.3425,
        59.94,
        49.56
      ],
      "math_scores": [
        85.82,
        54.6,
        20.48,
        18.55,
        6.67,
        74.39,
        84.61,
        52.4,
        18.61,
        18.1,
        13.33,
        73.78,
        85.37,
        52.6,
        17.62,
        18.69,
        10.0,
        73.78
      ],
      "code_scores": [
        71.98,
        8.96,
        68.29,
        17.29,
        70.82,
        6.45,
        68.29,
        15.59,
        71.6,
        3.94,
        68.9,
        17.97
      ],
      "reasoning_scores": [
        67.9,
        34.34,
        38.032609,
        34.4,
        68.84,
        32.83,
        38.195652,
        34.32,
        68.41,
        33.33,
        38.130435,
        34.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.27
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.45
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.81,
          "math_avg": -2.74,
          "code_avg": 0.91,
          "reasoning_avg": -0.57,
          "overall_avg": 0.35,
          "overall_efficiency": 0.017603,
          "general_efficiency": 0.190409,
          "math_efficiency": -0.13685,
          "code_efficiency": 0.045325,
          "reasoning_efficiency": -0.028474,
          "general_task_scores": [
            -9.74,
            17.56,
            2.05,
            5.38
          ],
          "math_task_scores": [
            5.29,
            3.0,
            -7.14,
            -17.46,
            3.33,
            -3.46
          ],
          "code_task_scores": [
            -0.13,
            -1.79,
            25.2,
            -19.65
          ],
          "reasoning_task_scores": [
            -1.08,
            -1.35,
            -1.08,
            1.23
          ]
        },
        "vs_instruct": {
          "general_avg": -10.95,
          "math_avg": -9.04,
          "code_avg": -0.84,
          "reasoning_avg": -3.69,
          "overall_avg": -6.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -8.63,
            -19.91,
            -7.65,
            -7.61
          ],
          "math_task_scores": [
            -7.0,
            -24.6,
            -10.5,
            -22.5,
            0.42,
            9.96
          ],
          "code_task_scores": [
            -3.24,
            -4.66,
            13.0,
            -8.47
          ],
          "reasoning_task_scores": [
            -3.24,
            -0.34,
            -4.33,
            -6.85
          ]
        }
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 59.17,
      "math_avg": 47.96,
      "code_avg": 44.63,
      "reasoning_avg": 48.21,
      "overall_avg": 49.99,
      "overall_efficiency": 0.010516,
      "general_efficiency": 0.017703,
      "math_efficiency": 0.004401,
      "code_efficiency": 0.010774,
      "reasoning_efficiency": 0.009187,
      "general_scores": [
        69.46,
        54.0475,
        61.73,
        49.3685714,
        71.3,
        55.2325,
        62.19,
        49.23,
        70.06,
        55.42,
        62.35,
        49.6428571
      ],
      "math_scores": [
        87.64,
        54.4,
        21.12,
        23.29,
        26.67,
        79.27,
        87.11,
        54.4,
        21.43,
        23.44,
        20.0,
        78.05,
        85.44,
        55.2,
        21.43,
        22.4,
        23.33,
        78.66
      ],
      "code_scores": [
        75.88,
        13.26,
        65.24,
        24.41,
        72.76,
        13.98,
        65.24,
        25.08,
        75.49,
        14.34,
        65.85,
        24.07
      ],
      "reasoning_scores": [
        67.17,
        36.36,
        44.717391,
        44.4,
        67.49,
        38.89,
        43.336957,
        44.24,
        68.17,
        35.86,
        43.282609,
        44.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.44
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.78
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.72,
          "math_avg": 1.92,
          "code_avg": 4.7,
          "reasoning_avg": 4.01,
          "overall_avg": 4.59,
          "overall_efficiency": 0.010516,
          "general_efficiency": 0.017703,
          "math_efficiency": 0.004401,
          "code_efficiency": 0.010774,
          "reasoning_efficiency": 0.009187,
          "general_task_scores": [
            1.96,
            19.41,
            4.35,
            5.17
          ],
          "math_task_scores": [
            6.75,
            4.47,
            -4.71,
            -12.87,
            16.66,
            1.22
          ],
          "code_task_scores": [
            3.11,
            5.62,
            22.15,
            -12.08
          ],
          "reasoning_task_scores": [
            -1.85,
            2.19,
            4.58,
            11.12
          ]
        },
        "vs_instruct": {
          "general_avg": -7.04,
          "math_avg": -4.38,
          "code_avg": 2.95,
          "reasoning_avg": 0.89,
          "overall_avg": -1.89,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.07,
            -18.06,
            -5.35,
            -7.82
          ],
          "math_task_scores": [
            -5.54,
            -23.13,
            -8.07,
            -17.91,
            13.75,
            14.64
          ],
          "code_task_scores": [
            0.0,
            2.75,
            9.95,
            -0.9
          ],
          "reasoning_task_scores": [
            -4.01,
            3.2,
            1.33,
            3.04
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 57.82,
      "math_avg": 44.33,
      "code_avg": 43.9,
      "reasoning_avg": 47.64,
      "overall_avg": 48.42,
      "overall_efficiency": 0.019279,
      "general_efficiency": 0.04075,
      "math_efficiency": -0.010953,
      "code_efficiency": 0.025326,
      "reasoning_efficiency": 0.021994,
      "general_scores": [
        70.6,
        51.1425,
        62.08,
        49.5023077,
        69.12,
        50.4425,
        62.41,
        48.7285714,
        68.23,
        50.2575,
        62.22,
        49.1421429
      ],
      "math_scores": [
        83.55,
        53.8,
        20.57,
        20.62,
        10.0,
        81.71,
        82.79,
        52.4,
        20.37,
        20.47,
        6.67,
        78.66,
        82.87,
        52.8,
        20.23,
        20.47,
        10.0,
        79.88
      ],
      "code_scores": [
        74.32,
        13.62,
        70.73,
        22.71,
        70.04,
        14.7,
        68.9,
        21.02,
        70.82,
        13.98,
        65.24,
        20.68
      ],
      "reasoning_scores": [
        67.62,
        36.87,
        41.380435,
        44.56,
        67.7,
        36.87,
        41.195652,
        43.76,
        68.4,
        36.36,
        42.804348,
        44.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.24
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.29
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.79
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.38,
          "math_avg": -1.71,
          "code_avg": 3.96,
          "reasoning_avg": 3.44,
          "overall_avg": 3.02,
          "overall_efficiency": 0.019279,
          "general_efficiency": 0.04075,
          "math_efficiency": -0.010953,
          "code_efficiency": 0.025326,
          "reasoning_efficiency": 0.021994,
          "general_task_scores": [
            1.01,
            15.12,
            4.5,
            4.88
          ],
          "math_task_scores": [
            3.09,
            2.8,
            -5.65,
            -15.39,
            2.22,
            2.64
          ],
          "code_task_scores": [
            0.13,
            5.86,
            25.0,
            -15.13
          ],
          "reasoning_task_scores": [
            -1.55,
            1.85,
            2.59,
            10.88
          ]
        },
        "vs_instruct": {
          "general_avg": -8.39,
          "math_avg": -8.01,
          "code_avg": 2.21,
          "reasoning_avg": 0.32,
          "overall_avg": -3.46,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.12,
            -22.35,
            -5.2,
            -8.11
          ],
          "math_task_scores": [
            -9.2,
            -24.8,
            -9.01,
            -20.43,
            -0.69,
            16.06
          ],
          "code_task_scores": [
            -2.98,
            2.99,
            12.8,
            -3.95
          ],
          "reasoning_task_scores": [
            -3.71,
            2.86,
            -0.66,
            2.8
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 58.18,
      "math_avg": 44.39,
      "code_avg": 44.07,
      "reasoning_avg": 48.35,
      "overall_avg": 48.75,
      "overall_efficiency": 0.030061,
      "general_efficiency": 0.060571,
      "math_efficiency": -0.014854,
      "code_efficiency": 0.037206,
      "reasoning_efficiency": 0.037321,
      "general_scores": [
        68.26,
        50.4625,
        62.86,
        48.8964286,
        69.85,
        51.2425,
        63.57,
        49.0857143,
        69.48,
        52.2175,
        63.09,
        49.1971429
      ],
      "math_scores": [
        86.35,
        55.0,
        21.03,
        20.18,
        6.67,
        78.66,
        86.2,
        54.4,
        20.05,
        20.33,
        3.33,
        77.44,
        85.6,
        55.6,
        20.53,
        21.66,
        6.67,
        79.27
      ],
      "code_scores": [
        72.37,
        11.83,
        67.07,
        23.39,
        71.6,
        12.19,
        69.51,
        23.05,
        73.15,
        11.47,
        69.51,
        23.73
      ],
      "reasoning_scores": [
        67.97,
        37.88,
        43.880435,
        43.6,
        68.39,
        35.35,
        43.891304,
        44.4,
        68.34,
        37.37,
        44.01087,
        45.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.83
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.7
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.93
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.74,
          "math_avg": -1.65,
          "code_avg": 4.14,
          "reasoning_avg": 4.15,
          "overall_avg": 3.34,
          "overall_efficiency": 0.030061,
          "general_efficiency": 0.060571,
          "math_efficiency": -0.014854,
          "code_efficiency": 0.037206,
          "reasoning_efficiency": 0.037321,
          "general_task_scores": [
            0.89,
            15.82,
            5.43,
            4.82
          ],
          "math_task_scores": [
            6.07,
            4.8,
            -5.5,
            -15.19,
            -1.11,
            1.02
          ],
          "code_task_scores": [
            0.77,
            3.59,
            25.41,
            -13.21
          ],
          "reasoning_task_scores": [
            -1.23,
            2.02,
            4.73,
            11.09
          ]
        },
        "vs_instruct": {
          "general_avg": -8.02,
          "math_avg": -7.95,
          "code_avg": 2.39,
          "reasoning_avg": 1.03,
          "overall_avg": -3.14,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.0,
            -21.65,
            -4.27,
            -8.17
          ],
          "math_task_scores": [
            -6.22,
            -22.8,
            -8.86,
            -20.23,
            -4.02,
            14.44
          ],
          "code_task_scores": [
            -2.34,
            0.72,
            13.21,
            -2.03
          ],
          "reasoning_task_scores": [
            -3.39,
            3.03,
            1.48,
            3.01
          ]
        }
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 56.86,
      "math_avg": 44.88,
      "code_avg": 44.24,
      "reasoning_avg": 46.75,
      "overall_avg": 48.18,
      "overall_efficiency": 0.036982,
      "general_efficiency": 0.07208,
      "math_efficiency": -0.015399,
      "code_efficiency": 0.05725,
      "reasoning_efficiency": 0.033999,
      "general_scores": [
        65.37,
        50.5175,
        64.19,
        49.625,
        70.6,
        45.6125,
        60.99,
        48.52429,
        71.56,
        46.1625,
        60.64,
        48.58429
      ],
      "math_scores": [
        87.19,
        63.6,
        26.31,
        28.49,
        16.67,
        80.49,
        84.69,
        52.0,
        23.4,
        16.62,
        0.0,
        71.95,
        85.22,
        49.8,
        24.23,
        16.32,
        8.33625,
        72.56
      ],
      "code_scores": [
        72.37,
        11.83,
        69.51,
        23.39,
        70.43,
        14.7,
        66.46,
        22.03,
        71.98,
        15.05,
        67.68,
        25.42
      ],
      "reasoning_scores": [
        69.52,
        36.36,
        41.336957,
        46.72,
        68.46,
        36.87,
        39.5217,
        38.56,
        68.76,
        34.85,
        39.1304,
        40.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.65
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.48
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.59
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.88
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.91
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": -1.16,
          "code_avg": 4.31,
          "reasoning_avg": 2.56,
          "overall_avg": 2.78,
          "overall_efficiency": 0.036982,
          "general_efficiency": 0.07208,
          "math_efficiency": -0.015399,
          "code_efficiency": 0.05725,
          "reasoning_efficiency": 0.033999,
          "general_task_scores": [
            0.87,
            11.94,
            4.2,
            4.67
          ],
          "math_task_scores": [
            5.72,
            4.93,
            -1.39,
            -15.43,
            1.67,
            -2.44
          ],
          "code_task_scores": [
            -0.01,
            5.62,
            24.59,
            -12.99
          ],
          "reasoning_task_scores": [
            -0.55,
            1.18,
            0.8,
            8.8
          ]
        },
        "vs_instruct": {
          "general_avg": -9.34,
          "math_avg": -7.46,
          "code_avg": 2.55,
          "reasoning_avg": -0.56,
          "overall_avg": -3.7,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.98,
            -25.53,
            -5.5,
            -8.32
          ],
          "math_task_scores": [
            -6.57,
            -22.67,
            -4.75,
            -20.47,
            -1.24,
            10.98
          ],
          "code_task_scores": [
            -3.12,
            2.75,
            12.39,
            -1.81
          ],
          "reasoning_task_scores": [
            -2.71,
            2.19,
            -2.45,
            0.72
          ]
        }
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 55.83,
      "math_avg": 42.49,
      "code_avg": 44.19,
      "reasoning_avg": 43.72,
      "overall_avg": 46.56,
      "overall_efficiency": 0.017384,
      "general_efficiency": 0.066052,
      "math_efficiency": -0.053544,
      "code_efficiency": 0.064148,
      "reasoning_efficiency": -0.007118,
      "general_scores": [
        70.25,
        48.3275,
        58.8,
        47.0585714,
        70.16,
        48.3925,
        58.86,
        47.2607143,
        69.17,
        45.895,
        58.28,
        47.4957143
      ],
      "math_scores": [
        85.97,
        41.4,
        22.34,
        16.77,
        10.0,
        78.05,
        85.44,
        42.0,
        23.06,
        17.95,
        13.33,
        75.61,
        84.38,
        41.0,
        21.59,
        16.91,
        13.33,
        75.61
      ],
      "code_scores": [
        73.15,
        12.19,
        73.17,
        16.61,
        73.54,
        12.9,
        75.0,
        18.64,
        71.98,
        13.26,
        69.51,
        20.34
      ],
      "reasoning_scores": [
        68.02,
        37.37,
        32.086957,
        38.08,
        67.36,
        36.36,
        32.782609,
        39.2,
        67.51,
        36.36,
        32.369565,
        37.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.27
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.26
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.42
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.78
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.56
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.38,
          "math_avg": -3.55,
          "code_avg": 4.26,
          "reasoning_avg": -0.47,
          "overall_avg": 1.15,
          "overall_efficiency": 0.017384,
          "general_efficiency": 0.066052,
          "math_efficiency": -0.053544,
          "code_efficiency": 0.064148,
          "reasoning_efficiency": -0.007118,
          "general_task_scores": [
            1.55,
            12.05,
            0.91,
            3.03
          ],
          "math_task_scores": [
            5.28,
            -8.73,
            -3.71,
            -18.7,
            5.55,
            -1.02
          ],
          "code_task_scores": [
            1.29,
            4.54,
            29.27,
            -18.07
          ],
          "reasoning_task_scores": [
            -1.83,
            1.85,
            -6.79,
            4.88
          ]
        },
        "vs_instruct": {
          "general_avg": -10.38,
          "math_avg": -9.85,
          "code_avg": 2.51,
          "reasoning_avg": -3.59,
          "overall_avg": -5.33,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.66,
            -25.42,
            -8.79,
            -9.96
          ],
          "math_task_scores": [
            -7.01,
            -36.33,
            -7.07,
            -23.74,
            2.64,
            12.4
          ],
          "code_task_scores": [
            -1.82,
            1.67,
            17.07,
            -6.89
          ],
          "reasoning_task_scores": [
            -3.99,
            2.86,
            -10.04,
            -3.2
          ]
        }
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 39.49,
      "code_avg": 38.48,
      "reasoning_avg": 44.68,
      "overall_avg": 43.53,
      "overall_efficiency": -0.033948,
      "general_efficiency": 0.000437,
      "math_efficiency": -0.118758,
      "code_efficiency": -0.026292,
      "reasoning_efficiency": 0.008822,
      "general_scores": [
        70.49,
        39.445,
        47.31,
        48.3821429,
        71.05,
        39.645,
        46.62,
        48.6735714,
        71.48,
        39.16,
        46.71,
        48.6564286
      ],
      "math_scores": [
        87.64,
        49.2,
        25.09,
        18.55,
        6.67,
        51.83,
        87.04,
        50.4,
        25.43,
        15.58,
        10.0,
        46.34,
        86.2,
        48.2,
        25.25,
        16.91,
        16.67,
        43.9
      ],
      "code_scores": [
        68.87,
        9.32,
        54.47,
        24.75,
        66.93,
        11.83,
        53.66,
        24.75,
        66.54,
        8.24,
        49.39,
        23.05
      ],
      "reasoning_scores": [
        68.44,
        31.31,
        38.967391,
        37.92,
        68.19,
        33.33,
        38.804348,
        38.56,
        67.94,
        34.85,
        39.413043,
        38.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.01
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.36
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.8
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.18
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.06
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.02,
          "math_avg": -6.55,
          "code_avg": -1.45,
          "reasoning_avg": 0.49,
          "overall_avg": -1.87,
          "overall_efficiency": -0.033948,
          "general_efficiency": 0.000437,
          "math_efficiency": -0.118758,
          "code_efficiency": -0.026292,
          "reasoning_efficiency": 0.008822,
          "general_task_scores": [
            2.7,
            3.93,
            -10.86,
            4.33
          ],
          "math_task_scores": [
            6.98,
            -0.93,
            -0.78,
            -18.9,
            4.44,
            -30.08
          ],
          "code_task_scores": [
            -4.15,
            1.56,
            9.22,
            -12.42
          ],
          "reasoning_task_scores": [
            -1.27,
            -1.69,
            -0.14,
            5.04
          ]
        },
        "vs_instruct": {
          "general_avg": -14.74,
          "math_avg": -12.84,
          "code_avg": -3.2,
          "reasoning_avg": -2.63,
          "overall_avg": -8.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.81,
            -33.54,
            -20.56,
            -8.66
          ],
          "math_task_scores": [
            -5.31,
            -28.53,
            -4.14,
            -23.94,
            1.53,
            -16.66
          ],
          "code_task_scores": [
            -7.26,
            -1.31,
            -2.98,
            -1.24
          ],
          "reasoning_task_scores": [
            -3.43,
            -0.68,
            -3.39,
            -3.04
          ]
        }
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 56.9,
      "math_avg": 42.94,
      "code_avg": 44.06,
      "reasoning_avg": 44.71,
      "overall_avg": 47.15,
      "overall_efficiency": 0.034542,
      "general_efficiency": 0.107613,
      "math_efficiency": -0.06118,
      "code_efficiency": 0.081538,
      "reasoning_efficiency": 0.010197,
      "general_scores": [
        72.33,
        45.205,
        61.36,
        48.8171429,
        72.0,
        46.14,
        60.68,
        49.1314286,
        72.58,
        45.295,
        60.26,
        48.9564286
      ],
      "math_scores": [
        84.53,
        50.6,
        23.28,
        16.77,
        6.67,
        72.56,
        85.52,
        51.8,
        23.01,
        17.95,
        6.67,
        71.95,
        85.29,
        51.4,
        23.92,
        16.62,
        10.0,
        74.39
      ],
      "code_scores": [
        73.54,
        13.98,
        70.73,
        21.02,
        71.21,
        13.98,
        67.07,
        21.69,
        71.21,
        14.34,
        65.24,
        24.75
      ],
      "reasoning_scores": [
        68.64,
        33.84,
        38.173913,
        38.56,
        67.61,
        35.35,
        38.108696,
        37.68,
        68.38,
        33.33,
        37.456522,
        39.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.11
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.97
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 14.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.21
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.91
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.45,
          "math_avg": -3.1,
          "code_avg": 4.13,
          "reasoning_avg": 0.52,
          "overall_avg": 1.75,
          "overall_efficiency": 0.034542,
          "general_efficiency": 0.107613,
          "math_efficiency": -0.06118,
          "code_efficiency": 0.081538,
          "reasoning_efficiency": 0.010197,
          "general_task_scores": [
            3.99,
            10.06,
            3.03,
            4.73
          ],
          "math_task_scores": [
            5.13,
            1.07,
            -2.64,
            -18.8,
            1.11,
            -4.47
          ],
          "code_task_scores": [
            0.39,
            5.86,
            24.39,
            -14.11
          ],
          "reasoning_task_scores": [
            -1.25,
            -0.68,
            -1.29,
            5.28
          ]
        },
        "vs_instruct": {
          "general_avg": -9.31,
          "math_avg": -9.4,
          "code_avg": 2.38,
          "reasoning_avg": -2.6,
          "overall_avg": -4.73,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            5.1,
            -27.41,
            -6.67,
            -8.26
          ],
          "math_task_scores": [
            -7.16,
            -26.53,
            -6.0,
            -23.84,
            -1.8,
            8.95
          ],
          "code_task_scores": [
            -2.72,
            2.99,
            12.19,
            -2.93
          ],
          "reasoning_task_scores": [
            -3.41,
            0.33,
            -4.54,
            -2.8
          ]
        }
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.6,
      "math_avg": 43.4,
      "code_avg": 42.23,
      "reasoning_avg": 47.43,
      "overall_avg": 47.41,
      "overall_efficiency": 0.001957,
      "general_efficiency": 0.005017,
      "math_efficiency": -0.002575,
      "code_efficiency": 0.002235,
      "reasoning_efficiency": 0.003152,
      "general_scores": [
        62.51,
        51.775,
        63.3,
        48.4585714,
        64.79,
        50.195,
        62.82,
        48.9107143,
        64.46,
        51.02,
        62.4,
        48.5342857
      ],
      "math_scores": [
        86.43,
        51.6,
        20.53,
        20.03,
        3.33,
        78.66,
        84.76,
        53.8,
        21.5,
        16.62,
        3.33,
        79.88,
        86.35,
        52.0,
        21.61,
        18.69,
        3.33,
        78.66
      ],
      "code_scores": [
        71.6,
        13.26,
        65.24,
        18.31,
        71.6,
        12.19,
        65.85,
        22.71,
        69.26,
        11.47,
        65.24,
        20.0
      ],
      "reasoning_scores": [
        65.39,
        36.36,
        43.847826,
        46.32,
        65.57,
        32.32,
        43.271739,
        46.24,
        66.51,
        33.84,
        44.75,
        44.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.31
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.44
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.34
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.15,
          "math_avg": -2.64,
          "code_avg": 2.3,
          "reasoning_avg": 3.24,
          "overall_avg": 2.01,
          "overall_efficiency": 0.001957,
          "general_efficiency": 0.005017,
          "math_efficiency": -0.002575,
          "code_efficiency": 0.002235,
          "reasoning_efficiency": 0.003152,
          "general_task_scores": [
            -4.39,
            15.51,
            5.1,
            4.39
          ],
          "math_task_scores": [
            5.87,
            2.27,
            -4.83,
            -17.46,
            -3.34,
            1.63
          ],
          "code_task_scores": [
            -0.78,
            4.07,
            22.15,
            -16.26
          ],
          "reasoning_task_scores": [
            -3.64,
            -0.68,
            4.76,
            12.51
          ]
        },
        "vs_instruct": {
          "general_avg": -9.61,
          "math_avg": -8.94,
          "code_avg": 0.54,
          "reasoning_avg": 0.12,
          "overall_avg": -4.47,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -3.28,
            -21.96,
            -4.6,
            -8.6
          ],
          "math_task_scores": [
            -6.42,
            -25.33,
            -8.19,
            -22.5,
            -6.25,
            15.05
          ],
          "code_task_scores": [
            -3.89,
            1.2,
            9.95,
            -5.08
          ],
          "reasoning_task_scores": [
            -5.8,
            0.33,
            1.51,
            4.43
          ]
        }
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 57.3,
      "math_avg": 51.84,
      "code_avg": 44.07,
      "reasoning_avg": 46.99,
      "overall_avg": 50.05,
      "overall_efficiency": 0.132822,
      "general_efficiency": 0.167394,
      "math_efficiency": 0.165703,
      "code_efficiency": 0.118289,
      "reasoning_efficiency": 0.079899,
      "general_scores": [
        62.16,
        51.76,
        65.13,
        50.87,
        61.34,
        51.2125,
        64.68,
        50.6557143,
        61.64,
        51.0075,
        65.07,
        52.1115385
      ],
      "math_scores": [
        90.3,
        68.0,
        27.48,
        32.79,
        13.33,
        78.05,
        89.31,
        72.0,
        27.08,
        33.83,
        13.33,
        76.22,
        89.46,
        69.2,
        27.51,
        33.23,
        13.33,
        78.66
      ],
      "code_scores": [
        73.93,
        10.04,
        73.78,
        21.36,
        73.54,
        11.47,
        68.9,
        22.37,
        73.54,
        9.68,
        71.95,
        18.31
      ],
      "reasoning_scores": [
        69.63,
        34.85,
        42.565217,
        38.32,
        69.84,
        37.37,
        41.673913,
        38.32,
        69.71,
        38.38,
        42.467391,
        40.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.71
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.96
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.69
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.28
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.64
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.4
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.86,
          "math_avg": 5.8,
          "code_avg": 4.14,
          "reasoning_avg": 2.8,
          "overall_avg": 4.65,
          "overall_efficiency": 0.132822,
          "general_efficiency": 0.167394,
          "math_efficiency": 0.165703,
          "code_efficiency": 0.118289,
          "reasoning_efficiency": 0.079899,
          "general_task_scores": [
            -6.6,
            15.84,
            7.22,
            6.97
          ],
          "math_task_scores": [
            9.71,
            19.53,
            1.32,
            -2.63,
            6.66,
            0.2
          ],
          "code_task_scores": [
            2.07,
            2.16,
            28.25,
            -15.92
          ],
          "reasoning_task_scores": [
            0.27,
            2.02,
            3.04,
            5.87
          ]
        },
        "vs_instruct": {
          "general_avg": -8.91,
          "math_avg": -0.5,
          "code_avg": 2.39,
          "reasoning_avg": -0.32,
          "overall_avg": -1.83,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.49,
            -21.63,
            -2.48,
            -6.02
          ],
          "math_task_scores": [
            -2.58,
            -8.07,
            -2.04,
            -7.67,
            3.75,
            13.62
          ],
          "code_task_scores": [
            -1.04,
            -0.71,
            16.05,
            -4.74
          ],
          "reasoning_task_scores": [
            -1.89,
            3.03,
            -0.21,
            -2.21
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 50.66,
      "math_avg": 51.04,
      "code_avg": 42.99,
      "reasoning_avg": 34.17,
      "overall_avg": 44.72,
      "overall_efficiency": -0.001422,
      "general_efficiency": -0.001611,
      "math_efficiency": 0.01032,
      "code_efficiency": 0.006317,
      "reasoning_efficiency": -0.020714,
      "general_scores": [
        62.79,
        56.4025,
        58.34,
        27.26,
        63.02,
        56.725,
        59.04,
        21.8107143,
        61.83,
        55.93,
        59.31,
        25.51571
      ],
      "math_scores": [
        88.32,
        65.4,
        26.51,
        35.91,
        6.67,
        79.27,
        89.46,
        66.4,
        25.56,
        32.94,
        20.0,
        76.83,
        89.08,
        66.4,
        26.08,
        29.38,
        13.335,
        81.1
      ],
      "code_scores": [
        76.65,
        0.0,
        72.56,
        24.07,
        74.32,
        0.0,
        71.34,
        23.05,
        75.1,
        0.0,
        74.39,
        24.41
      ],
      "reasoning_scores": [
        55.51,
        28.79,
        19.467391,
        26.56,
        63.21,
        32.32,
        19.26087,
        26.64,
        62.97,
        28.79,
        19.2391,
        27.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.55
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 56.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.05
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 75.36
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -0.78,
          "math_avg": 5.0,
          "code_avg": 3.06,
          "reasoning_avg": -10.03,
          "overall_avg": -0.69,
          "overall_efficiency": -0.001422,
          "general_efficiency": -0.001611,
          "math_efficiency": 0.01032,
          "code_efficiency": 0.006317,
          "reasoning_efficiency": -0.020714,
          "general_task_scores": [
            -5.76,
            20.86,
            1.16,
            -19.38
          ],
          "math_task_scores": [
            8.97,
            15.87,
            0.01,
            -3.17,
            6.67,
            1.63
          ],
          "code_task_scores": [
            3.76,
            -8.24,
            29.47,
            -12.76
          ],
          "reasoning_task_scores": [
            -8.9,
            -4.88,
            -19.88,
            -6.45
          ]
        },
        "vs_instruct": {
          "general_avg": -15.54,
          "math_avg": -1.3,
          "code_avg": 1.31,
          "reasoning_avg": -13.15,
          "overall_avg": -7.17,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.65,
            -16.61,
            -8.54,
            -32.37
          ],
          "math_task_scores": [
            -3.32,
            -11.73,
            -3.35,
            -8.21,
            3.76,
            15.05
          ],
          "code_task_scores": [
            0.65,
            -11.11,
            17.27,
            -1.58
          ],
          "reasoning_task_scores": [
            -11.06,
            -3.87,
            -23.13,
            -14.53
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 47.27,
      "math_avg": 31.49,
      "code_avg": 26.74,
      "reasoning_avg": 37.05,
      "overall_avg": 35.64,
      "overall_efficiency": -0.118464,
      "general_efficiency": -0.050627,
      "math_efficiency": -0.176461,
      "code_efficiency": -0.160047,
      "reasoning_efficiency": -0.086723,
      "general_scores": [
        55.63,
        38.3025,
        55.02,
        39.4907143,
        56.19,
        38.275,
        55.73,
        39.2192857,
        54.94,
        39.3875,
        55.13,
        39.935
      ],
      "math_scores": [
        78.39,
        38.6,
        20.69,
        8.61,
        3.33,
        38.41,
        78.09,
        39.4,
        18.16,
        9.2,
        6.67,
        40.24,
        76.8,
        37.8,
        19.26,
        9.35,
        6.67,
        37.2
      ],
      "code_scores": [
        52.14,
        0.0,
        30.49,
        25.42,
        51.36,
        0.0,
        30.49,
        24.41,
        49.42,
        0.0,
        31.71,
        25.42
      ],
      "reasoning_scores": [
        60.38,
        28.28,
        31.304348,
        27.76,
        60.26,
        28.79,
        31.51087,
        28.08,
        60.7,
        27.78,
        31.413043,
        28.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 38.62
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.9
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.45
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.05
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -4.17,
          "math_avg": -14.55,
          "code_avg": -13.19,
          "reasoning_avg": -7.15,
          "overall_avg": -9.77,
          "overall_efficiency": -0.118464,
          "general_efficiency": -0.050627,
          "math_efficiency": -0.176461,
          "code_efficiency": -0.160047,
          "reasoning_efficiency": -0.086723,
          "general_task_scores": [
            -12.72,
            3.17,
            -2.45,
            -4.69
          ],
          "math_task_scores": [
            -2.22,
            -11.6,
            -6.67,
            -26.86,
            -1.11,
            -38.82
          ],
          "code_task_scores": [
            -20.63,
            -8.24,
            -12.39,
            -11.52
          ],
          "reasoning_task_scores": [
            -9.01,
            -6.57,
            -7.79,
            -5.23
          ]
        },
        "vs_instruct": {
          "general_avg": -18.94,
          "math_avg": -20.84,
          "code_avg": -14.94,
          "reasoning_avg": -10.27,
          "overall_avg": -16.25,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.61,
            -34.3,
            -12.15,
            -17.68
          ],
          "math_task_scores": [
            -14.51,
            -39.2,
            -10.03,
            -31.9,
            -4.02,
            -25.4
          ],
          "code_task_scores": [
            -23.74,
            -11.11,
            -24.59,
            -0.34
          ],
          "reasoning_task_scores": [
            -11.17,
            -5.56,
            -11.04,
            -13.31
          ]
        }
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.05,
      "math_avg": 46.03,
      "code_avg": 41.28,
      "reasoning_avg": 45.98,
      "overall_avg": 47.33,
      "overall_efficiency": 0.024667,
      "general_efficiency": 0.058874,
      "math_efficiency": -0.000191,
      "code_efficiency": 0.017164,
      "reasoning_efficiency": 0.022819,
      "general_scores": [
        62.72,
        50.47,
        61.33,
        49.0335714,
        62.71,
        51.1875,
        60.27,
        48.7007143,
        63.12,
        52.885,
        60.87,
        49.33
      ],
      "math_scores": [
        84.76,
        57.6,
        23.28,
        18.84,
        10.0,
        78.05,
        85.9,
        59.2,
        22.83,
        21.36,
        16.67,
        72.56,
        84.84,
        57.8,
        23.37,
        20.62,
        13.33,
        77.44
      ],
      "code_scores": [
        71.21,
        12.9,
        64.63,
        18.31,
        71.98,
        12.54,
        60.98,
        19.32,
        69.65,
        12.54,
        61.59,
        19.66
      ],
      "reasoning_scores": [
        68.0,
        32.83,
        41.326087,
        41.84,
        67.95,
        32.83,
        41.228261,
        42.88,
        67.84,
        33.84,
        39.076087,
        42.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.17
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.02
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.4
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.54
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.61,
          "math_avg": -0.01,
          "code_avg": 1.34,
          "reasoning_avg": 1.79,
          "overall_avg": 1.93,
          "overall_efficiency": 0.024667,
          "general_efficiency": 0.058874,
          "math_efficiency": -0.000191,
          "code_efficiency": 0.017164,
          "reasoning_efficiency": 0.022819,
          "general_task_scores": [
            -5.46,
            16.02,
            3.08,
            4.78
          ],
          "math_task_scores": [
            5.19,
            8.0,
            -2.88,
            -15.64,
            6.66,
            -1.42
          ],
          "code_task_scores": [
            -0.65,
            4.42,
            19.11,
            -17.5
          ],
          "reasoning_task_scores": [
            -1.53,
            -1.68,
            1.34,
            9.01
          ]
        },
        "vs_instruct": {
          "general_avg": -10.16,
          "math_avg": -6.31,
          "code_avg": -0.41,
          "reasoning_avg": -1.33,
          "overall_avg": -4.55,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.35,
            -21.45,
            -6.62,
            -8.21
          ],
          "math_task_scores": [
            -7.1,
            -19.6,
            -6.24,
            -20.68,
            3.75,
            12.0
          ],
          "code_task_scores": [
            -3.76,
            1.55,
            6.91,
            -6.32
          ],
          "reasoning_task_scores": [
            -3.69,
            -0.67,
            -1.91,
            0.93
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.26,
      "math_avg": 41.8,
      "code_avg": 38.9,
      "reasoning_avg": 43.21,
      "overall_avg": 44.54,
      "overall_efficiency": -0.007069,
      "general_efficiency": 0.023064,
      "math_efficiency": -0.034802,
      "code_efficiency": -0.00846,
      "reasoning_efficiency": -0.00808,
      "general_scores": [
        55.69,
        52.59,
        58.7,
        49.46,
        56.2,
        53.985,
        58.44,
        49.1271429,
        55.85,
        53.6375,
        58.46,
        48.9485714
      ],
      "math_scores": [
        83.62,
        50.0,
        18.41,
        15.28,
        6.67,
        74.39,
        84.15,
        48.6,
        18.77,
        15.28,
        6.67,
        76.22,
        83.09,
        51.6,
        19.44,
        15.13,
        10.0,
        75.0
      ],
      "code_scores": [
        67.32,
        2.15,
        61.59,
        24.75,
        68.48,
        1.79,
        62.8,
        20.34,
        66.54,
        3.23,
        63.41,
        24.41
      ],
      "reasoning_scores": [
        67.06,
        32.32,
        38.369565,
        35.28,
        67.12,
        32.32,
        38.25,
        35.6,
        66.96,
        31.82,
        38.565217,
        34.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.62
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.2
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.39
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.6
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.17
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.81,
          "math_avg": -4.24,
          "code_avg": -1.03,
          "reasoning_avg": -0.99,
          "overall_avg": -0.86,
          "overall_efficiency": -0.007069,
          "general_efficiency": 0.023064,
          "math_efficiency": -0.034802,
          "code_efficiency": -0.00846,
          "reasoning_efficiency": -0.00808,
          "general_task_scores": [
            -12.4,
            17.91,
            0.79,
            4.94
          ],
          "math_task_scores": [
            3.64,
            -0.13,
            -7.17,
            -20.68,
            1.11,
            -2.24
          ],
          "code_task_scores": [
            -4.15,
            -5.85,
            19.31,
            -13.43
          ],
          "reasoning_task_scores": [
            -2.41,
            -2.7,
            -0.81,
            1.97
          ]
        },
        "vs_instruct": {
          "general_avg": -11.95,
          "math_avg": -10.54,
          "code_avg": -2.78,
          "reasoning_avg": -4.1,
          "overall_avg": -7.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -11.29,
            -19.56,
            -8.91,
            -8.05
          ],
          "math_task_scores": [
            -8.65,
            -27.73,
            -10.53,
            -25.72,
            -1.8,
            11.18
          ],
          "code_task_scores": [
            -7.26,
            -8.72,
            7.11,
            -2.25
          ],
          "reasoning_task_scores": [
            -4.57,
            -1.69,
            -4.06,
            -6.11
          ]
        }
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 57.02,
      "math_avg": 47.04,
      "code_avg": 39.9,
      "reasoning_avg": 44.63,
      "overall_avg": 47.15,
      "overall_efficiency": 0.093625,
      "general_efficiency": 0.299738,
      "math_efficiency": 0.05355,
      "code_efficiency": -0.001925,
      "reasoning_efficiency": 0.023138,
      "general_scores": [
        65.95,
        50.66,
        61.83,
        49.5642857,
        66.42,
        50.6575,
        61.69,
        49.6342857,
        65.74,
        51.18,
        61.75,
        49.2021429
      ],
      "math_scores": [
        84.69,
        58.2,
        24.86,
        23.44,
        13.33,
        76.22,
        84.76,
        59.8,
        25.29,
        23.0,
        13.33,
        74.39,
        85.52,
        56.4,
        24.86,
        24.18,
        20.0,
        74.39
      ],
      "code_scores": [
        71.6,
        0.36,
        69.51,
        21.69,
        67.32,
        0.0,
        68.9,
        20.0,
        69.26,
        0.0,
        67.07,
        23.05
      ],
      "reasoning_scores": [
        67.93,
        33.33,
        38.195652,
        39.36,
        68.66,
        31.82,
        38.565217,
        39.36,
        68.61,
        32.32,
        38.586957,
        38.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.04
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.76
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.47
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.99
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.58
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.58,
          "math_avg": 1.0,
          "code_avg": -0.04,
          "reasoning_avg": 0.43,
          "overall_avg": 1.74,
          "overall_efficiency": 0.093625,
          "general_efficiency": 0.299738,
          "math_efficiency": 0.05355,
          "code_efficiency": -0.001925,
          "reasoning_efficiency": 0.023138,
          "general_task_scores": [
            -2.27,
            15.34,
            4.02,
            5.23
          ],
          "math_task_scores": [
            5.01,
            7.93,
            -1.04,
            -12.37,
            8.88,
            -2.44
          ],
          "code_task_scores": [
            -2.21,
            -8.12,
            25.2,
            -15.02
          ],
          "reasoning_task_scores": [
            -1.06,
            -2.36,
            -0.75,
            5.89
          ]
        },
        "vs_instruct": {
          "general_avg": -9.19,
          "math_avg": -5.3,
          "code_avg": -1.79,
          "reasoning_avg": -2.69,
          "overall_avg": -4.74,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -1.16,
            -22.13,
            -5.68,
            -7.76
          ],
          "math_task_scores": [
            -7.28,
            -19.67,
            -4.4,
            -17.41,
            5.97,
            10.98
          ],
          "code_task_scores": [
            -5.32,
            -10.99,
            13.0,
            -3.84
          ],
          "reasoning_task_scores": [
            -3.22,
            -1.35,
            -4.0,
            -2.19
          ]
        }
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 59.34,
      "math_avg": 49.27,
      "code_avg": 41.43,
      "reasoning_avg": 47.15,
      "overall_avg": 49.3,
      "overall_efficiency": 0.17227,
      "general_efficiency": 0.349102,
      "math_efficiency": 0.14287,
      "code_efficiency": 0.066385,
      "reasoning_efficiency": 0.130725,
      "general_scores": [
        74.59,
        48.8025,
        63.94,
        48.7921429,
        76.02,
        50.9975,
        63.4,
        48.7921429,
        75.88,
        49.385,
        63.32,
        48.1242857
      ],
      "math_scores": [
        86.5,
        63.2,
        25.72,
        23.74,
        23.33,
        78.66,
        86.81,
        63.4,
        25.61,
        23.74,
        10.0,
        77.44,
        86.96,
        62.6,
        25.47,
        25.67,
        20.0,
        78.01
      ],
      "code_scores": [
        68.09,
        14.34,
        61.59,
        22.71,
        67.7,
        14.34,
        64.02,
        18.98,
        69.65,
        13.26,
        63.2,
        19.32
      ],
      "reasoning_scores": [
        67.14,
        34.34,
        38.25,
        48.88,
        67.06,
        34.34,
        39.032609,
        48.4,
        67.67,
        34.85,
        38.032609,
        47.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.04
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.94
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.34
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.29
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.89,
          "math_avg": 3.23,
          "code_avg": 1.5,
          "reasoning_avg": 2.96,
          "overall_avg": 3.89,
          "overall_efficiency": 0.17227,
          "general_efficiency": 0.349102,
          "math_efficiency": 0.14287,
          "code_efficiency": 0.066385,
          "reasoning_efficiency": 0.130725,
          "general_task_scores": [
            7.19,
            14.24,
            5.81,
            4.33
          ],
          "math_task_scores": [
            6.78,
            12.87,
            -0.44,
            -11.53,
            11.11,
            0.6
          ],
          "code_task_scores": [
            -3.12,
            5.74,
            19.65,
            -16.26
          ],
          "reasoning_task_scores": [
            -2.17,
            -0.34,
            -0.76,
            15.09
          ]
        },
        "vs_instruct": {
          "general_avg": -6.87,
          "math_avg": -3.07,
          "code_avg": -0.25,
          "reasoning_avg": -0.16,
          "overall_avg": -2.59,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            8.3,
            -23.23,
            -3.89,
            -8.66
          ],
          "math_task_scores": [
            -5.51,
            -14.73,
            -3.8,
            -16.57,
            8.2,
            14.02
          ],
          "code_task_scores": [
            -6.23,
            2.87,
            7.45,
            -5.08
          ],
          "reasoning_task_scores": [
            -4.33,
            0.67,
            -4.01,
            7.01
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 53.67,
      "math_avg": 38.62,
      "code_avg": 36.34,
      "reasoning_avg": 42.96,
      "overall_avg": 42.89,
      "overall_efficiency": -0.008236,
      "general_efficiency": 0.007296,
      "math_efficiency": -0.024361,
      "code_efficiency": -0.011799,
      "reasoning_efficiency": -0.004077,
      "general_scores": [
        68.75,
        42.085,
        59.03,
        45.8735714,
        65.46,
        41.9775,
        59.03,
        46.21,
        68.39,
        42.1625,
        59.03,
        46.0121429
      ],
      "math_scores": [
        83.4,
        58.2,
        24.71,
        19.29,
        13.33,
        37.8,
        83.4,
        58.2,
        24.68,
        22.4,
        13.33,
        26.22,
        83.4,
        58.2,
        24.68,
        19.44,
        13.33,
        31.1
      ],
      "code_scores": [
        64.2,
        6.81,
        48.17,
        26.78,
        64.2,
        6.81,
        50.0,
        26.78,
        64.2,
        6.81,
        44.51,
        26.78
      ],
      "reasoning_scores": [
        64.43,
        32.32,
        36.758242,
        39.76,
        64.43,
        30.3,
        36.456522,
        39.76,
        64.43,
        30.3,
        36.758242,
        39.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.69
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.71
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.81
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.78
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.66
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 2.22,
          "math_avg": -7.42,
          "code_avg": -3.59,
          "reasoning_avg": -1.24,
          "overall_avg": -2.51,
          "overall_efficiency": -0.008236,
          "general_efficiency": 0.007296,
          "math_efficiency": -0.024361,
          "code_efficiency": -0.011799,
          "reasoning_efficiency": -0.004077,
          "general_task_scores": [
            -0.78,
            6.59,
            1.29,
            1.79
          ],
          "math_task_scores": [
            3.42,
            8.0,
            -1.35,
            -15.53,
            6.66,
            -45.73
          ],
          "code_task_scores": [
            -7.4,
            -1.43,
            4.27,
            -9.82
          ],
          "reasoning_task_scores": [
            -5.03,
            -3.88,
            -2.54,
            6.48
          ]
        },
        "vs_instruct": {
          "general_avg": -12.54,
          "math_avg": -13.72,
          "code_avg": -5.35,
          "reasoning_avg": -4.36,
          "overall_avg": -8.99,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.33,
            -30.88,
            -8.41,
            -11.2
          ],
          "math_task_scores": [
            -8.87,
            -19.6,
            -4.71,
            -20.57,
            3.75,
            -32.31
          ],
          "code_task_scores": [
            -10.51,
            -4.3,
            -7.93,
            1.36
          ],
          "reasoning_task_scores": [
            -7.19,
            -2.87,
            -5.79,
            -1.6
          ]
        }
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 57.9,
      "math_avg": 52.55,
      "code_avg": 47.34,
      "reasoning_avg": 40.16,
      "overall_avg": 49.49,
      "overall_efficiency": 0.010741,
      "general_efficiency": 0.01699,
      "math_efficiency": 0.017121,
      "code_efficiency": 0.019482,
      "reasoning_efficiency": -0.010629,
      "general_scores": [
        62.35,
        53.8,
        64.65,
        52.5285714,
        62.15,
        52.785,
        65.69,
        52.3235714,
        58.0,
        53.7575,
        64.56,
        52.2142857
      ],
      "math_scores": [
        87.57,
        66.8,
        32.36,
        29.08,
        20.0,
        82.32,
        88.93,
        70.8,
        33.42,
        33.23,
        13.33,
        81.71,
        87.34,
        65.6,
        33.06,
        24.63,
        13.33,
        82.32
      ],
      "code_scores": [
        73.54,
        16.85,
        76.22,
        24.75,
        73.54,
        13.98,
        75.61,
        25.76,
        73.15,
        16.85,
        77.44,
        20.34
      ],
      "reasoning_scores": [
        68.07,
        32.83,
        32.945652,
        26.0,
        68.7,
        32.32,
        33.717391,
        26.72,
        68.64,
        33.84,
        34.119565,
        24.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.95
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.98
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 82.12
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.41
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.89
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.42
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.62
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.59
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.46,
          "math_avg": 6.51,
          "code_avg": 7.4,
          "reasoning_avg": -4.04,
          "overall_avg": 4.08,
          "overall_efficiency": 0.010741,
          "general_efficiency": 0.01699,
          "math_efficiency": 0.017121,
          "code_efficiency": 0.019482,
          "reasoning_efficiency": -0.010629,
          "general_task_scores": [
            -7.48,
            17.96,
            7.23,
            8.12
          ],
          "math_task_scores": [
            7.97,
            17.53,
            6.91,
            -6.93,
            8.88,
            4.68
          ],
          "code_task_scores": [
            1.81,
            7.65,
            33.13,
            -12.98
          ],
          "reasoning_task_scores": [
            -0.99,
            -1.85,
            -5.61,
            -7.71
          ]
        },
        "vs_instruct": {
          "general_avg": -8.31,
          "math_avg": 0.21,
          "code_avg": 5.65,
          "reasoning_avg": -7.16,
          "overall_avg": -2.4,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -6.37,
            -19.51,
            -2.47,
            -4.87
          ],
          "math_task_scores": [
            -4.32,
            -10.07,
            3.55,
            -11.97,
            5.97,
            18.1
          ],
          "code_task_scores": [
            -1.3,
            4.78,
            20.93,
            -1.8
          ],
          "reasoning_task_scores": [
            -3.15,
            -0.84,
            -8.86,
            -15.79
          ]
        }
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 57.41,
      "math_avg": 49.49,
      "code_avg": 44.17,
      "reasoning_avg": 47.63,
      "overall_avg": 49.68,
      "overall_efficiency": 0.039424,
      "general_efficiency": 0.055048,
      "math_efficiency": 0.031844,
      "code_efficiency": 0.039102,
      "reasoning_efficiency": 0.031701,
      "general_scores": [
        64.77,
        52.9625,
        62.53,
        49.9814286,
        64.64,
        52.7925,
        62.76,
        49.4192857,
        64.06,
        52.8175,
        62.54,
        49.6607143
      ],
      "math_scores": [
        88.78,
        69.4,
        26.65,
        23.74,
        13.33,
        78.05,
        88.48,
        65.8,
        26.76,
        24.33,
        10.0,
        80.49,
        89.01,
        65.6,
        25.86,
        21.07,
        16.67,
        76.83
      ],
      "code_scores": [
        72.76,
        12.54,
        68.29,
        20.0,
        71.6,
        11.83,
        66.46,
        26.1,
        71.6,
        13.26,
        69.51,
        26.1
      ],
      "reasoning_scores": [
        69.88,
        37.37,
        45.108696,
        37.6,
        70.41,
        36.36,
        44.326087,
        39.6,
        70.29,
        36.87,
        44.98913,
        38.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.49
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.61
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.69
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.42
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.07
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.87
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.81
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.97,
          "math_avg": 3.45,
          "code_avg": 4.24,
          "reasoning_avg": 3.44,
          "overall_avg": 4.27,
          "overall_efficiency": 0.039424,
          "general_efficiency": 0.055048,
          "math_efficiency": 0.031844,
          "code_efficiency": 0.039102,
          "reasoning_efficiency": 0.031701,
          "general_task_scores": [
            -3.82,
            17.37,
            4.87,
            5.45
          ],
          "math_task_scores": [
            8.78,
            16.73,
            0.38,
            -12.86,
            6.66,
            1.02
          ],
          "code_task_scores": [
            0.39,
            4.3,
            24.8,
            -12.53
          ],
          "reasoning_task_scores": [
            0.73,
            2.02,
            5.61,
            5.39
          ]
        },
        "vs_instruct": {
          "general_avg": -8.8,
          "math_avg": -2.85,
          "code_avg": 2.49,
          "reasoning_avg": 0.32,
          "overall_avg": -2.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.71,
            -20.1,
            -4.83,
            -7.54
          ],
          "math_task_scores": [
            -3.51,
            -10.87,
            -2.98,
            -17.9,
            3.75,
            14.44
          ],
          "code_task_scores": [
            -2.72,
            1.43,
            12.6,
            -1.35
          ],
          "reasoning_task_scores": [
            -1.43,
            3.03,
            2.36,
            -2.69
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 52.21,
      "math_avg": 38.16,
      "code_avg": 35.38,
      "reasoning_avg": 41.24,
      "overall_avg": 41.75,
      "overall_efficiency": -0.003506,
      "general_efficiency": 0.000738,
      "math_efficiency": -0.007557,
      "code_efficiency": -0.004368,
      "reasoning_efficiency": -0.002838,
      "general_scores": [
        57.05,
        49.1375,
        53.53,
        48.7123077,
        56.63,
        50.0225,
        54.01,
        48.7421429,
        59.75,
        49.3175,
        50.38,
        49.2942857
      ],
      "math_scores": [
        80.82,
        39.0,
        29.27,
        9.2,
        10.0,
        56.71,
        81.12,
        42.8,
        29.27,
        16.02,
        3.33,
        57.93,
        81.2,
        37.6,
        27.94,
        15.73,
        6.67,
        62.2
      ],
      "code_scores": [
        65.37,
        0.0,
        54.88,
        22.71,
        66.54,
        0.0,
        53.66,
        23.39,
        64.98,
        0.0,
        53.66,
        19.32
      ],
      "reasoning_scores": [
        63.37,
        39.39,
        38.391304,
        25.92,
        64.23,
        36.36,
        34.673913,
        26.32,
        64.29,
        35.35,
        38.543478,
        28.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.83
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 58.95
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 54.07
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.96
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.75
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 0.77,
          "math_avg": -7.88,
          "code_avg": -4.56,
          "reasoning_avg": -2.96,
          "overall_avg": -3.66,
          "overall_efficiency": -0.003506,
          "general_efficiency": 0.000738,
          "math_efficiency": -0.007557,
          "code_efficiency": -0.004368,
          "reasoning_efficiency": -0.002838,
          "general_task_scores": [
            -10.5,
            14.0,
            -5.1,
            4.68
          ],
          "math_task_scores": [
            1.07,
            -10.4,
            2.79,
            -22.26,
            0.0,
            -18.49
          ],
          "code_task_scores": [
            -5.97,
            -8.24,
            10.78,
            -14.79
          ],
          "reasoning_task_scores": [
            -5.5,
            2.18,
            -2.0,
            -6.53
          ]
        },
        "vs_instruct": {
          "general_avg": -13.99,
          "math_avg": -14.18,
          "code_avg": -6.31,
          "reasoning_avg": -6.08,
          "overall_avg": -10.14,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -9.39,
            -23.47,
            -14.8,
            -8.31
          ],
          "math_task_scores": [
            -11.22,
            -38.0,
            -0.57,
            -27.3,
            -2.91,
            -5.07
          ],
          "code_task_scores": [
            -9.08,
            -11.11,
            -1.42,
            -3.61
          ],
          "reasoning_task_scores": [
            -7.66,
            3.19,
            -5.25,
            -14.61
          ]
        }
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 56.87,
      "math_avg": 47.03,
      "code_avg": 41.99,
      "reasoning_avg": 45.11,
      "overall_avg": 47.75,
      "overall_efficiency": 0.023475,
      "general_efficiency": 0.05421,
      "math_efficiency": 0.009906,
      "code_efficiency": 0.020617,
      "reasoning_efficiency": 0.009169,
      "general_scores": [
        64.27,
        53.28,
        61.0,
        49.6371429,
        64.73,
        52.515,
        61.09,
        49.5992857,
        64.28,
        51.73,
        61.17,
        49.0842857
      ],
      "math_scores": [
        87.87,
        63.4,
        24.62,
        27.0,
        10.0,
        73.17,
        87.26,
        61.6,
        24.71,
        25.82,
        13.33,
        71.34,
        85.9,
        59.8,
        24.28,
        23.89,
        10.0,
        72.56
      ],
      "code_scores": [
        73.93,
        9.32,
        60.37,
        23.73,
        74.32,
        9.68,
        62.8,
        21.36,
        72.76,
        9.32,
        61.59,
        24.75
      ],
      "reasoning_scores": [
        66.92,
        33.33,
        42.434783,
        36.24,
        67.3,
        35.35,
        41.923913,
        35.12,
        67.3,
        37.37,
        41.923913,
        36.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.44
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.59
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.28
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.17
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.42,
          "math_avg": 0.99,
          "code_avg": 2.06,
          "reasoning_avg": 0.92,
          "overall_avg": 2.35,
          "overall_efficiency": 0.023475,
          "general_efficiency": 0.05421,
          "math_efficiency": 0.009906,
          "code_efficiency": 0.020617,
          "reasoning_efficiency": 0.009169,
          "general_task_scores": [
            -3.88,
            17.02,
            3.35,
            5.2
          ],
          "math_task_scores": [
            7.03,
            11.4,
            -1.5,
            -10.34,
            4.44,
            -5.08
          ],
          "code_task_scores": [
            2.07,
            1.2,
            18.3,
            -13.32
          ],
          "reasoning_task_scores": [
            -2.29,
            0.5,
            2.89,
            2.56
          ]
        },
        "vs_instruct": {
          "general_avg": -9.34,
          "math_avg": -5.31,
          "code_avg": 0.31,
          "reasoning_avg": -2.2,
          "overall_avg": -4.14,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.77,
            -20.45,
            -6.35,
            -7.79
          ],
          "math_task_scores": [
            -5.26,
            -16.2,
            -4.86,
            -15.38,
            1.53,
            8.34
          ],
          "code_task_scores": [
            -1.04,
            -1.67,
            6.1,
            -2.14
          ],
          "reasoning_task_scores": [
            -4.45,
            1.51,
            -0.36,
            -5.52
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 58.57,
      "math_avg": 52.84,
      "code_avg": 40.82,
      "reasoning_avg": 48.14,
      "overall_avg": 50.09,
      "overall_efficiency": 0.208426,
      "general_efficiency": 0.316896,
      "math_efficiency": 0.302,
      "code_efficiency": 0.039444,
      "reasoning_efficiency": 0.175361,
      "general_scores": [
        66.35,
        52.75,
        65.69,
        49.4478571,
        66.51,
        52.01,
        66.2,
        49.8314286,
        65.96,
        52.525,
        66.3,
        49.3214286
      ],
      "math_scores": [
        89.61,
        72.6,
        26.38,
        40.36,
        16.67,
        78.66,
        89.08,
        73.0,
        26.65,
        25.37,
        13.33,
        79.27,
        88.86,
        73.0,
        26.87,
        38.72,
        13.33,
        79.27
      ],
      "code_scores": [
        73.15,
        0.0,
        73.78,
        17.97,
        71.98,
        0.0,
        70.12,
        21.69,
        71.98,
        0.0,
        72.56,
        16.61
      ],
      "reasoning_scores": [
        71.07,
        35.86,
        44.152174,
        41.36,
        71.12,
        35.86,
        44.076087,
        41.76,
        70.92,
        36.36,
        43.73913,
        41.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.53
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.18
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.99
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.13,
          "math_avg": 6.8,
          "code_avg": 0.89,
          "reasoning_avg": 3.95,
          "overall_avg": 4.69,
          "overall_efficiency": 0.208426,
          "general_efficiency": 0.316896,
          "math_efficiency": 0.302,
          "code_efficiency": 0.039444,
          "reasoning_efficiency": 0.175361,
          "general_task_scores": [
            -2.04,
            16.94,
            8.32,
            5.29
          ],
          "math_task_scores": [
            9.2,
            22.67,
            0.59,
            -1.09,
            7.77,
            1.63
          ],
          "code_task_scores": [
            0.77,
            -8.24,
            28.86,
            -17.84
          ],
          "reasoning_task_scores": [
            1.58,
            1.18,
            4.79,
            8.24
          ]
        },
        "vs_instruct": {
          "general_avg": -7.63,
          "math_avg": 0.5,
          "code_avg": -0.86,
          "reasoning_avg": 0.83,
          "overall_avg": -1.79,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -0.93,
            -20.53,
            -1.38,
            -7.7
          ],
          "math_task_scores": [
            -3.09,
            -4.93,
            -2.77,
            -6.13,
            4.86,
            15.05
          ],
          "code_task_scores": [
            -2.34,
            -11.11,
            16.66,
            -6.66
          ],
          "reasoning_task_scores": [
            -0.58,
            2.19,
            1.54,
            0.16
          ]
        }
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 56.78,
      "math_avg": 46.42,
      "code_avg": 43.05,
      "reasoning_avg": 46.75,
      "overall_avg": 48.25,
      "overall_efficiency": 0.036215,
      "general_efficiency": 0.067895,
      "math_efficiency": 0.004815,
      "code_efficiency": 0.039642,
      "reasoning_efficiency": 0.032509,
      "general_scores": [
        64.72,
        52.2125,
        61.42,
        48.9078571,
        65.58,
        50.24,
        61.83,
        49.095,
        65.09,
        51.9275,
        61.48,
        48.85
      ],
      "math_scores": [
        85.6,
        59.8,
        26.63,
        23.0,
        13.33,
        75.61,
        84.31,
        58.0,
        25.61,
        22.26,
        10.0,
        72.56,
        84.31,
        62.2,
        25.95,
        20.47,
        13.33,
        72.56
      ],
      "code_scores": [
        73.93,
        11.11,
        64.63,
        23.39,
        71.21,
        11.83,
        64.63,
        23.39,
        70.04,
        10.39,
        65.24,
        26.78
      ],
      "reasoning_scores": [
        69.56,
        36.87,
        43.152174,
        38.4,
        68.61,
        37.88,
        42.391304,
        36.48,
        68.85,
        37.88,
        43.75,
        37.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.46
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.58
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.95
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.06
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.91
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.83
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.01
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.1
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.33,
          "math_avg": 0.38,
          "code_avg": 3.12,
          "reasoning_avg": 2.55,
          "overall_avg": 2.85,
          "overall_efficiency": 0.036215,
          "general_efficiency": 0.067895,
          "math_efficiency": 0.004815,
          "code_efficiency": 0.039642,
          "reasoning_efficiency": 0.032509,
          "general_task_scores": [
            -3.18,
            15.97,
            3.84,
            4.71
          ],
          "math_task_scores": [
            4.76,
            9.8,
            0.02,
            -14.0,
            5.55,
            -3.86
          ],
          "code_task_scores": [
            0.13,
            2.87,
            21.54,
            -12.08
          ],
          "reasoning_task_scores": [
            -0.45,
            2.69,
            3.9,
            4.08
          ]
        },
        "vs_instruct": {
          "general_avg": -9.43,
          "math_avg": -5.92,
          "code_avg": 1.37,
          "reasoning_avg": -0.56,
          "overall_avg": -3.64,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.07,
            -21.5,
            -5.86,
            -8.28
          ],
          "math_task_scores": [
            -7.53,
            -17.8,
            -3.34,
            -19.04,
            2.64,
            9.56
          ],
          "code_task_scores": [
            -2.98,
            0.0,
            9.34,
            -0.9
          ],
          "reasoning_task_scores": [
            -2.61,
            3.7,
            0.65,
            -4.0
          ]
        }
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 50.0,
      "math_avg": 34.67,
      "code_avg": 29.76,
      "reasoning_avg": 31.77,
      "overall_avg": 36.55,
      "overall_efficiency": -0.080927,
      "general_efficiency": -0.013202,
      "math_efficiency": -0.103964,
      "code_efficiency": -0.092983,
      "reasoning_efficiency": -0.11356,
      "general_scores": [
        49.87,
        37.7,
        59.5,
        49.1907143,
        58.46,
        35.775,
        60.32,
        48.5914286,
        53.01,
        38.29,
        60.65,
        48.6442857
      ],
      "math_scores": [
        80.74,
        48.4,
        19.2,
        7.27,
        12.89,
        52.44,
        83.55,
        52.0,
        19.11,
        0.89,
        10.0,
        48.17,
        76.57,
        40.8,
        18.59,
        1.63,
        13.33,
        38.41
      ],
      "code_scores": [
        71.6,
        12.54,
        10.37,
        24.75,
        71.6,
        12.9,
        11.59,
        24.75,
        71.6,
        12.54,
        12.2,
        20.68
      ],
      "reasoning_scores": [
        69.12,
        25.76,
        29.12714,
        5.04,
        69.09,
        23.74,
        28.98913,
        7.12,
        68.78,
        22.73,
        29.869565,
        1.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.16
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.81
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.29
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.97
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.07
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.39
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.44,
          "math_avg": -11.37,
          "code_avg": -10.17,
          "reasoning_avg": -12.42,
          "overall_avg": -8.85,
          "overall_efficiency": -0.080927,
          "general_efficiency": -0.013202,
          "math_efficiency": -0.103964,
          "code_efficiency": -0.092983,
          "reasoning_efficiency": -0.11356,
          "general_task_scores": [
            -14.53,
            1.76,
            2.42,
            4.57
          ],
          "math_task_scores": [
            0.31,
            -3.13,
            -7.07,
            -32.65,
            5.4,
            -31.1
          ],
          "code_task_scores": [
            0.0,
            4.42,
            -31.9,
            -13.21
          ],
          "reasoning_task_scores": [
            -0.46,
            -10.77,
            -9.87,
            -28.59
          ]
        },
        "vs_instruct": {
          "general_avg": -16.21,
          "math_avg": -17.67,
          "code_avg": -11.92,
          "reasoning_avg": -15.54,
          "overall_avg": -15.34,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -13.42,
            -35.71,
            -7.28,
            -8.42
          ],
          "math_task_scores": [
            -11.98,
            -30.73,
            -10.43,
            -37.69,
            2.49,
            -17.68
          ],
          "code_task_scores": [
            -3.11,
            1.55,
            -44.1,
            -2.03
          ],
          "reasoning_task_scores": [
            -2.62,
            -9.76,
            -13.12,
            -36.67
          ]
        }
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 57.59,
      "math_avg": 44.05,
      "code_avg": 43.32,
      "reasoning_avg": 46.51,
      "overall_avg": 47.87,
      "overall_efficiency": 0.008521,
      "general_efficiency": 0.021269,
      "math_efficiency": -0.006883,
      "code_efficiency": 0.011709,
      "reasoning_efficiency": 0.007991,
      "general_scores": [
        71.16,
        51.635,
        59.84,
        49.2021429,
        69.69,
        50.245,
        60.61,
        49.5564286,
        71.12,
        48.7625,
        59.94,
        49.3578571
      ],
      "math_scores": [
        82.49,
        51.6,
        22.15,
        18.84,
        16.67,
        78.66,
        81.5,
        53.4,
        22.06,
        18.1,
        10.0,
        77.44,
        81.65,
        53.4,
        22.02,
        20.03,
        6.67,
        76.22
      ],
      "code_scores": [
        68.09,
        12.19,
        68.29,
        17.97,
        68.09,
        13.26,
        72.56,
        24.41,
        68.48,
        13.98,
        67.07,
        25.42
      ],
      "reasoning_scores": [
        66.6,
        35.35,
        39.576087,
        45.84,
        66.02,
        35.86,
        38.217391,
        44.64,
        66.63,
        35.86,
        38.217391,
        45.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.66
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.13
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.22
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.14
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.31
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.42
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.15,
          "math_avg": -1.99,
          "code_avg": 3.39,
          "reasoning_avg": 2.31,
          "overall_avg": 2.46,
          "overall_efficiency": 0.008521,
          "general_efficiency": 0.021269,
          "math_efficiency": -0.006883,
          "code_efficiency": 0.011709,
          "reasoning_efficiency": 0.007991,
          "general_task_scores": [
            2.35,
            14.72,
            2.39,
            5.13
          ],
          "math_task_scores": [
            1.9,
            2.6,
            -3.96,
            -16.92,
            4.44,
            0.0
          ],
          "code_task_scores": [
            -3.38,
            4.9,
            26.02,
            -14.0
          ],
          "reasoning_task_scores": [
            -3.04,
            0.84,
            -0.53,
            11.97
          ]
        },
        "vs_instruct": {
          "general_avg": -8.62,
          "math_avg": -8.29,
          "code_avg": 1.63,
          "reasoning_avg": -0.81,
          "overall_avg": -4.02,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            3.46,
            -22.75,
            -7.31,
            -7.86
          ],
          "math_task_scores": [
            -10.39,
            -25.0,
            -7.32,
            -21.96,
            1.53,
            13.42
          ],
          "code_task_scores": [
            -6.49,
            2.03,
            13.82,
            -2.82
          ],
          "reasoning_task_scores": [
            -5.2,
            1.85,
            -3.78,
            3.89
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 58.01,
      "math_avg": 46.65,
      "code_avg": 41.78,
      "reasoning_avg": 47.1,
      "overall_avg": 48.38,
      "overall_efficiency": 0.040303,
      "general_efficiency": 0.088745,
      "math_efficiency": 0.008249,
      "code_efficiency": 0.024979,
      "reasoning_efficiency": 0.039238,
      "general_scores": [
        69.15,
        50.8675,
        62.02,
        49.4178571,
        70.87,
        50.395,
        62.52,
        49.3135714,
        68.9,
        50.9425,
        62.29,
        49.3764286
      ],
      "math_scores": [
        83.4,
        59.6,
        23.94,
        21.07,
        19.16625,
        79.88,
        83.02,
        56.4,
        23.55,
        21.22,
        10.0,
        78.66,
        84.0,
        57.8,
        23.87,
        20.62,
        16.67,
        76.83
      ],
      "code_scores": [
        71.6,
        12.54,
        62.8,
        20.68,
        70.82,
        11.11,
        62.8,
        18.31,
        70.82,
        12.19,
        64.63,
        23.05
      ],
      "reasoning_scores": [
        67.64,
        37.37,
        39.271739,
        44.88,
        66.84,
        36.36,
        39.521739,
        44.56,
        67.66,
        37.37,
        38.826087,
        44.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.64
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.79
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.28
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.46
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.95
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.56,
          "math_avg": 0.61,
          "code_avg": 1.85,
          "reasoning_avg": 2.9,
          "overall_avg": 2.98,
          "overall_efficiency": 0.040303,
          "general_efficiency": 0.088745,
          "math_efficiency": 0.008249,
          "code_efficiency": 0.024979,
          "reasoning_efficiency": 0.039238,
          "general_task_scores": [
            1.33,
            15.25,
            4.54,
            5.13
          ],
          "math_task_scores": [
            3.49,
            7.73,
            -2.25,
            -14.94,
            8.61,
            1.02
          ],
          "code_task_scores": [
            -0.52,
            3.71,
            20.12,
            -15.92
          ],
          "reasoning_task_scores": [
            -2.08,
            2.18,
            0.01,
            11.49
          ]
        },
        "vs_instruct": {
          "general_avg": -8.2,
          "math_avg": -5.69,
          "code_avg": 0.1,
          "reasoning_avg": -0.22,
          "overall_avg": -3.5,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            2.44,
            -22.22,
            -5.16,
            -7.86
          ],
          "math_task_scores": [
            -8.8,
            -19.87,
            -5.61,
            -19.98,
            5.7,
            14.44
          ],
          "code_task_scores": [
            -3.63,
            0.84,
            7.92,
            -4.74
          ],
          "reasoning_task_scores": [
            -4.24,
            3.19,
            -3.24,
            3.41
          ]
        }
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 55.02,
      "math_avg": 46.66,
      "code_avg": 41.49,
      "reasoning_avg": 42.98,
      "overall_avg": 46.54,
      "overall_efficiency": 0.02493,
      "general_efficiency": 0.078729,
      "math_efficiency": 0.013581,
      "code_efficiency": 0.034178,
      "reasoning_efficiency": -0.026771,
      "general_scores": [
        65.18,
        44.0975,
        64.11,
        48.3571429,
        63.91,
        44.235,
        63.89,
        47.945,
        64.25,
        42.895,
        63.92,
        47.4807143
      ],
      "math_scores": [
        85.9,
        62.2,
        23.19,
        23.74,
        10.0,
        68.9,
        85.97,
        66.4,
        23.53,
        26.26,
        16.67,
        68.9,
        86.05,
        63.6,
        23.64,
        25.37,
        10.0,
        69.51
      ],
      "code_scores": [
        71.6,
        12.54,
        57.32,
        25.08,
        71.98,
        14.34,
        59.15,
        23.39,
        70.43,
        13.26,
        56.71,
        22.03
      ],
      "reasoning_scores": [
        68.61,
        36.36,
        36.380435,
        30.72,
        68.52,
        36.87,
        36.141304,
        30.32,
        68.17,
        36.36,
        36.597826,
        30.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.93
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.45
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.1
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.38
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.73
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.58,
          "math_avg": 0.62,
          "code_avg": 1.55,
          "reasoning_avg": -1.22,
          "overall_avg": 1.13,
          "overall_efficiency": 0.02493,
          "general_efficiency": 0.078729,
          "math_efficiency": 0.013581,
          "code_efficiency": 0.034178,
          "reasoning_efficiency": -0.026771,
          "general_task_scores": [
            -3.86,
            8.25,
            6.23,
            3.69
          ],
          "math_task_scores": [
            5.99,
            13.87,
            -2.59,
            -10.79,
            5.55,
            -8.34
          ],
          "code_task_scores": [
            -0.26,
            5.14,
            14.44,
            -13.1
          ],
          "reasoning_task_scores": [
            -1.03,
            1.68,
            -2.83,
            -2.69
          ]
        },
        "vs_instruct": {
          "general_avg": -11.19,
          "math_avg": -5.68,
          "code_avg": -0.2,
          "reasoning_avg": -4.34,
          "overall_avg": -5.35,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.75,
            -29.22,
            -3.47,
            -9.3
          ],
          "math_task_scores": [
            -6.3,
            -13.73,
            -5.95,
            -15.83,
            2.64,
            5.08
          ],
          "code_task_scores": [
            -3.37,
            2.27,
            2.24,
            -1.92
          ],
          "reasoning_task_scores": [
            -3.19,
            2.69,
            -6.08,
            -10.77
          ]
        }
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 56.84,
      "math_avg": 44.67,
      "code_avg": 41.31,
      "reasoning_avg": 47.54,
      "overall_avg": 47.59,
      "overall_efficiency": 0.005216,
      "general_efficiency": 0.012881,
      "math_efficiency": -0.003277,
      "code_efficiency": 0.003285,
      "reasoning_efficiency": 0.007976,
      "general_scores": [
        67.42,
        47.275,
        64.11,
        48.685,
        65.42,
        46.285,
        64.6,
        48.6821429,
        69.08,
        47.0825,
        64.26,
        49.13
      ],
      "math_scores": [
        87.72,
        67.8,
        27.12,
        30.42,
        10.0,
        50.61,
        87.57,
        67.6,
        26.56,
        30.56,
        3.33,
        47.78,
        86.81,
        65.2,
        26.22,
        31.45,
        6.67,
        50.61
      ],
      "code_scores": [
        66.93,
        9.32,
        63.41,
        28.47,
        66.54,
        9.68,
        62.8,
        25.76,
        68.48,
        9.32,
        61.59,
        23.39
      ],
      "reasoning_scores": [
        67.82,
        34.85,
        39.728261,
        48.32,
        67.91,
        32.83,
        40.619565,
        48.24,
        67.61,
        31.31,
        42.152174,
        49.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.83
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.81
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.44
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.6
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.87
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.83
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.39,
          "math_avg": -1.37,
          "code_avg": 1.38,
          "reasoning_avg": 3.34,
          "overall_avg": 2.18,
          "overall_efficiency": 0.005216,
          "general_efficiency": 0.012881,
          "math_efficiency": -0.003277,
          "code_efficiency": 0.003285,
          "reasoning_efficiency": 0.007976,
          "general_task_scores": [
            -1.0,
            11.39,
            6.58,
            4.59
          ],
          "math_task_scores": [
            7.39,
            16.67,
            0.59,
            -5.1,
            0.0,
            -27.77
          ],
          "code_task_scores": [
            -4.28,
            1.2,
            19.31,
            -10.73
          ],
          "reasoning_task_scores": [
            -1.68,
            -1.85,
            1.63,
            15.25
          ]
        },
        "vs_instruct": {
          "general_avg": -9.37,
          "math_avg": -7.67,
          "code_avg": -0.38,
          "reasoning_avg": 0.22,
          "overall_avg": -4.3,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            0.11,
            -26.08,
            -3.12,
            -8.4
          ],
          "math_task_scores": [
            -4.9,
            -10.93,
            -2.77,
            -10.14,
            -2.91,
            -14.35
          ],
          "code_task_scores": [
            -7.39,
            -1.67,
            7.11,
            0.45
          ],
          "reasoning_task_scores": [
            -3.84,
            -0.84,
            -1.62,
            7.17
          ]
        }
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 71,
      "name": "Open-Platypus",
      "domain": "reasoning",
      "general_avg": 55.39,
      "math_avg": 41.21,
      "code_avg": 41.4,
      "reasoning_avg": 45.85,
      "overall_avg": 45.96,
      "overall_efficiency": 0.022512,
      "general_efficiency": 0.158322,
      "math_efficiency": -0.19364,
      "code_efficiency": 0.058874,
      "reasoning_efficiency": 0.066488,
      "general_scores": [
        61.28,
        48.8825,
        61.31,
        50.0907143
      ],
      "math_scores": [
        83.55,
        49.2,
        18.81,
        16.17,
        3.33,
        76.22
      ],
      "code_scores": [
        72.76,
        10.04,
        65.85,
        16.95
      ],
      "reasoning_scores": [
        67.48,
        32.83,
        39.98913,
        43.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.28
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.31
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.55
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.81
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.17
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.85
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.48
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.99
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.95,
          "math_avg": -4.83,
          "code_avg": 1.47,
          "reasoning_avg": 1.66,
          "overall_avg": 0.56,
          "overall_efficiency": 0.022512,
          "general_efficiency": 0.158322,
          "math_efficiency": -0.19364,
          "code_efficiency": 0.058874,
          "reasoning_efficiency": 0.066488,
          "general_task_scores": [
            -7.03,
            13.39,
            3.57,
            5.85
          ],
          "math_task_scores": [
            3.57,
            -1.0,
            -7.23,
            -19.74,
            -3.34,
            -1.22
          ],
          "code_task_scores": [
            1.16,
            1.8,
            22.56,
            -19.65
          ],
          "reasoning_task_scores": [
            -1.98,
            -2.02,
            0.79,
            9.84
          ]
        },
        "vs_instruct": {
          "general_avg": -10.82,
          "math_avg": -11.12,
          "code_avg": -0.28,
          "reasoning_avg": -1.46,
          "overall_avg": -5.92,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -5.92,
            -24.08,
            -6.13,
            -7.14
          ],
          "math_task_scores": [
            -8.72,
            -28.6,
            -10.59,
            -24.78,
            -6.25,
            12.2
          ],
          "code_task_scores": [
            -1.95,
            -1.07,
            10.36,
            -8.47
          ],
          "reasoning_task_scores": [
            -4.14,
            -1.01,
            -2.46,
            1.76
          ]
        }
      },
      "affiliation": "Boston University",
      "year": "2023",
      "size": "24.9k",
      "size_precise": "24926",
      "link": "https://huggingface.co/datasets/garage-bAInd/Open-Platypus",
      "paper_link": "https://arxiv.org/abs/2308.07317",
      "tag": "general,math,code,science"
    },
    {
      "id": 72,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 55.84,
      "math_avg": 50.81,
      "code_avg": 38.59,
      "reasoning_avg": 43.67,
      "overall_avg": 47.23,
      "overall_efficiency": 0.019453,
      "general_efficiency": 0.046883,
      "math_efficiency": 0.050836,
      "code_efficiency": -0.014296,
      "reasoning_efficiency": -0.005612,
      "general_scores": [
        82.73,
        48.36,
        62.45,
        29.8157143
      ],
      "math_scores": [
        93.4,
        82.0,
        32.45,
        51.04,
        16.67,
        29.27
      ],
      "code_scores": [
        74.71,
        8.24,
        52.44,
        18.98
      ],
      "reasoning_scores": [
        67.71,
        20.71,
        34.826087,
        51.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.73
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.4
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.45
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.27
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 8.24
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.44
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.71
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.83
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.39,
          "math_avg": 4.77,
          "code_avg": -1.34,
          "reasoning_avg": -0.53,
          "overall_avg": 1.82,
          "overall_efficiency": 0.019453,
          "general_efficiency": 0.046883,
          "math_efficiency": 0.050836,
          "code_efficiency": -0.014296,
          "reasoning_efficiency": -0.005612,
          "general_task_scores": [
            14.42,
            12.87,
            4.71,
            -14.42
          ],
          "math_task_scores": [
            13.42,
            31.8,
            6.41,
            15.13,
            10.0,
            -48.17
          ],
          "code_task_scores": [
            3.11,
            0.0,
            9.15,
            -17.62
          ],
          "reasoning_task_scores": [
            -1.75,
            -14.14,
            -4.37,
            18.16
          ]
        },
        "vs_instruct": {
          "general_avg": -10.37,
          "math_avg": -1.53,
          "code_avg": -3.09,
          "reasoning_avg": -3.64,
          "overall_avg": -4.66,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            15.53,
            -24.6,
            -4.99,
            -27.41
          ],
          "math_task_scores": [
            1.13,
            4.2,
            3.05,
            10.09,
            7.09,
            -34.75
          ],
          "code_task_scores": [
            0.0,
            -2.87,
            -3.05,
            -6.44
          ],
          "reasoning_task_scores": [
            -3.91,
            -13.13,
            -7.62,
            10.08
          ]
        }
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 73,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 61.04,
      "math_avg": 55.72,
      "code_avg": 45.11,
      "reasoning_avg": 44.76,
      "overall_avg": 51.66,
      "overall_efficiency": 0.04698,
      "general_efficiency": 0.072069,
      "math_efficiency": 0.072705,
      "code_efficiency": 0.038918,
      "reasoning_efficiency": 0.004228,
      "general_scores": [
        83.74,
        42.26,
        64.18,
        53.9678571
      ],
      "math_scores": [
        93.93,
        77.4,
        27.26,
        53.9678571,
        18.335,
        63.41
      ],
      "code_scores": [
        73.15,
        11.47,
        61.59,
        34.24
      ],
      "reasoning_scores": [
        71.11,
        26.77,
        34.76087,
        46.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 93.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 61.59
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.76
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.59,
          "math_avg": 9.68,
          "code_avg": 5.18,
          "reasoning_avg": 0.56,
          "overall_avg": 6.25,
          "overall_efficiency": 0.04698,
          "general_efficiency": 0.072069,
          "math_efficiency": 0.072705,
          "code_efficiency": 0.038918,
          "reasoning_efficiency": 0.004228,
          "general_task_scores": [
            15.43,
            6.77,
            6.44,
            9.73
          ],
          "math_task_scores": [
            13.95,
            27.2,
            1.22,
            18.06,
            11.67,
            -14.03
          ],
          "code_task_scores": [
            1.55,
            3.23,
            18.3,
            -2.36
          ],
          "reasoning_task_scores": [
            1.65,
            -8.08,
            -4.44,
            13.12
          ]
        },
        "vs_instruct": {
          "general_avg": -5.17,
          "math_avg": 3.38,
          "code_avg": 3.43,
          "reasoning_avg": -2.56,
          "overall_avg": -0.23,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.54,
            -30.7,
            -3.26,
            -3.26
          ],
          "math_task_scores": [
            1.66,
            -0.4,
            -2.14,
            13.02,
            8.76,
            -0.61
          ],
          "code_task_scores": [
            -1.56,
            0.36,
            6.1,
            8.82
          ],
          "reasoning_task_scores": [
            -0.51,
            -7.07,
            -7.69,
            5.04
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 74,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 60.02,
      "math_avg": 42.46,
      "code_avg": 43.5,
      "reasoning_avg": 42.69,
      "overall_avg": 47.17,
      "overall_efficiency": 0.012763,
      "general_efficiency": 0.062123,
      "math_efficiency": -0.025948,
      "code_efficiency": 0.025833,
      "reasoning_efficiency": -0.010953,
      "general_scores": [
        83.44,
        41.2775,
        64.71,
        50.6421429
      ],
      "math_scores": [
        92.12,
        75.2,
        25.07,
        39.76,
        18.335,
        4.27
      ],
      "code_scores": [
        72.37,
        13.26,
        64.63,
        23.73
      ],
      "reasoning_scores": [
        69.8,
        19.19,
        33.673913,
        48.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.07
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.76
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.27
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.63
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.8
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.57,
          "math_avg": -3.58,
          "code_avg": 3.56,
          "reasoning_avg": -1.51,
          "overall_avg": 1.76,
          "overall_efficiency": 0.012763,
          "general_efficiency": 0.062123,
          "math_efficiency": -0.025948,
          "code_efficiency": 0.025833,
          "reasoning_efficiency": -0.010953,
          "general_task_scores": [
            15.13,
            5.79,
            6.97,
            6.4
          ],
          "math_task_scores": [
            12.14,
            25.0,
            -0.97,
            3.85,
            11.67,
            -73.17
          ],
          "code_task_scores": [
            0.77,
            5.02,
            21.34,
            -12.87
          ],
          "reasoning_task_scores": [
            0.34,
            -15.66,
            -5.53,
            14.8
          ]
        },
        "vs_instruct": {
          "general_avg": -6.19,
          "math_avg": -9.88,
          "code_avg": 1.81,
          "reasoning_avg": -4.63,
          "overall_avg": -4.72,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.24,
            -31.68,
            -2.73,
            -6.59
          ],
          "math_task_scores": [
            -0.15,
            -2.6,
            -4.33,
            -1.19,
            8.76,
            -59.75
          ],
          "code_task_scores": [
            -2.34,
            2.15,
            9.14,
            -1.69
          ],
          "reasoning_task_scores": [
            -1.82,
            -14.65,
            -8.78,
            6.72
          ]
        }
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 75,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 37.29,
      "math_avg": 48.47,
      "code_avg": 50.44,
      "reasoning_avg": 31.76,
      "overall_avg": 41.99,
      "overall_efficiency": -0.204324,
      "general_efficiency": -0.846819,
      "math_efficiency": 0.145472,
      "code_efficiency": 0.628516,
      "reasoning_efficiency": -0.744464,
      "general_scores": [
        62.4,
        27.34,
        39.08,
        20.3564286
      ],
      "math_scores": [
        85.44,
        72.0,
        25.34,
        37.39,
        13.335,
        57.32
      ],
      "code_scores": [
        62.26,
        13.62,
        53.66,
        72.2
      ],
      "reasoning_scores": [
        41.41,
        19.19,
        26.27,
        40.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.44
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.32
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 62.26
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.62
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 53.66
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -14.15,
          "math_avg": 2.43,
          "code_avg": 10.5,
          "reasoning_avg": -12.44,
          "overall_avg": -3.41,
          "overall_efficiency": -0.204324,
          "general_efficiency": -0.846819,
          "math_efficiency": 0.145472,
          "code_efficiency": 0.628516,
          "reasoning_efficiency": -0.744464,
          "general_task_scores": [
            -5.91,
            -8.15,
            -18.66,
            -23.88
          ],
          "math_task_scores": [
            5.46,
            21.8,
            -0.7,
            1.48,
            6.67,
            -20.12
          ],
          "code_task_scores": [
            -9.34,
            5.38,
            10.37,
            35.6
          ],
          "reasoning_task_scores": [
            -28.05,
            -15.66,
            -12.93,
            6.88
          ]
        },
        "vs_instruct": {
          "general_avg": -28.91,
          "math_avg": -3.87,
          "code_avg": 8.75,
          "reasoning_avg": -15.56,
          "overall_avg": -9.9,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.8,
            -45.62,
            -28.36,
            -36.87
          ],
          "math_task_scores": [
            -6.83,
            -5.8,
            -4.06,
            -3.56,
            3.76,
            -6.7
          ],
          "code_task_scores": [
            -12.45,
            2.51,
            -1.83,
            46.78
          ],
          "reasoning_task_scores": [
            -30.21,
            -14.65,
            -16.18,
            -1.2
          ]
        }
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "size_precise": "16710",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k",
      "paper_link": "https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation",
      "tag": "general,math,code,science"
    },
    {
      "id": 76,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 59.64,
      "math_avg": 58.92,
      "code_avg": 45.01,
      "reasoning_avg": 48.22,
      "overall_avg": 52.95,
      "overall_efficiency": 0.066208,
      "general_efficiency": 0.071938,
      "math_efficiency": 0.113021,
      "code_efficiency": 0.044535,
      "reasoning_efficiency": 0.03534,
      "general_scores": [
        85.96,
        41.2125,
        55.83,
        55.5671429
      ],
      "math_scores": [
        92.27,
        83.6,
        32.99,
        53.26,
        22.4975,
        68.9
      ],
      "code_scores": [
        73.93,
        17.2,
        68.9,
        20.0
      ],
      "reasoning_scores": [
        72.38,
        32.32,
        37.23913,
        50.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.83
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.27
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.5
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.9
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.2
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.9
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.38
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.2,
          "math_avg": 12.88,
          "code_avg": 5.08,
          "reasoning_avg": 4.03,
          "overall_avg": 7.54,
          "overall_efficiency": 0.066208,
          "general_efficiency": 0.071938,
          "math_efficiency": 0.113021,
          "code_efficiency": 0.044535,
          "reasoning_efficiency": 0.03534,
          "general_task_scores": [
            17.65,
            5.72,
            -1.91,
            11.33
          ],
          "math_task_scores": [
            12.29,
            33.4,
            6.95,
            17.35,
            15.83,
            -8.54
          ],
          "code_task_scores": [
            2.33,
            8.96,
            25.61,
            -16.6
          ],
          "reasoning_task_scores": [
            2.92,
            -2.53,
            -1.96,
            17.68
          ]
        },
        "vs_instruct": {
          "general_avg": -6.57,
          "math_avg": 6.58,
          "code_avg": 3.33,
          "reasoning_avg": 0.91,
          "overall_avg": 1.06,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            18.76,
            -31.75,
            -11.61,
            -1.66
          ],
          "math_task_scores": [
            0.0,
            5.8,
            3.59,
            12.31,
            12.92,
            4.88
          ],
          "code_task_scores": [
            -0.78,
            6.09,
            13.41,
            -5.42
          ],
          "reasoning_task_scores": [
            0.76,
            -1.52,
            -5.21,
            9.6
          ]
        }
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,math,code,science"
    },
    {
      "id": 77,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 57.16,
      "math_avg": 45.82,
      "code_avg": 41.16,
      "reasoning_avg": 45.47,
      "overall_avg": 47.4,
      "overall_efficiency": 0.011642,
      "general_efficiency": 0.033308,
      "math_efficiency": -0.001301,
      "code_efficiency": 0.007137,
      "reasoning_efficiency": 0.007421,
      "general_scores": [
        68.95,
        46.615,
        63.45,
        49.6321429
      ],
      "math_scores": [
        87.41,
        60.0,
        23.98,
        31.6,
        6.67,
        65.24
      ],
      "code_scores": [
        68.48,
        11.11,
        64.02,
        21.02
      ],
      "reasoning_scores": [
        65.23,
        33.33,
        36.445652,
        46.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.45
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.98
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.24
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.02
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.72,
          "math_avg": -0.22,
          "code_avg": 1.23,
          "reasoning_avg": 1.27,
          "overall_avg": 2.0,
          "overall_efficiency": 0.011642,
          "general_efficiency": 0.033308,
          "math_efficiency": -0.001301,
          "code_efficiency": 0.007137,
          "reasoning_efficiency": 0.007421,
          "general_task_scores": [
            0.64,
            11.13,
            5.71,
            5.39
          ],
          "math_task_scores": [
            7.43,
            9.8,
            -2.06,
            -4.31,
            0.0,
            -12.2
          ],
          "code_task_scores": [
            -3.12,
            2.87,
            20.73,
            -15.58
          ],
          "reasoning_task_scores": [
            -4.23,
            -1.52,
            -2.75,
            13.6
          ]
        },
        "vs_instruct": {
          "general_avg": -9.05,
          "math_avg": -6.52,
          "code_avg": -0.52,
          "reasoning_avg": -1.84,
          "overall_avg": -4.48,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            1.75,
            -26.34,
            -3.99,
            -7.6
          ],
          "math_task_scores": [
            -4.86,
            -17.8,
            -5.42,
            -9.35,
            -2.91,
            1.22
          ],
          "code_task_scores": [
            -6.23,
            0.0,
            8.53,
            -4.4
          ],
          "reasoning_task_scores": [
            -6.39,
            -0.51,
            -6.0,
            5.52
          ]
        }
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 78,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 56.83,
      "math_avg": 51.27,
      "code_avg": 58.9,
      "reasoning_avg": 47.38,
      "overall_avg": 53.6,
      "overall_efficiency": 0.105483,
      "general_efficiency": 0.069374,
      "math_efficiency": 0.067366,
      "code_efficiency": 0.244191,
      "reasoning_efficiency": 0.040999,
      "general_scores": [
        77.74,
        37.1225,
        62.77,
        49.7028571
      ],
      "math_scores": [
        90.45,
        74.2,
        24.82,
        41.1,
        10.0,
        67.07
      ],
      "code_scores": [
        74.71,
        12.54,
        67.68,
        80.68
      ],
      "reasoning_scores": [
        72.49,
        33.84,
        36.0,
        47.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.82
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 5.39,
          "math_avg": 5.23,
          "code_avg": 18.97,
          "reasoning_avg": 3.18,
          "overall_avg": 8.19,
          "overall_efficiency": 0.105483,
          "general_efficiency": 0.069374,
          "math_efficiency": 0.067366,
          "code_efficiency": 0.244191,
          "reasoning_efficiency": 0.040999,
          "general_task_scores": [
            9.43,
            1.63,
            5.03,
            5.46
          ],
          "math_task_scores": [
            10.47,
            24.0,
            -1.22,
            5.19,
            3.33,
            -10.37
          ],
          "code_task_scores": [
            3.11,
            4.3,
            24.39,
            44.08
          ],
          "reasoning_task_scores": [
            3.03,
            -1.01,
            -3.2,
            13.92
          ]
        },
        "vs_instruct": {
          "general_avg": -9.37,
          "math_avg": -1.06,
          "code_avg": 17.22,
          "reasoning_avg": 0.07,
          "overall_avg": 1.71,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            10.54,
            -35.84,
            -4.67,
            -7.53
          ],
          "math_task_scores": [
            -1.82,
            -3.6,
            -4.58,
            0.15,
            0.42,
            3.05
          ],
          "code_task_scores": [
            0.0,
            1.43,
            12.19,
            55.26
          ],
          "reasoning_task_scores": [
            0.87,
            0.0,
            -6.45,
            5.84
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 79,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 55.46,
      "math_avg": 38.22,
      "code_avg": 40.73,
      "reasoning_avg": 48.82,
      "overall_avg": 45.81,
      "overall_efficiency": 1.238474,
      "general_efficiency": 12.28293,
      "math_efficiency": -23.909276,
      "code_efficiency": 2.446483,
      "reasoning_efficiency": 14.13376,
      "general_scores": [
        64.66,
        49.0625,
        59.22,
        48.90143
      ],
      "math_scores": [
        86.66,
        69.6,
        26.17,
        31.9,
        15.0,
        0.0
      ],
      "code_scores": [
        72.76,
        13.26,
        57.93,
        18.98
      ],
      "reasoning_scores": [
        70.74,
        34.85,
        40.086957,
        49.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.66
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.06
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.17
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.74
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.09
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.02,
          "math_avg": -7.82,
          "code_avg": 0.8,
          "reasoning_avg": 4.62,
          "overall_avg": 0.4,
          "overall_efficiency": 1.238474,
          "general_efficiency": 12.28293,
          "math_efficiency": -23.909276,
          "code_efficiency": 2.446483,
          "reasoning_efficiency": 14.13376,
          "general_task_scores": [
            -3.65,
            13.57,
            1.48,
            4.66
          ],
          "math_task_scores": [
            6.68,
            19.4,
            0.13,
            -4.01,
            8.33,
            -77.44
          ],
          "code_task_scores": [
            1.16,
            5.02,
            14.64,
            -17.62
          ],
          "reasoning_task_scores": [
            1.28,
            0.0,
            0.89,
            16.32
          ]
        },
        "vs_instruct": {
          "general_avg": -10.75,
          "math_avg": -14.12,
          "code_avg": -0.95,
          "reasoning_avg": 1.5,
          "overall_avg": -6.08,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -2.54,
            -23.9,
            -8.22,
            -8.33
          ],
          "math_task_scores": [
            -5.61,
            -8.2,
            -3.23,
            -9.05,
            5.42,
            -64.02
          ],
          "code_task_scores": [
            -1.95,
            2.15,
            2.44,
            -6.44
          ],
          "reasoning_task_scores": [
            -0.88,
            1.01,
            -2.36,
            8.24
          ]
        }
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 80,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 60.83,
      "math_avg": 41.5,
      "code_avg": 37.8,
      "reasoning_avg": 47.02,
      "overall_avg": 46.79,
      "overall_efficiency": 0.02201,
      "general_efficiency": 0.149183,
      "math_efficiency": -0.07215,
      "code_efficiency": -0.03381,
      "reasoning_efficiency": 0.044815,
      "general_scores": [
        83.76,
        42.22,
        63.94,
        53.4071429
      ],
      "math_scores": [
        91.28,
        76.4,
        29.18,
        42.14,
        10.0,
        0.0
      ],
      "code_scores": [
        72.37,
        10.04,
        0.0,
        68.81
      ],
      "reasoning_scores": [
        74.06,
        26.77,
        37.0,
        50.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.18
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.39,
          "math_avg": -4.54,
          "code_avg": -2.13,
          "reasoning_avg": 2.82,
          "overall_avg": 1.38,
          "overall_efficiency": 0.02201,
          "general_efficiency": 0.149183,
          "math_efficiency": -0.07215,
          "code_efficiency": -0.03381,
          "reasoning_efficiency": 0.044815,
          "general_task_scores": [
            15.45,
            6.73,
            6.2,
            9.17
          ],
          "math_task_scores": [
            11.3,
            26.2,
            3.14,
            6.23,
            3.33,
            -77.44
          ],
          "code_task_scores": [
            0.77,
            1.8,
            -43.29,
            32.21
          ],
          "reasoning_task_scores": [
            4.6,
            -8.08,
            -2.2,
            16.96
          ]
        },
        "vs_instruct": {
          "general_avg": -5.38,
          "math_avg": -10.84,
          "code_avg": -3.88,
          "reasoning_avg": -0.3,
          "overall_avg": -5.1,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.56,
            -30.74,
            -3.5,
            -3.82
          ],
          "math_task_scores": [
            -0.99,
            -1.4,
            -0.22,
            1.19,
            0.42,
            -64.02
          ],
          "code_task_scores": [
            -2.34,
            -1.07,
            -55.49,
            43.39
          ],
          "reasoning_task_scores": [
            2.44,
            -7.07,
            -5.45,
            8.88
          ]
        }
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 55.21,
      "math_avg": 49.0,
      "code_avg": 54.33,
      "reasoning_avg": 44.03,
      "overall_avg": 50.64,
      "overall_efficiency": 0.020957,
      "general_efficiency": 0.015073,
      "math_efficiency": 0.011827,
      "code_efficiency": 0.057608,
      "reasoning_efficiency": -0.000681,
      "general_scores": [
        81.75,
        34.23,
        59.47,
        45.3957143
      ],
      "math_scores": [
        88.48,
        72.4,
        29.97,
        39.02,
        11.665,
        52.44
      ],
      "code_scores": [
        70.43,
        7.53,
        51.22,
        88.14
      ],
      "reasoning_scores": [
        70.82,
        19.19,
        33.858696,
        52.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.75
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.23
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.48
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.02
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.66
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 70.43
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.53
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.22
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.86
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.77,
          "math_avg": 2.96,
          "code_avg": 14.4,
          "reasoning_avg": -0.17,
          "overall_avg": 5.24,
          "overall_efficiency": 0.020957,
          "general_efficiency": 0.015073,
          "math_efficiency": 0.011827,
          "code_efficiency": 0.057608,
          "reasoning_efficiency": -0.000681,
          "general_task_scores": [
            13.44,
            -1.26,
            1.73,
            1.16
          ],
          "math_task_scores": [
            8.5,
            22.2,
            3.93,
            3.11,
            4.99,
            -25.0
          ],
          "code_task_scores": [
            -1.17,
            -0.71,
            7.93,
            51.54
          ],
          "reasoning_task_scores": [
            1.36,
            -15.66,
            -5.34,
            18.96
          ]
        },
        "vs_instruct": {
          "general_avg": -11.0,
          "math_avg": -3.34,
          "code_avg": 12.65,
          "reasoning_avg": -3.29,
          "overall_avg": -1.25,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            14.55,
            -38.73,
            -7.97,
            -11.83
          ],
          "math_task_scores": [
            -3.79,
            -5.4,
            0.57,
            -1.93,
            2.08,
            -11.58
          ],
          "code_task_scores": [
            -4.28,
            -3.58,
            -4.27,
            62.72
          ],
          "reasoning_task_scores": [
            -0.8,
            -14.65,
            -8.59,
            10.88
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 61.14,
      "math_avg": 42.47,
      "code_avg": 55.08,
      "reasoning_avg": 44.49,
      "overall_avg": 50.79,
      "overall_efficiency": 0.035926,
      "general_efficiency": 0.064611,
      "math_efficiency": -0.023827,
      "code_efficiency": 0.100966,
      "reasoning_efficiency": 0.001953,
      "general_scores": [
        78.37,
        55.5325,
        63.26,
        47.3821429
      ],
      "math_scores": [
        90.14,
        47.2,
        19.06,
        19.29,
        1.665,
        77.44
      ],
      "code_scores": [
        74.71,
        15.41,
        72.56,
        57.63
      ],
      "reasoning_scores": [
        65.52,
        28.79,
        38.771739,
        44.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.06
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.41
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.56
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.52
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.79
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.77
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 9.69,
          "math_avg": -3.57,
          "code_avg": 15.15,
          "reasoning_avg": 0.29,
          "overall_avg": 5.39,
          "overall_efficiency": 0.035926,
          "general_efficiency": 0.064611,
          "math_efficiency": -0.023827,
          "code_efficiency": 0.100966,
          "reasoning_efficiency": 0.001953,
          "general_task_scores": [
            10.06,
            20.04,
            5.52,
            3.14
          ],
          "math_task_scores": [
            10.16,
            -3.0,
            -6.98,
            -16.62,
            -5.01,
            0.0
          ],
          "code_task_scores": [
            3.11,
            7.17,
            29.27,
            21.03
          ],
          "reasoning_task_scores": [
            -3.94,
            -6.06,
            -0.43,
            11.6
          ]
        },
        "vs_instruct": {
          "general_avg": -5.07,
          "math_avg": -9.87,
          "code_avg": 13.39,
          "reasoning_avg": -2.83,
          "overall_avg": -1.09,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            11.17,
            -17.43,
            -4.18,
            -9.85
          ],
          "math_task_scores": [
            -2.13,
            -30.6,
            -10.34,
            -21.66,
            -7.92,
            13.42
          ],
          "code_task_scores": [
            0.0,
            4.3,
            17.07,
            32.21
          ],
          "reasoning_task_scores": [
            -6.1,
            -5.05,
            -3.68,
            3.52
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 59.46,
      "math_avg": 45.76,
      "code_avg": 45.31,
      "reasoning_avg": 41.3,
      "overall_avg": 47.96,
      "overall_efficiency": 0.010214,
      "general_efficiency": 0.032055,
      "math_efficiency": -0.00112,
      "code_efficiency": 0.021517,
      "reasoning_efficiency": -0.011597,
      "general_scores": [
        88.1,
        41.1025,
        58.46,
        50.16
      ],
      "math_scores": [
        88.48,
        72.4,
        24.3,
        39.17,
        16.67,
        33.54
      ],
      "code_scores": [
        74.32,
        6.09,
        74.39,
        26.44
      ],
      "reasoning_scores": [
        69.28,
        15.66,
        33.456522,
        46.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.48
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.3
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.54
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.09
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.66
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.46
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.8
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 8.01,
          "math_avg": -0.28,
          "code_avg": 5.38,
          "reasoning_avg": -2.9,
          "overall_avg": 2.55,
          "overall_efficiency": 0.010214,
          "general_efficiency": 0.032055,
          "math_efficiency": -0.00112,
          "code_efficiency": 0.021517,
          "reasoning_efficiency": -0.011597,
          "general_task_scores": [
            19.79,
            5.61,
            0.72,
            5.92
          ],
          "math_task_scores": [
            8.5,
            22.2,
            -1.74,
            3.26,
            10.0,
            -43.9
          ],
          "code_task_scores": [
            2.72,
            -2.15,
            31.1,
            -10.16
          ],
          "reasoning_task_scores": [
            -0.18,
            -19.19,
            -5.74,
            13.52
          ]
        },
        "vs_instruct": {
          "general_avg": -6.75,
          "math_avg": -6.58,
          "code_avg": 3.63,
          "reasoning_avg": -6.02,
          "overall_avg": -3.93,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            20.9,
            -31.86,
            -8.98,
            -7.07
          ],
          "math_task_scores": [
            -3.79,
            -5.4,
            -5.1,
            -1.78,
            7.09,
            -30.48
          ],
          "code_task_scores": [
            -0.39,
            -5.02,
            18.9,
            1.02
          ],
          "reasoning_task_scores": [
            -2.34,
            -18.18,
            -8.99,
            5.44
          ]
        }
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 84,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 44.19,
      "math_avg": 29.98,
      "code_avg": 42.61,
      "reasoning_avg": 43.46,
      "overall_avg": 40.06,
      "overall_efficiency": -0.267782,
      "general_efficiency": -0.363541,
      "math_efficiency": -0.805046,
      "code_efficiency": 0.134377,
      "reasoning_efficiency": -0.036919,
      "general_scores": [
        28.43,
        44.7325,
        60.72,
        42.8935714
      ],
      "math_scores": [
        85.75,
        52.4,
        16.78,
        19.14,
        5.835,
        0.0
      ],
      "code_scores": [
        71.6,
        9.68,
        66.46,
        22.71
      ],
      "reasoning_scores": [
        67.97,
        31.31,
        31.684783,
        42.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.75
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.78
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.84
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.46
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.68
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -7.25,
          "math_avg": -16.06,
          "code_avg": 2.68,
          "reasoning_avg": -0.74,
          "overall_avg": -5.34,
          "overall_efficiency": -0.267782,
          "general_efficiency": -0.363541,
          "math_efficiency": -0.805046,
          "code_efficiency": 0.134377,
          "reasoning_efficiency": -0.036919,
          "general_task_scores": [
            -39.88,
            9.24,
            2.98,
            -1.35
          ],
          "math_task_scores": [
            5.77,
            2.2,
            -9.26,
            -16.77,
            -0.83,
            -77.44
          ],
          "code_task_scores": [
            0.0,
            1.44,
            23.17,
            -13.89
          ],
          "reasoning_task_scores": [
            -1.49,
            -3.54,
            -7.52,
            9.6
          ]
        },
        "vs_instruct": {
          "general_avg": -22.01,
          "math_avg": -22.35,
          "code_avg": 0.93,
          "reasoning_avg": -3.86,
          "overall_avg": -11.82,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -38.77,
            -28.23,
            -6.72,
            -14.34
          ],
          "math_task_scores": [
            -6.52,
            -25.4,
            -12.62,
            -21.81,
            -3.74,
            -64.02
          ],
          "code_task_scores": [
            -3.11,
            -1.43,
            10.97,
            -2.71
          ],
          "reasoning_task_scores": [
            -3.65,
            -2.53,
            -10.77,
            1.52
          ]
        }
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 85,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 55.16,
      "math_avg": 41.79,
      "code_avg": 35.02,
      "reasoning_avg": 46.72,
      "overall_avg": 44.67,
      "overall_efficiency": -0.037158,
      "general_efficiency": 0.188336,
      "math_efficiency": -0.215459,
      "code_efficiency": -0.249315,
      "reasoning_efficiency": 0.127805,
      "general_scores": [
        62.65,
        42.0875,
        64.67,
        51.2142857
      ],
      "math_scores": [
        89.76,
        71.2,
        27.35,
        34.72,
        12.4975,
        15.24
      ],
      "code_scores": [
        72.76,
        11.47,
        31.1,
        24.75
      ],
      "reasoning_scores": [
        70.33,
        30.3,
        37.913043,
        48.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.76
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.35
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.5
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.24
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.91
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 3.71,
          "math_avg": -4.25,
          "code_avg": -4.91,
          "reasoning_avg": 2.52,
          "overall_avg": -0.73,
          "overall_efficiency": -0.037158,
          "general_efficiency": 0.188336,
          "math_efficiency": -0.215459,
          "code_efficiency": -0.249315,
          "reasoning_efficiency": 0.127805,
          "general_task_scores": [
            -5.66,
            6.6,
            6.93,
            6.97
          ],
          "math_task_scores": [
            9.78,
            21.0,
            1.31,
            -1.19,
            5.83,
            -62.2
          ],
          "code_task_scores": [
            1.16,
            3.23,
            -12.19,
            -11.85
          ],
          "reasoning_task_scores": [
            0.87,
            -4.55,
            -1.29,
            15.04
          ]
        },
        "vs_instruct": {
          "general_avg": -11.05,
          "math_avg": -10.54,
          "code_avg": -6.66,
          "reasoning_avg": -0.6,
          "overall_avg": -7.21,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            -4.55,
            -30.87,
            -2.77,
            -6.02
          ],
          "math_task_scores": [
            -2.51,
            -6.6,
            -2.05,
            -6.23,
            2.92,
            -48.78
          ],
          "code_task_scores": [
            -1.95,
            0.36,
            -24.39,
            -0.67
          ],
          "reasoning_task_scores": [
            -1.29,
            -3.54,
            -4.54,
            6.96
          ]
        }
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 86,
      "name": "SCP-116K",
      "domain": "reasoning",
      "general_avg": 56.05,
      "math_avg": 52.46,
      "code_avg": 48.76,
      "reasoning_avg": 43.07,
      "overall_avg": 50.09,
      "overall_efficiency": 0.017081,
      "general_efficiency": 0.016797,
      "math_efficiency": 0.023418,
      "code_efficiency": 0.032216,
      "reasoning_efficiency": -0.004108,
      "general_scores": [
        83.48,
        33.5275,
        57.03,
        50.1614286
      ],
      "math_scores": [
        90.52,
        74.2,
        31.75,
        42.28,
        33.3325,
        42.68
      ],
      "code_scores": [
        60.7,
        10.04,
        37.2,
        87.12
      ],
      "reasoning_scores": [
        71.47,
        23.74,
        27.315217,
        49.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.28
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 60.7
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.2
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.61,
          "math_avg": 6.42,
          "code_avg": 8.83,
          "reasoning_avg": -1.13,
          "overall_avg": 4.68,
          "overall_efficiency": 0.017081,
          "general_efficiency": 0.016797,
          "math_efficiency": 0.023418,
          "code_efficiency": 0.032216,
          "reasoning_efficiency": -0.004108,
          "general_task_scores": [
            15.17,
            -1.96,
            -0.71,
            5.92
          ],
          "math_task_scores": [
            10.54,
            24.0,
            5.71,
            6.37,
            26.66,
            -34.76
          ],
          "code_task_scores": [
            -10.9,
            1.8,
            -6.09,
            50.52
          ],
          "reasoning_task_scores": [
            2.01,
            -11.11,
            -11.88,
            16.48
          ]
        },
        "vs_instruct": {
          "general_avg": -10.16,
          "math_avg": 0.12,
          "code_avg": 7.08,
          "reasoning_avg": -4.25,
          "overall_avg": -1.8,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.28,
            -39.43,
            -10.41,
            -7.07
          ],
          "math_task_scores": [
            -1.75,
            -3.6,
            2.35,
            1.33,
            23.75,
            -21.34
          ],
          "code_task_scores": [
            -14.01,
            -1.07,
            -18.29,
            61.7
          ],
          "reasoning_task_scores": [
            -0.15,
            -10.1,
            -15.13,
            8.4
          ]
        }
      },
      "affiliation": "Fudan University",
      "year": "2025",
      "size": "274k",
      "size_precise": "274166",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K",
      "paper_link": "https://arxiv.org/abs/2501.15587",
      "tag": "general,math,science"
    },
    {
      "id": 87,
      "name": "AM-Thinking-v1-Distilled-code",
      "domain": "reasoning",
      "general_avg": 49.86,
      "math_avg": 55.49,
      "code_avg": 69.76,
      "reasoning_avg": 34.89,
      "overall_avg": 52.5,
      "overall_efficiency": 0.021912,
      "general_efficiency": -0.004878,
      "math_efficiency": 0.029178,
      "code_efficiency": 0.092078,
      "reasoning_efficiency": -0.02873,
      "general_scores": [
        84.0,
        32.945,
        45.41,
        37.1014286
      ],
      "math_scores": [
        88.1,
        69.4,
        31.93,
        42.14,
        23.335,
        78.05
      ],
      "code_scores": [
        78.99,
        40.86,
        76.83,
        82.37
      ],
      "reasoning_scores": [
        55.88,
        11.11,
        28.57,
        44.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.41
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.93
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.05
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 78.99
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.88
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.11
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.57
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": -1.58,
          "math_avg": 9.45,
          "code_avg": 29.83,
          "reasoning_avg": -9.31,
          "overall_avg": 7.1,
          "overall_efficiency": 0.021912,
          "general_efficiency": -0.004878,
          "math_efficiency": 0.029178,
          "code_efficiency": 0.092078,
          "reasoning_efficiency": -0.02873,
          "general_task_scores": [
            15.69,
            -2.55,
            -12.33,
            -7.14
          ],
          "math_task_scores": [
            8.12,
            19.2,
            5.89,
            6.23,
            16.67,
            0.61
          ],
          "code_task_scores": [
            7.39,
            32.62,
            33.54,
            45.77
          ],
          "reasoning_task_scores": [
            -13.58,
            -23.74,
            -10.63,
            10.72
          ]
        },
        "vs_instruct": {
          "general_avg": -16.34,
          "math_avg": 3.16,
          "code_avg": 28.08,
          "reasoning_avg": -12.43,
          "overall_avg": 0.62,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            16.8,
            -40.02,
            -22.03,
            -20.13
          ],
          "math_task_scores": [
            -4.17,
            -8.4,
            2.53,
            1.19,
            13.76,
            14.03
          ],
          "code_task_scores": [
            4.28,
            29.75,
            21.34,
            56.95
          ],
          "reasoning_task_scores": [
            -15.74,
            -22.73,
            -13.88,
            2.64
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "324k",
      "size_precise": "323965",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/code.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "code"
    },
    {
      "id": 88,
      "name": "AM-Thinking-v1-Distilled-math",
      "domain": "reasoning",
      "general_avg": 57.74,
      "math_avg": 72.01,
      "code_avg": 40.71,
      "reasoning_avg": 42.63,
      "overall_avg": 53.27,
      "overall_efficiency": 0.014101,
      "general_efficiency": 0.01128,
      "math_efficiency": 0.046536,
      "code_efficiency": 0.001393,
      "reasoning_efficiency": -0.002804,
      "general_scores": [
        90.67,
        30.2025,
        61.55,
        48.5385714
      ],
      "math_scores": [
        94.16,
        96.2,
        60.55,
        73.0,
        60.0,
        48.17
      ],
      "code_scores": [
        57.98,
        8.96,
        42.68,
        53.22
      ],
      "reasoning_scores": [
        72.14,
        17.68,
        31.271739,
        49.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.2
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.54
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 94.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 96.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.55
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 60.0
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.17
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.14
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 6.3,
          "math_avg": 25.97,
          "code_avg": 0.78,
          "reasoning_avg": -1.56,
          "overall_avg": 7.87,
          "overall_efficiency": 0.014101,
          "general_efficiency": 0.01128,
          "math_efficiency": 0.046536,
          "code_efficiency": 0.001393,
          "reasoning_efficiency": -0.002804,
          "general_task_scores": [
            22.36,
            -5.29,
            3.81,
            4.3
          ],
          "math_task_scores": [
            14.18,
            46.0,
            34.51,
            37.09,
            53.33,
            -29.27
          ],
          "code_task_scores": [
            -13.62,
            0.72,
            -0.61,
            16.62
          ],
          "reasoning_task_scores": [
            2.68,
            -17.17,
            -7.93,
            16.16
          ]
        },
        "vs_instruct": {
          "general_avg": -8.47,
          "math_avg": 19.68,
          "code_avg": -0.97,
          "reasoning_avg": -4.68,
          "overall_avg": 1.39,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            23.47,
            -42.76,
            -5.89,
            -8.69
          ],
          "math_task_scores": [
            1.89,
            18.4,
            31.15,
            32.05,
            50.42,
            -15.85
          ],
          "code_task_scores": [
            -16.73,
            -2.15,
            -12.81,
            27.8
          ],
          "reasoning_task_scores": [
            0.52,
            -16.16,
            -11.18,
            8.08
          ]
        }
      },
      "affiliation": "a-m-team",
      "year": "2025",
      "size": "558k",
      "size_precise": "558129",
      "link": "https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled/blob/main/math.jsonl",
      "paper_link": "https://arxiv.org/abs/2505.14464",
      "tag": "math"
    },
    {
      "id": 89,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 55.54,
      "math_avg": 61.28,
      "code_avg": 46.9,
      "reasoning_avg": 45.32,
      "overall_avg": 52.26,
      "overall_efficiency": 0.08631,
      "general_efficiency": 0.051514,
      "math_efficiency": 0.191856,
      "code_efficiency": 0.087678,
      "reasoning_efficiency": 0.014194,
      "general_scores": [
        79.4,
        38.4875,
        58.01,
        46.2492857
      ],
      "math_scores": [
        92.04,
        88.0,
        43.34,
        60.24,
        38.335,
        45.73
      ],
      "code_scores": [
        66.15,
        3.23,
        40.24,
        77.97
      ],
      "reasoning_scores": [
        72.73,
        24.75,
        31.5,
        52.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.25
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.34
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.73
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 66.15
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.23
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.24
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.5
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 4.09,
          "math_avg": 15.24,
          "code_avg": 6.97,
          "reasoning_avg": 1.13,
          "overall_avg": 6.86,
          "overall_efficiency": 0.08631,
          "general_efficiency": 0.051514,
          "math_efficiency": 0.191856,
          "code_efficiency": 0.087678,
          "reasoning_efficiency": 0.014194,
          "general_task_scores": [
            11.09,
            3.0,
            0.27,
            2.01
          ],
          "math_task_scores": [
            12.06,
            37.8,
            17.3,
            24.33,
            31.67,
            -31.71
          ],
          "code_task_scores": [
            -5.45,
            -5.01,
            -3.05,
            41.37
          ],
          "reasoning_task_scores": [
            3.27,
            -10.1,
            -7.7,
            19.04
          ]
        },
        "vs_instruct": {
          "general_avg": -10.67,
          "math_avg": 8.94,
          "code_avg": 5.21,
          "reasoning_avg": -1.99,
          "overall_avg": 0.37,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            12.2,
            -34.47,
            -9.43,
            -10.98
          ],
          "math_task_scores": [
            -0.23,
            10.2,
            13.94,
            19.29,
            28.76,
            -18.29
          ],
          "code_task_scores": [
            -8.56,
            -7.88,
            -15.25,
            52.55
          ],
          "reasoning_task_scores": [
            1.11,
            -9.09,
            -10.95,
            10.96
          ]
        }
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,math,code,science"
    },
    {
      "id": 90,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 58.95,
      "math_avg": 53.88,
      "code_avg": 33.87,
      "reasoning_avg": 44.32,
      "overall_avg": 47.76,
      "overall_efficiency": 0.297832,
      "general_efficiency": 0.950553,
      "math_efficiency": 0.9923,
      "code_efficiency": -0.767405,
      "reasoning_efficiency": 0.015878,
      "general_scores": [
        84.94,
        43.3925,
        62.39,
        45.0928571
      ],
      "math_scores": [
        90.6,
        80.0,
        35.75,
        50.3,
        23.335,
        43.29
      ],
      "code_scores": [
        74.71,
        6.09,
        41.46,
        13.22
      ],
      "reasoning_scores": [
        66.05,
        29.8,
        33.521739,
        47.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.39
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.39
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.75
              }
            ]
          },
          {
            "task_name": "OlympiadBenchMath",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.34
              }
            ]
          },
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.29
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LCB(v5)",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.09
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.46
              }
            ]
          },
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.05
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.52
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.92
              }
            ]
          }
        ]
      },
      "improvement": {
        "vs_base": {
          "general_avg": 7.51,
          "math_avg": 7.84,
          "code_avg": -6.06,
          "reasoning_avg": 0.13,
          "overall_avg": 2.35,
          "overall_efficiency": 0.297832,
          "general_efficiency": 0.950553,
          "math_efficiency": 0.9923,
          "code_efficiency": -0.767405,
          "reasoning_efficiency": 0.015878,
          "general_task_scores": [
            16.63,
            7.9,
            4.65,
            0.85
          ],
          "math_task_scores": [
            10.62,
            29.8,
            9.71,
            14.39,
            16.67,
            -34.15
          ],
          "code_task_scores": [
            3.11,
            -2.15,
            -1.83,
            -23.38
          ],
          "reasoning_task_scores": [
            -3.41,
            -5.05,
            -5.68,
            14.64
          ]
        },
        "vs_instruct": {
          "general_avg": -7.25,
          "math_avg": 1.54,
          "code_avg": -7.81,
          "reasoning_avg": -2.99,
          "overall_avg": -4.13,
          "overall_efficiency": 0,
          "general_efficiency": 0,
          "math_efficiency": 0,
          "code_efficiency": 0,
          "reasoning_efficiency": 0,
          "general_task_scores": [
            17.74,
            -29.57,
            -5.05,
            -12.14
          ],
          "math_task_scores": [
            -1.67,
            2.2,
            6.35,
            9.35,
            13.76,
            -20.73
          ],
          "code_task_scores": [
            0.0,
            -5.02,
            -14.03,
            -12.2
          ],
          "reasoning_task_scores": [
            -5.57,
            -4.04,
            -8.93,
            6.56
          ]
        }
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ]
}