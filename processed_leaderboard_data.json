{
  "llama": [
    {
      "id": 0,
      "name": "meta-llama/Llama-3.1-8B",
      "domain": "base",
      "general_avg": 16.01,
      "math_avg": 13.45,
      "code_avg": 15.68,
      "reasoning_avg": 41.32,
      "overall_avg": 21.62,
      "general_task_scores": [
        21.83,
        20.3375,
        17.97,
        3.92071429
      ],
      "math_task_scores": [
        55.8,
        5.5,
        1.6,
        4.34,
        0.0
      ],
      "code_task_scores": [
        20.12,
        54.47,
        3.58,
        0.21,
        0.0
      ],
      "reasoning_task_scores": [
        80.34,
        62.56,
        0.312,
        22.08
      ]
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 27.88,
      "math_avg": 13.79,
      "code_avg": 26.31,
      "reasoning_avg": 31.94,
      "overall_avg": 24.98,
      "general_scores": [
        45.88,
        30.76,
        29.51,
        5.28642857,
        42.43,
        34.1625,
        29.15,
        6.52857143,
        43.37,
        31.745,
        30.01,
        5.76714286
      ],
      "math_scores": [
        43.06,
        9.2,
        11.0,
        7.66,
        3.33,
        36.54,
        9.76,
        10.4,
        7.7,
        0.0,
        42.53,
        8.78,
        9.6,
        7.36,
        0.0
      ],
      "code_scores": [
        32.32,
        45.53,
        4.3,
        27.56,
        9.73,
        39.02,
        50.58,
        1.79,
        29.23,
        17.65,
        35.98,
        49.03,
        3.58,
        29.85,
        18.55
      ],
      "reasoning_scores": [
        66.44,
        49.22,
        0.28815217,
        13.68,
        61.69,
        46.13,
        0.29815217,
        18.24,
        63.39,
        48.34,
        0.31217391,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.56
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.88
              },
              {
                "metric": "lcb_test_output",
                "score": 15.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.87,
        "math_avg": 0.35,
        "code_avg": 10.64,
        "reasoning_avg": -9.38,
        "overall_avg": 3.37,
        "general_task_scores": [
          22.06,
          11.88,
          11.59,
          1.94
        ],
        "math_task_scores": [
          -15.09,
          3.75,
          8.73,
          3.23,
          1.11
        ],
        "code_task_scores": [
          15.65,
          -6.09,
          -0.36,
          28.67,
          15.31
        ],
        "reasoning_task_scores": [
          -16.5,
          -14.66,
          -0.01,
          -6.35
        ]
      },
      "affiliation": "nomic-ai",
      "year": "2023",
      "size": "809k",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 34.07,
      "math_avg": 4.07,
      "code_avg": 23.98,
      "reasoning_avg": 32.65,
      "overall_avg": 23.69,
      "general_scores": [
        38.3,
        42.8125,
        33.56,
        16.8542857,
        48.65,
        44.035,
        35.03,
        12.1478571,
        49.07,
        36.945,
        35.66,
        15.7421429
      ],
      "math_scores": [
        1.82,
        4.62,
        4.6,
        6.05,
        0.0,
        2.58,
        4.74,
        4.8,
        6.01,
        3.33,
        4.17,
        4.64,
        4.0,
        6.35,
        3.33
      ],
      "code_scores": [
        32.32,
        45.91,
        3.23,
        24.84,
        5.43,
        30.49,
        50.19,
        2.51,
        27.56,
        18.1,
        29.27,
        48.64,
        3.94,
        28.18,
        9.05
      ],
      "reasoning_scores": [
        69.83,
        37.81,
        0.3026087,
        20.0,
        70.85,
        43.84,
        0.29945652,
        20.64,
        69.15,
        38.37,
        0.30956522,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.86
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.35
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.05,
        "math_avg": -9.38,
        "code_avg": 8.3,
        "reasoning_avg": -8.67,
        "overall_avg": 2.08,
        "general_task_scores": [
          23.51,
          20.92,
          16.78,
          10.99
        ],
        "math_task_scores": [
          -52.94,
          -0.83,
          2.87,
          1.8,
          2.22
        ],
        "code_task_scores": [
          10.57,
          -6.22,
          -0.35,
          26.65,
          10.86
        ],
        "reasoning_task_scores": [
          -10.4,
          -22.55,
          -0.01,
          -1.73
        ]
      },
      "affiliation": "tatsu-lab",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 32.52,
      "math_avg": 7.57,
      "code_avg": 28.28,
      "reasoning_avg": 28.52,
      "overall_avg": 24.23,
      "general_scores": [
        25.39,
        41.765,
        33.13,
        12.5321429,
        47.94,
        41.5825,
        33.19,
        13.2785714,
        50.1,
        42.3275,
        34.28,
        14.7607143
      ],
      "math_scores": [
        18.04,
        8.54,
        8.8,
        9.35,
        0.0,
        14.86,
        7.06,
        7.4,
        9.98,
        0.0,
        8.72,
        6.76,
        6.0,
        8.06,
        0.0
      ],
      "code_scores": [
        37.2,
        52.14,
        2.51,
        23.8,
        20.36,
        41.46,
        53.7,
        3.58,
        21.92,
        20.36,
        42.68,
        56.81,
        6.09,
        21.29,
        20.36
      ],
      "reasoning_scores": [
        54.58,
        37.05,
        0.32521739,
        23.36,
        55.93,
        29.86,
        0.3526087,
        22.0,
        56.27,
        39.27,
        0.32434783,
        22.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.45
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.34
              },
              {
                "metric": "lcb_test_output",
                "score": 20.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.51,
        "math_avg": -5.88,
        "code_avg": 12.61,
        "reasoning_avg": -12.8,
        "overall_avg": 2.61,
        "general_task_scores": [
          19.31,
          21.55,
          15.56,
          9.6
        ],
        "math_task_scores": [
          -41.93,
          1.95,
          5.8,
          4.79,
          0.0
        ],
        "code_task_scores": [
          20.33,
          -0.25,
          0.48,
          22.13,
          20.36
        ],
        "reasoning_task_scores": [
          -24.75,
          -27.17,
          0.02,
          0.69
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 33.77,
      "math_avg": 6.33,
      "code_avg": 16.73,
      "reasoning_avg": 22.82,
      "overall_avg": 19.91,
      "general_scores": [
        67.24,
        38.7275,
        23.84,
        3.29928571,
        68.3,
        37.7775,
        27.33,
        2.54714286,
        66.8,
        42.2075,
        24.01,
        3.13642857
      ],
      "math_scores": [
        11.52,
        6.46,
        7.6,
        8.69,
        0.0,
        10.92,
        6.98,
        6.4,
        7.9,
        0.0,
        5.53,
        6.5,
        8.0,
        8.42,
        0.0
      ],
      "code_scores": [
        6.71,
        54.86,
        2.87,
        6.47,
        4.75,
        1.22,
        55.64,
        2.87,
        17.95,
        2.26,
        21.95,
        56.03,
        1.43,
        14.61,
        1.36
      ],
      "reasoning_scores": [
        20.34,
        52.1,
        0.33684783,
        19.36,
        30.51,
        49.95,
        0.3623913,
        18.4,
        19.32,
        43.9,
        0.35630435,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.99
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.96
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.01
              },
              {
                "metric": "lcb_test_output",
                "score": 2.79
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.75,
        "math_avg": -7.12,
        "code_avg": 1.06,
        "reasoning_avg": -18.51,
        "overall_avg": -1.7,
        "general_task_scores": [
          45.62,
          19.23,
          7.09,
          -0.93
        ],
        "math_task_scores": [
          -46.48,
          1.15,
          5.73,
          4.0,
          0.0
        ],
        "code_task_scores": [
          -10.16,
          1.04,
          -1.19,
          12.8,
          2.79
        ],
        "reasoning_task_scores": [
          -56.95,
          -13.91,
          0.04,
          -3.2
        ]
      },
      "affiliation": "databricks",
      "year": "2023",
      "size": "15k",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 41.63,
      "math_avg": 21.85,
      "code_avg": 30.9,
      "reasoning_avg": 35.27,
      "overall_avg": 32.41,
      "general_scores": [
        61.23,
        33.62,
        22.8685714,
        60.34,
        49.6325,
        34.8,
        23.5921429,
        62.71,
        48.0125,
        35.05,
        26.0314286
      ],
      "math_scores": [
        38.59,
        13.14,
        13.8,
        42.99,
        13.18,
        13.8,
        34.42,
        12.74,
        14.0
      ],
      "code_scores": [
        42.68,
        57.98,
        7.89,
        30.06,
        21.27,
        44.51,
        53.7,
        6.45,
        31.52,
        12.67,
        39.63,
        55.64,
        6.09,
        28.81,
        24.66
      ],
      "reasoning_scores": [
        60.68,
        59.9,
        0.33619565,
        24.4,
        55.93,
        57.8,
        0.3301087,
        23.04,
        58.31,
        58.85,
        0.34380435,
        23.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.13
              },
              {
                "metric": "lcb_test_output",
                "score": 19.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 25.61,
        "math_avg": 8.4,
        "code_avg": 15.23,
        "reasoning_avg": -6.05,
        "overall_avg": 10.8,
        "general_task_scores": [
          39.6,
          28.48,
          16.52,
          20.24
        ],
        "math_task_scores": [
          -17.13,
          7.52,
          12.27
        ],
        "code_task_scores": [
          22.15,
          1.3,
          3.23,
          29.92,
          19.53
        ],
        "reasoning_task_scores": [
          -22.03,
          -3.71,
          0.03,
          1.52
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 38.04,
      "math_avg": 15.47,
      "code_avg": 28.16,
      "reasoning_avg": 37.96,
      "overall_avg": 29.91,
      "general_scores": [
        41.16,
        47.9275,
        35.71,
        26.5814286,
        49.22,
        49.1075,
        33.58,
        20.6507143,
        45.08,
        49.3525,
        34.68,
        23.4607143
      ],
      "math_scores": [
        45.11,
        12.7,
        13.8,
        8.11,
        3.33,
        41.09,
        12.92,
        11.8,
        7.72,
        0.0,
        40.41,
        12.96,
        14.4,
        7.7,
        0.0
      ],
      "code_scores": [
        43.9,
        55.64,
        5.02,
        12.32,
        18.33,
        50.61,
        52.53,
        3.23,
        18.16,
        17.42,
        45.12,
        57.59,
        5.73,
        22.34,
        14.48
      ],
      "reasoning_scores": [
        71.86,
        54.41,
        0.31336957,
        24.72,
        69.83,
        55.51,
        0.35206522,
        25.28,
        70.51,
        56.47,
        0.33565217,
        25.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.61
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 22.03,
        "math_avg": 2.02,
        "code_avg": 12.49,
        "reasoning_avg": -3.36,
        "overall_avg": 8.29,
        "general_task_scores": [
          23.32,
          28.46,
          16.69,
          19.64
        ],
        "math_task_scores": [
          -13.6,
          7.36,
          11.73,
          3.5,
          1.11
        ],
        "code_task_scores": [
          26.42,
          0.78,
          1.08,
          17.4,
          16.74
        ],
        "reasoning_task_scores": [
          -9.61,
          -7.1,
          0.02,
          3.23
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 30.01,
      "math_avg": 5.61,
      "code_avg": 23.54,
      "reasoning_avg": 27.38,
      "overall_avg": 21.64,
      "general_scores": [
        66.09,
        29.2525,
        24.29,
        0.24857143,
        66.03,
        30.36,
        23.85,
        0.17142857,
        65.83,
        29.9575,
        23.84,
        0.23357143
      ],
      "math_scores": [
        7.73,
        5.6,
        4.0,
        10.52,
        0.0,
        7.73,
        5.6,
        4.6,
        10.46,
        0.0,
        7.51,
        5.76,
        4.2,
        10.48,
        0.0
      ],
      "code_scores": [
        40.24,
        59.14,
        0.36,
        17.95,
        0.45,
        40.85,
        58.75,
        0.36,
        18.16,
        0.23,
        38.41,
        58.37,
        0.36,
        19.21,
        0.23
      ],
      "reasoning_scores": [
        43.05,
        61.43,
        0.27391304,
        5.68,
        42.37,
        61.26,
        0.27641304,
        5.6,
        42.03,
        61.3,
        0.27717391,
        5.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.44
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.0,
        "math_avg": -7.84,
        "code_avg": 7.86,
        "reasoning_avg": -13.94,
        "overall_avg": 0.02,
        "general_task_scores": [
          44.15,
          9.52,
          6.02,
          -3.7
        ],
        "math_task_scores": [
          -48.14,
          0.15,
          2.67,
          6.15,
          0.0
        ],
        "code_task_scores": [
          19.71,
          4.28,
          -3.22,
          18.23,
          0.3
        ],
        "reasoning_task_scores": [
          -37.86,
          -1.23,
          -0.03,
          -16.64
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "link": "https://huggingface.co/datasets/GAIR/lima"
    },
    {
      "id": 8,
      "name": "orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 47.35,
      "math_avg": 24.72,
      "code_avg": 29.6,
      "reasoning_avg": 42.53,
      "overall_avg": 36.05,
      "general_scores": [
        70.08,
        54.355,
        37.42,
        28.1542857,
        69.38,
        55.04,
        39.08,
        28.6621429,
        67.95,
        54.7825,
        38.46,
        24.82
      ],
      "math_scores": [
        47.84,
        30.1,
        34.2,
        13.37,
        0.0,
        51.78,
        31.4,
        30.6,
        14.23,
        0.0,
        47.46,
        28.44,
        28.6,
        12.76,
        0.0
      ],
      "code_scores": [
        45.73,
        48.64,
        7.53,
        18.37,
        29.64,
        45.73,
        50.58,
        6.45,
        13.15,
        26.7,
        40.85,
        49.42,
        6.81,
        29.44,
        24.89
      ],
      "reasoning_scores": [
        75.93,
        62.69,
        0.34293478,
        31.28,
        77.97,
        60.88,
        0.36043478,
        31.28,
        74.92,
        62.07,
        0.3451087,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.32
              },
              {
                "metric": "lcb_test_output",
                "score": 27.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.88
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 31.33,
        "math_avg": 11.27,
        "code_avg": 13.92,
        "reasoning_avg": 1.2,
        "overall_avg": 14.43,
        "general_task_scores": [
          47.31,
          34.39,
          20.35,
          23.29
        ],
        "math_task_scores": [
          -6.77,
          24.48,
          29.53,
          9.11,
          0.0
        ],
        "code_task_scores": [
          23.98,
          -4.92,
          3.35,
          20.11,
          27.08
        ],
        "reasoning_task_scores": [
          -4.07,
          -0.68,
          0.04,
          9.52
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 44.34,
      "math_avg": 20.21,
      "code_avg": 30.31,
      "reasoning_avg": 36.93,
      "overall_avg": 32.95,
      "general_scores": [
        64.06,
        50.9275,
        39.9,
        25.0585714,
        58.97,
        52.215,
        39.98,
        24.3621429,
        60.95,
        50.3825,
        39.81,
        25.4792857
      ],
      "math_scores": [
        60.2,
        17.3,
        19.8,
        11.72,
        0.0,
        60.5,
        13.56,
        11.0,
        10.28,
        0.0,
        60.12,
        14.62,
        14.4,
        9.64,
        0.0
      ],
      "code_scores": [
        46.95,
        52.53,
        6.09,
        29.65,
        15.16,
        49.39,
        52.53,
        3.58,
        27.56,
        16.29,
        50.0,
        56.42,
        4.66,
        27.56,
        16.29
      ],
      "reasoning_scores": [
        76.61,
        54.92,
        0.38608696,
        15.52,
        79.32,
        54.08,
        0.3676087,
        17.76,
        78.98,
        54.66,
        0.34847826,
        10.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.26
              },
              {
                "metric": "lcb_test_output",
                "score": 15.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 28.33,
        "math_avg": 6.76,
        "code_avg": 14.63,
        "reasoning_avg": -4.39,
        "overall_avg": 11.33,
        "general_task_scores": [
          39.5,
          30.84,
          21.93,
          21.05
        ],
        "math_task_scores": [
          4.47,
          9.66,
          13.47,
          6.21,
          0.0
        ],
        "code_task_scores": [
          28.66,
          -0.64,
          1.2,
          28.05,
          15.91
        ],
        "reasoning_task_scores": [
          -2.04,
          -8.01,
          0.06,
          -7.57
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 9.56,
      "math_avg": 0.72,
      "code_avg": 5.23,
      "reasoning_avg": 10.96,
      "overall_avg": 6.62,
      "general_scores": [
        17.68,
        3.62,
        7.94071429,
        5.56,
        8.61,
        12.9621429,
        17.73,
        3.9,
        8.04428571
      ],
      "math_scores": [
        0.61,
        0.92,
        0.6,
        0.0,
        0.38,
        1.84,
        1.6,
        0.0,
        0.38,
        1.36,
        1.0,
        0.0
      ],
      "code_scores": [
        7.93,
        14.79,
        0.0,
        3.76,
        0.0,
        9.15,
        19.46,
        0.0,
        0.21,
        0.0,
        6.1,
        17.12,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        12.2,
        22.01,
        0.25108696,
        7.68,
        15.59,
        22.93,
        0.25021739,
        7.36,
        22.71,
        16.11,
        0.23391304,
        4.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.32
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.83
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.45,
        "math_avg": -12.72,
        "code_avg": -10.44,
        "reasoning_avg": -30.37,
        "overall_avg": -15.0,
        "general_task_scores": [
          -8.17,
          -12.59,
          5.73
        ],
        "math_task_scores": [
          -55.34,
          -4.13,
          -0.53,
          0.0
        ],
        "code_task_scores": [
          -12.39,
          -37.35,
          -3.58,
          1.11,
          0.0
        ],
        "reasoning_task_scores": [
          -63.51,
          -42.21,
          -0.06,
          -15.68
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "252k",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 47.31,
      "math_avg": 36.33,
      "code_avg": 35.33,
      "reasoning_avg": 36.08,
      "overall_avg": 38.76,
      "general_scores": [
        65.4,
        72.9025,
        36.45,
        18.58,
        59.28,
        71.025,
        37.68,
        28.5221429,
        48.68,
        69.4225,
        34.94,
        24.8178571
      ],
      "math_scores": [
        73.39,
        30.14,
        31.0,
        13.98,
        73.77,
        28.76,
        29.4,
        13.89,
        73.16,
        27.5,
        27.8,
        13.21
      ],
      "code_scores": [
        64.02,
        57.59,
        5.02,
        36.53,
        18.33,
        60.98,
        57.98,
        7.53,
        29.02,
        18.1,
        61.59,
        53.31,
        3.23,
        32.78,
        23.98
      ],
      "reasoning_scores": [
        53.56,
        56.83,
        0.39130435,
        24.72,
        64.07,
        56.42,
        0.36663043,
        27.6,
        64.07,
        57.98,
        0.35043478,
        26.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 71.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.69
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 20.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 31.29,
        "math_avg": 22.89,
        "code_avg": 19.66,
        "reasoning_avg": -5.24,
        "overall_avg": 17.15,
        "general_task_scores": [
          35.96,
          50.78,
          18.39,
          20.05
        ],
        "math_task_scores": [
          17.64,
          23.3,
          27.8,
          9.35
        ],
        "code_task_scores": [
          42.08,
          1.82,
          1.68,
          32.57,
          20.14
        ],
        "reasoning_task_scores": [
          -19.77,
          -5.48,
          0.06,
          4.24
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "939k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 28.38,
      "math_avg": 3.6,
      "code_avg": 6.79,
      "reasoning_avg": 27.09,
      "overall_avg": 16.47,
      "general_scores": [
        67.91,
        14.0225,
        30.84,
        0.02928571,
        67.22,
        14.905,
        32.86,
        0.00857143,
        67.01,
        13.04,
        32.64,
        0.05071429
      ],
      "math_scores": [
        9.86,
        4.52,
        1.8,
        6.71,
        0.0,
        2.43,
        2.68,
        3.4,
        6.82,
        0.0,
        3.94,
        3.1,
        2.2,
        6.53,
        0.0
      ],
      "code_scores": [
        0.0,
        49.03,
        0.0,
        1.25,
        0.0,
        0.0,
        9.73,
        0.0,
        0.21,
        0.0,
        0.0,
        41.25,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        65.08,
        40.3,
        0.22956522,
        3.12,
        66.44,
        37.15,
        0.24478261,
        3.12,
        64.41,
        41.17,
        0.24021739,
        3.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.36,
        "math_avg": -9.85,
        "code_avg": -8.88,
        "reasoning_avg": -14.23,
        "overall_avg": -5.15,
        "general_task_scores": [
          45.55,
          -6.35,
          14.14,
          -3.89
        ],
        "math_task_scores": [
          -50.39,
          -2.07,
          0.87,
          2.35,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -21.13,
          -3.58,
          0.42,
          0.0
        ],
        "reasoning_task_scores": [
          -15.03,
          -23.02,
          -0.07,
          -18.8
        ]
      },
      "affiliation": "CAS",
      "year": "2023",
      "size": "-",
      "link": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 35.36,
      "math_avg": 3.85,
      "code_avg": 23.01,
      "reasoning_avg": 33.28,
      "overall_avg": 23.87,
      "general_scores": [
        62.76,
        36.08,
        35.07,
        0.0,
        69.32,
        34.5625,
        36.44,
        11.3207143,
        66.29,
        36.8025,
        35.63,
        0.0
      ],
      "math_scores": [
        3.94,
        4.02,
        4.0,
        8.36,
        0.0,
        4.17,
        3.8,
        3.2,
        7.68,
        0.0,
        2.65,
        4.5,
        3.8,
        7.61,
        0.0
      ],
      "code_scores": [
        22.56,
        41.63,
        2.51,
        26.93,
        19.23,
        25.0,
        43.58,
        2.15,
        29.02,
        22.85,
        25.0,
        41.25,
        2.15,
        26.51,
        14.71
      ],
      "reasoning_scores": [
        76.27,
        50.26,
        0.33782609,
        5.92,
        74.24,
        50.79,
        0.36076087,
        5.36,
        76.95,
        51.53,
        0.36576087,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.88
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.27
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.49
              },
              {
                "metric": "lcb_test_output",
                "score": 18.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.34,
        "math_avg": -9.6,
        "code_avg": 7.33,
        "reasoning_avg": -8.04,
        "overall_avg": 2.26,
        "general_task_scores": [
          44.29,
          15.48,
          17.74,
          -0.15
        ],
        "math_task_scores": [
          -52.21,
          -1.39,
          2.07,
          3.54,
          0.0
        ],
        "code_task_scores": [
          4.07,
          -12.32,
          -1.31,
          27.28,
          18.93
        ],
        "reasoning_task_scores": [
          -4.52,
          -11.7,
          0.04,
          -16.0
        ]
      },
      "affiliation": "cognitivecomputations",
      "year": "2023",
      "size": "892k",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 24.29,
      "math_avg": 21.25,
      "code_avg": 30.15,
      "reasoning_avg": 36.58,
      "overall_avg": 28.07,
      "general_scores": [
        26.03,
        34.355,
        38.69,
        0.0,
        35.82,
        35.99,
        38.33,
        0.0,
        8.33,
        35.845,
        38.09,
        0.0
      ],
      "math_scores": [
        52.31,
        18.02,
        18.8,
        9.55,
        3.33,
        56.1,
        19.98,
        24.4,
        9.71,
        0.0,
        52.39,
        19.14,
        25.0,
        9.98,
        0.0
      ],
      "code_scores": [
        48.17,
        57.59,
        6.81,
        19.21,
        26.24,
        48.17,
        57.59,
        8.96,
        11.06,
        27.6,
        47.56,
        57.59,
        7.89,
        2.51,
        25.34
      ],
      "reasoning_scores": [
        52.88,
        59.83,
        0.26684783,
        30.0,
        60.0,
        58.97,
        0.27586957,
        29.52,
        57.29,
        60.5,
        0.26858696,
        29.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.37
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.97
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.59
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 10.93
              },
              {
                "metric": "lcb_test_output",
                "score": 26.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.72
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.28,
        "math_avg": 7.8,
        "code_avg": 14.48,
        "reasoning_avg": -4.74,
        "overall_avg": 6.45,
        "general_task_scores": [
          1.56,
          15.06,
          20.4,
          -3.92
        ],
        "math_task_scores": [
          -2.2,
          13.55,
          21.13,
          5.41,
          1.11
        ],
        "code_task_scores": [
          27.85,
          3.12,
          4.31,
          10.72,
          26.39
        ],
        "reasoning_task_scores": [
          -23.62,
          -2.79,
          -0.04,
          7.49
        ]
      },
      "affiliation": "Mxode",
      "year": "2024",
      "size": "10k",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 31.99,
      "math_avg": 4.2,
      "code_avg": 21.3,
      "reasoning_avg": 25.02,
      "overall_avg": 20.63,
      "general_scores": [
        43.88,
        40.2925,
        31.15,
        15.5421429,
        43.7,
        40.24,
        31.13,
        15.1221429,
        36.21,
        41.0375,
        30.46,
        15.0628571
      ],
      "math_scores": [
        9.86,
        3.86,
        2.6,
        5.06,
        0.0,
        6.82,
        3.76,
        5.0,
        5.69,
        0.0,
        10.16,
        3.7,
        1.6,
        4.92,
        0.0
      ],
      "code_scores": [
        25.61,
        38.52,
        3.23,
        22.76,
        13.12,
        29.88,
        28.79,
        3.23,
        25.68,
        19.68,
        31.71,
        31.13,
        3.23,
        22.76,
        20.14
      ],
      "reasoning_scores": [
        60.34,
        25.85,
        0.32641304,
        19.84,
        59.66,
        27.84,
        0.32673913,
        7.28,
        58.31,
        22.61,
        0.32184783,
        17.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.95
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.81
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.73
              },
              {
                "metric": "lcb_test_output",
                "score": 17.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 15.97,
        "math_avg": -9.25,
        "code_avg": 5.62,
        "reasoning_avg": -16.3,
        "overall_avg": -0.99,
        "general_task_scores": [
          19.43,
          20.18,
          12.94,
          11.32
        ],
        "math_task_scores": [
          -46.85,
          -1.73,
          1.47,
          0.88,
          0.0
        ],
        "code_task_scores": [
          8.95,
          -21.66,
          -0.35,
          23.52,
          17.65
        ],
        "reasoning_task_scores": [
          -20.9,
          -37.13,
          0.01,
          -7.2
        ]
      },
      "affiliation": "hakurei",
      "year": "2023",
      "size": "499k",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 28.98,
      "math_avg": 9.19,
      "code_avg": 9.22,
      "reasoning_avg": 12.71,
      "overall_avg": 15.03,
      "general_scores": [
        62.3,
        28.2,
        24.45,
        1.09357143,
        61.21,
        29.2975,
        23.76,
        1.555
      ],
      "math_scores": [
        20.09,
        12.0,
        10.4,
        6.53,
        0.0,
        0.0,
        21.76,
        12.42,
        11.2,
        6.64,
        0.0
      ],
      "code_scores": [
        9.76,
        38.91,
        0.0,
        0.0,
        0.0,
        13.41,
        29.96,
        0.0,
        0.21,
        0.0
      ],
      "reasoning_scores": [
        29.15,
        24.08,
        0.2076087,
        3.68,
        0.13545455,
        29.15,
        24.5,
        0.20478261,
        3.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.75
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.58
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 34.44
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.1
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.29
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.97,
        "math_avg": -4.26,
        "code_avg": -6.45,
        "reasoning_avg": -28.61,
        "overall_avg": -6.59,
        "general_task_scores": [
          39.93,
          8.41,
          6.13,
          -2.6
        ],
        "math_task_scores": [
          -34.88,
          6.71,
          9.2,
          2.24,
          0.0
        ],
        "code_task_scores": [
          -8.54,
          -20.03,
          -3.58,
          -0.11,
          0.0
        ],
        "reasoning_task_scores": [
          -51.19,
          -38.27,
          -0.13,
          -18.6
        ]
      },
      "affiliation": "cyzhh",
      "year": "2024",
      "size": "135k",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS/viewer/default/train?row=0"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 24.15,
      "math_avg": 1.47,
      "code_avg": 4.68,
      "reasoning_avg": 15.67,
      "overall_avg": 11.49,
      "general_scores": [
        68.66,
        19.3875,
        9.05,
        0.09285714,
        70.04,
        18.1275,
        8.25,
        0.13,
        70.38,
        16.5275,
        9.12,
        0.09214286
      ],
      "math_scores": [
        0.0,
        0.2,
        0.0,
        5.67,
        0.0,
        0.16,
        0.0,
        5.94,
        0.0,
        0.2,
        0.0,
        5.49
      ],
      "code_scores": [
        0.0,
        24.51,
        0.0,
        0.42,
        0.0,
        0.0,
        14.79,
        0.0,
        0.42,
        0.0,
        0.0,
        29.18,
        0.0,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        12.88,
        50.39,
        0.12402174,
        0.24,
        12.2,
        49.48,
        0.11141304,
        0.08,
        12.54,
        49.66,
        0.12402174,
        0.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 8.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.7
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.56
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.14,
        "math_avg": -11.98,
        "code_avg": -11.0,
        "reasoning_avg": -25.66,
        "overall_avg": -10.12,
        "general_task_scores": [
          47.86,
          -2.33,
          -9.16,
          -3.82
        ],
        "math_task_scores": [
          -55.8,
          -5.31,
          -1.6,
          1.36
        ],
        "code_task_scores": [
          -20.12,
          -31.64,
          -3.58,
          0.35,
          0.0
        ],
        "reasoning_task_scores": [
          -67.8,
          -12.72,
          -0.19,
          -21.92
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "link": "https://huggingface.co/datasets/openai/gsm8k"
    },
    {
      "id": 18,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 32.19,
      "math_avg": 42.14,
      "code_avg": 11.81,
      "reasoning_avg": 21.93,
      "overall_avg": 27.01,
      "general_scores": [
        70.44,
        24.0,
        31.77,
        0.05285714,
        74.18,
        28.025,
        30.4,
        0.04642857,
        69.91,
        26.545,
        30.68,
        0.20142857
      ],
      "math_scores": [
        73.54,
        56.88,
        54.6,
        18.27,
        3.33,
        74.75,
        57.16,
        56.0,
        18.9,
        3.33,
        81.35,
        58.18,
        57.4,
        18.34,
        0.0
      ],
      "code_scores": [
        9.76,
        40.86,
        0.72,
        0.21,
        4.98,
        9.15,
        37.74,
        1.79,
        0.0,
        9.5,
        14.63,
        36.58,
        2.15,
        0.0,
        9.05
      ],
      "reasoning_scores": [
        50.51,
        24.09,
        0.18771739,
        7.12,
        54.24,
        32.11,
        0.20554348,
        7.52,
        48.47,
        32.63,
        0.20445652,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.17,
        "math_avg": 28.69,
        "code_avg": -3.87,
        "reasoning_avg": -19.4,
        "overall_avg": 5.4,
        "general_task_scores": [
          49.68,
          5.85,
          12.98,
          -3.82
        ],
        "math_task_scores": [
          20.75,
          51.91,
          54.4,
          14.16,
          2.22
        ],
        "code_task_scores": [
          -8.94,
          -16.08,
          -2.03,
          -0.14,
          7.84
        ],
        "reasoning_task_scores": [
          -29.27,
          -32.95,
          -0.11,
          -15.25
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"
    },
    {
      "id": 19,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 26.68,
      "math_avg": 40.16,
      "code_avg": 9.46,
      "reasoning_avg": 20.09,
      "overall_avg": 24.1,
      "general_scores": [
        56.61,
        24.8925,
        29.94,
        0.21571429,
        55.38,
        25.495,
        29.78,
        0.11142857,
        44.78,
        23.0425,
        29.69,
        0.27
      ],
      "math_scores": [
        80.52,
        48.78,
        49.2,
        19.76,
        0.0,
        78.7,
        48.74,
        51.8,
        19.2,
        3.33,
        80.14,
        50.44,
        48.4,
        19.99,
        3.33
      ],
      "code_scores": [
        12.2,
        40.86,
        0.36,
        0.21,
        2.26,
        1.22,
        40.08,
        0.36,
        0.0,
        0.23,
        6.71,
        36.19,
        0.36,
        0.0,
        0.9
      ],
      "reasoning_scores": [
        41.69,
        24.77,
        0.16913043,
        15.36,
        39.66,
        30.17,
        0.16945652,
        14.4,
        41.02,
        20.88,
        0.16804348,
        12.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.26
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.48
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.79
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.71
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.67,
        "math_avg": 26.71,
        "code_avg": -6.21,
        "reasoning_avg": -21.23,
        "overall_avg": 2.48,
        "general_task_scores": [
          30.43,
          4.14,
          11.83,
          -3.72
        ],
        "math_task_scores": [
          23.99,
          43.82,
          48.2,
          15.31,
          2.22
        ],
        "code_task_scores": [
          -13.41,
          -15.43,
          -3.22,
          -0.14,
          1.13
        ],
        "reasoning_task_scores": [
          -39.55,
          -37.29,
          -0.14,
          -7.95
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT"
    },
    {
      "id": 20,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 33.08,
      "math_avg": 25.01,
      "code_avg": 23.14,
      "reasoning_avg": 17.35,
      "overall_avg": 24.64,
      "general_scores": [
        55.39,
        31.35,
        31.26,
        19.7371429,
        45.26,
        31.675,
        31.11,
        21.6564286,
        47.03,
        31.505,
        31.24,
        19.7942857
      ],
      "math_scores": [
        34.34,
        24.28,
        25.0,
        14.86,
        36.09,
        24.58,
        26.2,
        15.15,
        34.8,
        24.76,
        25.2,
        14.81
      ],
      "code_scores": [
        53.66,
        52.53,
        7.17,
        2.3,
        0.23,
        51.83,
        55.25,
        5.02,
        2.3,
        0.0,
        51.22,
        55.25,
        6.09,
        3.76,
        0.45
      ],
      "reasoning_scores": [
        52.2,
        16.99,
        0.17597826,
        3.52,
        49.15,
        13.46,
        0.17934783,
        4.8,
        49.15,
        15.13,
        0.17119565,
        3.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.79
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.07,
        "math_avg": 11.56,
        "code_avg": 7.46,
        "reasoning_avg": -23.97,
        "overall_avg": 3.03,
        "general_task_scores": [
          27.4,
          11.17,
          13.23,
          16.48
        ],
        "math_task_scores": [
          -20.72,
          19.04,
          23.87,
          10.6
        ],
        "code_task_scores": [
          32.12,
          -0.13,
          2.51,
          2.58,
          0.23
        ],
        "reasoning_task_scores": [
          -30.17,
          -47.37,
          -0.13,
          -18.21
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR"
    },
    {
      "id": 21,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 25.67,
      "math_avg": 29.41,
      "code_avg": 4.59,
      "reasoning_avg": 20.47,
      "overall_avg": 20.04,
      "general_scores": [
        63.63,
        22.3325,
        14.36,
        0.11928571,
        66.62,
        23.03,
        16.5,
        0.09071429,
        64.79,
        22.7125,
        13.7,
        0.15571429
      ],
      "math_scores": [
        73.54,
        30.94,
        31.6,
        10.34,
        0.0,
        75.74,
        31.74,
        30.6,
        9.96,
        0.0,
        75.97,
        31.5,
        29.4,
        9.78,
        0.0
      ],
      "code_scores": [
        0.0,
        19.84,
        0.0,
        0.84,
        0.68,
        0.0,
        17.9,
        0.0,
        4.59,
        0.23,
        0.0,
        20.62,
        0.0,
        3.97,
        0.23
      ],
      "reasoning_scores": [
        39.32,
        35.66,
        0.15347826,
        12.56,
        35.59,
        35.94,
        0.14304348,
        5.2,
        31.86,
        35.44,
        0.16956522,
        13.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 19.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.13
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.66,
        "math_avg": 15.96,
        "code_avg": -11.08,
        "reasoning_avg": -20.85,
        "overall_avg": -1.58,
        "general_task_scores": [
          43.18,
          2.35,
          -3.12,
          -3.8
        ],
        "math_task_scores": [
          19.28,
          25.89,
          28.93,
          5.69,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -35.02,
          -3.58,
          2.92,
          0.38
        ],
        "reasoning_task_scores": [
          -44.75,
          -26.88,
          -0.15,
          -11.63
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA"
    },
    {
      "id": 22,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 29.84,
      "math_avg": 21.07,
      "code_avg": 17.6,
      "reasoning_avg": 30.49,
      "overall_avg": 24.75,
      "general_scores": [
        43.69,
        34.2625,
        32.41,
        2.02428571,
        45.54,
        35.44,
        34.22,
        0.54357143,
        63.45,
        32.9325,
        32.27,
        1.26071429
      ],
      "math_scores": [
        59.21,
        19.58,
        21.2,
        9.08,
        0.0,
        57.01,
        18.0,
        16.6,
        8.81,
        0.0,
        58.76,
        18.98,
        19.8,
        8.99,
        0.0
      ],
      "code_scores": [
        29.88,
        43.97,
        0.72,
        6.05,
        7.01,
        31.1,
        43.58,
        0.0,
        5.85,
        4.07,
        30.49,
        42.41,
        2.51,
        11.9,
        4.52
      ],
      "reasoning_scores": [
        58.64,
        49.76,
        0.25086957,
        9.52,
        61.69,
        50.93,
        0.2573913,
        8.56,
        62.71,
        49.5,
        0.27836957,
        13.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.08
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.93
              },
              {
                "metric": "lcb_test_output",
                "score": 5.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.82,
        "math_avg": 7.62,
        "code_avg": 1.93,
        "reasoning_avg": -10.83,
        "overall_avg": 3.14,
        "general_task_scores": [
          29.06,
          13.87,
          15.0,
          -2.64
        ],
        "math_task_scores": [
          2.53,
          13.35,
          17.6,
          4.62,
          0.0
        ],
        "code_task_scores": [
          10.37,
          -11.15,
          -2.5,
          7.72,
          5.2
        ],
        "reasoning_task_scores": [
          -19.33,
          -12.5,
          -0.05,
          -11.44
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct"
    },
    {
      "id": 23,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 35.08,
      "math_avg": 4.32,
      "code_avg": 16.89,
      "reasoning_avg": 23.49,
      "overall_avg": 19.94,
      "general_scores": [
        71.93,
        31.235,
        31.68,
        2.60785714,
        73.08,
        32.4475,
        32.67,
        3.575,
        73.01,
        34.6325,
        31.73,
        2.355
      ],
      "math_scores": [
        2.65,
        5.2,
        5.4,
        10.7,
        3.33,
        0.53,
        3.1,
        3.0,
        11.86,
        0.0,
        0.76,
        3.58,
        3.2,
        11.43,
        0.0
      ],
      "code_scores": [
        22.56,
        38.13,
        2.15,
        2.71,
        13.12,
        27.44,
        40.47,
        2.51,
        6.05,
        10.86,
        23.17,
        39.3,
        0.72,
        8.98,
        15.16
      ],
      "reasoning_scores": [
        32.54,
        41.37,
        0.21934783,
        14.48,
        37.97,
        45.11,
        0.2176087,
        14.64,
        36.27,
        41.01,
        0.22934783,
        17.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.31
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.79
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.91
              },
              {
                "metric": "lcb_test_output",
                "score": 13.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.06,
        "math_avg": -9.13,
        "code_avg": 1.21,
        "reasoning_avg": -17.83,
        "overall_avg": -1.67,
        "general_task_scores": [
          50.84,
          12.43,
          14.06,
          -1.07
        ],
        "math_task_scores": [
          -54.49,
          -1.54,
          2.27,
          6.99,
          1.11
        ],
        "code_task_scores": [
          4.27,
          -15.17,
          -1.79,
          5.7,
          13.05
        ],
        "reasoning_task_scores": [
          -44.75,
          -20.06,
          -0.09,
          -6.43
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 24,
      "name": "DART-Math",
      "domain": "math",
      "general_avg": 32.77,
      "math_avg": 41.3,
      "code_avg": 15.45,
      "reasoning_avg": 29.95,
      "overall_avg": 29.87,
      "general_scores": [
        76.17,
        25.66,
        30.71,
        0.25285714,
        76.17,
        25.81,
        30.71,
        0.25285714,
        73.21,
        24.2825,
        29.78,
        0.17785714
      ],
      "math_scores": [
        83.09,
        44.7,
        47.8,
        15.29,
        83.09,
        44.7,
        47.8,
        15.27,
        3.33,
        83.24,
        44.64,
        47.0,
        14.97,
        3.33
      ],
      "code_scores": [
        24.39,
        42.8,
        0.72,
        2.09,
        6.33,
        24.39,
        42.8,
        0.72,
        2.09,
        6.33,
        26.22,
        45.53,
        0.72,
        2.71,
        3.85
      ],
      "reasoning_scores": [
        57.29,
        42.99,
        0.19228261,
        18.88,
        57.29,
        42.99,
        0.19228261,
        18.88,
        56.95,
        42.99,
        0.18543478,
        20.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.3
              },
              {
                "metric": "lcb_test_output",
                "score": 5.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.75,
        "math_avg": 27.86,
        "code_avg": -0.23,
        "reasoning_avg": -11.37,
        "overall_avg": 8.25,
        "general_task_scores": [
          53.35,
          4.91,
          12.43,
          -3.69
        ],
        "math_task_scores": [
          27.34,
          39.18,
          45.93,
          10.84,
          3.33
        ],
        "code_task_scores": [
          4.88,
          -10.76,
          -2.86,
          2.09,
          5.5
        ],
        "reasoning_task_scores": [
          -23.16,
          -19.57,
          -0.12,
          -2.64
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-uniform https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 25,
      "name": "MathQA",
      "domain": "math",
      "general_avg": 17.66,
      "math_avg": 2.27,
      "code_avg": 5.65,
      "reasoning_avg": 1.8,
      "overall_avg": 6.85,
      "general_scores": [
        41.08,
        26.495,
        0.92,
        0.0,
        42.03,
        25.9,
        1.58,
        0.0,
        46.17,
        26.5475,
        1.24,
        0.0
      ],
      "math_scores": [
        2.88,
        0.56,
        0.0,
        5.85,
        0.0,
        4.93,
        1.66,
        0.4,
        6.12,
        0.0,
        4.4,
        1.06,
        0.0,
        6.17,
        0.0
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        0.42,
        0.0,
        0.0,
        26.46,
        0.0,
        1.25,
        0.0,
        0.61,
        27.63,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        0.34,
        3.9,
        0.13880435,
        2.32,
        0.34,
        3.64,
        0.16217391,
        2.64,
        0.68,
        4.71,
        0.1548913,
        2.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 1.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.65,
        "math_avg": -11.18,
        "code_avg": -10.02,
        "reasoning_avg": -39.52,
        "overall_avg": -14.77,
        "general_task_scores": [
          21.26,
          5.97,
          -16.72,
          -3.92
        ],
        "math_task_scores": [
          -51.73,
          -4.41,
          -1.47,
          1.71,
          0.0
        ],
        "code_task_scores": [
          -19.92,
          -27.1,
          -3.58,
          0.49,
          0.0
        ],
        "reasoning_task_scores": [
          -79.89,
          -58.48,
          -0.16,
          -19.57
        ]
      },
      "affiliation": "AllenAI",
      "year": "2019",
      "size": "29.8k",
      "link": "https://huggingface.co/datasets/allenai/math_qa"
    },
    {
      "id": 26,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 30.01,
      "math_avg": 17.26,
      "code_avg": 12.64,
      "reasoning_avg": 25.92,
      "overall_avg": 21.46,
      "general_scores": [
        71.36,
        21.0,
        25.47,
        0.02071429,
        72.35,
        24.7925,
        25.27,
        0.08857143,
        72.93,
        21.8575,
        24.91,
        0.02214286
      ],
      "math_scores": [
        40.86,
        17.18,
        18.4,
        8.79,
        0.0,
        41.24,
        17.9,
        17.6,
        9.55,
        0.0,
        38.67,
        18.1,
        18.4,
        8.9,
        3.33
      ],
      "code_scores": [
        0.0,
        54.09,
        0.36,
        5.22,
        0.9,
        1.22,
        55.25,
        1.43,
        3.34,
        2.26,
        1.22,
        56.81,
        1.08,
        5.01,
        1.36
      ],
      "reasoning_scores": [
        48.14,
        54.08,
        0.16869565,
        3.84,
        37.63,
        55.61,
        0.16869565,
        12.08,
        41.36,
        52.05,
        0.15163043,
        5.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.52
              },
              {
                "metric": "lcb_test_output",
                "score": 1.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.99,
        "math_avg": 3.81,
        "code_avg": -3.04,
        "reasoning_avg": -15.4,
        "overall_avg": -0.16,
        "general_task_scores": [
          50.38,
          2.21,
          7.25,
          -3.88
        ],
        "math_task_scores": [
          -15.54,
          12.23,
          16.53,
          4.74,
          1.11
        ],
        "code_task_scores": [
          -19.31,
          0.91,
          -2.62,
          4.31,
          1.51
        ],
        "reasoning_task_scores": [
          -37.96,
          -8.65,
          -0.15,
          -14.85
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "link": "https://huggingface.co/datasets/camel-ai/math"
    },
    {
      "id": 27,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 32.93,
      "math_avg": 10.81,
      "code_avg": 13.31,
      "reasoning_avg": 28.4,
      "overall_avg": 21.36,
      "general_scores": [
        55.66,
        33.68,
        35.43,
        6.08071429,
        58.42,
        32.125,
        36.03,
        3.90071429,
        61.88,
        31.85,
        35.99,
        4.13571429
      ],
      "math_scores": [
        21.3,
        11.1,
        10.8,
        11.36,
        0.0,
        21.68,
        12.36,
        9.8,
        11.36,
        0.0,
        19.94,
        11.28,
        10.2,
        11.0,
        0.0
      ],
      "code_scores": [
        8.54,
        55.64,
        2.51,
        3.13,
        1.36,
        2.44,
        52.14,
        2.87,
        4.59,
        2.26,
        1.22,
        54.47,
        2.51,
        4.38,
        1.58
      ],
      "reasoning_scores": [
        55.25,
        39.81,
        0.28271739,
        21.2,
        53.22,
        39.74,
        0.28728261,
        19.76,
        53.56,
        38.89,
        0.28793478,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.48
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.92,
        "math_avg": -2.64,
        "code_avg": -2.37,
        "reasoning_avg": -12.93,
        "overall_avg": -0.25,
        "general_task_scores": [
          36.82,
          12.21,
          17.85,
          0.79
        ],
        "math_task_scores": [
          -34.83,
          6.08,
          8.67,
          6.9,
          0.0
        ],
        "code_task_scores": [
          -16.05,
          -0.39,
          -0.95,
          3.82,
          1.73
        ],
        "reasoning_task_scores": [
          -26.33,
          -23.08,
          -0.02,
          -2.27
        ]
      },
      "affiliation": "SkunkworksAI",
      "year": "2025",
      "size": "29.9k",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 22.58,
      "reasoning_avg": 20.09,
      "overall_avg": 24.06,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.6875,
        21.54,
        0.43785714,
        45.71,
        33.075,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.21,
        0.0,
        57.7,
        27.26,
        39.4,
        15.15,
        0.0
      ],
      "code_scores": [
        26.22,
        55.64,
        6.45,
        0.42,
        16.74,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68
      ],
      "reasoning_scores": [
        25.42,
        34.29,
        0.27228261,
        21.36,
        24.07,
        31.44,
        0.25282609,
        21.92,
        28.81,
        32.99,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.02,
        "math_avg": 14.1,
        "code_avg": 6.9,
        "reasoning_avg": -21.23,
        "overall_avg": 2.45,
        "general_task_scores": [
          27.19,
          11.69,
          4.68,
          -3.48
        ],
        "math_task_scores": [
          -0.13,
          21.51,
          37.73,
          10.29,
          1.11
        ],
        "code_task_scores": [
          11.18,
          1.3,
          3.83,
          0.49,
          17.72
        ],
        "reasoning_task_scores": [
          -54.24,
          -29.65,
          -0.05,
          -0.99
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math"
    },
    {
      "id": 29,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 25.29,
      "math_avg": 26.6,
      "code_avg": 8.05,
      "reasoning_avg": 19.05,
      "overall_avg": 19.75,
      "general_scores": [
        56.04,
        26.94,
        21.86,
        1.06928571,
        52.56,
        25.765,
        21.19,
        1.61928571,
        48.93,
        25.1,
        21.35,
        1.01357143
      ],
      "math_scores": [
        68.69,
        19.14,
        24.0,
        11.5,
        0.0,
        63.99,
        17.28,
        22.2,
        10.84,
        0.0,
        65.96,
        18.84,
        23.4
      ],
      "code_scores": [
        0.61,
        48.64,
        0.0,
        0.0,
        0.68,
        0.0,
        35.8,
        0.0,
        0.0,
        0.0,
        1.22,
        33.07,
        0.0,
        0.0,
        0.68
      ],
      "reasoning_scores": [
        21.36,
        34.07,
        0.2425,
        18.88,
        22.37,
        38.59,
        0.23369565,
        17.84,
        21.69,
        33.76,
        0.23336957,
        19.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.17
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.27,
        "math_avg": 13.16,
        "code_avg": -7.63,
        "reasoning_avg": -22.28,
        "overall_avg": -1.87,
        "general_task_scores": [
          30.68,
          5.6,
          3.5,
          -2.69
        ],
        "math_task_scores": [
          10.41,
          12.92,
          21.6,
          6.83,
          0.0
        ],
        "code_task_scores": [
          -19.51,
          -15.3,
          -3.58,
          -0.21,
          0.45
        ],
        "reasoning_task_scores": [
          -58.53,
          -27.09,
          -0.07,
          -3.41
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "20k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra"
    },
    {
      "id": 30,
      "name": "tulu-3-sft-personas-math-grade",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 22.55,
      "reasoning_avg": 20.09,
      "overall_avg": 24.06,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.67,
        21.54,
        0.43785714,
        45.71,
        33.0275,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.23,
        0.0,
        57.7,
        27.26,
        39.4,
        15.13,
        0.0
      ],
      "code_scores": [
        26.22,
        55.25,
        6.45,
        0.42,
        16.74,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68
      ],
      "reasoning_scores": [
        25.42,
        34.29,
        0.27228261,
        21.36,
        24.07,
        31.44,
        0.25282609,
        21.92,
        28.81,
        32.99,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.64
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.01,
        "math_avg": 14.1,
        "code_avg": 6.88,
        "reasoning_avg": -21.23,
        "overall_avg": 2.44,
        "general_task_scores": [
          27.19,
          11.67,
          4.68,
          -3.48
        ],
        "math_task_scores": [
          -0.13,
          21.51,
          37.73,
          10.29,
          1.11
        ],
        "code_task_scores": [
          11.18,
          1.17,
          3.83,
          0.49,
          17.72
        ],
        "reasoning_task_scores": [
          -54.24,
          -29.65,
          -0.05,
          -0.99
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "50k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade"
    },
    {
      "id": 31,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 34.8,
      "math_avg": 48.35,
      "code_avg": 11.81,
      "reasoning_avg": 16.92,
      "overall_avg": 27.97,
      "general_scores": [
        81.28,
        23.6725,
        32.08,
        0.0,
        83.17,
        24.2225,
        30.46,
        2.06785714,
        81.97,
        23.88,
        0.03357143
      ],
      "math_scores": [
        86.66,
        63.18,
        64.2,
        21.0,
        6.67,
        89.08,
        63.24,
        64.8,
        20.57,
        0.0,
        87.41,
        63.06,
        67.8,
        20.89,
        6.67
      ],
      "code_scores": [
        16.46,
        30.74,
        2.51,
        0.0,
        7.47,
        21.34,
        35.02,
        2.15,
        0.0,
        1.81,
        22.56,
        31.91,
        0.36,
        2.3,
        2.49
      ],
      "reasoning_scores": [
        45.76,
        20.33,
        0.18402174,
        0.16,
        47.8,
        22.91,
        0.18641304,
        0.4,
        44.41,
        20.61,
        0.18130435,
        0.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.72
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.67
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.77
              },
              {
                "metric": "lcb_test_output",
                "score": 3.92
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.79,
        "math_avg": 34.9,
        "code_avg": -3.87,
        "reasoning_avg": -24.41,
        "overall_avg": 6.35,
        "general_task_scores": [
          60.31,
          3.58,
          13.3,
          -3.22
        ],
        "math_task_scores": [
          31.92,
          57.66,
          64.0,
          16.48,
          4.45
        ],
        "code_task_scores": [
          0.0,
          -21.91,
          -1.91,
          0.56,
          3.92
        ],
        "reasoning_task_scores": [
          -34.35,
          -41.28,
          -0.13,
          -21.87
        ]
      },
      "affiliation": "Soochow Univ",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math"
    },
    {
      "id": 32,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 30.35,
      "math_avg": 37.55,
      "code_avg": 9.53,
      "reasoning_avg": 28.34,
      "overall_avg": 26.44,
      "general_scores": [
        64.14,
        23.81,
        33.19,
        2.58571429,
        52.56,
        24.03,
        32.54,
        3.58714286,
        65.16,
        25.4725,
        33.58,
        3.49785714
      ],
      "math_scores": [
        66.34,
        55.7,
        45.4,
        16.24,
        3.33,
        67.02,
        56.58,
        49.2,
        16.46,
        3.33,
        66.64,
        55.84,
        45.4,
        15.83,
        0.0
      ],
      "code_scores": [
        0.0,
        43.19,
        0.0,
        0.21,
        0.23,
        3.05,
        45.91,
        0.0,
        0.21,
        0.68,
        0.0,
        43.19,
        0.0,
        6.05,
        0.23
      ],
      "reasoning_scores": [
        65.76,
        34.49,
        0.20608696,
        12.88,
        63.73,
        34.21,
        0.21119565,
        13.36,
        66.1,
        37.13,
        0.20913043,
        11.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.44
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.2
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.33,
        "math_avg": 24.11,
        "code_avg": -6.15,
        "reasoning_avg": -12.98,
        "overall_avg": 4.83,
        "general_task_scores": [
          38.79,
          4.1,
          15.13,
          -0.7
        ],
        "math_task_scores": [
          10.87,
          50.54,
          45.07,
          11.84,
          2.22
        ],
        "code_task_scores": [
          -19.1,
          -10.37,
          -3.58,
          1.95,
          0.38
        ],
        "reasoning_task_scores": [
          -15.14,
          -27.28,
          -0.1,
          -9.39
        ]
      },
      "affiliation": "rubenroy",
      "year": "2025",
      "size": "170k",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k/viewer/default/train?row=0"
    },
    {
      "id": 33,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 34.26,
      "math_avg": 3.92,
      "code_avg": 17.17,
      "reasoning_avg": 24.35,
      "overall_avg": 19.92,
      "general_scores": [
        72.08,
        29.815,
        31.69,
        1.43785714,
        70.87,
        29.4225,
        32.93,
        3.26928571,
        74.2,
        31.1675,
        31.81,
        2.36857143
      ],
      "math_scores": [
        1.74,
        3.2,
        4.4,
        11.18,
        0.0,
        0.53,
        3.16,
        3.8,
        10.98,
        0.0,
        0.91,
        3.58,
        4.2,
        11.11,
        0.0
      ],
      "code_scores": [
        23.78,
        36.19,
        1.08,
        5.43,
        11.31,
        28.66,
        40.86,
        1.79,
        1.46,
        14.25,
        25.0,
        39.3,
        1.08,
        14.41,
        12.9
      ],
      "reasoning_scores": [
        34.24,
        43.53,
        0.22565217,
        16.72,
        44.75,
        44.5,
        0.21793478,
        13.44,
        35.25,
        44.49,
        0.2223913,
        14.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.06
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.31
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.1
              },
              {
                "metric": "lcb_test_output",
                "score": 12.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.24,
        "math_avg": -9.53,
        "code_avg": 1.49,
        "reasoning_avg": -16.97,
        "overall_avg": -1.69,
        "general_task_scores": [
          50.55,
          9.8,
          14.17,
          -1.56
        ],
        "math_task_scores": [
          -54.74,
          -2.19,
          2.53,
          6.75,
          0.0
        ],
        "code_task_scores": [
          5.69,
          -15.69,
          -2.26,
          6.89,
          12.82
        ],
        "reasoning_task_scores": [
          -42.26,
          -18.39,
          -0.09,
          -7.15
        ]
      },
      "affiliation": "microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 34,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 32.57,
      "math_avg": 38.19,
      "code_avg": 15.65,
      "reasoning_avg": 30.01,
      "overall_avg": 29.11,
      "general_scores": [
        75.02,
        24.375,
        30.52,
        0.23571429,
        74.78,
        25.16,
        30.61,
        0.19571429,
        74.98,
        23.74,
        31.02,
        0.19714286
      ],
      "math_scores": [
        83.24,
        45.36,
        46.0,
        14.61,
        3.33,
        83.17,
        44.8,
        48.4,
        14.84,
        0.0,
        82.34,
        44.44,
        44.4,
        14.59,
        3.33
      ],
      "code_scores": [
        26.83,
        40.86,
        0.36,
        3.13,
        4.75,
        21.95,
        47.86,
        1.08,
        2.51,
        5.66,
        23.78,
        47.47,
        2.15,
        2.09,
        4.3
      ],
      "reasoning_scores": [
        58.31,
        44.06,
        0.19586957,
        19.12,
        54.92,
        42.44,
        0.21054348,
        18.96,
        57.97,
        43.34,
        0.19315217,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.93
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.68
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.58
              },
              {
                "metric": "lcb_test_output",
                "score": 4.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.55,
        "math_avg": 24.74,
        "code_avg": -0.02,
        "reasoning_avg": -11.31,
        "overall_avg": 7.49,
        "general_task_scores": [
          53.1,
          4.08,
          12.75,
          -3.71
        ],
        "math_task_scores": [
          27.12,
          39.37,
          44.67,
          10.34,
          2.22
        ],
        "code_task_scores": [
          4.07,
          -9.07,
          -2.38,
          2.37,
          4.9
        ],
        "reasoning_task_scores": [
          -23.27,
          -19.28,
          -0.11,
          -2.59
        ]
      },
      "affiliation": "hkust-nlp",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 35,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 29.85,
      "math_avg": 1.89,
      "code_avg": 9.28,
      "reasoning_avg": 15.17,
      "overall_avg": 14.05,
      "general_scores": [
        73.71,
        22.8025,
        23.56,
        0.60428571,
        73.18,
        21.865,
        24.41,
        0.49428571,
        71.82,
        21.5775,
        23.45,
        0.69857143
      ],
      "math_scores": [
        0.0,
        0.34,
        0.2,
        7.48,
        0.0,
        0.0,
        0.4,
        0.6,
        7.32,
        0.0,
        0.0,
        0.28,
        0.6,
        7.77,
        3.33
      ],
      "code_scores": [
        0.0,
        41.63,
        0.0,
        0.63,
        0.0,
        0.0,
        47.08,
        0.0,
        1.25,
        0.0,
        0.0,
        47.08,
        0.0,
        1.46,
        0.0
      ],
      "reasoning_scores": [
        29.49,
        25.66,
        0.18434783,
        2.0,
        31.86,
        29.84,
        0.17978261,
        2.48,
        30.85,
        27.13,
        0.18173913,
        2.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.9
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.6
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.83,
        "math_avg": -11.56,
        "code_avg": -6.4,
        "reasoning_avg": -26.15,
        "overall_avg": -7.57,
        "general_task_scores": [
          51.07,
          1.74,
          5.84,
          -3.32
        ],
        "math_task_scores": [
          -55.8,
          -5.16,
          -1.13,
          3.18,
          1.11
        ],
        "code_task_scores": [
          -20.12,
          -9.21,
          -3.58,
          0.9,
          0.0
        ],
        "reasoning_task_scores": [
          -49.61,
          -35.02,
          -0.13,
          -19.84
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "23.6k",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1"
    },
    {
      "id": 36,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 41.4,
      "math_avg": 14.36,
      "code_avg": 36.85,
      "reasoning_avg": 38.24,
      "overall_avg": 32.71,
      "general_scores": [
        66.61,
        42.195,
        36.15,
        17.8557143,
        65.46,
        42.1725,
        35.81,
        25.2014286,
        65.67,
        41.9,
        35.6,
        22.2078571
      ],
      "math_scores": [
        30.1,
        14.06,
        15.0,
        10.89,
        3.33,
        31.01,
        16.04,
        14.8,
        11.11,
        3.33,
        27.52,
        14.46,
        12.0,
        11.7,
        0.0
      ],
      "code_scores": [
        69.51,
        66.93,
        6.81,
        20.88,
        20.59,
        65.85,
        64.98,
        5.02,
        16.08,
        29.19,
        67.68,
        64.2,
        5.02,
        21.71,
        28.28
      ],
      "reasoning_scores": [
        64.41,
        58.95,
        0.3301087,
        28.72,
        66.1,
        58.69,
        0.34315217,
        30.4,
        61.69,
        60.49,
        0.32434783,
        28.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.56
              },
              {
                "metric": "lcb_test_output",
                "score": 26.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 25.39,
        "math_avg": 0.91,
        "code_avg": 21.17,
        "reasoning_avg": -3.08,
        "overall_avg": 11.1,
        "general_task_scores": [
          44.08,
          21.75,
          17.88,
          17.84
        ],
        "math_task_scores": [
          -26.26,
          9.35,
          12.33,
          6.89,
          2.22
        ],
        "code_task_scores": [
          47.56,
          10.9,
          2.04,
          19.35,
          26.02
        ],
        "reasoning_task_scores": [
          -16.27,
          -3.18,
          0.02,
          7.12
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "970k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College"
    },
    {
      "id": 37,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 23.34,
      "math_avg": 11.95,
      "code_avg": 21.97,
      "reasoning_avg": 20.15,
      "overall_avg": 19.35,
      "general_scores": [
        38.16,
        32.3675,
        25.72,
        0.0,
        19.57,
        28.535,
        25.06,
        0.0,
        56.67,
        28.1425,
        25.88,
        0.0
      ],
      "math_scores": [
        32.07,
        11.9,
        9.6,
        9.85,
        0.0,
        22.29,
        11.14,
        12.6,
        10.34,
        0.0,
        25.63,
        11.36,
        11.0,
        11.52,
        0.0
      ],
      "code_scores": [
        36.59,
        45.53,
        2.87,
        20.67,
        11.31,
        31.1,
        45.53,
        3.23,
        14.2,
        14.03,
        31.71,
        46.3,
        2.87,
        6.26,
        17.42
      ],
      "reasoning_scores": [
        42.03,
        18.38,
        0.22717391,
        23.04,
        40.34,
        15.63,
        0.21304348,
        21.52,
        36.95,
        19.87,
        0.2198913,
        23.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.13
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.71
              },
              {
                "metric": "lcb_test_output",
                "score": 14.25
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.33,
        "math_avg": -1.49,
        "code_avg": 6.3,
        "reasoning_avg": -21.17,
        "overall_avg": -2.26,
        "general_task_scores": [
          16.3,
          9.34,
          7.58,
          -3.92
        ],
        "math_task_scores": [
          -29.14,
          5.97,
          9.47,
          6.23,
          0.0
        ],
        "code_task_scores": [
          13.01,
          -8.68,
          -0.59,
          13.5,
          14.25
        ],
        "reasoning_task_scores": [
          -40.57,
          -44.6,
          -0.09,
          0.56
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "98k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School"
    },
    {
      "id": 38,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 31.86,
      "math_avg": 15.55,
      "code_avg": 22.01,
      "reasoning_avg": 17.06,
      "overall_avg": 21.62,
      "general_scores": [
        71.26,
        27.6075,
        23.9,
        3.22285714,
        71.34,
        28.75,
        24.11,
        5.53928571,
        70.42,
        28.1825,
        23.5,
        4.49428571
      ],
      "math_scores": [
        11.75,
        19.32,
        23.6,
        16.78,
        3.33,
        16.0,
        22.28,
        27.8,
        17.1,
        0.0,
        11.3,
        19.86,
        27.8,
        16.26,
        0.0
      ],
      "code_scores": [
        35.98,
        60.7,
        3.58,
        0.0,
        9.5,
        36.59,
        61.87,
        2.87,
        0.0,
        9.73,
        35.98,
        61.48,
        3.94,
        0.0,
        7.92
      ],
      "reasoning_scores": [
        25.42,
        24.23,
        0.235,
        17.76,
        26.1,
        22.58,
        0.24684783,
        16.72,
        28.47,
        26.43,
        0.24369565,
        16.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.42
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.41
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 15.85,
        "math_avg": 2.1,
        "code_avg": 6.33,
        "reasoning_avg": -24.26,
        "overall_avg": 0.0,
        "general_task_scores": [
          49.18,
          7.84,
          5.87,
          0.5
        ],
        "math_task_scores": [
          -42.78,
          14.99,
          24.8,
          12.37,
          1.11
        ],
        "code_task_scores": [
          16.06,
          6.88,
          -0.12,
          -0.21,
          9.05
        ],
        "reasoning_task_scores": [
          -53.68,
          -38.15,
          -0.07,
          -5.15
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT"
    },
    {
      "id": 39,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 36.41,
      "math_avg": 34.4,
      "code_avg": 22.8,
      "reasoning_avg": 29.78,
      "overall_avg": 30.85,
      "general_scores": [
        81.04,
        33.9675,
        31.7,
        0.56642857,
        80.91,
        31.5525,
        26.8,
        3.80857143,
        81.72,
        34.055,
        27.9,
        2.92928571
      ],
      "math_scores": [
        83.24,
        36.94,
        35.4,
        13.14,
        0.0,
        82.03,
        39.58,
        38.8,
        13.71,
        0.0,
        81.96,
        39.06,
        38.6,
        13.48,
        0.0
      ],
      "code_scores": [
        35.98,
        60.7,
        3.58,
        0.63,
        9.73,
        38.41,
        56.03,
        4.3,
        1.04,
        11.99,
        42.07,
        60.31,
        3.58,
        1.46,
        12.22
      ],
      "reasoning_scores": [
        50.17,
        55.0,
        0.28184783,
        11.52,
        53.22,
        51.36,
        0.27869565,
        12.96,
        57.29,
        50.53,
        0.28467391,
        14.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 38.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.82
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.04
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 20.4,
        "math_avg": 20.95,
        "code_avg": 7.13,
        "reasoning_avg": -11.54,
        "overall_avg": 9.23,
        "general_task_scores": [
          59.39,
          12.85,
          10.83,
          -1.49
        ],
        "math_task_scores": [
          26.61,
          33.03,
          36.0,
          9.1,
          0.0
        ],
        "code_task_scores": [
          18.7,
          4.54,
          0.24,
          0.83,
          11.31
        ],
        "reasoning_task_scores": [
          -26.78,
          -10.26,
          -0.03,
          -9.09
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus"
    },
    {
      "id": 40,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 38.29,
      "math_avg": 31.43,
      "code_avg": 24.86,
      "reasoning_avg": 18.74,
      "overall_avg": 28.33,
      "general_scores": [
        79.58,
        33.4775,
        31.77,
        9.49,
        79.67,
        33.3025,
        32.28,
        6.36857143,
        80.48,
        33.2825,
        30.88,
        8.90928571
      ],
      "math_scores": [
        59.14,
        37.92,
        39.4,
        14.68,
        6.67,
        64.82,
        37.88,
        38.8,
        15.2,
        3.33,
        57.16,
        37.08,
        37.8,
        14.93,
        6.67
      ],
      "code_scores": [
        44.51,
        55.25,
        5.38,
        0.84,
        17.87,
        46.34,
        55.64,
        6.81,
        0.21,
        14.71,
        45.12,
        55.64,
        7.89,
        0.21,
        16.52
      ],
      "reasoning_scores": [
        31.86,
        21.41,
        0.27847826,
        23.36,
        33.56,
        20.92,
        0.27195652,
        21.12,
        29.83,
        21.02,
        0.26173913,
        20.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.26
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 16.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 22.28,
        "math_avg": 17.98,
        "code_avg": 9.19,
        "reasoning_avg": -22.59,
        "overall_avg": 6.72,
        "general_task_scores": [
          58.08,
          13.01,
          13.67,
          4.34
        ],
        "math_task_scores": [
          4.57,
          32.13,
          37.07,
          10.6,
          5.56
        ],
        "code_task_scores": [
          25.2,
          1.04,
          3.11,
          0.21,
          16.37
        ],
        "reasoning_task_scores": [
          -48.59,
          -41.44,
          -0.04,
          -0.27
        ]
      },
      "affiliation": "PawanKrd",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k"
    },
    {
      "id": 41,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 28.14,
      "math_avg": 42.11,
      "code_avg": 10.28,
      "reasoning_avg": 24.86,
      "overall_avg": 26.35,
      "general_scores": [
        59.7,
        25.6175,
        28.99,
        0.17714286,
        57.06,
        25.19,
        30.52,
        0.48285714,
        53.75,
        26.26,
        29.74,
        0.245
      ],
      "math_scores": [
        82.64,
        53.2,
        55.0,
        19.49,
        10.0,
        81.73,
        51.56,
        52.4,
        19.83,
        0.0,
        81.5,
        52.16,
        49.2,
        19.58,
        3.33
      ],
      "code_scores": [
        12.2,
        36.96,
        0.36,
        0.21,
        0.23,
        10.98,
        33.46,
        1.43,
        0.21,
        0.68,
        14.02,
        40.86,
        1.08,
        0.0,
        1.58
      ],
      "reasoning_scores": [
        52.2,
        39.04,
        0.21836957,
        10.64,
        48.47,
        32.48,
        0.20858696,
        8.96,
        57.97,
        35.3,
        0.21782609,
        12.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.96
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.4
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 37.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 0.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.13,
        "math_avg": 28.66,
        "code_avg": -5.39,
        "reasoning_avg": -16.47,
        "overall_avg": 4.73,
        "general_task_scores": [
          35.01,
          5.35,
          11.78,
          -3.62
        ],
        "math_task_scores": [
          26.16,
          46.81,
          50.6,
          15.29,
          4.44
        ],
        "code_task_scores": [
          -7.72,
          -17.38,
          -2.62,
          -0.07,
          0.83
        ],
        "reasoning_task_scores": [
          -27.46,
          -26.95,
          -0.1,
          -11.36
        ]
      },
      "affiliation": "EricLu",
      "year": "2025",
      "size": "274k",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K"
    },
    {
      "id": 42,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 32.46,
      "math_avg": 34.75,
      "code_avg": 18.21,
      "reasoning_avg": 28.31,
      "overall_avg": 28.43,
      "general_scores": [
        68.26,
        34.855,
        25.73,
        0.0,
        69.26,
        34.1825,
        27.37,
        0.0
      ],
      "math_scores": [
        59.74,
        36.16,
        34.0,
        3.33,
        66.26,
        36.56,
        38.6,
        3.33
      ],
      "code_scores": [
        22.56,
        51.36,
        2.51,
        0.21,
        17.19,
        18.9,
        53.31,
        1.08,
        0.0,
        14.93
      ],
      "reasoning_scores": [
        33.9,
        51.66,
        0.27717391,
        25.68,
        35.25,
        54.27,
        0.28543478,
        25.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.1
              },
              {
                "metric": "lcb_test_output",
                "score": 16.06
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.44,
        "math_avg": 21.3,
        "code_avg": 2.53,
        "reasoning_avg": -13.02,
        "overall_avg": 6.81,
        "general_task_scores": [
          46.93,
          14.18,
          8.58,
          -3.92
        ],
        "math_task_scores": [
          7.2,
          30.86,
          34.7,
          3.33
        ],
        "code_task_scores": [
          0.61,
          -2.13,
          -1.78,
          -0.11,
          16.06
        ],
        "reasoning_task_scores": [
          -45.76,
          -9.6,
          -0.03,
          3.32
        ]
      },
      "affiliation": "AI-MO",
      "year": "2025",
      "size": "896k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5"
    },
    {
      "id": 43,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 45.83,
      "math_avg": 20.9,
      "code_avg": 37.12,
      "reasoning_avg": 43.32,
      "overall_avg": 36.79,
      "general_scores": [
        59.32,
        54.035,
        39.36,
        31.23,
        62.96,
        53.8975,
        38.69,
        24.98,
        61.03,
        54.4775,
        39.14,
        30.785
      ],
      "math_scores": [
        44.35,
        20.88,
        23.4,
        11.99,
        3.33,
        46.93,
        20.76,
        21.4,
        11.09,
        0.0,
        54.28,
        21.66,
        22.2,
        11.2,
        0.0
      ],
      "code_scores": [
        67.68,
        65.37,
        7.17,
        19.42,
        25.79,
        65.85,
        65.37,
        7.53,
        19.62,
        25.57,
        68.9,
        66.93,
        7.17,
        17.95,
        26.47
      ],
      "reasoning_scores": [
        77.97,
        64.32,
        0.34152174,
        29.28,
        80.0,
        62.75,
        0.38271739,
        31.6,
        77.97,
        62.72,
        0.37793478,
        32.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.43
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.48
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.0
              },
              {
                "metric": "lcb_test_output",
                "score": 25.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.26
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.01
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 29.81,
        "math_avg": 7.45,
        "code_avg": 21.44,
        "reasoning_avg": 2.0,
        "overall_avg": 15.18,
        "general_task_scores": [
          39.27,
          33.8,
          21.09,
          25.08
        ],
        "math_task_scores": [
          -7.28,
          15.6,
          20.73,
          7.09,
          1.11
        ],
        "code_task_scores": [
          47.36,
          11.42,
          3.71,
          18.79,
          25.94
        ],
        "reasoning_task_scores": [
          -1.69,
          0.7,
          0.06,
          8.93
        ]
      },
      "affiliation": "Locutusque",
      "year": "2024",
      "size": "463k",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true"
    },
    {
      "id": 44,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 35.7,
      "math_avg": 4.41,
      "code_avg": 25.61,
      "reasoning_avg": 30.92,
      "overall_avg": 24.16,
      "general_scores": [
        55.24,
        45.455,
        33.64,
        9.75142857,
        57.68,
        44.5575,
        35.0,
        7.50071429,
        56.06,
        44.5625,
        31.89,
        7.05714286
      ],
      "math_scores": [
        3.34,
        4.52,
        5.4,
        6.39,
        0.0,
        3.49,
        5.0,
        6.6,
        6.28,
        3.33,
        3.18,
        4.42,
        4.6,
        6.23,
        3.33
      ],
      "code_scores": [
        34.15,
        46.69,
        0.0,
        26.1,
        23.08,
        32.93,
        42.41,
        0.0,
        25.05,
        23.08,
        31.1,
        47.08,
        0.0,
        27.14,
        25.34
      ],
      "reasoning_scores": [
        63.05,
        45.78,
        0.29,
        16.56,
        62.03,
        49.66,
        0.29076087,
        18.72,
        52.88,
        45.28,
        0.30923913,
        16.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 32.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.1
              },
              {
                "metric": "lcb_test_output",
                "score": 23.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.68,
        "math_avg": -9.04,
        "code_avg": 9.93,
        "reasoning_avg": -10.41,
        "overall_avg": 2.54,
        "general_task_scores": [
          34.5,
          24.52,
          15.54,
          4.18
        ],
        "math_task_scores": [
          -52.46,
          -0.85,
          3.93,
          1.96,
          2.22
        ],
        "code_task_scores": [
          12.61,
          -9.08,
          -3.58,
          25.89,
          23.83
        ],
        "reasoning_task_scores": [
          -21.02,
          -15.65,
          -0.01,
          -4.93
        ]
      },
      "affiliation": "glaiveAI",
      "year": "2023",
      "size": "20k",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"
    },
    {
      "id": 45,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 29.52,
      "math_avg": 13.17,
      "code_avg": 38.17,
      "reasoning_avg": 31.9,
      "overall_avg": 28.19,
      "general_scores": [
        32.19,
        37.875,
        30.06,
        12.8085714,
        30.96,
        41.305,
        33.27,
        9.16071429,
        46.25,
        39.6725,
        33.54,
        7.20142857
      ],
      "math_scores": [
        29.57,
        9.48,
        10.4,
        14.77,
        0.0,
        31.16,
        10.12,
        9.2,
        15.45,
        6.67,
        23.96,
        10.72,
        11.0,
        15.02,
        0.0
      ],
      "code_scores": [
        78.05,
        63.81,
        7.53,
        18.79,
        26.7,
        76.22,
        67.7,
        7.53,
        17.12,
        22.4,
        75.61,
        65.76,
        6.81,
        18.79,
        19.68
      ],
      "reasoning_scores": [
        52.88,
        48.99,
        0.27619565,
        20.08,
        61.02,
        48.14,
        0.30434783,
        24.24,
        57.97,
        46.37,
        0.28576087,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.23
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.23
              },
              {
                "metric": "lcb_test_output",
                "score": 22.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.51,
        "math_avg": -0.28,
        "code_avg": 22.49,
        "reasoning_avg": -9.42,
        "overall_avg": 6.57,
        "general_task_scores": [
          14.64,
          19.28,
          14.32,
          5.8
        ],
        "math_task_scores": [
          -27.57,
          4.61,
          8.6,
          10.74,
          2.22
        ],
        "code_task_scores": [
          56.51,
          11.29,
          3.71,
          18.02,
          22.93
        ],
        "reasoning_task_scores": [
          -23.05,
          -14.73,
          -0.02,
          0.11
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2"
    },
    {
      "id": 46,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 37.4,
      "math_avg": 13.97,
      "code_avg": 33.46,
      "reasoning_avg": 37.58,
      "overall_avg": 30.6,
      "general_scores": [
        53.9,
        45.0025,
        35.89,
        12.9128571,
        54.34,
        45.9125,
        33.82,
        17.3835714,
        54.99,
        44.7925,
        34.61,
        15.3
      ],
      "math_scores": [
        33.06,
        10.14,
        8.6,
        11.4,
        0.0,
        37.38,
        11.94,
        13.8,
        10.23,
        3.33,
        37.0,
        10.44,
        11.0,
        11.16,
        0.0
      ],
      "code_scores": [
        66.46,
        57.59,
        6.45,
        19.42,
        20.14,
        67.68,
        53.31,
        7.53,
        17.12,
        20.81,
        67.07,
        57.59,
        5.38,
        15.66,
        19.68
      ],
      "reasoning_scores": [
        73.22,
        54.55,
        0.26608696,
        25.84,
        70.85,
        53.7,
        0.24445652,
        26.96,
        70.51,
        50.86,
        0.25206522,
        23.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.4
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 21.39,
        "math_avg": 0.52,
        "code_avg": 17.78,
        "reasoning_avg": -3.75,
        "overall_avg": 8.99,
        "general_task_scores": [
          32.58,
          24.9,
          16.8,
          11.28
        ],
        "math_task_scores": [
          -19.99,
          5.34,
          9.53,
          6.59,
          1.11
        ],
        "code_task_scores": [
          46.95,
          1.69,
          2.87,
          17.19,
          20.21
        ],
        "reasoning_task_scores": [
          -8.81,
          -9.52,
          -0.06,
          3.41
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "157k",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction"
    },
    {
      "id": 47,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 34.16,
      "math_avg": 12.4,
      "code_avg": 37.19,
      "reasoning_avg": 36.84,
      "overall_avg": 30.15,
      "general_scores": [
        47.04,
        39.87,
        34.75,
        12.6614286,
        49.14,
        41.98,
        35.69,
        12.2407143,
        47.41,
        42.24,
        35.69,
        11.1778571
      ],
      "math_scores": [
        22.06,
        10.0,
        9.2,
        16.06,
        0.0,
        24.41,
        9.92,
        8.2,
        15.29,
        6.67,
        29.04,
        9.88,
        10.4,
        14.84,
        0.0
      ],
      "code_scores": [
        69.51,
        58.37,
        5.02,
        31.11,
        18.78,
        69.51,
        59.14,
        5.38,
        36.95,
        16.74,
        69.51,
        59.14,
        6.81,
        28.6,
        23.3
      ],
      "reasoning_scores": [
        64.07,
        53.03,
        0.25380435,
        26.0,
        68.47,
        53.36,
        0.27521739,
        26.08,
        69.15,
        54.76,
        0.27380435,
        26.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.74
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.22
              },
              {
                "metric": "lcb_test_output",
                "score": 19.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.14,
        "math_avg": -1.05,
        "code_avg": 21.52,
        "reasoning_avg": -4.49,
        "overall_avg": 8.53,
        "general_task_scores": [
          26.03,
          21.02,
          17.41,
          8.11
        ],
        "math_task_scores": [
          -30.63,
          4.43,
          7.67,
          11.06,
          2.22
        ],
        "code_task_scores": [
          49.39,
          4.41,
          2.16,
          32.01,
          19.61
        ],
        "reasoning_task_scores": [
          -13.11,
          -8.84,
          -0.04,
          4.05
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "111k",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1"
    },
    {
      "id": 48,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 33.9,
      "math_avg": 11.25,
      "code_avg": 35.88,
      "reasoning_avg": 36.22,
      "overall_avg": 29.31,
      "general_scores": [
        50.66,
        41.49,
        34.39,
        13.7185714,
        38.57,
        44.045,
        34.82,
        13.0278571,
        47.54,
        42.6375,
        36.22,
        9.72642857
      ],
      "math_scores": [
        22.67,
        9.46,
        7.2,
        14.7,
        0.0,
        28.35,
        9.62,
        8.2,
        15.99,
        0.0,
        22.67,
        8.22,
        7.8,
        13.91,
        0.0
      ],
      "code_scores": [
        66.46,
        59.92,
        3.58,
        27.14,
        19.68,
        70.73,
        58.37,
        4.3,
        26.1,
        24.21,
        72.56,
        56.81,
        4.3,
        23.17,
        20.81
      ],
      "reasoning_scores": [
        68.47,
        53.95,
        0.27586957,
        26.4,
        62.71,
        53.95,
        0.28597826,
        26.56,
        60.34,
        54.85,
        0.29130435,
        26.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.72
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.47
              },
              {
                "metric": "lcb_test_output",
                "score": 21.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.89,
        "math_avg": -2.2,
        "code_avg": 20.2,
        "reasoning_avg": -5.1,
        "overall_avg": 7.7,
        "general_task_scores": [
          23.76,
          22.38,
          17.17,
          8.24
        ],
        "math_task_scores": [
          -31.24,
          3.6,
          6.13,
          10.53,
          0.0
        ],
        "code_task_scores": [
          49.8,
          3.9,
          0.48,
          25.26,
          21.57
        ],
        "reasoning_task_scores": [
          -16.5,
          -8.31,
          -0.03,
          4.43
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "110k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K"
    },
    {
      "id": 49,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 33.31,
      "math_avg": 7.77,
      "code_avg": 20.72,
      "reasoning_avg": 28.51,
      "overall_avg": 22.58,
      "general_scores": [
        63.06,
        35.24,
        33.08,
        3.01714286,
        60.64,
        35.8975,
        33.1,
        2.31428571,
        61.74,
        34.9925,
        32.61,
        4.02285714
      ],
      "math_scores": [
        16.91,
        7.08,
        6.0,
        8.15,
        0.0,
        14.18,
        7.44,
        8.0,
        8.58,
        0.0,
        14.56,
        7.94,
        9.2,
        8.45,
        0.0
      ],
      "code_scores": [
        47.56,
        52.53,
        2.51,
        0.21,
        0.23,
        45.73,
        52.53,
        3.94,
        0.0,
        0.0,
        48.17,
        53.7,
        2.87,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        64.41,
        27.74,
        0.22836957,
        18.24,
        65.08,
        28.16,
        0.2373913,
        17.6,
        66.78,
        37.2,
        0.23119565,
        16.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.29,
        "math_avg": -5.68,
        "code_avg": 5.05,
        "reasoning_avg": -12.81,
        "overall_avg": 0.96,
        "general_task_scores": [
          39.98,
          15.04,
          14.96,
          -0.8
        ],
        "math_task_scores": [
          -40.58,
          1.99,
          6.13,
          4.05,
          0.0
        ],
        "code_task_scores": [
          27.03,
          -1.55,
          -0.47,
          0.14,
          0.08
        ],
        "reasoning_task_scores": [
          -14.92,
          -31.53,
          -0.08,
          -4.72
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "75.2k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K"
    },
    {
      "id": 50,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 32.25,
      "math_avg": 9.75,
      "code_avg": 29.19,
      "reasoning_avg": 27.02,
      "overall_avg": 24.55,
      "general_scores": [
        45.42,
        42.0775,
        31.85,
        11.7392857,
        54.05,
        37.0,
        33.67,
        10.0478571,
        42.18,
        37.8975,
        31.69,
        9.36214286
      ],
      "math_scores": [
        13.72,
        5.96,
        6.0,
        17.48,
        6.67,
        14.03,
        5.86,
        6.2,
        17.66,
        0.0,
        17.82,
        7.16,
        7.0,
        17.34,
        3.33
      ],
      "code_scores": [
        54.27,
        59.14,
        8.96,
        0.42,
        20.36,
        57.93,
        58.37,
        6.81,
        0.21,
        24.89,
        61.59,
        56.42,
        7.53,
        0.21,
        20.81
      ],
      "reasoning_scores": [
        55.59,
        29.31,
        0.22,
        18.08,
        61.69,
        32.06,
        0.23684783,
        15.68,
        60.34,
        32.29,
        0.22423913,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.77
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 22.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.21
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.23,
        "math_avg": -3.7,
        "code_avg": 13.52,
        "reasoning_avg": -14.31,
        "overall_avg": 2.94,
        "general_task_scores": [
          25.39,
          18.65,
          14.43,
          6.46
        ],
        "math_task_scores": [
          -40.61,
          0.83,
          4.8,
          13.15,
          3.33
        ],
        "code_task_scores": [
          37.81,
          3.51,
          4.19,
          0.07,
          22.02
        ],
        "reasoning_task_scores": [
          -21.13,
          -31.34,
          -0.08,
          -4.67
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "66.4k",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback"
    },
    {
      "id": 51,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 18.3,
      "math_avg": 2.27,
      "code_avg": 13.69,
      "reasoning_avg": 24.1,
      "overall_avg": 14.59,
      "general_scores": [
        17.96,
        24.61,
        26.21,
        3.15428571,
        15.06,
        23.915,
        25.4,
        6.20571429,
        21.4,
        23.7825,
        24.58,
        7.35571429
      ],
      "math_scores": [
        1.82,
        2.14,
        3.4,
        2.76,
        0.0,
        2.81,
        2.5,
        2.6,
        3.21,
        0.0,
        2.96,
        3.12,
        4.2,
        2.51,
        0.0
      ],
      "code_scores": [
        14.02,
        47.47,
        0.36,
        12.94,
        0.45,
        8.54,
        46.3,
        0.72,
        10.44,
        4.3,
        6.71,
        46.69,
        1.08,
        4.18,
        1.13
      ],
      "reasoning_scores": [
        40.68,
        48.76,
        0.15815217,
        16.64,
        34.24,
        37.61,
        0.14782609,
        15.04,
        37.97,
        40.61,
        0.14130435,
        17.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.19
              },
              {
                "metric": "lcb_test_output",
                "score": 1.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.29,
        "math_avg": -11.18,
        "code_avg": -1.99,
        "reasoning_avg": -17.22,
        "overall_avg": -7.03,
        "general_task_scores": [
          -3.69,
          3.76,
          7.43,
          1.65
        ],
        "math_task_scores": [
          -53.27,
          -2.91,
          1.8,
          -1.51,
          0.0
        ],
        "code_task_scores": [
          -10.36,
          -7.65,
          -2.86,
          8.98,
          1.96
        ],
        "reasoning_task_scores": [
          -42.71,
          -20.23,
          -0.16,
          -5.79
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "55.1k",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT"
    },
    {
      "id": 52,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 22.02,
      "math_avg": 7.57,
      "code_avg": 25.3,
      "reasoning_avg": 17.45,
      "overall_avg": 18.09,
      "general_scores": [
        32.95,
        30.26,
        20.24,
        4.34785714,
        31.67,
        34.065,
        22.19,
        8.03071429,
        24.39,
        29.7375,
        21.23,
        5.07357143
      ],
      "math_scores": [
        8.64,
        4.24,
        4.0,
        15.65,
        0.0,
        15.85,
        4.66,
        6.0,
        16.46,
        0.0,
        11.6,
        4.98,
        5.8,
        15.74,
        0.0
      ],
      "code_scores": [
        48.17,
        55.64,
        3.58,
        0.21,
        18.78,
        46.34,
        55.25,
        3.94,
        0.21,
        19.91,
        45.12,
        57.2,
        3.23,
        0.0,
        21.95
      ],
      "reasoning_scores": [
        30.51,
        26.58,
        0.14956522,
        7.6,
        32.54,
        33.66,
        0.15380435,
        9.92,
        31.19,
        29.33,
        0.15793478,
        7.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.03
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.0,
        "math_avg": -5.87,
        "code_avg": 9.63,
        "reasoning_avg": -23.87,
        "overall_avg": -3.53,
        "general_task_scores": [
          7.84,
          11.01,
          3.25,
          1.9
        ],
        "math_task_scores": [
          -43.77,
          -0.87,
          3.67,
          11.61,
          0.0
        ],
        "code_task_scores": [
          26.42,
          1.56,
          0.0,
          -0.07,
          20.21
        ],
        "reasoning_task_scores": [
          -48.93,
          -32.7,
          -0.16,
          -13.71
        ]
      },
      "affiliation": "bigcode (Huggingface)",
      "year": "2024",
      "size": "50.7k",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k"
    },
    {
      "id": 53,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 35.27,
      "math_avg": 15.37,
      "code_avg": 33.27,
      "reasoning_avg": 35.26,
      "overall_avg": 29.79,
      "general_scores": [
        41.75,
        46.4425,
        31.8,
        13.5342857,
        49.1,
        47.25,
        32.37,
        19.9864286,
        50.57,
        46.66,
        32.78,
        11.0335714
      ],
      "math_scores": [
        50.19,
        12.72,
        8.6,
        7.0,
        0.0,
        50.04,
        12.3,
        8.0,
        7.45,
        0.0,
        50.8,
        12.6,
        3.2,
        7.59,
        0.0
      ],
      "code_scores": [
        62.2,
        32.3,
        3.23,
        29.23,
        19.23,
        65.24,
        59.92,
        2.87,
        30.48,
        24.66,
        64.02,
        54.47,
        2.51,
        31.73,
        16.97
      ],
      "reasoning_scores": [
        66.44,
        51.65,
        0.39184783,
        21.84,
        63.39,
        52.43,
        0.40597826,
        23.52,
        67.8,
        50.81,
        0.35032609,
        24.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 20.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.26,
        "math_avg": 1.92,
        "code_avg": 17.59,
        "reasoning_avg": -6.06,
        "overall_avg": 8.18,
        "general_task_scores": [
          25.31,
          26.44,
          14.35,
          10.93
        ],
        "math_task_scores": [
          -5.46,
          7.04,
          5.0,
          3.01,
          0.0
        ],
        "code_task_scores": [
          43.7,
          -5.57,
          -0.71,
          30.27,
          20.29
        ],
        "reasoning_task_scores": [
          -14.46,
          -10.93,
          0.07,
          1.07
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1"
    },
    {
      "id": 54,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 21.49,
      "math_avg": 4.36,
      "code_avg": 18.88,
      "reasoning_avg": 20.0,
      "overall_avg": 16.18,
      "general_scores": [
        9.02,
        37.9425,
        17.48,
        8.68857143,
        16.57,
        35.685,
        19.77,
        13.1114286,
        29.23,
        39.87,
        17.6,
        12.875
      ],
      "math_scores": [
        5.46,
        1.94,
        2.0,
        14.54,
        0.0,
        1.52,
        1.72,
        1.6,
        16.62,
        0.0,
        2.65,
        1.88,
        3.2,
        12.33,
        0.0
      ],
      "code_scores": [
        41.46,
        50.97,
        1.43,
        1.25,
        0.23,
        41.46,
        47.47,
        1.43,
        4.59,
        0.9,
        42.68,
        47.86,
        0.36,
        0.63,
        0.45
      ],
      "reasoning_scores": [
        16.61,
        46.75,
        0.10391304,
        12.08,
        22.71,
        44.4,
        0.10847826,
        10.4,
        22.37,
        47.29,
        0.09326087,
        17.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.47,
        "math_avg": -9.08,
        "code_avg": 3.2,
        "reasoning_avg": -21.33,
        "overall_avg": -5.43,
        "general_task_scores": [
          -3.56,
          17.49,
          0.31,
          7.64
        ],
        "math_task_scores": [
          -52.59,
          -3.65,
          0.67,
          10.16,
          0.0
        ],
        "code_task_scores": [
          21.75,
          -5.7,
          -2.51,
          1.95,
          0.53
        ],
        "reasoning_task_scores": [
          -59.78,
          -16.41,
          -0.21,
          -8.91
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "35k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code"
    },
    {
      "id": 55,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 20.84,
      "math_avg": 3.44,
      "code_avg": 24.78,
      "reasoning_avg": 5.94,
      "overall_avg": 13.75,
      "general_scores": [
        20.64,
        30.5625,
        21.16,
        12.3721429,
        23.19,
        29.95,
        22.1,
        12.3792857,
        22.08,
        31.2475,
        16.43,
        7.91285714
      ],
      "math_scores": [
        0.23,
        0.08,
        0.2,
        16.73,
        0.0,
        0.0,
        0.02,
        0.0,
        18.34,
        0.0,
        0.08,
        0.02,
        0.0,
        15.85,
        0.0
      ],
      "code_scores": [
        60.98,
        60.7,
        0.0,
        0.42,
        0.0,
        65.24,
        57.2,
        0.0,
        0.21,
        0.23,
        65.24,
        61.48,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        13.9,
        3.87,
        0.14717391,
        8.0,
        13.9,
        3.95,
        0.14663043,
        5.84,
        14.24,
        4.39,
        0.15869565,
        2.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.97
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.82,
        "math_avg": -10.01,
        "code_avg": 9.1,
        "reasoning_avg": -35.38,
        "overall_avg": -7.87,
        "general_task_scores": [
          0.14,
          10.25,
          1.93,
          6.97
        ],
        "math_task_scores": [
          -55.7,
          -5.46,
          -1.53,
          12.63,
          0.0
        ],
        "code_task_scores": [
          43.7,
          5.32,
          -3.58,
          0.0,
          0.08
        ],
        "reasoning_task_scores": [
          -66.33,
          -58.49,
          -0.16,
          -16.56
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "444k",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1"
    },
    {
      "id": 56,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 27.09,
      "math_avg": 4.74,
      "code_avg": 14.94,
      "reasoning_avg": 27.81,
      "overall_avg": 18.65,
      "general_scores": [
        45.84,
        37.3625,
        18.36,
        7.40285714,
        41.66,
        39.5775,
        15.6,
        5.87214286,
        51.92,
        38.46,
        19.07,
        3.98
      ],
      "math_scores": [
        5.08,
        3.94,
        5.2,
        13.53,
        0.0,
        3.64,
        4.1,
        4.8,
        8.42,
        0.0,
        4.85,
        3.4,
        4.0,
        10.21,
        0.0
      ],
      "code_scores": [
        21.95,
        37.74,
        0.0,
        11.06,
        1.13,
        20.12,
        39.69,
        0.0,
        19.21,
        0.9,
        20.73,
        36.96,
        0.0,
        13.78,
        0.9
      ],
      "reasoning_scores": [
        37.29,
        50.86,
        0.23043478,
        17.76,
        43.73,
        49.59,
        0.26793478,
        18.96,
        44.07,
        51.84,
        0.23804348,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.68
              },
              {
                "metric": "lcb_test_output",
                "score": 0.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.76
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.08,
        "math_avg": -8.7,
        "code_avg": -0.73,
        "reasoning_avg": -13.51,
        "overall_avg": -2.97,
        "general_task_scores": [
          24.64,
          18.13,
          -0.29,
          1.83
        ],
        "math_task_scores": [
          -51.28,
          -1.69,
          3.07,
          6.38,
          0.0
        ],
        "code_task_scores": [
          0.81,
          -16.34,
          -3.58,
          14.47,
          0.98
        ],
        "reasoning_task_scores": [
          -38.64,
          -11.8,
          -0.06,
          -3.55
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "5k",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder"
    },
    {
      "id": 57,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 36.01,
      "math_avg": 11.62,
      "code_avg": 29.55,
      "reasoning_avg": 38.52,
      "overall_avg": 28.93,
      "general_scores": [
        43.11,
        48.8325,
        36.7,
        9.22214286,
        47.59,
        47.2175,
        36.09,
        17.1,
        50.64,
        47.5275,
        36.17,
        11.9664286
      ],
      "math_scores": [
        35.33,
        10.36,
        10.0,
        7.0,
        0.0,
        27.6,
        10.16,
        8.6,
        6.71,
        0.0,
        31.39,
        9.84,
        9.8,
        7.45,
        0.0
      ],
      "code_scores": [
        52.44,
        53.7,
        1.08,
        23.8,
        19.68,
        46.95,
        53.7,
        2.51,
        29.02,
        12.9,
        48.78,
        53.31,
        1.08,
        26.72,
        17.65
      ],
      "reasoning_scores": [
        77.97,
        54.67,
        0.35402174,
        21.92,
        77.97,
        53.19,
        0.36195652,
        20.48,
        78.31,
        55.03,
        0.35543478,
        21.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.56
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.51
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 20.0,
        "math_avg": -1.83,
        "code_avg": 13.88,
        "reasoning_avg": -2.81,
        "overall_avg": 7.31,
        "general_task_scores": [
          25.28,
          27.52,
          18.35,
          8.84
        ],
        "math_task_scores": [
          -24.36,
          4.62,
          7.87,
          2.71,
          0.0
        ],
        "code_task_scores": [
          29.27,
          -0.9,
          -2.02,
          26.3,
          16.74
        ],
        "reasoning_task_scores": [
          -2.26,
          -8.26,
          0.05,
          -0.75
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1"
    },
    {
      "id": 58,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 35.01,
      "math_avg": 5.56,
      "code_avg": 22.51,
      "reasoning_avg": 29.54,
      "overall_avg": 23.15,
      "general_scores": [
        48.8,
        42.0,
        29.99,
        21.575,
        48.26,
        44.71,
        27.82,
        18.9085714,
        48.3,
        43.045,
        29.01,
        17.6428571
      ],
      "math_scores": [
        8.72,
        6.84,
        7.2,
        6.59,
        0.0,
        9.86,
        6.38,
        7.6,
        6.66,
        0.0,
        5.53,
        5.96,
        5.6,
        6.46,
        0.0
      ],
      "code_scores": [
        43.29,
        50.58,
        0.0,
        15.24,
        6.33,
        40.85,
        54.09,
        0.0,
        13.78,
        4.07,
        42.68,
        45.53,
        0.0,
        5.64,
        15.61
      ],
      "reasoning_scores": [
        45.42,
        45.04,
        0.28347826,
        21.6,
        46.1,
        46.26,
        0.275,
        18.96,
        62.37,
        46.6,
        0.27206522,
        21.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.55
              },
              {
                "metric": "lcb_test_output",
                "score": 8.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.61
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.99,
        "math_avg": -7.89,
        "code_avg": 6.84,
        "reasoning_avg": -11.78,
        "overall_avg": 1.54,
        "general_task_scores": [
          26.62,
          22.91,
          10.97,
          15.46
        ],
        "math_task_scores": [
          -47.76,
          0.89,
          5.2,
          2.23,
          0.0
        ],
        "code_task_scores": [
          22.15,
          -4.4,
          -3.58,
          11.34,
          8.67
        ],
        "reasoning_task_scores": [
          -29.04,
          -16.59,
          -0.03,
          -1.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca"
    },
    {
      "id": 59,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 39.97,
      "math_avg": 7.14,
      "code_avg": 23.02,
      "reasoning_avg": 36.11,
      "overall_avg": 26.56,
      "general_scores": [
        46.95,
        46.8975,
        32.6,
        29.5671429,
        51.47,
        48.0325,
        33.12,
        27.4607143,
        53.45,
        47.0875,
        34.54,
        28.4692857
      ],
      "math_scores": [
        7.51,
        8.14,
        11.0,
        7.36,
        0.0,
        10.24,
        8.12,
        9.0,
        7.38,
        3.33,
        8.34,
        7.44,
        9.0,
        6.91,
        3.33
      ],
      "code_scores": [
        37.2,
        54.47,
        0.0,
        19.62,
        9.05,
        36.59,
        52.53,
        0.0,
        10.86,
        11.31,
        39.63,
        54.47,
        0.0,
        6.05,
        13.57
      ],
      "reasoning_scores": [
        70.85,
        53.47,
        0.30804348,
        23.6,
        64.07,
        53.88,
        0.32304348,
        23.44,
        65.76,
        53.76,
        0.30043478,
        23.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.18
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.89
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 23.96,
        "math_avg": -6.31,
        "code_avg": 7.35,
        "reasoning_avg": -5.21,
        "overall_avg": 4.95,
        "general_task_scores": [
          28.79,
          27.0,
          15.45,
          24.58
        ],
        "math_task_scores": [
          -47.1,
          2.4,
          8.07,
          2.88,
          2.22
        ],
        "code_task_scores": [
          17.69,
          -0.65,
          -3.58,
          11.97,
          11.31
        ],
        "reasoning_task_scores": [
          -13.45,
          -8.86,
          -0.0,
          1.47
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "18.6k",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca"
    },
    {
      "id": 60,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.16,
      "math_avg": 9.34,
      "code_avg": 29.57,
      "reasoning_avg": 34.15,
      "overall_avg": 28.81,
      "general_scores": [
        66.16,
        45.2575,
        34.66,
        21.9871429,
        66.55,
        44.4375,
        35.39,
        23.7114286,
        66.19,
        44.0875,
        34.03,
        23.5171429
      ],
      "math_scores": [
        21.46,
        9.32,
        10.0,
        8.2,
        0.0,
        21.08,
        9.22,
        8.8,
        8.31,
        0.0,
        17.06,
        8.54,
        9.8,
        8.27,
        0.0
      ],
      "code_scores": [
        51.22,
        56.42,
        7.89,
        16.91,
        22.17,
        48.78,
        55.64,
        8.24,
        5.22,
        25.34,
        47.56,
        54.09,
        6.45,
        14.61,
        23.08
      ],
      "reasoning_scores": [
        53.56,
        57.22,
        0.29184783,
        26.0,
        53.22,
        57.96,
        0.29641304,
        26.08,
        51.19,
        57.88,
        0.3075,
        25.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.25
              },
              {
                "metric": "lcb_test_output",
                "score": 23.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 26.15,
        "math_avg": -4.11,
        "code_avg": 13.9,
        "reasoning_avg": -7.17,
        "overall_avg": 7.19,
        "general_task_scores": [
          44.47,
          24.25,
          16.72,
          19.15
        ],
        "math_task_scores": [
          -35.93,
          3.53,
          7.93,
          3.92,
          0.0
        ],
        "code_task_scores": [
          29.07,
          0.91,
          3.95,
          12.04,
          23.53
        ],
        "reasoning_task_scores": [
          -27.68,
          -4.87,
          -0.01,
          3.89
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "22.6k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT"
    },
    {
      "id": 61,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 19.4,
      "math_avg": 3.95,
      "code_avg": 8.24,
      "reasoning_avg": 19.68,
      "overall_avg": 12.82,
      "general_scores": [
        40.6,
        23.7175,
        11.78,
        0.74142857,
        39.93,
        24.685,
        10.92,
        0.19285714,
        40.34,
        27.2425,
        11.89,
        0.8
      ],
      "math_scores": [
        1.59,
        2.86,
        3.2,
        6.35,
        3.33,
        1.67,
        3.0,
        4.8,
        6.75,
        3.33,
        2.73,
        3.2,
        3.4,
        6.39,
        6.67
      ],
      "code_scores": [
        1.83,
        39.3,
        0.0,
        0.0,
        0.0,
        0.61,
        39.3,
        0.0,
        0.0,
        0.0,
        0.61,
        42.02,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        44.41,
        29.34,
        0.20933333,
        4.16,
        33.22,
        28.57,
        0.226,
        6.08,
        51.19,
        32.73,
        0.20533333,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.21
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.39,
        "math_avg": -9.5,
        "code_avg": -7.43,
        "reasoning_avg": -21.64,
        "overall_avg": -8.8,
        "general_task_scores": [
          18.46,
          4.88,
          -6.44,
          -3.34
        ],
        "math_task_scores": [
          -53.8,
          -2.48,
          2.2,
          2.16,
          4.44
        ],
        "code_task_scores": [
          -19.1,
          -14.26,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -37.4,
          -32.35,
          -0.1,
          -16.72
        ]
      },
      "affiliation": "smcleod",
      "year": "2024",
      "size": "305k",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder/viewer/default/train?row=93"
    },
    {
      "id": 62,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 17.17,
      "math_avg": 11.25,
      "code_avg": 23.94,
      "reasoning_avg": 10.73,
      "overall_avg": 15.77,
      "general_scores": [
        5.68,
        31.015,
        24.76,
        0.0,
        21.59,
        31.8075,
        23.62,
        0.0,
        13.14,
        30.205,
        24.28,
        0.0
      ],
      "math_scores": [
        17.59,
        6.72,
        6.4,
        31.82,
        0.0,
        14.33,
        6.74,
        6.4,
        30.76,
        0.0,
        8.64,
        4.08,
        2.8,
        32.48,
        0.0
      ],
      "code_scores": [
        60.98,
        21.4,
        8.6,
        0.0,
        6.79,
        58.54,
        54.47,
        10.39,
        0.0,
        10.86,
        58.54,
        54.09,
        8.6,
        0.0,
        5.88
      ],
      "reasoning_scores": [
        28.47,
        6.76,
        0.13891304,
        4.4,
        31.53,
        7.43,
        0.12956522,
        7.04,
        29.49,
        6.15,
        0.12315217,
        7.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.35
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.83
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.16,
        "math_avg": -2.2,
        "code_avg": 8.27,
        "reasoning_avg": -30.6,
        "overall_avg": -5.84,
        "general_task_scores": [
          -8.36,
          10.67,
          6.25,
          -3.92
        ],
        "math_task_scores": [
          -42.28,
          0.35,
          3.6,
          27.35,
          0.0
        ],
        "code_task_scores": [
          39.23,
          -11.15,
          5.62,
          -0.21,
          7.84
        ],
        "reasoning_task_scores": [
          -50.51,
          -55.78,
          -0.18,
          -15.92
        ]
      },
      "affiliation": "microsoft",
      "year": "2025",
      "size": "380k",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k"
    },
    {
      "id": 63,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 43.46,
      "math_avg": 10.98,
      "code_avg": 30.67,
      "reasoning_avg": 37.16,
      "overall_avg": 30.57,
      "general_scores": [
        69.13,
        45.01,
        32.72,
        26.9557143,
        69.82,
        44.8675,
        32.23,
        28.5228571,
        68.85,
        43.4825,
        32.75,
        27.2242857
      ],
      "math_scores": [
        19.26,
        8.9,
        7.2,
        12.08,
        0.0,
        27.52,
        10.66,
        9.6,
        11.34,
        3.33,
        24.64,
        10.48,
        7.8,
        11.88,
        0.0
      ],
      "code_scores": [
        49.39,
        57.59,
        1.08,
        14.82,
        28.51,
        50.61,
        61.09,
        3.94,
        17.33,
        29.64,
        51.83,
        59.53,
        1.08,
        6.05,
        27.6
      ],
      "reasoning_scores": [
        68.14,
        55.67,
        0.17163043,
        23.44,
        73.56,
        56.42,
        0.18043478,
        19.52,
        70.17,
        57.16,
        0.17152174,
        21.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.03
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.73
              },
              {
                "metric": "lcb_test_output",
                "score": 28.58
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 27.45,
        "math_avg": -2.47,
        "code_avg": 15.0,
        "reasoning_avg": -4.16,
        "overall_avg": 8.95,
        "general_task_scores": [
          47.44,
          24.11,
          14.6,
          23.65
        ],
        "math_task_scores": [
          -31.99,
          4.51,
          6.6,
          7.43,
          1.11
        ],
        "code_task_scores": [
          30.49,
          4.93,
          -1.55,
          12.52,
          28.58
        ],
        "reasoning_task_scores": [
          -9.72,
          -6.14,
          -0.14,
          -0.64
        ]
      },
      "affiliation": "likaixin",
      "year": "2023",
      "size": "108k",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder"
    },
    {
      "id": 64,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 24.02,
      "math_avg": 4.14,
      "code_avg": 18.0,
      "reasoning_avg": 17.56,
      "overall_avg": 15.93,
      "general_scores": [
        32.32,
        29.495,
        26.9,
        18.985,
        23.31,
        30.8075,
        18.87,
        16.0421429,
        30.22,
        33.365,
        18.76,
        9.20214286
      ],
      "math_scores": [
        0.08,
        0.02,
        0.0,
        20.82,
        0.0,
        0.08,
        0.02,
        0.0,
        21.54,
        0.0,
        0.23,
        0.04,
        0.0,
        19.24,
        0.0
      ],
      "code_scores": [
        37.2,
        47.08,
        0.0,
        0.0,
        0.0,
        37.8,
        55.25,
        0.0,
        0.0,
        0.0,
        38.41,
        53.7,
        0.0,
        0.63,
        0.0
      ],
      "reasoning_scores": [
        60.34,
        5.96,
        0.03206522,
        9.76,
        54.58,
        4.53,
        0.01586957,
        5.12,
        58.31,
        2.91,
        0.01293478,
        9.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.74
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.01,
        "math_avg": -9.31,
        "code_avg": 2.33,
        "reasoning_avg": -23.77,
        "overall_avg": -5.68,
        "general_task_scores": [
          6.79,
          10.88,
          3.54,
          10.82
        ],
        "math_task_scores": [
          -55.67,
          -5.47,
          -1.6,
          16.19,
          0.0
        ],
        "code_task_scores": [
          17.68,
          -2.46,
          -3.58,
          0.0,
          0.0
        ],
        "reasoning_task_scores": [
          -22.6,
          -58.09,
          -0.29,
          -14.08
        ]
      },
      "affiliation": "likaixin",
      "year": "2024",
      "size": "1043k",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified"
    },
    {
      "id": 65,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 17.4,
      "math_avg": 0.81,
      "code_avg": 0.08,
      "reasoning_avg": 7.84,
      "overall_avg": 6.53,
      "general_scores": [
        29.71,
        25.09,
        13.99,
        0.67214286,
        29.97,
        24.2225,
        12.53,
        2.99428571
      ],
      "math_scores": [
        0.08,
        0.02,
        1.6,
        3.0,
        0.0,
        0.0,
        0.0,
        0.2,
        3.16,
        0.0
      ],
      "code_scores": [
        0.0,
        0.39,
        0.0,
        0.0,
        0.0,
        0.0,
        0.39,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        25.42,
        6.22,
        0.0401087,
        0.4,
        23.05,
        7.57,
        0.02380435,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.83
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.38,
        "math_avg": -12.64,
        "code_avg": -15.6,
        "reasoning_avg": -33.48,
        "overall_avg": -15.08,
        "general_task_scores": [
          8.01,
          4.32,
          -4.71,
          -2.09
        ],
        "math_task_scores": [
          -55.76,
          -5.49,
          -0.7,
          -1.26,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -54.08,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -56.1,
          -55.66,
          -0.28,
          -21.88
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "100k",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql"
    },
    {
      "id": 66,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 27.71,
      "math_avg": 5.37,
      "code_avg": 18.2,
      "reasoning_avg": 23.89,
      "overall_avg": 18.79,
      "general_scores": [
        33.84,
        41.3275,
        25.41,
        12.4314286,
        29.66,
        45.1325,
        32.35,
        13.9057143,
        25.22,
        40.27,
        24.38,
        8.57928571
      ],
      "math_scores": [
        2.65,
        3.44,
        4.2,
        17.66,
        3.33,
        3.71,
        4.68,
        4.4,
        13.98,
        0.0,
        1.21,
        2.06,
        2.2,
        16.98,
        0.0
      ],
      "code_scores": [
        37.2,
        42.41,
        0.0,
        20.46,
        2.49,
        34.76,
        35.02,
        0.0,
        13.99,
        2.26,
        31.71,
        44.75,
        0.0,
        5.22,
        2.71
      ],
      "reasoning_scores": [
        59.32,
        29.31,
        0.08576087,
        5.04,
        63.73,
        31.61,
        0.08815217,
        10.0,
        64.75,
        20.22,
        0.06619565,
        2.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.56
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.22
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.69,
        "math_avg": -8.08,
        "code_avg": 2.52,
        "reasoning_avg": -17.43,
        "overall_avg": -2.82,
        "general_task_scores": [
          7.74,
          21.9,
          9.41,
          7.72
        ],
        "math_task_scores": [
          -53.28,
          -2.11,
          2.0,
          11.87,
          1.11
        ],
        "code_task_scores": [
          14.44,
          -13.74,
          -3.58,
          13.01,
          2.49
        ],
        "reasoning_task_scores": [
          -17.74,
          -35.51,
          -0.23,
          -16.24
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "22.5k",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1"
    },
    {
      "id": 67,
      "name": "RAG-v1",
      "domain": "code",
      "general_avg": 14.2,
      "math_avg": 3.58,
      "code_avg": 10.52,
      "reasoning_avg": 9.64,
      "overall_avg": 9.49,
      "general_scores": [
        9.15,
        17.085,
        21.15,
        9.11928571,
        9.68,
        17.6125,
        21.47,
        9.11928571,
        9.94,
        15.955,
        21.02,
        9.11928571
      ],
      "math_scores": [
        1.14,
        2.44,
        4.8,
        5.42,
        0.0,
        6.14,
        5.12,
        5.4,
        7.29,
        3.33,
        0.45,
        1.36,
        2.8,
        4.7,
        3.33
      ],
      "code_scores": [
        0.0,
        50.19,
        0.0,
        0.0,
        0.0,
        0.61,
        52.92,
        0.0,
        0.0,
        0.0,
        0.0,
        54.09,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        27.12,
        11.12,
        0.11869565,
        0.96,
        25.76,
        11.09,
        0.11402174,
        0.96,
        25.76,
        11.28,
        0.1125,
        1.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.21
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.81,
        "math_avg": -9.87,
        "code_avg": -5.16,
        "reasoning_avg": -31.68,
        "overall_avg": -12.13,
        "general_task_scores": [
          -12.24,
          -3.46,
          3.24,
          5.2
        ],
        "math_task_scores": [
          -53.22,
          -2.53,
          2.73,
          1.46,
          2.22
        ],
        "code_task_scores": [
          -19.92,
          -2.07,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -54.13,
          -51.4,
          -0.19,
          -21.01
        ]
      },
      "affiliation": "glaiveai",
      "year": "2024",
      "size": "51.4k",
      "link": "https://huggingface.co/datasets/glaiveai/RAG-v1"
    },
    {
      "id": 68,
      "name": "Code-290k-ShareGPT-Vicuna",
      "domain": "code",
      "general_avg": 41.43,
      "math_avg": 15.16,
      "code_avg": 36.63,
      "reasoning_avg": 38.12,
      "overall_avg": 32.84,
      "general_scores": [
        66.26,
        41.69,
        35.11,
        19.05,
        66.63,
        43.42,
        35.55,
        24.8442857,
        64.23,
        41.3525,
        35.48,
        23.5635714
      ],
      "math_scores": [
        29.11,
        14.48,
        14.8,
        11.56,
        6.67,
        32.98,
        16.52,
        16.2,
        11.34,
        3.33,
        29.72,
        14.74,
        14.2,
        11.74,
        0.0
      ],
      "code_scores": [
        66.46,
        65.37,
        7.17,
        25.89,
        23.3,
        65.24,
        64.98,
        3.58,
        13.99,
        25.79,
        67.68,
        65.37,
        6.81,
        19.83,
        28.05
      ],
      "reasoning_scores": [
        64.75,
        60.15,
        0.32402174,
        28.16,
        63.39,
        59.12,
        0.33869565,
        28.96,
        61.36,
        61.17,
        0.31847826,
        29.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.71
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.49
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.85
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.9
              },
              {
                "metric": "lcb_test_output",
                "score": 25.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 25.42,
        "math_avg": 1.71,
        "code_avg": 20.96,
        "reasoning_avg": -3.2,
        "overall_avg": 11.22,
        "general_task_scores": [
          43.88,
          21.81,
          17.41,
          18.57
        ],
        "math_task_scores": [
          -25.2,
          9.75,
          13.47,
          7.21,
          3.33
        ],
        "code_task_scores": [
          46.34,
          10.77,
          2.27,
          19.69,
          25.71
        ],
        "reasoning_task_scores": [
          -17.17,
          -2.41,
          0.02,
          6.77
        ]
      },
      "affiliation": "cognitivecomputations",
      "year": "2024",
      "size": "289k",
      "link": "https://huggingface.co/datasets/cognitivecomputations/Code-290k-ShareGPT-Vicuna"
    },
    {
      "id": 69,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 14.53,
      "math_avg": 1.24,
      "code_avg": 9.23,
      "reasoning_avg": 7.64,
      "overall_avg": 8.16,
      "general_scores": [
        16.12,
        26.68,
        12.63,
        0.00857143,
        18.69,
        23.56,
        12.6,
        3.35285714,
        23.97,
        21.7775,
        11.67,
        3.28
      ],
      "math_scores": [
        0.0,
        0.18,
        0.4,
        4.04,
        0.0,
        1.74,
        2.32,
        2.2,
        3.84,
        0.0,
        0.08,
        0.58,
        1.2,
        1.99,
        0.0
      ],
      "code_scores": [
        0.0,
        42.41,
        0.0,
        1.04,
        0.0,
        0.0,
        46.3,
        0.0,
        0.0,
        0.0,
        0.0,
        48.64,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        25.76,
        0.09,
        0.11967391,
        3.92,
        30.17,
        2.62,
        0.14521739,
        3.84,
        22.03,
        0.02,
        0.13413043,
        2.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 12.3
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.49,
        "math_avg": -12.21,
        "code_avg": -6.45,
        "reasoning_avg": -33.68,
        "overall_avg": -13.46,
        "general_task_scores": [
          -2.24,
          3.67,
          -5.67,
          -1.71
        ],
        "math_task_scores": [
          -55.19,
          -4.47,
          -0.33,
          -1.05,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -8.69,
          -3.58,
          0.14,
          0.0
        ],
        "reasoning_task_scores": [
          -54.35,
          -61.65,
          -0.18,
          -18.53
        ]
      },
      "affiliation": "b-mc2",
      "year": "2023",
      "size": "78.6k",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context/viewer/default/train?row=0"
    },
    {
      "id": 70,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 7.56,
      "math_avg": 0.32,
      "code_avg": 8.33,
      "reasoning_avg": 3.26,
      "overall_avg": 4.87,
      "general_scores": [
        0.01,
        16.575,
        0.7,
        0.015,
        20.42,
        12.955,
        9.79,
        0.0
      ],
      "math_scores": [
        0.0,
        0.04,
        0.0,
        0.09,
        0.0,
        3.33,
        0.0,
        0.0,
        0.0,
        0.07,
        0.0
      ],
      "code_scores": [
        0.0,
        36.58,
        0.0,
        0.0,
        0.0,
        0.0,
        46.69,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        23.73,
        0.0,
        0.00076087,
        0.0,
        1.36,
        0.95,
        0.0001087,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.24
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 41.64
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.48
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -8.46,
        "math_avg": -13.13,
        "code_avg": -7.35,
        "reasoning_avg": -38.07,
        "overall_avg": -16.75,
        "general_task_scores": [
          -11.61,
          -5.58,
          -12.73,
          -3.91
        ],
        "math_task_scores": [
          -55.8,
          -5.48,
          -1.6,
          -4.26,
          1.11
        ],
        "code_task_scores": [
          -20.12,
          -12.83,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -67.8,
          -62.08,
          -0.31,
          -22.08
        ]
      },
      "affiliation": "argilla",
      "year": "2024",
      "size": "109k",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling"
    },
    {
      "id": 71,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 41.43,
      "math_avg": 15.16,
      "code_avg": 36.63,
      "reasoning_avg": 38.12,
      "overall_avg": 32.84,
      "general_scores": [
        66.26,
        41.69,
        35.11,
        19.05,
        66.63,
        43.42,
        35.55,
        24.8442857,
        64.23,
        41.3525,
        35.48,
        23.5635714
      ],
      "math_scores": [
        29.11,
        14.48,
        14.8,
        11.56,
        6.67,
        32.98,
        16.52,
        16.2,
        11.34,
        3.33,
        29.72,
        14.74,
        14.2,
        11.74,
        0.0
      ],
      "code_scores": [
        66.46,
        65.37,
        7.17,
        25.89,
        23.3,
        65.24,
        64.98,
        3.58,
        13.99,
        25.79,
        67.68,
        65.37,
        6.81,
        19.83,
        28.05
      ],
      "reasoning_scores": [
        64.75,
        60.15,
        0.32402174,
        28.16,
        63.39,
        59.12,
        0.33869565,
        28.96,
        61.36,
        61.17,
        0.31847826,
        29.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.71
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.49
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.85
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.9
              },
              {
                "metric": "lcb_test_output",
                "score": 25.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 25.42,
        "math_avg": 1.71,
        "code_avg": 20.96,
        "reasoning_avg": -3.2,
        "overall_avg": 11.22,
        "general_task_scores": [
          43.88,
          21.81,
          17.41,
          18.57
        ],
        "math_task_scores": [
          -25.2,
          9.75,
          13.47,
          7.21,
          3.33
        ],
        "code_task_scores": [
          46.34,
          10.77,
          2.27,
          19.69,
          25.71
        ],
        "reasoning_task_scores": [
          -17.17,
          -2.41,
          0.02,
          6.77
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "290k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT"
    },
    {
      "id": 72,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 39.75,
      "math_avg": 13.16,
      "code_avg": 33.03,
      "reasoning_avg": 35.11,
      "overall_avg": 30.26,
      "general_scores": [
        68.75,
        29.31,
        34.56,
        0.69714286,
        71.27,
        48.2975,
        36.37,
        16.58,
        70.78,
        47.68,
        35.97,
        16.7542857
      ],
      "math_scores": [
        8.26,
        6.28,
        5.8,
        0.0,
        35.56,
        17.18,
        15.4,
        11.02,
        0.0,
        38.29,
        16.96,
        18.0,
        11.56,
        0.0
      ],
      "code_scores": [
        50.0,
        59.92,
        3.94,
        8.35,
        0.23,
        65.24,
        65.37,
        7.89,
        26.93,
        24.66,
        67.07,
        62.26,
        8.24,
        23.8,
        21.49
      ],
      "reasoning_scores": [
        55.93,
        43.83,
        0.21619565,
        10.48,
        64.41,
        62.48,
        0.31043478,
        28.64,
        63.73,
        62.85,
        0.29043478,
        28.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 62.52
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.69
              },
              {
                "metric": "lcb_test_output",
                "score": 15.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 23.74,
        "math_avg": -0.28,
        "code_avg": 17.35,
        "reasoning_avg": -6.21,
        "overall_avg": 8.65,
        "general_task_scores": [
          48.44,
          21.42,
          17.66,
          7.42
        ],
        "math_task_scores": [
          -28.43,
          7.97,
          11.47,
          6.95,
          0.0
        ],
        "code_task_scores": [
          40.65,
          8.05,
          3.11,
          19.48,
          15.46
        ],
        "reasoning_task_scores": [
          -18.98,
          -6.17,
          -0.04,
          0.35
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "74k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT"
    },
    {
      "id": 73,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 17.81,
      "math_avg": 5.95,
      "code_avg": 25.06,
      "reasoning_avg": 15.93,
      "overall_avg": 16.19,
      "general_scores": [
        20.61,
        30.395,
        24.21,
        0.0,
        15.94,
        28.015,
        25.28,
        0.0,
        19.02,
        26.9925,
        23.25,
        0.0
      ],
      "math_scores": [
        2.2,
        2.1,
        2.2,
        20.3,
        3.33,
        3.03,
        2.5,
        3.2,
        19.47,
        3.33,
        1.59,
        1.92,
        2.4,
        21.7,
        0.0
      ],
      "code_scores": [
        38.41,
        48.64,
        5.02,
        10.02,
        24.21,
        38.41,
        54.09,
        5.38,
        10.02,
        23.08,
        35.98,
        50.58,
        5.02,
        4.38,
        22.62
      ],
      "reasoning_scores": [
        40.34,
        9.88,
        0.17923913,
        3.76,
        59.32,
        14.92,
        0.18532609,
        3.36,
        47.12,
        9.84,
        0.16793478,
        2.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.52
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.17
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.6
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.14
              },
              {
                "metric": "lcb_test_output",
                "score": 23.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.79,
        "math_avg": -7.5,
        "code_avg": 9.38,
        "reasoning_avg": -25.39,
        "overall_avg": -5.43,
        "general_task_scores": [
          -3.31,
          8.13,
          6.28,
          -3.92
        ],
        "math_task_scores": [
          -53.53,
          -3.33,
          1.0,
          16.15,
          2.22
        ],
        "code_task_scores": [
          17.48,
          -3.37,
          1.56,
          7.93,
          23.3
        ],
        "reasoning_task_scores": [
          -31.41,
          -51.01,
          -0.13,
          -19.01
        ]
      },
      "affiliation": "Tensoic",
      "year": "2023",
      "size": "45k",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook"
    },
    {
      "id": 74,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 18.68,
      "math_avg": 1.76,
      "code_avg": 7.4,
      "reasoning_avg": 12.3,
      "overall_avg": 10.04,
      "general_scores": [
        27.15,
        31.1675,
        17.83,
        0.0,
        27.4,
        31.3975,
        16.76,
        0.0,
        24.81,
        30.2225,
        17.42,
        0.0
      ],
      "math_scores": [
        2.2,
        1.68,
        2.0,
        0.0,
        3.18,
        2.56,
        3.2,
        0.0,
        1.29,
        1.12,
        0.6,
        3.33
      ],
      "code_scores": [
        0.0,
        38.13,
        0.0,
        0.84,
        0.0,
        0.0,
        35.41,
        0.0,
        0.0,
        0.0,
        0.0,
        36.58,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        18.98,
        22.24,
        0.18326087,
        3.84,
        26.44,
        23.48,
        0.19141304,
        3.36,
        31.53,
        14.76,
        0.20543478,
        2.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.93
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 36.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.67,
        "math_avg": -11.68,
        "code_avg": -8.28,
        "reasoning_avg": -29.02,
        "overall_avg": -11.58,
        "general_task_scores": [
          4.62,
          10.59,
          -0.63,
          -3.92
        ],
        "math_task_scores": [
          -53.58,
          -3.71,
          0.33,
          1.11
        ],
        "code_task_scores": [
          -20.12,
          -17.76,
          -3.58,
          0.07,
          0.0
        ],
        "reasoning_task_scores": [
          -54.69,
          -42.4,
          -0.12,
          -18.88
        ]
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python"
    },
    {
      "id": 75,
      "name": "OpenR1-Math-220k(default)",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 58.15,
      "code_avg": 6.25,
      "reasoning_avg": 0,
      "overall_avg": 32.2,
      "general_scores": [],
      "math_scores": [
        87.26,
        77.64,
        76.8,
        35.73,
        13.33
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        1.88,
        1.36
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.73
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.88
              },
              {
                "metric": "lcb_test_output",
                "score": 1.36
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 44.7,
        "code_avg": -9.42,
        "reasoning_avg": -41.32,
        "overall_avg": 10.59,
        "general_task_scores": [],
        "math_task_scores": [
          31.46,
          72.14,
          75.2,
          31.39,
          13.33
        ],
        "code_task_scores": [
          -20.12,
          -26.45,
          -3.58,
          1.67,
          1.36
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k"
    },
    {
      "id": 76,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 53.58,
      "code_avg": 10.67,
      "reasoning_avg": 0,
      "overall_avg": 32.12,
      "general_scores": [],
      "math_scores": [
        86.81,
        61.38,
        62.8,
        3.33
      ],
      "code_scores": [
        0.0,
        44.36,
        5.02,
        1.46,
        2.49
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.38
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 44.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 40.13,
        "code_avg": -5.01,
        "reasoning_avg": -41.32,
        "overall_avg": 10.51,
        "general_task_scores": [],
        "math_task_scores": [
          31.01,
          55.88,
          61.2,
          3.33
        ],
        "code_task_scores": [
          -20.12,
          -10.11,
          1.44,
          1.25,
          2.49
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Yonsei University",
      "year": "2025",
      "size": "130k",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K"
    },
    {
      "id": 77,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 34.52,
      "code_avg": 10.79,
      "reasoning_avg": 0,
      "overall_avg": 22.66,
      "general_scores": [],
      "math_scores": [
        71.42,
        29.2,
        30.8,
        6.67
      ],
      "code_scores": [
        1.83,
        46.3,
        4.3,
        0.63,
        0.9
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 21.07,
        "code_avg": -4.88,
        "reasoning_avg": -41.32,
        "overall_avg": 1.04,
        "general_task_scores": [],
        "math_task_scores": [
          15.62,
          23.7,
          29.2,
          6.67
        ],
        "code_task_scores": [
          -18.29,
          -8.17,
          0.72,
          0.42,
          0.9
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Yonsei University",
      "year": "2025",
      "size": "138k",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2"
    },
    {
      "id": 78,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 21.46,
      "math_avg": 20.94,
      "code_avg": 21.8,
      "reasoning_avg": 18.47,
      "overall_avg": 20.67,
      "general_scores": [
        31.42,
        28.3025,
        25.66,
        0.12142857,
        38.8,
        27.275,
        24.39,
        0.105,
        29.52,
        27.4475,
        24.46,
        0.065
      ],
      "math_scores": [
        4.4,
        42.86,
        40.8,
        15.22,
        0.0,
        3.49,
        42.54,
        40.0,
        15.06,
        0.0,
        5.08,
        44.16,
        42.2,
        15.0,
        3.33
      ],
      "code_scores": [
        38.41,
        56.03,
        9.68,
        9.6,
        0.45,
        42.68,
        52.53,
        9.32,
        2.51,
        0.0,
        39.02,
        52.92,
        8.96,
        4.18,
        0.68
      ],
      "reasoning_scores": [
        47.12,
        22.95,
        0.17032609,
        7.12,
        45.42,
        22.43,
        0.16684783,
        6.48,
        40.68,
        22.03,
        0.16467391,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.43
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.45,
        "math_avg": 7.49,
        "code_avg": 6.12,
        "reasoning_avg": -22.85,
        "overall_avg": -0.95,
        "general_task_scores": [
          11.42,
          7.34,
          6.87,
          -3.82
        ],
        "math_task_scores": [
          -51.48,
          37.69,
          39.4,
          10.75,
          1.11
        ],
        "code_task_scores": [
          19.92,
          -0.64,
          5.74,
          5.22,
          0.38
        ],
        "reasoning_task_scores": [
          -35.93,
          -40.09,
          -0.14,
          -15.23
        ]
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k"
    },
    {
      "id": 79,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 66.02,
      "code_avg": 19.57,
      "reasoning_avg": 0,
      "overall_avg": 42.8,
      "general_scores": [],
      "math_scores": [
        84.76,
        75.6,
        70.4,
        33.33
      ],
      "code_scores": [
        29.27,
        49.42,
        10.39,
        8.77,
        0.0
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.77
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 52.57,
        "code_avg": 3.89,
        "reasoning_avg": -41.32,
        "overall_avg": 21.18,
        "general_task_scores": [],
        "math_task_scores": [
          28.96,
          70.1,
          68.8,
          33.33
        ],
        "code_task_scores": [
          9.15,
          -5.05,
          6.81,
          8.56,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Open Thoughts",
      "year": "2025",
      "size": "114k",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"
    },
    {
      "id": 80,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 32.04,
      "code_avg": 3.51,
      "reasoning_avg": 0,
      "overall_avg": 17.77,
      "general_scores": [],
      "math_scores": [
        64.52,
        32.82,
        30.8,
        0.0
      ],
      "code_scores": [
        0.0,
        17.12,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 18.59,
        "code_avg": -12.17,
        "reasoning_avg": -41.32,
        "overall_avg": -3.84,
        "general_task_scores": [],
        "math_task_scores": [
          8.72,
          27.32,
          29.2,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -37.35,
          -3.58,
          0.21,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT"
    },
    {
      "id": 81,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 38.88,
      "code_avg": 19.93,
      "reasoning_avg": 0,
      "overall_avg": 29.4,
      "general_scores": [],
      "math_scores": [
        77.26,
        37.64,
        40.6,
        0.0
      ],
      "code_scores": [
        3.05,
        51.36,
        4.3,
        16.28,
        24.66
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.28
              },
              {
                "metric": "lcb_test_output",
                "score": 24.66
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 25.43,
        "code_avg": 4.25,
        "reasoning_avg": -41.32,
        "overall_avg": 7.79,
        "general_task_scores": [],
        "math_task_scores": [
          21.46,
          32.14,
          39.0,
          0.0
        ],
        "code_task_scores": [
          -17.07,
          -3.11,
          0.72,
          16.07,
          24.66
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "O1-OPEN",
      "year": "2025",
      "size": "77.7k",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"
    },
    {
      "id": 82,
      "name": "GAIR/o1-journey",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 0.08,
      "code_avg": 10.82,
      "reasoning_avg": 0,
      "overall_avg": 5.45,
      "general_scores": [],
      "math_scores": [
        0.08,
        0.04,
        0.2,
        0.0
      ],
      "code_scores": [
        0.0,
        54.09,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": -13.37,
        "code_avg": -4.86,
        "reasoning_avg": -41.32,
        "overall_avg": -16.17,
        "general_task_scores": [],
        "math_task_scores": [
          -55.72,
          -5.46,
          -1.4,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -0.38,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey"
    },
    {
      "id": 83,
      "name": "zwhe99/DeepMath-103K",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 68.34,
      "code_avg": 6.46,
      "reasoning_avg": 0,
      "overall_avg": 37.4,
      "general_scores": [],
      "math_scores": [
        83.09,
        85.88,
        84.4,
        20.0
      ],
      "code_scores": [
        0.61,
        25.68,
        1.43,
        2.3,
        2.26
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.09
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 25.68
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.3
              },
              {
                "metric": "lcb_test_output",
                "score": 2.26
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 54.89,
        "code_avg": -9.22,
        "reasoning_avg": -41.32,
        "overall_avg": 15.78,
        "general_task_scores": [],
        "math_task_scores": [
          27.29,
          80.38,
          82.8,
          20.0
        ],
        "code_task_scores": [
          -19.51,
          -28.79,
          -2.15,
          2.09,
          2.26
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "zwhe99",
      "year": "2025",
      "size": "309k",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K"
    },
    {
      "id": 84,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 34.63,
      "code_avg": 16.13,
      "reasoning_avg": 0,
      "overall_avg": 25.38,
      "general_scores": [],
      "math_scores": [
        75.28,
        31.24,
        32.0,
        0.0
      ],
      "code_scores": [
        0.0,
        56.42,
        7.17,
        9.39,
        7.69
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.24
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.17
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.39
              },
              {
                "metric": "lcb_test_output",
                "score": 7.69
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 21.18,
        "code_avg": 0.46,
        "reasoning_avg": -41.32,
        "overall_avg": 3.77,
        "general_task_scores": [],
        "math_task_scores": [
          19.48,
          25.74,
          30.4,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          1.95,
          3.59,
          9.18,
          7.69
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1/viewer/default/train"
    },
    {
      "id": 85,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 50.53,
      "math_avg": 28.32,
      "code_avg": 32.97,
      "reasoning_avg": 40.0,
      "overall_avg": 37.95,
      "general_scores": [
        72.79,
        54.23,
        41.44,
        32.9271429,
        72.22,
        54.27,
        40.8,
        32.2985714,
        73.64,
        55.87,
        41.74,
        34.1107143
      ],
      "math_scores": [
        74.68,
        27.02,
        27.8,
        11.77,
        3.33,
        74.3,
        25.66,
        26.6,
        12.01,
        0.0,
        74.22,
        26.88,
        28.2,
        12.26,
        0.0
      ],
      "code_scores": [
        55.49,
        64.98,
        10.39,
        5.85,
        19.91,
        57.93,
        64.98,
        8.96,
        20.46,
        20.59,
        60.37,
        64.98,
        10.04,
        5.43,
        24.21
      ],
      "reasoning_scores": [
        81.36,
        46.23,
        0.34467391000000003,
        32.0,
        83.05,
        39.6,
        0.33173913,
        33.44,
        83.73,
        46.27,
        0.34184783,
        33.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.79
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.33
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.4
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.01
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 10.58
              },
              {
                "metric": "lcb_test_output",
                "score": 21.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.91
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 34.51,
        "math_avg": 14.87,
        "code_avg": 17.3,
        "reasoning_avg": -1.32,
        "overall_avg": 16.34,
        "general_task_scores": [
          51.05,
          34.45,
          23.36,
          29.19
        ],
        "math_task_scores": [
          18.6,
          21.02,
          25.93,
          7.67,
          1.11
        ],
        "code_task_scores": [
          37.81,
          10.51,
          6.22,
          10.37,
          21.57
        ],
        "reasoning_task_scores": [
          2.37,
          -18.53,
          0.03,
          10.83
        ]
      },
      "affiliation": "Magpie-Align",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K"
    },
    {
      "id": 86,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 48.49,
      "code_avg": 13.6,
      "reasoning_avg": 0,
      "overall_avg": 31.04,
      "general_scores": [],
      "math_scores": [
        78.62,
        51.74,
        53.6,
        10.0
      ],
      "code_scores": [
        0.61,
        55.25,
        3.23,
        4.38,
        4.52
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.38
              },
              {
                "metric": "lcb_test_output",
                "score": 4.52
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": 35.04,
        "code_avg": -2.08,
        "reasoning_avg": -41.32,
        "overall_avg": 9.43,
        "general_task_scores": [],
        "math_task_scores": [
          22.82,
          46.24,
          52.0,
          10.0
        ],
        "code_task_scores": [
          -19.51,
          0.78,
          -0.35,
          4.17,
          4.52
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Magpie-Align",
      "year": "2025",
      "size": "250k",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ"
    },
    {
      "id": 87,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 10.96,
      "code_avg": 14.81,
      "reasoning_avg": 0,
      "overall_avg": 12.89,
      "general_scores": [],
      "math_scores": [
        14.33,
        13.92,
        15.6,
        0.0
      ],
      "code_scores": [
        8.54,
        59.92,
        2.87,
        0.0,
        2.71
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 8.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 2.71
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -16.01,
        "math_avg": -2.49,
        "code_avg": -0.87,
        "reasoning_avg": -41.32,
        "overall_avg": -8.73,
        "general_task_scores": [],
        "math_task_scores": [
          -41.47,
          8.42,
          14.0,
          0.0
        ],
        "code_task_scores": [
          -11.58,
          5.45,
          -0.71,
          -0.21,
          2.71
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "KingNish",
      "year": "2024",
      "size": "19.9k",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k"
    },
    {
      "id": 88,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 9.51,
      "math_avg": 0.37,
      "code_avg": 10.77,
      "reasoning_avg": 8.31,
      "overall_avg": 7.24,
      "general_scores": [
        7.64,
        10.3225,
        19.25,
        0.0,
        7.85,
        11.29,
        19.47,
        0.0,
        8.34,
        10.71,
        19.24,
        0.0
      ],
      "math_scores": [
        0.0,
        0.0,
        0.0,
        1.24,
        0.0,
        0.0,
        0.26,
        0.0,
        2.96,
        0.0,
        0.0,
        0.0,
        0.0,
        1.15,
        0.0
      ],
      "code_scores": [
        0.0,
        57.2,
        0.0,
        0.0,
        0.0,
        0.0,
        57.98,
        0.0,
        0.0,
        0.0,
        0.0,
        46.3,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        21.36,
        10.62,
        0.1975,
        0.0,
        24.07,
        7.19,
        0.19771739,
        0.0,
        22.37,
        13.56,
        0.19847826,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.77
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 10.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.51,
        "math_avg": -13.07,
        "code_avg": -4.91,
        "reasoning_avg": -33.01,
        "overall_avg": -14.37,
        "general_task_scores": [
          -13.89,
          -9.57,
          1.35,
          -3.92
        ],
        "math_task_scores": [
          -55.8,
          -5.41,
          -1.6,
          -2.56,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -0.64,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -57.74,
          -52.1,
          -0.11,
          -22.08
        ]
      },
      "affiliation": "FreedomIntelligence",
      "year": "2024",
      "size": "19.7k",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT"
    }
  ],
  "qwen": [
    {
      "id": 0,
      "name": "Qwen/Qwen2.5-7B",
      "domain": "base",
      "general_avg": 43.31,
      "math_avg": 50.47,
      "code_avg": 38.72,
      "reasoning_avg": 34.93,
      "overall_avg": 41.86,
      "general_task_scores": [
        66.99,
        35.49,
        44.86,
        25.91
      ],
      "math_task_scores": [
        87.34,
        64.88,
        67.4,
        26.04,
        6.67
      ],
      "code_task_scores": [
        75.61,
        71.6,
        8.24,
        1.04,
        37.1
      ],
      "reasoning_task_scores": [
        36.6,
        69.46,
        0.392,
        33.28
      ]
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 51.61,
      "math_avg": 39.59,
      "code_avg": 48.03,
      "reasoning_avg": 32.15,
      "overall_avg": 42.85,
      "general_scores": [
        64.81,
        43.28,
        56.03,
        40.0042857,
        64.89,
        45.345,
        56.56,
        39.2021429,
        67.31,
        46.2225,
        56.54,
        39.1364286
      ],
      "math_scores": [
        80.44,
        49.06,
        50.6,
        19.04,
        3.33,
        80.89,
        48.22,
        47.0,
        18.9,
        0.0,
        78.47,
        49.12,
        48.8,
        20.03,
        0.0
      ],
      "code_scores": [
        76.22,
        73.93,
        12.9,
        41.54,
        41.63,
        75.61,
        74.71,
        12.54,
        42.17,
        30.09,
        75.61,
        73.93,
        12.54,
        41.34,
        35.75
      ],
      "reasoning_scores": [
        27.8,
        66.9,
        0.46956522,
        30.08,
        28.47,
        67.1,
        0.44793478,
        29.76,
        35.59,
        67.55,
        0.46315217,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.68
              },
              {
                "metric": "lcb_test_output",
                "score": 35.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.3,
        "math_avg": -10.87,
        "code_avg": 9.32,
        "reasoning_avg": -2.79,
        "overall_avg": 0.99,
        "general_task_scores": [
          -1.32,
          9.46,
          11.52,
          13.54
        ],
        "math_task_scores": [
          -7.41,
          -16.08,
          -18.6,
          -6.72,
          -5.56
        ],
        "code_task_scores": [
          0.2,
          2.59,
          4.42,
          40.64,
          -1.28
        ],
        "reasoning_task_scores": [
          -5.98,
          -2.28,
          0.07,
          -2.96
        ]
      },
      "affiliation": "nomic-ai",
      "year": "2023",
      "size": "809k",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.35,
      "math_avg": 37.16,
      "code_avg": 46.75,
      "reasoning_avg": 32.24,
      "overall_avg": 42.88,
      "general_scores": [
        65.44,
        54.5275,
        57.92,
        41.5621429,
        65.7,
        54.805,
        58.13,
        43.3885714,
        65.7,
        55.52,
        58.19,
        43.3107143
      ],
      "math_scores": [
        80.59,
        42.94,
        42.6,
        13.28,
        3.33,
        80.74,
        42.5,
        43.2,
        13.62,
        6.67,
        80.29,
        43.42,
        44.0,
        13.62,
        6.67
      ],
      "code_scores": [
        71.34,
        71.98,
        10.75,
        42.38,
        37.1,
        72.56,
        72.76,
        11.11,
        42.59,
        33.26,
        73.17,
        72.37,
        9.68,
        41.13,
        39.14
      ],
      "reasoning_scores": [
        29.49,
        68.37,
        0.38934783,
        28.88,
        29.49,
        68.65,
        0.39,
        29.84,
        32.88,
        68.45,
        0.39347826,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 36.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.04,
        "math_avg": -13.3,
        "code_avg": 8.04,
        "reasoning_avg": -2.69,
        "overall_avg": 1.02,
        "general_task_scores": [
          -1.38,
          19.46,
          13.22,
          16.84
        ],
        "math_task_scores": [
          -6.8,
          -21.93,
          -24.13,
          -12.53,
          -1.11
        ],
        "code_task_scores": [
          -3.25,
          0.77,
          2.27,
          40.99,
          -0.6
        ],
        "reasoning_task_scores": [
          -5.98,
          -0.97,
          -0.0,
          -3.81
        ]
      },
      "affiliation": "tatsu-lab",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 55.85,
      "math_avg": 44.42,
      "code_avg": 49.88,
      "reasoning_avg": 33.76,
      "overall_avg": 45.98,
      "general_scores": [
        72.8,
        51.095,
        58.83,
        42.3871429,
        73.14,
        50.86,
        58.98,
        39.0971429,
        72.02,
        51.565,
        58.89,
        40.5192857
      ],
      "math_scores": [
        87.95,
        53.36,
        55.2,
        21.3,
        6.67,
        87.19,
        53.06,
        53.2,
        21.48,
        6.67,
        86.88,
        52.8,
        52.4,
        21.48,
        6.67
      ],
      "code_scores": [
        76.22,
        72.76,
        12.54,
        37.16,
        46.61,
        76.83,
        75.1,
        11.47,
        39.04,
        47.51,
        77.44,
        73.15,
        13.26,
        41.54,
        47.51
      ],
      "reasoning_scores": [
        36.27,
        66.0,
        0.45934783,
        33.6,
        34.24,
        65.93,
        0.45108696,
        33.68,
        35.59,
        66.24,
        0.45119565,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.67
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.42
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.25
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.54,
        "math_avg": -6.05,
        "code_avg": 11.16,
        "reasoning_avg": -1.17,
        "overall_avg": 4.12,
        "general_task_scores": [
          5.66,
          15.68,
          14.04,
          14.76
        ],
        "math_task_scores": [
          0.0,
          -11.81,
          -13.8,
          -4.62,
          0.0
        ],
        "code_task_scores": [
          1.22,
          2.07,
          4.18,
          38.21,
          10.11
        ],
        "reasoning_task_scores": [
          -1.23,
          -3.4,
          0.06,
          -0.11
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 52.22,
      "math_avg": 39.73,
      "code_avg": 45.41,
      "reasoning_avg": 32.38,
      "overall_avg": 42.44,
      "general_scores": [
        71.97,
        45.8825,
        58.32,
        32.6907143,
        71.08,
        44.0925,
        58.16,
        34.7157143,
        74.04,
        43.6075,
        57.55,
        34.5392857
      ],
      "math_scores": [
        75.59,
        47.64,
        42.8,
        20.91,
        10.0,
        73.69,
        48.24,
        46.0,
        20.75,
        13.33,
        74.68,
        48.18,
        44.0,
        20.19,
        10.0
      ],
      "code_scores": [
        73.78,
        73.15,
        11.11,
        41.96,
        30.77,
        73.78,
        71.21,
        10.04,
        44.05,
        26.92,
        73.17,
        72.37,
        11.83,
        42.38,
        24.66
      ],
      "reasoning_scores": [
        33.9,
        65.45,
        0.4548913,
        27.84,
        34.58,
        65.32,
        0.44652174,
        26.16,
        38.98,
        65.98,
        0.44967391,
        28.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 27.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.58
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.91,
        "math_avg": -10.73,
        "code_avg": 6.69,
        "reasoning_avg": -2.56,
        "overall_avg": 0.58,
        "general_task_scores": [
          5.37,
          9.04,
          13.15,
          8.07
        ],
        "math_task_scores": [
          -12.69,
          -16.86,
          -23.13,
          -5.42,
          4.44
        ],
        "code_task_scores": [
          -2.03,
          0.64,
          2.75,
          41.76,
          -9.65
        ],
        "reasoning_task_scores": [
          -0.78,
          -3.88,
          0.06,
          -5.63
        ]
      },
      "affiliation": "databricks",
      "year": "2023",
      "size": "15k",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 55.11,
      "math_avg": 50.21,
      "code_avg": 50.24,
      "reasoning_avg": 34.93,
      "overall_avg": 47.62,
      "general_scores": [
        67.49,
        53.35,
        56.41,
        41.225,
        69.97,
        52.2825,
        56.43,
        41.0078571,
        69.02,
        55.37,
        56.57,
        42.1971429
      ],
      "math_scores": [
        84.53,
        47.66,
        48.4,
        18.54,
        84.31,
        47.94,
        51.2,
        19.11,
        83.32,
        47.98,
        51.0,
        18.56
      ],
      "code_scores": [
        78.66,
        73.54,
        10.75,
        43.01,
        43.21,
        78.66,
        73.15,
        12.19,
        42.59,
        47.29,
        76.22,
        73.93,
        11.47,
        42.38,
        46.61
      ],
      "reasoning_scores": [
        37.29,
        66.88,
        0.45163043,
        36.16,
        37.29,
        67.34,
        0.44543478,
        36.32,
        32.54,
        67.74,
        0.44858696,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.67
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.48
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.74
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.66
              },
              {
                "metric": "lcb_test_output",
                "score": 45.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.8,
        "math_avg": -0.25,
        "code_avg": 11.53,
        "reasoning_avg": -0.0,
        "overall_avg": 5.77,
        "general_task_scores": [
          1.84,
          18.18,
          11.61,
          15.57
        ],
        "math_task_scores": [
          -3.29,
          -17.02,
          -17.2,
          -7.3
        ],
        "code_task_scores": [
          2.24,
          1.94,
          3.23,
          41.62,
          8.6
        ],
        "reasoning_task_scores": [
          -0.89,
          -2.14,
          0.06,
          2.96
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 55.43,
      "math_avg": 39.17,
      "code_avg": 49.71,
      "reasoning_avg": 34.1,
      "overall_avg": 44.6,
      "general_scores": [
        71.25,
        52.5275,
        58.1,
        40.08,
        72.25,
        53.2075,
        58.15,
        39.6814286,
        71.33,
        51.6175,
        57.95,
        39.0557143
      ],
      "math_scores": [
        82.18,
        47.22,
        47.6,
        19.26,
        0.0,
        81.2,
        48.04,
        48.8,
        19.31,
        0.0,
        80.89,
        47.14,
        46.8,
        19.13,
        0.0
      ],
      "code_scores": [
        80.49,
        74.71,
        11.11,
        40.71,
        42.99,
        78.05,
        74.32,
        10.75,
        41.96,
        42.08,
        78.66,
        72.76,
        11.83,
        40.71,
        44.57
      ],
      "reasoning_scores": [
        32.2,
        67.14,
        0.41554348,
        35.36,
        35.25,
        66.9,
        0.41228261,
        36.4,
        32.2,
        67.14,
        0.41163043,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.61
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.13
              },
              {
                "metric": "lcb_test_output",
                "score": 43.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.12,
        "math_avg": -11.29,
        "code_avg": 11.0,
        "reasoning_avg": -0.83,
        "overall_avg": 2.75,
        "general_task_scores": [
          4.62,
          16.96,
          13.21,
          13.7
        ],
        "math_task_scores": [
          -5.92,
          -17.41,
          -19.67,
          -6.81,
          -6.67
        ],
        "code_task_scores": [
          3.46,
          2.33,
          2.99,
          40.09,
          6.11
        ],
        "reasoning_task_scores": [
          -3.38,
          -2.4,
          0.02,
          2.43
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 52.44,
      "math_avg": 41.23,
      "code_avg": 49.73,
      "reasoning_avg": 32.47,
      "overall_avg": 43.97,
      "general_scores": [
        77.31,
        43.3625,
        57.86,
        29.9021429,
        76.91,
        41.78,
        57.18,
        33.8178571,
        77.2,
        43.195,
        57.96,
        32.7728571
      ],
      "math_scores": [
        72.33,
        50.04,
        49.6,
        22.99,
        10.0,
        71.11,
        50.36,
        48.6,
        22.9,
        16.67,
        71.19,
        50.08,
        46.0,
        23.24,
        13.33
      ],
      "code_scores": [
        74.39,
        70.82,
        11.47,
        45.93,
        42.08,
        72.56,
        69.65,
        10.75,
        46.35,
        51.13,
        73.78,
        69.65,
        12.19,
        45.93,
        49.32
      ],
      "reasoning_scores": [
        34.24,
        67.42,
        0.40869565,
        28.24,
        34.92,
        67.46,
        0.39793478,
        26.32,
        32.88,
        66.94,
        0.40413043,
        30.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.07
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.13,
        "math_avg": -9.24,
        "code_avg": 11.02,
        "reasoning_avg": -2.46,
        "overall_avg": 2.11,
        "general_task_scores": [
          10.15,
          7.29,
          12.81,
          6.25
        ],
        "math_task_scores": [
          -15.8,
          -14.72,
          -19.33,
          -3.0,
          6.66
        ],
        "code_task_scores": [
          -2.03,
          -1.56,
          3.23,
          45.03,
          10.41
        ],
        "reasoning_task_scores": [
          -2.59,
          -2.19,
          0.01,
          -5.09
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "link": "https://huggingface.co/datasets/GAIR/lima"
    },
    {
      "id": 8,
      "name": "orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.36,
      "math_avg": 47.62,
      "code_avg": 48.01,
      "reasoning_avg": 50.09,
      "overall_avg": 51.02,
      "general_scores": [
        76.72,
        57.77,
        55.71,
        42.8821429,
        75.95,
        58.145,
        55.19,
        42.1885714,
        76.05,
        60.0675,
        55.23,
        44.3628571
      ],
      "math_scores": [
        86.88,
        59.16,
        59.4,
        21.27,
        10.0,
        87.57,
        59.84,
        60.2,
        21.18,
        13.33,
        85.67,
        60.6,
        61.2,
        21.36,
        6.67
      ],
      "code_scores": [
        75.0,
        71.98,
        12.54,
        39.67,
        40.95,
        75.0,
        74.32,
        12.9,
        34.86,
        40.95,
        72.56,
        74.32,
        14.34,
        40.71,
        40.05
      ],
      "reasoning_scores": [
        87.46,
        69.34,
        0.37869565,
        41.52,
        87.12,
        70.42,
        0.37706522,
        43.2,
        89.15,
        69.39,
        0.37978261,
        42.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 15.04,
        "math_avg": -2.84,
        "code_avg": 9.29,
        "reasoning_avg": 15.16,
        "overall_avg": 9.16,
        "general_task_scores": [
          9.25,
          23.17,
          10.52,
          17.23
        ],
        "math_task_scores": [
          -0.63,
          -5.01,
          -7.13,
          -4.77,
          3.33
        ],
        "code_task_scores": [
          -1.42,
          1.94,
          5.02,
          37.37,
          3.55
        ],
        "reasoning_task_scores": [
          51.31,
          0.26,
          -0.01,
          9.09
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 53.96,
      "math_avg": 35.28,
      "code_avg": 46.77,
      "reasoning_avg": 32.76,
      "overall_avg": 42.2,
      "general_scores": [
        70.36,
        56.3125,
        57.35,
        34.2271429,
        70.12,
        55.5125,
        57.8,
        35.0807143,
        70.74,
        54.6775,
        58.39,
        26.9914286
      ],
      "math_scores": [
        82.71,
        41.02,
        40.2,
        19.76,
        3.33,
        82.41,
        34.3,
        29.4,
        19.67,
        0.0,
        84.08,
        35.48,
        34.2,
        19.33,
        3.33
      ],
      "code_scores": [
        77.44,
        70.82,
        12.9,
        36.95,
        43.67,
        73.17,
        71.21,
        12.19,
        33.82,
        41.18,
        71.34,
        69.65,
        12.9,
        33.4,
        40.95
      ],
      "reasoning_scores": [
        29.49,
        67.31,
        0.41141304,
        32.8,
        33.22,
        67.85,
        0.41532609,
        27.52,
        37.63,
        66.4,
        0.4398913,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.72
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.65,
        "math_avg": -15.18,
        "code_avg": 8.05,
        "reasoning_avg": -2.17,
        "overall_avg": 0.34,
        "general_task_scores": [
          3.42,
          20.01,
          12.99,
          6.19
        ],
        "math_task_scores": [
          -4.27,
          -27.95,
          -32.8,
          -6.45,
          -4.45
        ],
        "code_task_scores": [
          -1.63,
          -1.04,
          4.42,
          33.68,
          4.83
        ],
        "reasoning_task_scores": [
          -3.15,
          -2.27,
          0.03,
          -3.28
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.52,
      "math_avg": 6.75,
      "code_avg": 14.57,
      "reasoning_avg": 27.38,
      "overall_avg": 22.05,
      "general_scores": [
        49.32,
        39.67,
        30.6535714,
        48.58,
        42.07,
        29.4264286,
        46.43,
        43.67,
        25.8307143
      ],
      "math_scores": [
        5.16,
        7.2,
        7.8,
        6.67,
        4.78,
        6.5,
        5.8,
        6.67,
        10.46,
        7.06,
        6.2,
        6.67
      ],
      "code_scores": [
        13.41,
        28.4,
        0.0,
        31.73,
        0.0,
        14.63,
        28.4,
        0.0,
        30.69,
        0.0,
        12.8,
        27.63,
        0.0,
        30.9,
        0.0
      ],
      "reasoning_scores": [
        32.54,
        54.05,
        0.33826087,
        20.48,
        33.56,
        54.92,
        0.33630435,
        19.28,
        37.63,
        54.57,
        0.33554348,
        20.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.8,
        "math_avg": -43.72,
        "code_avg": -24.15,
        "reasoning_avg": -7.56,
        "overall_avg": -19.8,
        "general_task_scores": [
          -18.88,
          -3.06,
          2.73
        ],
        "math_task_scores": [
          -80.54,
          -57.96,
          -60.8,
          0.0
        ],
        "code_task_scores": [
          -62.0,
          -43.46,
          -8.24,
          30.07,
          -37.1
        ],
        "reasoning_task_scores": [
          -2.02,
          -14.95,
          -0.05,
          -13.2
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "252k",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 62.84,
      "math_avg": 43.62,
      "code_avg": 49.26,
      "reasoning_avg": 46.3,
      "overall_avg": 50.51,
      "general_scores": [
        73.41,
        71.4675,
        58.56,
        50.8528571,
        66.87,
        72.1525,
        57.62,
        49.5364286,
        72.8,
        72.685,
        58.78,
        49.3721429
      ],
      "math_scores": [
        87.41,
        53.8,
        56.4,
        19.49,
        3.33,
        86.43,
        53.42,
        54.8,
        19.99,
        6.67,
        84.61,
        52.82,
        53.0,
        18.83,
        3.33
      ],
      "code_scores": [
        76.83,
        71.21,
        11.83,
        39.04,
        45.93,
        79.88,
        72.76,
        11.47,
        38.62,
        44.34,
        78.05,
        71.6,
        13.62,
        37.58,
        46.15
      ],
      "reasoning_scores": [
        82.71,
        62.09,
        0.44315217,
        40.88,
        83.05,
        61.85,
        0.44576087,
        39.6,
        81.02,
        61.82,
        0.43684783,
        41.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.15
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.86
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 45.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.26
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.53,
        "math_avg": -6.84,
        "code_avg": 10.54,
        "reasoning_avg": 11.36,
        "overall_avg": 8.65,
        "general_task_scores": [
          4.04,
          36.61,
          13.46,
          24.01
        ],
        "math_task_scores": [
          -1.19,
          -11.53,
          -12.67,
          -6.6,
          -2.23
        ],
        "code_task_scores": [
          2.64,
          0.26,
          4.07,
          37.37,
          8.37
        ],
        "reasoning_task_scores": [
          45.66,
          -7.54,
          0.05,
          7.28
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "939k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 35.76,
      "math_avg": 7.48,
      "code_avg": 18.52,
      "reasoning_avg": 24.47,
      "overall_avg": 21.56,
      "general_scores": [
        76.37,
        18.9625,
        48.83,
        0.00785714,
        75.92,
        17.9,
        48.41,
        0.00857143,
        76.1,
        19.6425,
        46.9,
        0.01571429
      ],
      "math_scores": [
        0.99,
        8.42,
        14.2,
        13.39,
        0.0,
        0.83,
        7.74,
        12.4,
        13.91,
        0.0,
        1.59,
        9.1,
        15.8,
        13.87,
        0.0
      ],
      "code_scores": [
        9.76,
        72.76,
        4.3,
        4.18,
        0.9,
        9.76,
        72.37,
        3.94,
        4.18,
        0.23,
        15.24,
        71.21,
        4.3,
        3.55,
        1.13
      ],
      "reasoning_scores": [
        42.37,
        51.89,
        0.29358696,
        4.08,
        51.68,
        0.29782609,
        8.56,
        40.34,
        55.03,
        0.31021739,
        8.4,
        40.0,
        55.03,
        0.31021739,
        8.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.59
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.11
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.97
              },
              {
                "metric": "lcb_test_output",
                "score": 0.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.41
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.56,
        "math_avg": -42.98,
        "code_avg": -20.2,
        "reasoning_avg": -10.47,
        "overall_avg": -20.3,
        "general_task_scores": [
          9.14,
          -16.66,
          3.19,
          -25.9
        ],
        "math_task_scores": [
          -86.2,
          -56.46,
          -53.27,
          -12.32,
          -6.67
        ],
        "code_task_scores": [
          -64.02,
          0.51,
          -4.06,
          2.93,
          -36.35
        ],
        "reasoning_task_scores": [
          4.3,
          -16.05,
          -0.09,
          -25.92
        ]
      },
      "affiliation": "CAS",
      "year": "2023",
      "size": "-",
      "link": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 46.58,
      "math_avg": 34.84,
      "code_avg": 48.43,
      "reasoning_avg": 35.19,
      "overall_avg": 41.26,
      "general_scores": [
        77.7,
        49.3625,
        58.75,
        0.0,
        77.8,
        49.5325,
        58.6,
        0.0,
        78.1,
        50.65,
        58.48,
        0.0
      ],
      "math_scores": [
        54.74,
        45.52,
        43.6,
        22.0,
        3.33,
        67.02,
        48.24,
        46.4,
        22.36,
        3.33,
        50.42,
        43.74,
        43.0,
        22.27,
        6.67
      ],
      "code_scores": [
        71.34,
        74.71,
        7.17,
        40.71,
        44.34,
        72.56,
        75.1,
        7.17,
        42.8,
        42.76,
        79.27,
        75.1,
        11.47,
        40.29,
        41.63
      ],
      "reasoning_scores": [
        50.17,
        69.87,
        0.42967391,
        25.76,
        48.81,
        69.82,
        0.43793478,
        26.32,
        33.9,
        69.0,
        0.44043478,
        27.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.87
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.61
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.6
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.27
              },
              {
                "metric": "lcb_test_output",
                "score": 42.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.27,
        "math_avg": -15.62,
        "code_avg": 9.71,
        "reasoning_avg": 0.26,
        "overall_avg": -0.6,
        "general_task_scores": [
          10.88,
          14.36,
          13.75,
          -25.91
        ],
        "math_task_scores": [
          -29.95,
          -19.05,
          -23.07,
          -3.83,
          -2.23
        ],
        "code_task_scores": [
          -1.22,
          3.37,
          0.36,
          40.23,
          5.81
        ],
        "reasoning_task_scores": [
          7.69,
          0.1,
          0.05,
          -6.8
        ]
      },
      "affiliation": "cognitivecomputations",
      "year": "2023",
      "size": "892k",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 47.12,
      "math_avg": 57.18,
      "code_avg": 51.64,
      "reasoning_avg": 35.05,
      "overall_avg": 47.75,
      "general_scores": [
        76.64,
        51.6575,
        60.23,
        0.0,
        75.95,
        51.7175,
        60.44,
        0.0,
        76.59,
        52.435,
        59.83,
        0.0
      ],
      "math_scores": [
        91.89,
        72.78,
        75.6,
        28.16,
        16.67,
        91.43,
        72.54,
        74.4,
        28.68,
        16.67,
        92.19,
        72.58,
        76.0,
        28.14,
        20.0
      ],
      "code_scores": [
        84.15,
        72.76,
        12.54,
        43.63,
        46.83,
        82.32,
        73.15,
        12.9,
        44.05,
        44.12,
        82.93,
        73.54,
        12.54,
        43.63,
        45.48
      ],
      "reasoning_scores": [
        33.56,
        70.64,
        0.44032609,
        35.36,
        36.27,
        70.41,
        0.43695652,
        35.28,
        31.19,
        71.03,
        0.43891304,
        35.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.13
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 45.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.39
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.81,
        "math_avg": 6.72,
        "code_avg": 12.92,
        "reasoning_avg": 0.12,
        "overall_avg": 5.89,
        "general_task_scores": [
          9.4,
          16.45,
          15.31,
          -25.91
        ],
        "math_task_scores": [
          4.5,
          7.75,
          7.93,
          2.29,
          11.11
        ],
        "code_task_scores": [
          7.52,
          1.55,
          4.42,
          42.73,
          8.38
        ],
        "reasoning_task_scores": [
          -2.93,
          1.23,
          0.05,
          2.11
        ]
      },
      "affiliation": "Mxode",
      "year": "2024",
      "size": "10k",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 48.13,
      "math_avg": 35.7,
      "code_avg": 46.76,
      "reasoning_avg": 33.94,
      "overall_avg": 41.13,
      "general_scores": [
        65.33,
        50.49,
        54.45,
        43.6957143,
        65.16,
        51.0625,
        54.89,
        0.0
      ],
      "math_scores": [
        75.36,
        44.28,
        44.0,
        17.39,
        3.33,
        69.75,
        42.62,
        40.6,
        16.31,
        3.33
      ],
      "code_scores": [
        67.07,
        70.43,
        11.83,
        41.96,
        42.53,
        69.51,
        71.21,
        10.04,
        41.13,
        41.86
      ],
      "reasoning_scores": [
        38.98,
        65.5,
        0.4026087,
        32.72,
        35.93,
        64.31,
        0.41565217,
        33.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.3
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.29
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.54
              },
              {
                "metric": "lcb_test_output",
                "score": 42.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.82,
        "math_avg": -14.77,
        "code_avg": 8.04,
        "reasoning_avg": -0.99,
        "overall_avg": -0.72,
        "general_task_scores": [
          -1.75,
          15.29,
          9.81,
          -4.06
        ],
        "math_task_scores": [
          -14.78,
          -21.43,
          -25.1,
          -9.19,
          -3.34
        ],
        "code_task_scores": [
          -7.32,
          -0.78,
          2.69,
          40.5,
          5.1
        ],
        "reasoning_task_scores": [
          0.86,
          -4.56,
          0.02,
          -0.28
        ]
      },
      "affiliation": "hakurei",
      "year": "2023",
      "size": "499k",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 38.41,
      "math_avg": 20.1,
      "code_avg": 38.52,
      "reasoning_avg": 28.88,
      "overall_avg": 31.48,
      "general_scores": [
        59.62,
        47.1375,
        42.17,
        0.57571429,
        68.01,
        47.0325,
        41.49,
        0.625,
        63.62,
        46.9625,
        42.76,
        0.91071429
      ],
      "math_scores": [
        37.68,
        26.06,
        25.6,
        10.93,
        0.0,
        37.6,
        25.94,
        25.0,
        10.95,
        0.0,
        36.54,
        25.68,
        25.6,
        10.61,
        3.33
      ],
      "code_scores": [
        57.93,
        66.54,
        7.17,
        36.74,
        28.96,
        59.15,
        64.98,
        7.17,
        29.65,
        31.45,
        56.71,
        65.37,
        5.73,
        31.94,
        28.28
      ],
      "reasoning_scores": [
        35.59,
        63.46,
        0.3225,
        17.04,
        35.59,
        62.6,
        0.31532609,
        16.56,
        37.29,
        62.2,
        0.32271739,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.75
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.04
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 29.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.16
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.9,
        "math_avg": -30.36,
        "code_avg": -0.2,
        "reasoning_avg": -6.05,
        "overall_avg": -10.38,
        "general_task_scores": [
          -3.24,
          11.55,
          -2.72,
          -25.21
        ],
        "math_task_scores": [
          -50.07,
          -38.99,
          -42.0,
          -15.21,
          -5.56
        ],
        "code_task_scores": [
          -17.68,
          -5.97,
          -1.55,
          31.74,
          -7.54
        ],
        "reasoning_task_scores": [
          -0.44,
          -6.71,
          -0.07,
          -16.99
        ]
      },
      "affiliation": "cyzhh",
      "year": "2024",
      "size": "135k",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS/viewer/default/train?row=0"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 50.0,
      "math_avg": 18.9,
      "code_avg": 42.18,
      "reasoning_avg": 31.75,
      "overall_avg": 35.71,
      "general_scores": [
        67.93,
        47.84,
        56.65,
        28.9,
        66.19,
        48.6625,
        56.56,
        27.7364286,
        68.39,
        46.19,
        56.65,
        28.2664286
      ],
      "math_scores": [
        1.29,
        29.72,
        28.0,
        18.72,
        2.12,
        29.06,
        25.6,
        18.25,
        2.27,
        28.78,
        25.6,
        17.39
      ],
      "code_scores": [
        69.51,
        71.6,
        11.83,
        29.23,
        29.41,
        70.12,
        70.43,
        10.75,
        26.51,
        24.66,
        71.95,
        72.37,
        11.83,
        26.51,
        35.97
      ],
      "reasoning_scores": [
        34.24,
        68.77,
        0.42119565,
        24.16,
        33.56,
        68.88,
        0.40652174,
        24.72,
        31.86,
        68.92,
        0.40217391,
        24.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.12
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.42
              },
              {
                "metric": "lcb_test_output",
                "score": 30.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.68,
        "math_avg": -31.57,
        "code_avg": 3.46,
        "reasoning_avg": -3.18,
        "overall_avg": -6.15,
        "general_task_scores": [
          0.51,
          12.07,
          11.76,
          2.39
        ],
        "math_task_scores": [
          -85.45,
          -35.69,
          -41.0,
          -7.92
        ],
        "code_task_scores": [
          -5.08,
          -0.13,
          3.23,
          26.38,
          -7.09
        ],
        "reasoning_task_scores": [
          -3.38,
          -0.6,
          0.02,
          -8.77
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "link": "https://huggingface.co/datasets/openai/gsm8k"
    },
    {
      "id": 18,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 46.06,
      "math_avg": 49.65,
      "code_avg": 46.82,
      "reasoning_avg": 29.74,
      "overall_avg": 43.07,
      "general_scores": [
        74.76,
        45.74,
        51.74,
        8.66142857,
        75.4,
        45.3225,
        50.96,
        13.9285714,
        74.32,
        46.115,
        50.35,
        15.465
      ],
      "math_scores": [
        90.9,
        64.28,
        64.2,
        22.56,
        6.67,
        91.21,
        64.56,
        64.6,
        22.47,
        6.67,
        90.67,
        63.28,
        63.4,
        22.54,
        6.67
      ],
      "code_scores": [
        68.9,
        73.15,
        14.7,
        34.66,
        40.05,
        71.34,
        73.54,
        13.26,
        30.06,
        43.21,
        73.17,
        72.37,
        13.62,
        37.79,
        42.53
      ],
      "reasoning_scores": [
        35.59,
        65.67,
        0.3223913,
        18.32,
        32.2,
        66.25,
        0.32119565,
        18.16,
        35.25,
        67.55,
        0.31902174,
        16.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.14
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.17
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.35
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.75,
        "math_avg": -0.82,
        "code_avg": 8.11,
        "reasoning_avg": -5.2,
        "overall_avg": 1.21,
        "general_task_scores": [
          7.84,
          10.24,
          6.16,
          -13.23
        ],
        "math_task_scores": [
          3.59,
          -0.84,
          -3.33,
          -3.52,
          0.0
        ],
        "code_task_scores": [
          -4.47,
          1.42,
          5.62,
          33.13,
          4.83
        ],
        "reasoning_task_scores": [
          -2.25,
          -2.97,
          -0.07,
          -15.49
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"
    },
    {
      "id": 19,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 45.01,
      "math_avg": 48.64,
      "code_avg": 39.16,
      "reasoning_avg": 36.0,
      "overall_avg": 42.2,
      "general_scores": [
        72.67,
        46.485,
        52.23,
        6.05928571,
        72.99,
        47.7125,
        52.46,
        8.03142857,
        73.75,
        47.54,
        53.36,
        6.83714286
      ],
      "math_scores": [
        85.75,
        61.88,
        61.8,
        26.22,
        10.0,
        84.61,
        62.6,
        63.6,
        25.86,
        13.33,
        84.46,
        62.1,
        60.6,
        26.78,
        0.0
      ],
      "code_scores": [
        56.71,
        68.09,
        10.04,
        20.04,
        40.27,
        54.27,
        68.48,
        5.73,
        23.38,
        39.37,
        55.49,
        67.7,
        4.66,
        30.9,
        42.31
      ],
      "reasoning_scores": [
        43.05,
        59.51,
        0.37402174,
        35.36,
        54.58,
        61.59,
        0.38934783,
        32.96,
        50.51,
        59.03,
        0.39336957,
        34.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.77
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.7,
        "math_avg": -1.83,
        "code_avg": 0.44,
        "reasoning_avg": 1.07,
        "overall_avg": 0.35,
        "general_task_scores": [
          6.15,
          11.76,
          7.82,
          -18.93
        ],
        "math_task_scores": [
          -2.4,
          -2.69,
          -5.4,
          0.25,
          1.11
        ],
        "code_task_scores": [
          -20.12,
          -3.51,
          -1.43,
          23.73,
          3.55
        ],
        "reasoning_task_scores": [
          12.78,
          -9.42,
          -0.0,
          0.91
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT"
    },
    {
      "id": 20,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 56.0,
      "math_avg": 45.72,
      "code_avg": 50.18,
      "reasoning_avg": 33.38,
      "overall_avg": 46.32,
      "general_scores": [
        78.27,
        52.5125,
        53.53,
        38.9935714,
        78.53,
        53.2775,
        54.33,
        38.6521429,
        78.27,
        53.16,
        54.19,
        38.235
      ],
      "math_scores": [
        68.61,
        47.22,
        48.0,
        18.95,
        68.54,
        46.5,
        49.2,
        19.63,
        67.85,
        46.92,
        48.2,
        19.08
      ],
      "code_scores": [
        83.54,
        72.76,
        14.34,
        38.41,
        44.12,
        83.54,
        73.54,
        13.62,
        40.29,
        41.63,
        79.88,
        72.76,
        13.62,
        38.41,
        42.31
      ],
      "reasoning_scores": [
        35.59,
        68.96,
        0.37423913,
        28.08,
        38.31,
        68.44,
        0.37815217,
        28.24,
        35.25,
        68.52,
        0.37880435,
        28.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 82.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.04
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.64
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.68,
        "math_avg": -4.74,
        "code_avg": 11.47,
        "reasoning_avg": -1.56,
        "overall_avg": 4.46,
        "general_task_scores": [
          11.37,
          17.49,
          9.16,
          12.72
        ],
        "math_task_scores": [
          -19.01,
          -18.0,
          -18.93,
          -6.82
        ],
        "code_task_scores": [
          6.71,
          1.42,
          5.62,
          38.0,
          5.59
        ],
        "reasoning_task_scores": [
          -0.22,
          -0.82,
          -0.01,
          -5.17
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR"
    },
    {
      "id": 21,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 54.02,
      "math_avg": 40.29,
      "code_avg": 47.14,
      "reasoning_avg": 32.6,
      "overall_avg": 43.51,
      "general_scores": [
        75.29,
        47.59,
        50.75,
        42.155,
        75.05,
        48.8025,
        50.76,
        41.6971429,
        74.84,
        47.615,
        52.0,
        41.6564286
      ],
      "math_scores": [
        83.78,
        49.72,
        50.4,
        19.44,
        0.0,
        82.41,
        49.06,
        48.8,
        18.2,
        3.33,
        83.78,
        48.9,
        47.6,
        18.9,
        0.0
      ],
      "code_scores": [
        73.17,
        71.98,
        9.32,
        44.05,
        38.46,
        73.78,
        69.65,
        9.68,
        42.38,
        35.75,
        78.66,
        68.48,
        10.04,
        40.29,
        41.4
      ],
      "reasoning_scores": [
        38.64,
        67.34,
        0.435,
        28.56,
        35.25,
        68.13,
        0.43043478,
        28.24,
        27.8,
        67.16,
        0.43597826,
        28.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.06
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.24
              },
              {
                "metric": "lcb_test_output",
                "score": 38.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.7,
        "math_avg": -10.18,
        "code_avg": 8.42,
        "reasoning_avg": -2.33,
        "overall_avg": 1.65,
        "general_task_scores": [
          8.07,
          12.51,
          6.31,
          15.93
        ],
        "math_task_scores": [
          -4.02,
          -15.65,
          -18.47,
          -7.19,
          -5.56
        ],
        "code_task_scores": [
          -0.41,
          -1.56,
          1.44,
          41.2,
          1.44
        ],
        "reasoning_task_scores": [
          -2.7,
          -1.92,
          0.04,
          -4.75
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA"
    },
    {
      "id": 22,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 55.72,
      "math_avg": 37.08,
      "code_avg": 49.12,
      "reasoning_avg": 32.49,
      "overall_avg": 43.6,
      "general_scores": [
        77.55,
        50.54,
        56.73,
        38.2635714,
        76.59,
        50.905,
        56.77,
        35.5721429,
        77.55,
        51.535,
        56.75,
        39.8385714
      ],
      "math_scores": [
        79.08,
        45.06,
        41.0,
        18.61,
        0.0,
        78.77,
        46.1,
        42.8,
        19.4,
        0.0,
        76.88,
        46.22,
        43.0,
        19.29,
        0.0
      ],
      "code_scores": [
        78.66,
        71.21,
        13.26,
        41.54,
        41.63,
        76.83,
        73.93,
        11.11,
        41.34,
        42.08,
        76.83,
        72.37,
        10.39,
        42.8,
        42.76
      ],
      "reasoning_scores": [
        33.22,
        67.91,
        0.47076087,
        28.96,
        32.54,
        67.21,
        0.44173913,
        31.28,
        29.49,
        68.22,
        0.42152174,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.59
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.89
              },
              {
                "metric": "lcb_test_output",
                "score": 42.16
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.4,
        "math_avg": -13.39,
        "code_avg": 10.4,
        "reasoning_avg": -2.45,
        "overall_avg": 1.74,
        "general_task_scores": [
          10.24,
          15.5,
          11.89,
          11.98
        ],
        "math_task_scores": [
          -9.1,
          -19.09,
          -25.13,
          -6.94,
          -6.67
        ],
        "code_task_scores": [
          1.83,
          0.9,
          3.35,
          40.85,
          5.06
        ],
        "reasoning_task_scores": [
          -4.85,
          -1.68,
          0.05,
          -3.31
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct"
    },
    {
      "id": 23,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 56.93,
      "math_avg": 39.59,
      "code_avg": 50.26,
      "reasoning_avg": 35.26,
      "overall_avg": 45.51,
      "general_scores": [
        76.89,
        53.39,
        58.6,
        46.8271429,
        76.06,
        52.1325,
        58.76,
        47.5414286,
        75.05,
        50.765,
        57.84,
        29.2514286
      ],
      "math_scores": [
        75.36,
        52.54,
        51.2,
        24.44,
        6.67,
        73.92,
        51.36,
        49.6,
        24.07,
        13.33,
        62.4,
        42.26,
        43.0,
        20.39,
        3.33
      ],
      "code_scores": [
        78.05,
        75.1,
        13.98,
        42.38,
        49.32,
        79.27,
        75.1,
        13.26,
        43.84,
        49.1,
        65.24,
        71.98,
        12.54,
        42.17,
        42.53
      ],
      "reasoning_scores": [
        39.66,
        68.32,
        0.41423913,
        39.44,
        33.9,
        68.11,
        0.40315217,
        38.08,
        26.44,
        66.65,
        0.39,
        41.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.06
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 46.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.61,
        "math_avg": -10.87,
        "code_avg": 11.54,
        "reasoning_avg": 0.32,
        "overall_avg": 3.65,
        "general_task_scores": [
          9.01,
          16.61,
          13.54,
          15.3
        ],
        "math_task_scores": [
          -16.78,
          -16.16,
          -19.47,
          -3.07,
          1.11
        ],
        "code_task_scores": [
          -1.42,
          2.46,
          5.02,
          41.76,
          9.88
        ],
        "reasoning_task_scores": [
          -3.27,
          -1.77,
          0.01,
          6.32
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 24,
      "name": "DART-Math",
      "domain": "math",
      "general_avg": 46.13,
      "math_avg": 46.04,
      "code_avg": 45.9,
      "reasoning_avg": 28.13,
      "overall_avg": 41.55,
      "general_scores": [
        81.6,
        44.02,
        57.16,
        1.0,
        81.91,
        43.745,
        57.85,
        0.99785714,
        82.12,
        45.09,
        57.07,
        1.03357143
      ],
      "math_scores": [
        89.23,
        58.68,
        60.4,
        20.33,
        0.0,
        90.14,
        57.58,
        58.2,
        20.28,
        3.33,
        90.22,
        57.44,
        60.6,
        20.8,
        3.33
      ],
      "code_scores": [
        67.07,
        68.09,
        14.7,
        41.34,
        41.4,
        67.68,
        67.7,
        15.05,
        39.46,
        38.69,
        66.46,
        67.32,
        13.98,
        39.46,
        40.05
      ],
      "reasoning_scores": [
        32.88,
        41.41,
        0.42532609,
        36.0,
        33.56,
        40.69,
        0.42467391,
        36.4,
        38.31,
        41.34,
        0.43315217,
        35.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.47
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.7
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.09
              },
              {
                "metric": "lcb_test_output",
                "score": 40.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.92
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.82,
        "math_avg": -4.43,
        "code_avg": 7.18,
        "reasoning_avg": -6.8,
        "overall_avg": -0.31,
        "general_task_scores": [
          14.89,
          8.79,
          12.5,
          -24.9
        ],
        "math_task_scores": [
          2.52,
          -6.98,
          -7.67,
          -5.57,
          -4.45
        ],
        "code_task_scores": [
          -8.54,
          -3.9,
          6.34,
          39.05,
          2.95
        ],
        "reasoning_task_scores": [
          -1.68,
          -28.31,
          0.04,
          2.75
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-uniform https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 25,
      "name": "MathQA",
      "domain": "math",
      "general_avg": 50.77,
      "math_avg": 29.53,
      "code_avg": 47.79,
      "reasoning_avg": 30.73,
      "overall_avg": 39.7,
      "general_scores": [
        69.48,
        50.355,
        51.55,
        28.4207143,
        67.08,
        52.5325,
        52.9,
        31.5907143,
        70.78,
        51.1575,
        53.12,
        30.265
      ],
      "math_scores": [
        48.37,
        37.32,
        30.6,
        18.93,
        13.33,
        48.52,
        37.74,
        31.2,
        20.01,
        10.0,
        49.51,
        38.28,
        33.4,
        19.06,
        6.67
      ],
      "code_scores": [
        67.07,
        69.26,
        11.83,
        44.26,
        45.25,
        73.17,
        69.65,
        10.04,
        42.59,
        38.91,
        73.78,
        68.09,
        12.19,
        43.63,
        47.06
      ],
      "reasoning_scores": [
        33.56,
        66.98,
        0.42391304,
        23.28,
        27.8,
        66.58,
        0.4251087,
        24.88,
        33.22,
        66.67,
        0.42032609,
        24.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.35
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 43.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.46,
        "math_avg": -20.94,
        "code_avg": 9.07,
        "reasoning_avg": -4.21,
        "overall_avg": -2.15,
        "general_task_scores": [
          2.12,
          15.86,
          7.66,
          4.18
        ],
        "math_task_scores": [
          -38.54,
          -27.1,
          -35.67,
          -6.71,
          3.33
        ],
        "code_task_scores": [
          -4.27,
          -2.6,
          3.11,
          42.45,
          6.64
        ],
        "reasoning_task_scores": [
          -5.07,
          -2.72,
          0.03,
          -9.07
        ]
      },
      "affiliation": "AllenAI",
      "year": "2019",
      "size": "29.8k",
      "link": "https://huggingface.co/datasets/allenai/math_qa"
    },
    {
      "id": 26,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 51.69,
      "math_avg": 34.4,
      "code_avg": 47.28,
      "reasoning_avg": 33.96,
      "overall_avg": 41.83,
      "general_scores": [
        77.61,
        46.9175,
        50.87,
        32.205,
        77.95,
        46.815,
        50.53,
        32.2257143,
        76.95,
        45.7325,
        50.67,
        31.8085714
      ],
      "math_scores": [
        59.21,
        45.0,
        43.8,
        18.68,
        6.67,
        62.85,
        45.44,
        45.8,
        18.97,
        0.0,
        60.96,
        45.54,
        44.8,
        18.34,
        0.0
      ],
      "code_scores": [
        75.61,
        71.6,
        12.19,
        43.84,
        29.64,
        75.0,
        73.15,
        11.83,
        43.42,
        35.29,
        75.61,
        70.43,
        12.19,
        44.05,
        35.29
      ],
      "reasoning_scores": [
        32.2,
        67.88,
        0.4225,
        32.56,
        68.62,
        0.42228261,
        33.52,
        33.56,
        68.98,
        0.41619565,
        33.68,
        35.25
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 33.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.38,
        "math_avg": -16.06,
        "code_avg": 8.56,
        "reasoning_avg": -0.97,
        "overall_avg": -0.02,
        "general_task_scores": [
          10.51,
          11.0,
          5.83,
          6.17
        ],
        "math_task_scores": [
          -26.33,
          -19.55,
          -22.6,
          -7.38,
          -4.45
        ],
        "code_task_scores": [
          -0.2,
          0.13,
          3.83,
          42.73,
          -3.69
        ],
        "reasoning_task_scores": [
          -2.93,
          -0.97,
          0.03,
          -0.03
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "link": "https://huggingface.co/datasets/camel-ai/math"
    },
    {
      "id": 27,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 38.26,
      "math_avg": 10.48,
      "code_avg": 25.05,
      "reasoning_avg": 30.07,
      "overall_avg": 25.97,
      "general_scores": [
        57.63,
        37.445,
        48.61,
        7.2,
        59.14,
        37.19,
        48.41,
        9.63714286,
        59.3,
        38.0825,
        49.5,
        6.98
      ],
      "math_scores": [
        14.1,
        11.3,
        6.8,
        19.29,
        0.0,
        13.12,
        11.62,
        7.8,
        19.38,
        0.0,
        14.25,
        12.12,
        7.4,
        20.01,
        0.0
      ],
      "code_scores": [
        2.44,
        72.76,
        8.6,
        6.47,
        38.69,
        4.27,
        71.6,
        6.45,
        7.52,
        31.22,
        6.71,
        71.6,
        8.24,
        4.8,
        34.39
      ],
      "reasoning_scores": [
        29.15,
        61.45,
        0.30858696,
        25.84,
        31.19,
        60.35,
        0.30521739,
        25.36,
        36.95,
        61.07,
        0.30413043,
        28.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.76
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.26
              },
              {
                "metric": "lcb_test_output",
                "score": 34.77
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.05,
        "math_avg": -39.99,
        "code_avg": -13.67,
        "reasoning_avg": -4.86,
        "overall_avg": -15.89,
        "general_task_scores": [
          -8.3,
          2.08,
          3.98,
          -17.97
        ],
        "math_task_scores": [
          -73.52,
          -53.2,
          -60.07,
          -6.48,
          -6.67
        ],
        "code_task_scores": [
          -71.14,
          0.39,
          -0.48,
          5.22,
          -2.33
        ],
        "reasoning_task_scores": [
          -4.17,
          -8.5,
          -0.08,
          -6.69
        ]
      },
      "affiliation": "SkunkworksAI",
      "year": "2025",
      "size": "29.9k",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 43.67,
      "math_avg": 41.51,
      "code_avg": 41.66,
      "reasoning_avg": 31.99,
      "overall_avg": 39.71,
      "general_scores": [
        75.73,
        47.58,
        48.91,
        0.10928571,
        75.64,
        48.9925,
        48.55,
        0.09428571,
        76.02,
        49.7,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.45,
        6.67,
        69.75,
        29.6,
        67.8,
        27.35,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15
      ],
      "reasoning_scores": [
        31.86,
        68.7,
        0.35880435,
        24.56,
        29.83,
        68.11,
        0.36271739,
        25.84,
        36.61,
        70.3,
        0.37706522,
        26.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.36,
        "math_avg": -8.96,
        "code_avg": 2.95,
        "reasoning_avg": -2.94,
        "overall_avg": -2.15,
        "general_task_scores": [
          8.81,
          13.27,
          4.85,
          -25.5
        ],
        "math_task_scores": [
          -15.14,
          -36.67,
          0.73,
          0.73,
          5.55
        ],
        "code_task_scores": [
          -32.52,
          0.12,
          4.78,
          34.8,
          7.55
        ],
        "reasoning_task_scores": [
          -3.83,
          -0.42,
          -0.02,
          -7.49
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math"
    },
    {
      "id": 29,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 48.62,
      "math_avg": 43.07,
      "code_avg": 44.39,
      "reasoning_avg": 29.68,
      "overall_avg": 41.44,
      "general_scores": [
        74.88,
        49.64,
        49.5,
        19.2128571,
        75.1,
        48.8,
        49.16,
        19.1157143,
        75.39,
        48.3725,
        48.62,
        25.6778571
      ],
      "math_scores": [
        86.96,
        37.56,
        35.0,
        27.46,
        13.33,
        86.66,
        36.68,
        36.6,
        86.13,
        36.7,
        36.6,
        26.94,
        13.33
      ],
      "code_scores": [
        80.49,
        73.93,
        12.9,
        6.47,
        47.06,
        81.1,
        73.93,
        12.19,
        7.93,
        47.29,
        82.93,
        73.93,
        12.19,
        7.1,
        46.38
      ],
      "reasoning_scores": [
        33.22,
        67.6,
        0.37391304,
        15.92,
        35.59,
        69.85,
        0.37108696,
        16.8,
        31.53,
        68.1,
        0.37076087,
        16.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 81.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.17
              },
              {
                "metric": "lcb_test_output",
                "score": 46.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.31,
        "math_avg": -7.39,
        "code_avg": 5.67,
        "reasoning_avg": -5.25,
        "overall_avg": -0.42,
        "general_task_scores": [
          8.13,
          13.45,
          4.23,
          -4.57
        ],
        "math_task_scores": [
          -0.76,
          -27.9,
          -31.33,
          1.16,
          6.66
        ],
        "code_task_scores": [
          5.9,
          2.33,
          4.19,
          6.13,
          9.81
        ],
        "reasoning_task_scores": [
          -3.15,
          -0.94,
          -0.02,
          -16.88
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "20k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra"
    },
    {
      "id": 30,
      "name": "tulu-3-sft-personas-math-grade",
      "domain": "math",
      "general_avg": 43.68,
      "math_avg": 41.51,
      "code_avg": 41.66,
      "reasoning_avg": 31.99,
      "overall_avg": 39.71,
      "general_scores": [
        75.73,
        47.6575,
        48.91,
        0.10928571,
        75.64,
        49.0225,
        48.55,
        0.09428571,
        76.02,
        49.775,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.49,
        6.67,
        69.75,
        29.6,
        67.8,
        27.39,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15
      ],
      "reasoning_scores": [
        31.86,
        68.7,
        0.35880435,
        24.56,
        25.84,
        29.83,
        68.11,
        0.36271739,
        26.96,
        36.61,
        70.3,
        0.37706522
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.37,
        "math_avg": -8.95,
        "code_avg": 2.95,
        "reasoning_avg": -2.94,
        "overall_avg": -2.14,
        "general_task_scores": [
          8.81,
          13.33,
          4.85,
          -25.5
        ],
        "math_task_scores": [
          -15.14,
          -36.67,
          0.73,
          0.76,
          5.55
        ],
        "code_task_scores": [
          -32.52,
          0.12,
          4.78,
          34.8,
          7.55
        ],
        "reasoning_task_scores": [
          -3.83,
          -0.42,
          -0.02,
          -7.49
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "50k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade"
    },
    {
      "id": 31,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 42.36,
      "math_avg": 54.56,
      "code_avg": 36.0,
      "reasoning_avg": 25.15,
      "overall_avg": 39.52,
      "general_scores": [
        87.67,
        35.41,
        46.59,
        0.00785714,
        87.29,
        36.31,
        45.59,
        0.0,
        87.32,
        37.18,
        44.91,
        0.0
      ],
      "math_scores": [
        88.1,
        72.64,
        74.0,
        26.78,
        13.33,
        88.17,
        72.22,
        74.0,
        27.12,
        10.0,
        88.1,
        72.2,
        73.8,
        27.87,
        10.0
      ],
      "code_scores": [
        64.63,
        73.54,
        9.68,
        20.88,
        13.57,
        61.59,
        72.76,
        11.47,
        20.46,
        11.76,
        64.02,
        73.54,
        9.32,
        18.79,
        14.03
      ],
      "reasoning_scores": [
        33.9,
        63.97,
        0.22065217,
        1.44,
        37.29,
        64.9,
        0.2201087,
        0.96,
        32.88,
        64.72,
        0.21913043,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.16
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.04
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.96,
        "math_avg": 4.09,
        "code_avg": -2.72,
        "reasoning_avg": -9.79,
        "overall_avg": -2.34,
        "general_task_scores": [
          20.44,
          0.81,
          0.84,
          -25.91
        ],
        "math_task_scores": [
          0.78,
          7.47,
          6.53,
          1.22,
          4.44
        ],
        "code_task_scores": [
          -12.2,
          1.68,
          1.92,
          19.0,
          -23.98
        ],
        "reasoning_task_scores": [
          -1.91,
          -4.93,
          -0.17,
          -32.13
        ]
      },
      "affiliation": "Soochow Univ",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math"
    },
    {
      "id": 32,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 46.5,
      "math_avg": 42.49,
      "code_avg": 37.4,
      "reasoning_avg": 30.01,
      "overall_avg": 39.1,
      "general_scores": [
        76.33,
        44.9875,
        53.35,
        13.125,
        76.14,
        42.615,
        54.28,
        12.385,
        76.17,
        43.595,
        53.24,
        11.7964286
      ],
      "math_scores": [
        67.48,
        62.72,
        57.4,
        21.3,
        0.0,
        71.65,
        61.1,
        58.0,
        22.52,
        3.33,
        66.49,
        62.54,
        58.2,
        21.34,
        3.33
      ],
      "code_scores": [
        61.59,
        71.98,
        10.75,
        36.33,
        11.09,
        31.71,
        73.54,
        8.96,
        41.96,
        26.92,
        56.1,
        72.76,
        12.19,
        37.37,
        7.69
      ],
      "reasoning_scores": [
        38.64,
        65.72,
        0.31782609,
        19.12,
        31.86,
        66.5,
        0.34163043,
        18.64,
        31.53,
        66.29,
        0.31782609,
        20.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 15.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.19,
        "math_avg": -7.97,
        "code_avg": -1.32,
        "reasoning_avg": -4.92,
        "overall_avg": -2.76,
        "general_task_scores": [
          9.22,
          8.24,
          8.76,
          -13.47
        ],
        "math_task_scores": [
          -18.8,
          -2.76,
          -9.53,
          -4.32,
          -4.45
        ],
        "code_task_scores": [
          -25.81,
          1.16,
          2.39,
          37.51,
          -21.87
        ],
        "reasoning_task_scores": [
          -2.59,
          -3.29,
          -0.06,
          -13.73
        ]
      },
      "affiliation": "rubenroy",
      "year": "2025",
      "size": "170k",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k/viewer/default/train?row=0"
    },
    {
      "id": 33,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 58.79,
      "math_avg": 42.67,
      "code_avg": 51.88,
      "reasoning_avg": 34.82,
      "overall_avg": 47.04,
      "general_scores": [
        76.57,
        52.4675,
        58.61,
        46.345,
        76.84,
        53.2375,
        58.49,
        47.52,
        76.92,
        52.4825,
        59.12,
        46.87
      ],
      "math_scores": [
        75.06,
        52.5,
        51.6,
        23.4,
        10.0,
        74.53,
        52.26,
        52.8,
        24.77,
        6.67,
        75.89,
        51.66,
        51.0,
        24.59,
        13.33
      ],
      "code_scores": [
        78.05,
        73.54,
        13.62,
        43.01,
        47.74,
        79.27,
        75.88,
        13.62,
        44.47,
        49.77,
        81.1,
        73.54,
        12.9,
        43.22,
        48.42
      ],
      "reasoning_scores": [
        33.56,
        68.52,
        0.41097826,
        39.12,
        30.85,
        67.72,
        0.40532608999999997,
        38.16,
        31.86,
        67.55,
        0.40902174,
        39.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.16
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.57
              },
              {
                "metric": "lcb_test_output",
                "score": 48.64
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.09
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 15.48,
        "math_avg": -7.8,
        "code_avg": 13.16,
        "reasoning_avg": -0.11,
        "overall_avg": 5.18,
        "general_task_scores": [
          9.79,
          17.24,
          13.88,
          21.0
        ],
        "math_task_scores": [
          -12.18,
          -12.74,
          -15.6,
          -1.79,
          3.33
        ],
        "code_task_scores": [
          3.86,
          2.72,
          5.14,
          42.53,
          11.54
        ],
        "reasoning_task_scores": [
          -4.51,
          -1.53,
          0.02,
          5.57
        ]
      },
      "affiliation": "microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 34,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 45.99,
      "math_avg": 46.67,
      "code_avg": 46.25,
      "reasoning_avg": 27.71,
      "overall_avg": 41.66,
      "general_scores": [
        82.12,
        43.0725,
        57.8,
        0.96571429,
        82.74,
        43.3675,
        57.64,
        0.96714286,
        82.12,
        42.605,
        57.62,
        0.81857143
      ],
      "math_scores": [
        89.23,
        58.26,
        61.4,
        20.51,
        6.67,
        90.45,
        57.96,
        61.2,
        20.17,
        0.0,
        89.39,
        57.68,
        60.0,
        20.44,
        6.67
      ],
      "code_scores": [
        62.8,
        69.26,
        14.34,
        41.34,
        43.44,
        65.85,
        69.26,
        14.7,
        41.13,
        42.76,
        65.24,
        67.32,
        13.98,
        40.08,
        42.31
      ],
      "reasoning_scores": [
        33.22,
        40.46,
        0.4423913,
        36.24,
        29.83,
        40.35,
        0.43956522,
        36.4,
        37.97,
        40.22,
        0.44532609,
        36.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.69
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.61
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.34
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.85
              },
              {
                "metric": "lcb_test_output",
                "score": 42.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.67,
        "math_avg": -3.8,
        "code_avg": 7.54,
        "reasoning_avg": -7.22,
        "overall_avg": -0.2,
        "general_task_scores": [
          15.34,
          7.52,
          12.83,
          -24.99
        ],
        "math_task_scores": [
          2.35,
          -6.91,
          -6.53,
          -5.67,
          -2.22
        ],
        "code_task_scores": [
          -10.98,
          -2.99,
          6.1,
          39.81,
          5.74
        ],
        "reasoning_task_scores": [
          -2.93,
          -29.12,
          0.05,
          3.12
        ]
      },
      "affiliation": "hkust-nlp",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 35,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 45.42,
      "math_avg": 5.54,
      "code_avg": 26.26,
      "reasoning_avg": 28.6,
      "overall_avg": 26.46,
      "general_scores": [
        86.43,
        48.0325,
        43.76,
        2.31071429,
        86.5,
        47.8875,
        45.15,
        1.92285714,
        86.65,
        48.885,
        45.65,
        1.87928571
      ],
      "math_scores": [
        0.08,
        2.12,
        1.0,
        19.67,
        6.67,
        0.0,
        1.9,
        0.6,
        18.99,
        6.67,
        0.0,
        2.2,
        0.6,
        19.24,
        3.33
      ],
      "code_scores": [
        3.66,
        72.76,
        13.98,
        21.09,
        21.04,
        3.66,
        73.15,
        12.9,
        20.04,
        22.62,
        0.61,
        73.15,
        11.83,
        21.29,
        22.17
      ],
      "reasoning_scores": [
        35.93,
        70.71,
        0.31369565,
        8.64,
        31.86,
        70.27,
        0.31293478,
        9.52,
        37.29,
        70.68,
        0.31934783,
        7.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.64
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.9
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.81
              },
              {
                "metric": "lcb_test_output",
                "score": 21.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.11,
        "math_avg": -44.93,
        "code_avg": -12.45,
        "reasoning_avg": -6.33,
        "overall_avg": -15.4,
        "general_task_scores": [
          19.54,
          12.78,
          -0.01,
          -23.87
        ],
        "math_task_scores": [
          -87.31,
          -62.81,
          -66.67,
          -6.74,
          -1.11
        ],
        "code_task_scores": [
          -72.97,
          1.42,
          4.66,
          19.77,
          -15.16
        ],
        "reasoning_task_scores": [
          -1.57,
          1.09,
          -0.07,
          -24.77
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "23.6k",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1"
    },
    {
      "id": 36,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 54.77,
      "math_avg": 41.88,
      "code_avg": 45.31,
      "reasoning_avg": 33.7,
      "overall_avg": 43.91,
      "general_scores": [
        69.51,
        49.795,
        55.2,
        44.2921429,
        70.48,
        50.0625,
        56.78,
        43.7028571,
        69.22,
        48.6925,
        55.65,
        43.8715385
      ],
      "math_scores": [
        80.21,
        49.88,
        46.8,
        21.66,
        13.33,
        76.12,
        48.8,
        48.0,
        22.06,
        10.0,
        77.79,
        49.48,
        46.4,
        21.0,
        16.67
      ],
      "code_scores": [
        78.66,
        68.48,
        12.19,
        12.53,
        42.31,
        78.66,
        67.7,
        13.26,
        32.15,
        41.86,
        79.27,
        69.26,
        12.19,
        29.23,
        41.86
      ],
      "reasoning_scores": [
        37.97,
        66.36,
        0.39804348,
        34.32,
        33.22,
        65.98,
        0.37967391,
        34.4,
        30.17,
        66.31,
        0.37902174,
        34.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.64
              },
              {
                "metric": "lcb_test_output",
                "score": 42.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.46,
        "math_avg": -8.59,
        "code_avg": 6.59,
        "reasoning_avg": -1.24,
        "overall_avg": 2.06,
        "general_task_scores": [
          2.75,
          14.03,
          11.02,
          18.05
        ],
        "math_task_scores": [
          -9.3,
          -15.49,
          -20.33,
          -4.47,
          6.66
        ],
        "code_task_scores": [
          3.25,
          -3.12,
          4.31,
          23.6,
          4.91
        ],
        "reasoning_task_scores": [
          -2.81,
          -3.24,
          -0.0,
          1.12
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "970k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College"
    },
    {
      "id": 37,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 47.17,
      "math_avg": 35.3,
      "code_avg": 30.64,
      "reasoning_avg": 32.41,
      "overall_avg": 36.38,
      "general_scores": [
        74.96,
        46.36,
        56.09,
        10.9892857,
        75.63,
        46.2175,
        56.19,
        10.9892857,
        75.39,
        44.9275,
        57.01,
        11.3207143
      ],
      "math_scores": [
        76.27,
        37.56,
        43.2,
        17.73,
        0.0,
        75.74,
        38.08,
        42.4,
        16.76,
        3.33,
        77.48,
        38.06,
        41.8,
        17.71,
        3.33
      ],
      "code_scores": [
        1.83,
        66.54,
        9.32,
        37.16,
        35.75,
        3.66,
        67.7,
        11.11,
        36.53,
        34.16,
        4.27,
        66.54,
        10.75,
        36.95,
        37.33
      ],
      "reasoning_scores": [
        33.56,
        60.83,
        0.48076087,
        35.44,
        34.24,
        60.9,
        0.4851087,
        34.08,
        30.51,
        62.62,
        0.48771739,
        35.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.43
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.88
              },
              {
                "metric": "lcb_test_output",
                "score": 35.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.45
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.48
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.86,
        "math_avg": -15.17,
        "code_avg": -8.08,
        "reasoning_avg": -2.52,
        "overall_avg": -5.48,
        "general_task_scores": [
          8.34,
          10.35,
          11.57,
          -14.81
        ],
        "math_task_scores": [
          -10.84,
          -26.98,
          -24.93,
          -8.64,
          -4.45
        ],
        "code_task_scores": [
          -72.36,
          -4.67,
          2.15,
          35.84,
          -1.35
        ],
        "reasoning_task_scores": [
          -3.83,
          -8.01,
          0.09,
          1.65
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "98k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School"
    },
    {
      "id": 38,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 60.32,
      "math_avg": 54.37,
      "code_avg": 45.21,
      "reasoning_avg": 34.66,
      "overall_avg": 48.64,
      "general_scores": [
        80.37,
        51.715,
        60.66,
        47.0,
        80.14,
        53.0675,
        60.32,
        48.0571429,
        80.43,
        53.955,
        60.41,
        47.665
      ],
      "math_scores": [
        86.66,
        69.92,
        73.8,
        26.74,
        20.0,
        87.87,
        70.18,
        72.6,
        27.01,
        13.33,
        88.32,
        70.26,
        72.4,
        26.51,
        10.0
      ],
      "code_scores": [
        53.05,
        73.15,
        14.34,
        42.38,
        42.99,
        51.22,
        73.54,
        15.05,
        41.75,
        44.12,
        54.27,
        72.76,
        13.26,
        42.8,
        43.44
      ],
      "reasoning_scores": [
        34.92,
        69.49,
        0.42576087,
        34.08,
        32.88,
        69.24,
        0.41304348,
        34.48,
        34.92,
        69.3,
        0.41934783,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.91
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 43.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.0,
        "math_avg": 3.91,
        "code_avg": 6.49,
        "reasoning_avg": -0.27,
        "overall_avg": 6.78,
        "general_task_scores": [
          13.32,
          17.42,
          15.6,
          21.66
        ],
        "math_task_scores": [
          0.28,
          5.24,
          5.53,
          0.71,
          7.77
        ],
        "code_task_scores": [
          -22.76,
          1.55,
          5.98,
          41.27,
          6.42
        ],
        "reasoning_task_scores": [
          -2.36,
          -0.12,
          0.03,
          1.36
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT"
    },
    {
      "id": 39,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 55.02,
      "math_avg": 40.7,
      "code_avg": 48.32,
      "reasoning_avg": 32.65,
      "overall_avg": 44.17,
      "general_scores": [
        75.41,
        49.68,
        55.01,
        40.2392857,
        74.43,
        49.875,
        53.68,
        41.8692857
      ],
      "math_scores": [
        82.79,
        49.68,
        48.6,
        18.41,
        3.33,
        85.14,
        49.94,
        51.0,
        18.13,
        0.0
      ],
      "code_scores": [
        72.56,
        68.87,
        12.9,
        42.59,
        41.63,
        73.17,
        74.71,
        12.19,
        40.92,
        43.67
      ],
      "reasoning_scores": [
        29.15,
        65.8,
        0.42076087,
        36.64,
        29.49,
        66.39,
        0.41130435,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.05
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.96
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.1
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.71,
        "math_avg": -9.76,
        "code_avg": 9.6,
        "reasoning_avg": -2.29,
        "overall_avg": 2.32,
        "general_task_scores": [
          7.93,
          14.29,
          9.48,
          15.14
        ],
        "math_task_scores": [
          -3.38,
          -15.07,
          -17.6,
          -7.77,
          -5.01
        ],
        "code_task_scores": [
          -2.74,
          0.19,
          4.3,
          40.72,
          5.55
        ],
        "reasoning_task_scores": [
          -7.28,
          -3.36,
          0.03,
          1.48
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus"
    },
    {
      "id": 40,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 61.04,
      "math_avg": 55.74,
      "code_avg": 51.95,
      "reasoning_avg": 35.07,
      "overall_avg": 50.95,
      "general_scores": [
        79.48,
        51.7625,
        61.1,
        51.39,
        80.36,
        52.635,
        60.93,
        51.0807143,
        79.89,
        52.88,
        60.9,
        50.1085714
      ],
      "math_scores": [
        91.43,
        72.5,
        75.0,
        28.27,
        13.33,
        90.9,
        72.06,
        76.6,
        27.46,
        6.67,
        91.13,
        72.02,
        77.2,
        28.18,
        13.33
      ],
      "code_scores": [
        82.32,
        75.1,
        13.98,
        41.34,
        45.93,
        80.49,
        75.1,
        14.34,
        42.38,
        47.06,
        79.27,
        75.1,
        15.05,
        43.22,
        48.64
      ],
      "reasoning_scores": [
        35.93,
        69.07,
        0.42130435,
        37.28,
        32.54,
        69.06,
        0.4251087,
        37.6,
        30.51,
        68.98,
        0.42630435,
        38.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.15
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 75.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.73,
        "math_avg": 5.27,
        "code_avg": 13.24,
        "reasoning_avg": 0.14,
        "overall_avg": 9.1,
        "general_task_scores": [
          12.92,
          16.94,
          16.12,
          24.95
        ],
        "math_task_scores": [
          3.81,
          7.31,
          8.87,
          1.93,
          4.44
        ],
        "code_task_scores": [
          5.08,
          3.5,
          6.22,
          41.27,
          10.11
        ],
        "reasoning_task_scores": [
          -3.61,
          -0.42,
          0.03,
          4.56
        ]
      },
      "affiliation": "PawanKrd",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k"
    },
    {
      "id": 41,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 42.3,
      "math_avg": 50.32,
      "code_avg": 36.63,
      "reasoning_avg": 32.86,
      "overall_avg": 40.53,
      "general_scores": [
        67.68,
        48.38,
        52.38,
        0.11928571,
        70.7,
        46.18,
        51.79,
        0.28214286,
        69.57,
        47.225,
        52.68,
        0.64428571
      ],
      "math_scores": [
        88.25,
        65.72,
        68.8,
        24.16,
        10.0,
        87.04,
        66.78,
        69.2,
        23.26,
        6.67,
        88.17,
        65.38,
        66.2,
        25.23,
        0.0
      ],
      "code_scores": [
        39.63,
        66.54,
        7.89,
        40.92,
        36.65,
        42.07,
        67.32,
        11.83,
        34.86,
        23.98,
        26.83,
        69.26,
        9.32,
        38.0,
        34.39
      ],
      "reasoning_scores": [
        28.14,
        61.3,
        0.38054348,
        26.48,
        41.69,
        62.08,
        0.37902174,
        29.84,
        51.19,
        59.93,
        0.38913043,
        32.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.93
              },
              {
                "metric": "lcb_test_output",
                "score": 31.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.01,
        "math_avg": -0.14,
        "code_avg": -2.09,
        "reasoning_avg": -2.08,
        "overall_avg": -1.33,
        "general_task_scores": [
          2.33,
          11.77,
          7.42,
          -25.56
        ],
        "math_task_scores": [
          0.48,
          1.08,
          0.67,
          -1.82,
          -1.11
        ],
        "code_task_scores": [
          -39.43,
          -3.89,
          1.44,
          36.89,
          -5.43
        ],
        "reasoning_task_scores": [
          3.74,
          -8.36,
          -0.01,
          -3.68
        ]
      },
      "affiliation": "EricLu",
      "year": "2025",
      "size": "274k",
      "link": "https://huggingface.co/datasets/EricLu/SCP-116K"
    },
    {
      "id": 42,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 46.95,
      "math_avg": 55.59,
      "code_avg": 51.22,
      "reasoning_avg": 35.08,
      "overall_avg": 47.21,
      "general_scores": [
        76.64,
        52.4775,
        58.68,
        0.0,
        76.14,
        52.055,
        58.89,
        0.0,
        77.24,
        52.395,
        58.91,
        0.0
      ],
      "math_scores": [
        84.0,
        64.02,
        64.2,
        13.33,
        77.79,
        64.62,
        65.8,
        10.0,
        81.58,
        64.52,
        67.2,
        10.0
      ],
      "code_scores": [
        76.22,
        72.37,
        14.7,
        43.01,
        48.87,
        78.66,
        71.98,
        12.19,
        44.89,
        47.51,
        76.83,
        73.93,
        13.98,
        44.05,
        49.1
      ],
      "reasoning_scores": [
        32.2,
        70.05,
        0.4301087,
        36.8,
        33.22,
        69.93,
        0.42826087,
        36.48,
        34.92,
        69.87,
        0.43923913,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.83
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.98
              },
              {
                "metric": "lcb_test_output",
                "score": 48.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.95
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.64,
        "math_avg": 5.12,
        "code_avg": 12.5,
        "reasoning_avg": 0.15,
        "overall_avg": 5.35,
        "general_task_scores": [
          9.68,
          16.82,
          13.97,
          -25.91
        ],
        "math_task_scores": [
          -6.22,
          -0.49,
          -1.67,
          4.44
        ],
        "code_task_scores": [
          1.63,
          1.16,
          5.38,
          42.94,
          11.39
        ],
        "reasoning_task_scores": [
          -3.15,
          0.49,
          0.04,
          3.23
        ]
      },
      "affiliation": "AI-MO",
      "year": "2025",
      "size": "896k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5"
    },
    {
      "id": 43,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 54.38,
      "math_avg": 40.05,
      "code_avg": 48.66,
      "reasoning_avg": 34.83,
      "overall_avg": 44.48,
      "general_scores": [
        73.09,
        53.1125,
        58.37,
        31.3078571,
        73.29,
        53.95,
        58.2,
        33.72
      ],
      "math_scores": [
        79.98,
        47.88,
        48.0,
        18.88,
        3.33,
        82.11,
        46.62,
        47.4,
        19.67,
        6.67
      ],
      "code_scores": [
        77.44,
        72.76,
        11.83,
        38.83,
        42.08,
        77.44,
        73.54,
        10.75,
        39.87,
        42.08
      ],
      "reasoning_scores": [
        34.58,
        69.31,
        0.43934783,
        34.96,
        36.95,
        68.6,
        0.44413043,
        33.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.51
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.7
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.35
              },
              {
                "metric": "lcb_test_output",
                "score": 42.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.76
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.07,
        "math_avg": -10.41,
        "code_avg": 9.94,
        "reasoning_avg": -0.1,
        "overall_avg": 2.62,
        "general_task_scores": [
          6.2,
          18.04,
          13.42,
          6.6
        ],
        "math_task_scores": [
          -6.3,
          -17.63,
          -19.7,
          -6.77,
          -1.67
        ],
        "code_task_scores": [
          1.83,
          1.55,
          3.05,
          38.31,
          4.98
        ],
        "reasoning_task_scores": [
          -0.84,
          -0.5,
          0.05,
          0.88
        ]
      },
      "affiliation": "Locutusque",
      "year": "2024",
      "size": "463k",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true"
    },
    {
      "id": 44,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 53.46,
      "math_avg": 41.78,
      "code_avg": 48.27,
      "reasoning_avg": 31.99,
      "overall_avg": 43.88,
      "general_scores": [
        70.76,
        52.305,
        56.6,
        36.5364286,
        69.01,
        53.4925,
        56.83,
        33.3778571,
        67.16,
        53.3425,
        57.03,
        35.135
      ],
      "math_scores": [
        81.05,
        50.6,
        52.2,
        20.48,
        6.67,
        79.98,
        49.42,
        50.0,
        18.61,
        13.33,
        77.63,
        49.1,
        50.0,
        17.62,
        10.0
      ],
      "code_scores": [
        77.44,
        71.98,
        8.96,
        44.05,
        44.8,
        73.17,
        70.82,
        6.45,
        42.59,
        43.67,
        75.61,
        71.6,
        3.94,
        44.68,
        44.34
      ],
      "reasoning_scores": [
        34.24,
        67.9,
        0.38032609,
        29.44,
        27.8,
        68.84,
        0.38195652,
        29.52,
        26.78,
        68.41,
        0.38130435,
        29.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 44.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.15,
        "math_avg": -8.69,
        "code_avg": 9.56,
        "reasoning_avg": -2.94,
        "overall_avg": 2.02,
        "general_task_scores": [
          1.99,
          17.56,
          11.96,
          9.11
        ],
        "math_task_scores": [
          -7.79,
          -15.17,
          -16.67,
          -7.14,
          3.33
        ],
        "code_task_scores": [
          -0.2,
          -0.13,
          -1.79,
          42.73,
          7.17
        ],
        "reasoning_task_scores": [
          -6.99,
          -1.08,
          -0.01,
          -3.68
        ]
      },
      "affiliation": "glaiveAI",
      "year": "2023",
      "size": "20k",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"
    },
    {
      "id": 45,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 57.83,
      "math_avg": 46.32,
      "code_avg": 45.9,
      "reasoning_avg": 34.08,
      "overall_avg": 46.03,
      "general_scores": [
        73.95,
        54.0475,
        57.61,
        45.75,
        73.28,
        55.2325,
        58.17,
        45.3114286,
        72.35,
        55.42,
        58.46,
        44.4035714
      ],
      "math_scores": [
        86.81,
        49.72,
        51.2,
        21.12,
        26.67,
        86.28,
        49.78,
        51.6,
        21.43,
        20.0,
        84.69,
        48.5,
        52.2,
        21.43,
        23.33
      ],
      "code_scores": [
        78.66,
        75.88,
        13.26,
        40.92,
        16.52,
        79.27,
        72.76,
        13.98,
        41.13,
        17.65,
        82.32,
        75.49,
        14.34,
        38.0,
        28.28
      ],
      "reasoning_scores": [
        33.56,
        67.17,
        0.44717391,
        35.6,
        33.22,
        67.49,
        0.43336957,
        35.76,
        32.54,
        68.17,
        0.43282609,
        34.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.02
              },
              {
                "metric": "lcb_test_output",
                "score": 20.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.52,
        "math_avg": -4.15,
        "code_avg": 7.18,
        "reasoning_avg": -0.86,
        "overall_avg": 4.17,
        "general_task_scores": [
          6.2,
          19.41,
          13.22,
          19.25
        ],
        "math_task_scores": [
          -1.41,
          -15.55,
          -15.73,
          -4.71,
          16.66
        ],
        "code_task_scores": [
          4.47,
          3.11,
          5.62,
          38.98,
          -16.28
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.85,
          0.05,
          1.87
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2"
    },
    {
      "id": 46,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 55.87,
      "math_avg": 42.32,
      "code_avg": 49.43,
      "reasoning_avg": 32.65,
      "overall_avg": 45.07,
      "general_scores": [
        71.55,
        51.1425,
        57.7,
        42.3457143,
        72.22,
        50.4425,
        58.0,
        43.0657143,
        72.94,
        50.2575,
        58.04,
        42.7221429
      ],
      "math_scores": [
        81.96,
        49.72,
        52.6,
        20.57,
        10.0,
        79.76,
        49.92,
        50.8,
        20.37,
        6.67,
        82.11,
        48.64,
        51.4,
        20.23,
        10.0
      ],
      "code_scores": [
        78.66,
        74.32,
        13.62,
        39.04,
        43.67,
        79.27,
        70.04,
        14.7,
        38.41,
        44.34,
        80.49,
        70.82,
        13.98,
        38.2,
        41.86
      ],
      "reasoning_scores": [
        28.47,
        67.62,
        0.41380435,
        33.44,
        28.14,
        67.7,
        0.41195652,
        32.8,
        29.83,
        68.4,
        0.42804348,
        34.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.56,
        "math_avg": -8.15,
        "code_avg": 10.71,
        "reasoning_avg": -2.28,
        "overall_avg": 3.21,
        "general_task_scores": [
          5.25,
          15.12,
          13.05,
          16.8
        ],
        "math_task_scores": [
          -6.06,
          -15.45,
          -15.8,
          -5.65,
          2.22
        ],
        "code_task_scores": [
          3.86,
          0.13,
          5.86,
          37.51,
          6.19
        ],
        "reasoning_task_scores": [
          -7.79,
          -1.55,
          0.03,
          0.19
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "157k",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction"
    },
    {
      "id": 47,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 55.97,
      "math_avg": 42.77,
      "code_avg": 48.81,
      "reasoning_avg": 33.63,
      "overall_avg": 45.29,
      "general_scores": [
        73.46,
        50.4625,
        58.39,
        40.425,
        72.79,
        51.2425,
        58.7,
        42.015,
        73.71,
        52.2175,
        58.47,
        39.75
      ],
      "math_scores": [
        85.97,
        48.64,
        51.8,
        21.03,
        6.67,
        85.75,
        50.6,
        52.6,
        20.05,
        3.33,
        85.22,
        49.72,
        53.0,
        20.53,
        6.67
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        37.58,
        42.76,
        78.66,
        71.6,
        12.19,
        36.95,
        43.21,
        78.05,
        73.15,
        11.47,
        38.41,
        43.44
      ],
      "reasoning_scores": [
        32.2,
        67.97,
        0.43880435,
        33.6,
        30.17,
        68.39,
        0.43891304,
        34.64,
        31.86,
        68.34,
        0.4401087,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.66,
        "math_avg": -7.69,
        "code_avg": 10.09,
        "reasoning_avg": -1.31,
        "overall_avg": 3.44,
        "general_task_scores": [
          6.33,
          15.82,
          13.66,
          14.82
        ],
        "math_task_scores": [
          -1.69,
          -15.23,
          -14.93,
          -5.5,
          -1.11
        ],
        "code_task_scores": [
          3.46,
          0.77,
          3.59,
          36.61,
          6.04
        ],
        "reasoning_task_scores": [
          -5.19,
          -1.23,
          0.05,
          1.15
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "111k",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1"
    },
    {
      "id": 48,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 56.18,
      "math_avg": 43.35,
      "code_avg": 48.67,
      "reasoning_avg": 33.49,
      "overall_avg": 45.42,
      "general_scores": [
        72.15,
        52.31,
        59.02,
        40.0664286,
        73.45,
        50.7375,
        58.68,
        41.9592857,
        73.67,
        52.3975,
        58.3,
        41.36
      ],
      "math_scores": [
        84.69,
        50.9,
        53.4,
        20.71,
        10.0,
        85.14,
        49.32,
        50.6,
        19.87,
        6.67,
        85.22,
        49.98,
        52.6,
        21.14,
        10.0
      ],
      "code_scores": [
        78.05,
        73.54,
        11.47,
        38.41,
        45.48,
        79.88,
        71.21,
        12.19,
        35.49,
        43.21,
        78.66,
        72.37,
        10.75,
        36.53,
        42.76
      ],
      "reasoning_scores": [
        33.56,
        67.98,
        0.44086957,
        34.24,
        27.8,
        67.78,
        0.44271739,
        34.48,
        30.17,
        67.71,
        0.42641304,
        36.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.81
              },
              {
                "metric": "lcb_test_output",
                "score": 43.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.51
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.86,
        "math_avg": -7.12,
        "code_avg": 9.95,
        "reasoning_avg": -1.44,
        "overall_avg": 3.56,
        "general_task_scores": [
          6.1,
          16.33,
          13.81,
          15.22
        ],
        "math_task_scores": [
          -2.32,
          -14.81,
          -15.2,
          -5.47,
          2.22
        ],
        "code_task_scores": [
          3.25,
          0.77,
          3.23,
          35.77,
          6.72
        ],
        "reasoning_task_scores": [
          -6.09,
          -1.64,
          0.05,
          1.92
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "110k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K"
    },
    {
      "id": 49,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 57.03,
      "math_avg": 48.88,
      "code_avg": 48.19,
      "reasoning_avg": 33.52,
      "overall_avg": 46.91,
      "general_scores": [
        74.18,
        50.5175,
        58.27,
        44.5807143,
        75.17,
        49.6875,
        58.73,
        45.1207143
      ],
      "math_scores": [
        77.86,
        61.66,
        61.2,
        26.31,
        16.67,
        81.27,
        61.66,
        60.0,
        25.5,
        16.67
      ],
      "code_scores": [
        79.27,
        72.37,
        11.83,
        42.8,
        35.29,
        79.88,
        71.6,
        12.54,
        42.38,
        33.94
      ],
      "reasoning_scores": [
        35.25,
        69.52,
        0.41336957,
        29.12,
        34.24,
        70.48,
        0.40923913,
        28.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.57
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.59
              },
              {
                "metric": "lcb_test_output",
                "score": 34.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.92
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.72,
        "math_avg": -1.59,
        "code_avg": 9.47,
        "reasoning_avg": -1.41,
        "overall_avg": 5.05,
        "general_task_scores": [
          7.69,
          14.61,
          13.64,
          18.94
        ],
        "math_task_scores": [
          -7.78,
          -3.22,
          -6.8,
          -0.14,
          10.0
        ],
        "code_task_scores": [
          3.96,
          0.38,
          3.94,
          41.55,
          -2.49
        ],
        "reasoning_task_scores": [
          -1.85,
          0.54,
          0.02,
          -4.36
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "75.2k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K"
    },
    {
      "id": 50,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 34.24,
      "code_avg": 45.39,
      "reasoning_avg": 32.58,
      "overall_avg": 40.92,
      "general_scores": [
        75.82,
        48.3275,
        55.52,
        27.7192857,
        76.13,
        48.3925,
        54.78,
        25.4685714,
        76.39,
        45.895,
        54.65,
        28.5435714
      ],
      "math_scores": [
        76.8,
        30.94,
        29.0,
        22.34,
        10.0,
        75.44,
        31.74,
        30.2,
        23.06,
        13.33,
        73.92,
        31.56,
        30.4,
        21.59,
        13.33
      ],
      "code_scores": [
        75.0,
        73.15,
        12.19,
        24.01,
        44.34,
        73.17,
        73.54,
        12.9,
        26.1,
        41.4,
        71.34,
        71.98,
        13.26,
        27.35,
        41.18
      ],
      "reasoning_scores": [
        33.22,
        68.02,
        0.32086957,
        28.96,
        30.17,
        67.36,
        0.32782609,
        30.16,
        35.93,
        67.51,
        0.32369565,
        28.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.82
              },
              {
                "metric": "lcb_test_output",
                "score": 42.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.16,
        "math_avg": -16.22,
        "code_avg": 6.68,
        "reasoning_avg": -2.35,
        "overall_avg": -0.94,
        "general_task_scores": [
          9.12,
          12.05,
          10.12,
          1.33
        ],
        "math_task_scores": [
          -11.95,
          -33.47,
          -37.53,
          -3.71,
          5.55
        ],
        "code_task_scores": [
          -2.44,
          1.29,
          4.54,
          24.78,
          5.21
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.83,
          -0.07,
          -4.03
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "66.4k",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback"
    },
    {
      "id": 51,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 49.96,
      "math_avg": 36.48,
      "code_avg": 39.29,
      "reasoning_avg": 30.68,
      "overall_avg": 39.1,
      "general_scores": [
        71.86,
        39.445,
        44.32,
        42.75,
        72.08,
        39.645,
        45.15,
        44.2685714,
        72.69,
        39.16,
        44.39,
        43.7485714
      ],
      "math_scores": [
        68.08,
        40.54,
        36.6,
        25.09,
        6.67,
        69.9,
        42.76,
        39.6,
        25.43,
        10.0,
        66.26,
        39.7,
        34.6,
        25.25,
        16.67
      ],
      "code_scores": [
        69.51,
        68.87,
        9.32,
        40.71,
        0.45,
        70.73,
        66.93,
        11.83,
        46.76,
        3.17,
        69.51,
        66.54,
        8.24,
        48.43,
        8.37
      ],
      "reasoning_scores": [
        33.22,
        68.44,
        0.38967391,
        19.6,
        34.24,
        68.19,
        0.38804348,
        21.92,
        31.53,
        67.94,
        0.39413043,
        21.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 45.3
              },
              {
                "metric": "lcb_test_output",
                "score": 4.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.65,
        "math_avg": -13.99,
        "code_avg": 0.57,
        "reasoning_avg": -4.25,
        "overall_avg": -2.76,
        "general_task_scores": [
          5.22,
          3.93,
          -0.24,
          17.68
        ],
        "math_task_scores": [
          -19.26,
          -23.88,
          -30.47,
          -0.78,
          4.44
        ],
        "code_task_scores": [
          -5.69,
          -4.15,
          1.56,
          44.26,
          -33.1
        ],
        "reasoning_task_scores": [
          -3.6,
          -1.27,
          -0.0,
          -12.13
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "55.1k",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT"
    },
    {
      "id": 52,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 53.39,
      "math_avg": 35.08,
      "code_avg": 42.42,
      "reasoning_avg": 30.13,
      "overall_avg": 40.26,
      "general_scores": [
        75.76,
        45.205,
        55.36,
        37.7707143,
        75.5,
        46.14,
        54.79,
        37.9335714,
        75.76,
        45.295,
        55.3,
        35.8214286
      ],
      "math_scores": [
        61.11,
        45.02,
        42.8,
        23.28,
        6.67,
        55.5,
        43.84,
        42.4,
        23.01,
        6.67,
        56.33,
        44.3,
        41.4,
        23.92,
        10.0
      ],
      "code_scores": [
        72.56,
        73.54,
        13.98,
        24.43,
        28.73,
        71.95,
        71.21,
        13.98,
        25.47,
        26.47,
        71.95,
        71.21,
        14.34,
        27.14,
        29.41
      ],
      "reasoning_scores": [
        31.19,
        68.64,
        0.38173913,
        19.6,
        31.86,
        67.61,
        0.38108696,
        18.56,
        34.24,
        68.38,
        0.37456522,
        20.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.68
              },
              {
                "metric": "lcb_test_output",
                "score": 28.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.07,
        "math_avg": -15.38,
        "code_avg": 3.71,
        "reasoning_avg": -4.8,
        "overall_avg": -1.6,
        "general_task_scores": [
          8.68,
          10.06,
          10.29,
          11.27
        ],
        "math_task_scores": [
          -29.69,
          -20.49,
          -25.2,
          -2.64,
          1.11
        ],
        "code_task_scores": [
          -3.46,
          0.39,
          5.86,
          24.64,
          -8.9
        ],
        "reasoning_task_scores": [
          -4.17,
          -1.25,
          -0.01,
          -13.79
        ]
      },
      "affiliation": "bigcode (Huggingface)",
      "year": "2024",
      "size": "50.7k",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k"
    },
    {
      "id": 53,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.5,
      "math_avg": 34.35,
      "code_avg": 46.65,
      "reasoning_avg": 32.18,
      "overall_avg": 42.42,
      "general_scores": [
        74.85,
        51.775,
        58.8,
        42.7107143,
        74.91,
        50.195,
        58.67,
        41.5028571,
        73.57,
        51.02,
        57.95,
        42.0721429
      ],
      "math_scores": [
        58.68,
        46.02,
        41.6,
        20.53,
        3.33,
        54.06,
        44.66,
        42.4,
        21.5,
        3.33,
        62.77,
        47.18,
        44.2,
        21.61,
        3.33
      ],
      "code_scores": [
        76.83,
        71.6,
        13.26,
        37.16,
        43.21,
        78.05,
        71.6,
        12.19,
        27.97,
        42.76,
        81.71,
        69.26,
        11.47,
        18.58,
        44.12
      ],
      "reasoning_scores": [
        25.76,
        65.39,
        0.43847826,
        33.52,
        32.88,
        65.57,
        0.43271739,
        34.16,
        28.14,
        66.51,
        0.4475,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.9
              },
              {
                "metric": "lcb_test_output",
                "score": 43.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.19,
        "math_avg": -16.12,
        "code_avg": 7.93,
        "reasoning_avg": -2.76,
        "overall_avg": 0.56,
        "general_task_scores": [
          7.45,
          15.51,
          13.61,
          16.19
        ],
        "math_task_scores": [
          -28.84,
          -18.93,
          -24.67,
          -4.83,
          -3.34
        ],
        "code_task_scores": [
          3.25,
          -0.78,
          4.07,
          26.86,
          6.26
        ],
        "reasoning_task_scores": [
          -7.67,
          -3.64,
          0.05,
          0.24
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1"
    },
    {
      "id": 54,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 54.71,
      "math_avg": 51.5,
      "code_avg": 40.42,
      "reasoning_avg": 32.58,
      "overall_avg": 44.8,
      "general_scores": [
        72.01,
        51.76,
        59.64,
        37.0535714,
        70.65,
        51.2125,
        58.86,
        37.1528571,
        70.48,
        51.0075,
        58.75,
        37.935
      ],
      "math_scores": [
        83.17,
        65.76,
        66.6,
        27.48,
        13.33,
        81.58,
        66.6,
        70.6,
        27.08,
        13.33,
        81.5,
        66.88,
        67.8,
        27.51,
        13.33
      ],
      "code_scores": [
        77.44,
        73.93,
        10.04,
        43.01,
        0.0,
        75.61,
        73.54,
        11.47,
        40.92,
        0.0,
        78.66,
        73.54,
        9.68,
        38.41,
        0.0
      ],
      "reasoning_scores": [
        28.47,
        69.63,
        0.42565217,
        28.64,
        30.85,
        69.84,
        0.41673913,
        29.76,
        30.85,
        69.71,
        0.42467391,
        31.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.78
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.06
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.4,
        "math_avg": 1.04,
        "code_avg": 1.7,
        "reasoning_avg": -2.35,
        "overall_avg": 2.94,
        "general_task_scores": [
          4.06,
          15.84,
          14.22,
          11.47
        ],
        "math_task_scores": [
          -5.26,
          1.53,
          0.93,
          1.32,
          6.66
        ],
        "code_task_scores": [
          1.63,
          2.07,
          2.16,
          39.74,
          -37.1
        ],
        "reasoning_task_scores": [
          -6.54,
          0.27,
          0.03,
          -3.17
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "35k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code"
    },
    {
      "id": 55,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 53.94,
      "math_avg": 48.18,
      "code_avg": 35.22,
      "reasoning_avg": 25.19,
      "overall_avg": 40.63,
      "general_scores": [
        66.21,
        56.4025,
        51.46,
        42.5707143,
        65.93,
        56.725,
        51.79,
        39.9364286,
        66.4,
        55.995,
        52.15,
        41.6585714
      ],
      "math_scores": [
        74.83,
        63.16,
        63.4,
        26.51,
        6.67,
        75.28,
        61.7,
        65.6,
        25.56,
        20.0,
        73.39,
        60.18,
        60.2,
        26.17,
        20.0
      ],
      "code_scores": [
        77.44,
        76.65,
        0.0,
        22.76,
        0.0,
        81.1,
        74.32,
        0.0,
        19.42,
        0.45,
        78.05,
        77.82,
        0.0,
        20.04,
        0.23
      ],
      "reasoning_scores": [
        32.54,
        55.51,
        0.19467391,
        7.92,
        33.22,
        63.21,
        0.1926087,
        10.48,
        32.2,
        58.36,
        0.19097826,
        8.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 56.37
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.39
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 76.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.74
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.62,
        "math_avg": -2.29,
        "code_avg": -3.5,
        "reasoning_avg": -9.74,
        "overall_avg": -1.23,
        "general_task_scores": [
          -0.81,
          20.88,
          6.94,
          15.48
        ],
        "math_task_scores": [
          -12.84,
          -3.2,
          -4.33,
          0.04,
          8.89
        ],
        "code_task_scores": [
          3.25,
          4.66,
          -8.24,
          19.7,
          -36.87
        ],
        "reasoning_task_scores": [
          -3.95,
          -10.43,
          -0.2,
          -24.4
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "444k",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1"
    },
    {
      "id": 56,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 34.86,
      "math_avg": 19.28,
      "code_avg": 26.66,
      "reasoning_avg": 29.38,
      "overall_avg": 27.55,
      "general_scores": [
        19.9,
        38.3025,
        51.93,
        26.8121429,
        26.92,
        38.275,
        51.86,
        25.6107143,
        23.18,
        39.3875,
        51.8,
        24.3114286
      ],
      "math_scores": [
        25.78,
        21.88,
        25.2,
        20.69,
        3.33,
        21.46,
        22.5,
        25.8,
        18.16,
        6.67,
        23.73,
        22.08,
        26.0,
        19.26,
        6.67
      ],
      "code_scores": [
        42.68,
        52.14,
        0.0,
        35.7,
        3.85,
        42.68,
        51.36,
        0.0,
        36.53,
        4.07,
        42.68,
        49.42,
        0.0,
        36.53,
        2.26
      ],
      "reasoning_scores": [
        34.24,
        60.38,
        0.31304348,
        22.32,
        30.85,
        60.26,
        0.3151087,
        23.04,
        37.63,
        60.7,
        0.31413043,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.25
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.45
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -8.46,
        "math_avg": -31.19,
        "code_avg": -12.06,
        "reasoning_avg": -5.55,
        "overall_avg": -14.31,
        "general_task_scores": [
          -43.66,
          3.17,
          7.0,
          -0.33
        ],
        "math_task_scores": [
          -63.68,
          -42.73,
          -41.73,
          -6.67,
          -1.11
        ],
        "code_task_scores": [
          -32.93,
          -20.63,
          -8.24,
          35.21,
          -33.71
        ],
        "reasoning_task_scores": [
          -2.36,
          -9.01,
          -0.08,
          -10.75
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "5k",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder"
    },
    {
      "id": 57,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.32,
      "math_avg": 44.84,
      "code_avg": 49.23,
      "reasoning_avg": 34.05,
      "overall_avg": 46.11,
      "general_scores": [
        74.26,
        50.47,
        57.39,
        41.7371429,
        74.82,
        51.1875,
        56.61,
        42.4564286,
        74.81,
        52.885,
        56.95,
        42.315
      ],
      "math_scores": [
        76.35,
        54.98,
        56.2,
        23.28,
        10.0,
        77.71,
        54.5,
        57.4,
        22.83,
        16.67,
        73.39,
        55.54,
        57.0,
        23.37,
        13.33
      ],
      "code_scores": [
        77.44,
        71.21,
        12.9,
        41.34,
        45.48,
        75.61,
        71.98,
        12.54,
        40.08,
        46.15,
        78.05,
        69.65,
        12.54,
        40.08,
        43.44
      ],
      "reasoning_scores": [
        33.9,
        68.0,
        0.41326087,
        33.52,
        67.95,
        0.41228261,
        33.44,
        31.53,
        67.95,
        0.41228261,
        33.44,
        38.31,
        67.84,
        0.39076087,
        33.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.63
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.17
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.03
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.5
              },
              {
                "metric": "lcb_test_output",
                "score": 45.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.01,
        "math_avg": -5.63,
        "code_avg": 10.51,
        "reasoning_avg": -0.89,
        "overall_avg": 4.25,
        "general_task_scores": [
          7.64,
          16.02,
          12.12,
          16.26
        ],
        "math_task_scores": [
          -11.52,
          -9.87,
          -10.53,
          -2.88,
          6.66
        ],
        "code_task_scores": [
          1.42,
          -0.65,
          4.42,
          39.46,
          7.92
        ],
        "reasoning_task_scores": [
          -2.02,
          -1.52,
          0.02,
          0.12
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1"
    },
    {
      "id": 58,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.09,
      "math_avg": 40.41,
      "code_avg": 45.38,
      "reasoning_avg": 32.64,
      "overall_avg": 43.13,
      "general_scores": [
        65.44,
        52.59,
        55.78,
        42.89,
        63.63,
        53.985,
        55.37,
        42.6692857,
        64.56,
        53.6375,
        55.57,
        42.9792857
      ],
      "math_scores": [
        81.12,
        46.72,
        48.6,
        18.41,
        6.67,
        82.34,
        47.38,
        47.0,
        18.77,
        6.67,
        77.26,
        46.42,
        49.4,
        19.44,
        10.0
      ],
      "code_scores": [
        74.39,
        67.32,
        2.15,
        42.17,
        39.59,
        73.78,
        68.48,
        1.79,
        42.17,
        43.21,
        73.17,
        66.54,
        3.23,
        40.92,
        41.86
      ],
      "reasoning_scores": [
        36.61,
        67.06,
        0.38369565,
        30.32,
        30.51,
        67.12,
        0.3825,
        30.08,
        32.88,
        66.96,
        0.38565217,
        29.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.54
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 41.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.78,
        "math_avg": -10.05,
        "code_avg": 6.67,
        "reasoning_avg": -2.29,
        "overall_avg": 1.28,
        "general_task_scores": [
          -2.45,
          17.91,
          10.71,
          16.94
        ],
        "math_task_scores": [
          -7.1,
          -18.04,
          -19.07,
          -7.17,
          1.11
        ],
        "code_task_scores": [
          -1.83,
          -4.15,
          -5.85,
          40.71,
          4.45
        ],
        "reasoning_task_scores": [
          -3.27,
          -2.41,
          -0.01,
          -3.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca"
    },
    {
      "id": 59,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 56.57,
      "math_avg": 46.13,
      "code_avg": 44.38,
      "reasoning_avg": 33.98,
      "overall_avg": 45.26,
      "general_scores": [
        75.1,
        50.66,
        58.01,
        42.9478571,
        74.71,
        50.6575,
        57.61,
        43.2478571,
        74.48,
        51.18,
        57.74,
        42.5057143
      ],
      "math_scores": [
        80.52,
        54.22,
        54.2,
        24.86,
        13.33,
        81.05,
        54.94,
        56.6,
        25.29,
        13.33,
        81.43,
        54.54,
        52.8,
        24.86,
        20.0
      ],
      "code_scores": [
        75.61,
        71.6,
        0.36,
        42.17,
        34.62,
        73.78,
        67.32,
        0.0,
        42.38,
        35.97,
        73.78,
        69.26,
        0.0,
        42.38,
        36.43
      ],
      "reasoning_scores": [
        35.59,
        67.93,
        0.38195652,
        32.08,
        32.88,
        68.66,
        0.38565217,
        32.32,
        35.93,
        68.61,
        0.38586957,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.12
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 35.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.26,
        "math_avg": -4.33,
        "code_avg": 5.66,
        "reasoning_avg": -0.96,
        "overall_avg": 3.41,
        "general_task_scores": [
          7.77,
          15.34,
          12.93,
          16.99
        ],
        "math_task_scores": [
          -6.34,
          -10.31,
          -12.87,
          -1.04,
          8.88
        ],
        "code_task_scores": [
          -1.22,
          -2.21,
          -8.12,
          41.27,
          -1.43
        ],
        "reasoning_task_scores": [
          -1.8,
          -1.06,
          -0.01,
          -0.96
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "18.6k",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca"
    },
    {
      "id": 60,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 57.46,
      "math_avg": 49.84,
      "code_avg": 44.32,
      "reasoning_avg": 35.29,
      "overall_avg": 46.73,
      "general_scores": [
        76.64,
        48.8025,
        57.65,
        46.33,
        76.22,
        50.9975,
        57.76,
        45.9364286,
        76.23,
        49.385,
        57.47,
        46.1164286
      ],
      "math_scores": [
        82.56,
        61.22,
        60.8,
        25.72,
        23.33,
        84.15,
        61.54,
        61.2,
        25.61,
        10.0,
        83.24,
        61.2,
        61.6,
        25.47,
        20.0
      ],
      "code_scores": [
        75.61,
        68.09,
        14.34,
        17.95,
        44.8,
        79.27,
        67.7,
        14.34,
        15.24,
        44.8,
        77.44,
        69.65,
        13.26,
        15.87,
        46.38
      ],
      "reasoning_scores": [
        35.93,
        67.14,
        0.3825,
        37.84,
        35.59,
        67.06,
        0.39032609,
        37.92,
        35.59,
        67.67,
        0.38032609,
        37.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.98
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.35
              },
              {
                "metric": "lcb_test_output",
                "score": 45.33
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.29
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.15,
        "math_avg": -0.62,
        "code_avg": 5.6,
        "reasoning_avg": 0.36,
        "overall_avg": 4.87,
        "general_task_scores": [
          9.37,
          14.24,
          12.77,
          20.22
        ],
        "math_task_scores": [
          -4.02,
          -3.56,
          -6.2,
          -0.44,
          11.11
        ],
        "code_task_scores": [
          1.83,
          -3.12,
          5.74,
          15.31,
          8.23
        ],
        "reasoning_task_scores": [
          -0.9,
          -2.17,
          -0.01,
          4.51
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "22.6k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT"
    },
    {
      "id": 61,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 49.64,
      "math_avg": 43.09,
      "code_avg": 31.9,
      "reasoning_avg": 27.14,
      "overall_avg": 37.94,
      "general_scores": [
        72.51,
        42.085,
        55.23,
        28.7535714,
        72.51,
        41.9775,
        55.23,
        28.7535714,
        72.51,
        42.1625,
        55.23,
        28.7535714
      ],
      "math_scores": [
        68.99,
        55.02,
        53.4,
        24.71,
        13.33,
        68.99,
        55.02,
        53.4,
        24.68,
        13.33,
        68.99,
        55.02,
        53.4,
        24.68,
        13.33
      ],
      "code_scores": [
        31.1,
        64.2,
        6.81,
        38.41,
        19.0,
        31.1,
        64.2,
        6.81,
        38.41,
        19.0,
        31.1,
        64.2,
        6.81,
        38.41,
        19.0
      ],
      "reasoning_scores": [
        34.58,
        64.43,
        0.36758242,
        9.2,
        34.58,
        64.43,
        0.36456522,
        9.2,
        34.58,
        64.43,
        0.36758242,
        9.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.99
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 19.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.33,
        "math_avg": -7.38,
        "code_avg": -6.81,
        "reasoning_avg": -7.79,
        "overall_avg": -3.91,
        "general_task_scores": [
          5.52,
          6.59,
          10.37,
          2.84
        ],
        "math_task_scores": [
          -18.35,
          -9.86,
          -14.0,
          -1.35,
          6.66
        ],
        "code_task_scores": [
          -44.51,
          -7.4,
          -1.43,
          37.37,
          -18.1
        ],
        "reasoning_task_scores": [
          -2.02,
          -5.03,
          -0.02,
          -24.08
        ]
      },
      "affiliation": "smcleod",
      "year": "2024",
      "size": "305k",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder/viewer/default/train?row=93"
    },
    {
      "id": 62,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 46.34,
      "math_avg": 53.41,
      "code_avg": 45.98,
      "reasoning_avg": 29.89,
      "overall_avg": 43.91,
      "general_scores": [
        73.54,
        53.8,
        57.18,
        0.0,
        75.48,
        52.785,
        57.98,
        0.0,
        74.51,
        53.7575,
        57.04,
        0.0
      ],
      "math_scores": [
        82.03,
        63.48,
        64.4,
        32.36,
        20.0,
        86.43,
        64.94,
        68.0,
        33.42,
        13.33,
        81.58,
        62.06,
        62.4,
        13.33
      ],
      "code_scores": [
        82.93,
        73.54,
        16.85,
        37.58,
        20.59,
        83.54,
        73.54,
        13.98,
        35.28,
        20.81,
        83.54,
        73.15,
        16.85,
        37.37,
        20.14
      ],
      "reasoning_scores": [
        33.9,
        68.07,
        0.32945652,
        20.08,
        31.86,
        68.7,
        0.33717391,
        21.44,
        27.12,
        68.64,
        0.34119565,
        17.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.35
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.89
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.41
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 15.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.74
              },
              {
                "metric": "lcb_test_output",
                "score": 20.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.96
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.03,
        "math_avg": 2.95,
        "code_avg": 7.26,
        "reasoning_avg": -5.04,
        "overall_avg": 2.05,
        "general_task_scores": [
          7.52,
          17.96,
          12.54,
          -25.91
        ],
        "math_task_scores": [
          -3.99,
          -1.39,
          -2.47,
          6.85,
          8.88
        ],
        "code_task_scores": [
          7.73,
          1.81,
          7.65,
          35.7,
          -16.59
        ],
        "reasoning_task_scores": [
          -5.64,
          -0.99,
          -0.05,
          -13.47
        ]
      },
      "affiliation": "microsoft",
      "year": "2025",
      "size": "380k",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k"
    },
    {
      "id": 63,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 55.67,
      "math_avg": 51.45,
      "code_avg": 49.96,
      "reasoning_avg": 34.16,
      "overall_avg": 47.81,
      "general_scores": [
        67.03,
        52.9625,
        59.54,
        44.4192857,
        65.75,
        52.7925,
        59.69,
        43.6664286,
        66.11,
        52.8175,
        59.29,
        43.9657143
      ],
      "math_scores": [
        87.95,
        64.04,
        68.6,
        26.65,
        13.33,
        87.49,
        63.68,
        64.6,
        26.76,
        10.0,
        88.25,
        63.6,
        64.2,
        25.86,
        16.67
      ],
      "code_scores": [
        76.83,
        72.76,
        12.54,
        42.17,
        46.83,
        75.0,
        71.6,
        11.83,
        42.38,
        49.77,
        75.0,
        71.6,
        13.26,
        40.71,
        47.06
      ],
      "reasoning_scores": [
        29.83,
        69.88,
        0.45108696,
        31.68,
        35.25,
        70.41,
        0.44326087,
        33.36,
        35.25,
        70.29,
        0.4498913,
        32.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.9
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 47.89
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.36,
        "math_avg": 0.98,
        "code_avg": 11.24,
        "reasoning_avg": -0.77,
        "overall_avg": 5.95,
        "general_task_scores": [
          -0.69,
          17.37,
          14.65,
          18.11
        ],
        "math_task_scores": [
          0.56,
          -1.11,
          -1.6,
          0.38,
          6.66
        ],
        "code_task_scores": [
          0.0,
          0.39,
          4.3,
          40.71,
          10.79
        ],
        "reasoning_task_scores": [
          -3.16,
          0.73,
          0.06,
          -0.72
        ]
      },
      "affiliation": "likaixin",
      "year": "2023",
      "size": "108k",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder"
    },
    {
      "id": 64,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 45.38,
      "math_avg": 31.8,
      "code_avg": 28.33,
      "reasoning_avg": 27.36,
      "overall_avg": 33.22,
      "general_scores": [
        53.06,
        49.1375,
        50.24,
        28.8142857,
        50.65,
        50.0225,
        50.83,
        26.9571429,
        58.75,
        49.3175,
        47.94,
        28.805
      ],
      "math_scores": [
        67.48,
        30.8,
        24.8,
        29.27,
        10.0,
        67.32,
        30.54,
        25.8,
        29.27,
        3.33,
        72.1,
        30.26,
        21.4,
        27.94,
        6.67
      ],
      "code_scores": [
        54.27,
        65.37,
        0.0,
        11.9,
        1.81,
        56.1,
        66.54,
        0.0,
        19.21,
        2.94,
        60.98,
        64.98,
        0.0,
        19.0,
        1.81
      ],
      "reasoning_scores": [
        28.47,
        63.37,
        0.38391304,
        16.24,
        32.2,
        64.23,
        0.34673913,
        16.32,
        27.46,
        64.29,
        0.38543478,
        14.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.7
              },
              {
                "metric": "lcb_test_output",
                "score": 2.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.06,
        "math_avg": -18.67,
        "code_avg": -10.39,
        "reasoning_avg": -7.57,
        "overall_avg": -8.64,
        "general_task_scores": [
          -12.84,
          14.0,
          4.81,
          2.28
        ],
        "math_task_scores": [
          -18.37,
          -34.35,
          -43.4,
          2.79,
          0.0
        ],
        "code_task_scores": [
          -18.49,
          -5.97,
          -8.24,
          15.66,
          -34.91
        ],
        "reasoning_task_scores": [
          -7.22,
          -5.5,
          -0.02,
          -17.55
        ]
      },
      "affiliation": "likaixin",
      "year": "2024",
      "size": "1043k",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified"
    },
    {
      "id": 65,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 53.93,
      "math_avg": 37.73,
      "code_avg": 46.88,
      "reasoning_avg": 31.85,
      "overall_avg": 42.59,
      "general_scores": [
        64.72,
        53.28,
        58.75,
        38.9128571,
        63.18,
        52.515,
        58.74,
        40.565,
        63.53,
        51.73,
        59.17,
        42.0135714
      ],
      "math_scores": [
        57.32,
        52.54,
        51.6,
        24.62,
        10.0,
        51.25,
        49.54,
        50.8,
        24.71,
        13.33,
        51.33,
        47.96,
        46.6,
        24.28,
        10.0
      ],
      "code_scores": [
        75.61,
        73.93,
        9.32,
        44.89,
        37.1,
        41.63,
        73.78,
        74.32,
        9.68,
        44.05,
        32.81,
        72.56,
        72.76,
        9.32,
        45.51,
        32.81
      ],
      "reasoning_scores": [
        35.59,
        66.92,
        0.42434783,
        28.48,
        31.53,
        66.94,
        0.41684783,
        27.36,
        31.19,
        67.3,
        0.41923913,
        27.04,
        31.19,
        67.3,
        0.41923913,
        27.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.89
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.3
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.82
              },
              {
                "metric": "lcb_test_output",
                "score": 36.09
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.61,
        "math_avg": -12.74,
        "code_avg": 8.16,
        "reasoning_avg": -3.09,
        "overall_avg": 0.74,
        "general_task_scores": [
          -3.18,
          17.02,
          14.03,
          14.59
        ],
        "math_task_scores": [
          -34.04,
          -14.87,
          -17.73,
          -1.5,
          4.44
        ],
        "code_task_scores": [
          -1.63,
          2.07,
          1.2,
          43.78,
          -1.01
        ],
        "reasoning_task_scores": [
          -4.22,
          -2.34,
          0.03,
          -5.8
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "100k",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql"
    },
    {
      "id": 66,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 53.11,
      "math_avg": 52.77,
      "code_avg": 45.06,
      "reasoning_avg": 31.99,
      "overall_avg": 45.73,
      "general_scores": [
        69.26,
        52.75,
        59.54,
        31.3753846,
        68.18,
        52.01,
        59.96,
        31.0557143,
        69.12,
        52.525,
        60.13,
        31.4728571
      ],
      "math_scores": [
        83.17,
        68.9,
        72.0,
        26.38,
        16.67,
        80.82,
        69.02,
        73.0,
        26.65,
        13.33,
        79.83,
        69.18,
        72.4,
        26.87,
        13.33
      ],
      "code_scores": [
        79.27,
        73.15,
        0.0,
        39.25,
        39.14,
        79.27,
        71.98,
        0.0,
        38.0,
        33.71,
        78.05,
        71.98,
        0.0,
        38.83,
        33.26
      ],
      "reasoning_scores": [
        27.8,
        71.07,
        0.44152174,
        28.56,
        30.17,
        71.12,
        0.44076087,
        27.68,
        27.12,
        70.92,
        0.4373913,
        28.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.69
              },
              {
                "metric": "lcb_test_output",
                "score": 35.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.8,
        "math_avg": 2.3,
        "code_avg": 6.34,
        "reasoning_avg": -2.94,
        "overall_avg": 3.88,
        "general_task_scores": [
          1.86,
          16.94,
          15.02,
          5.39
        ],
        "math_task_scores": [
          -6.07,
          4.15,
          5.07,
          0.59,
          7.77
        ],
        "code_task_scores": [
          3.25,
          0.77,
          -8.24,
          37.65,
          -1.73
        ],
        "reasoning_task_scores": [
          -8.24,
          1.58,
          0.05,
          -5.15
        ]
      },
      "affiliation": "gretelai",
      "year": "2024",
      "size": "22.5k",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1"
    },
    {
      "id": 67,
      "name": "RAG-v1",
      "domain": "code",
      "general_avg": 45.67,
      "math_avg": 52.31,
      "code_avg": 39.19,
      "reasoning_avg": 33.32,
      "overall_avg": 42.62,
      "general_scores": [
        66.05,
        51.4475,
        57.39,
        11.765,
        66.91,
        50.0775,
        57.51,
        0.0,
        66.45,
        51.6225,
        57.06,
        11.765
      ],
      "math_scores": [
        86.88,
        68.54,
        72.8,
        26.76,
        6.67,
        86.13,
        68.74,
        72.4,
        26.31,
        10.0,
        87.41,
        69.26,
        70.0,
        26.02,
        6.67
      ],
      "code_scores": [
        14.02,
        73.15,
        11.83,
        44.68,
        45.93,
        22.56,
        73.93,
        14.34,
        44.47,
        48.42,
        14.63,
        73.54,
        13.26,
        44.26,
        48.87
      ],
      "reasoning_scores": [
        31.53,
        69.41,
        0.42163043,
        31.76,
        31.86,
        69.15,
        0.4175,
        31.92,
        32.54,
        69.26,
        0.41728261,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.47
              },
              {
                "metric": "lcb_test_output",
                "score": 47.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.98
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.36,
        "math_avg": 1.84,
        "code_avg": 0.47,
        "reasoning_avg": -1.62,
        "overall_avg": 0.76,
        "general_task_scores": [
          -0.52,
          15.56,
          12.46,
          -18.07
        ],
        "math_task_scores": [
          -0.53,
          3.97,
          4.33,
          0.32,
          1.11
        ],
        "code_task_scores": [
          -58.54,
          1.94,
          4.9,
          43.43,
          10.64
        ],
        "reasoning_task_scores": [
          -4.62,
          -0.19,
          0.03,
          -1.68
        ]
      },
      "affiliation": "glaiveai",
      "year": "2024",
      "size": "51.4k",
      "link": "https://huggingface.co/datasets/glaiveai/RAG-v1"
    },
    {
      "id": 68,
      "name": "Code-290k-ShareGPT-Vicuna",
      "domain": "code",
      "general_avg": 54.99,
      "math_avg": 41.83,
      "code_avg": 45.13,
      "reasoning_avg": 33.87,
      "overall_avg": 43.96,
      "general_scores": [
        70.47,
        51.635,
        54.75,
        43.6728571,
        70.72,
        50.245,
        56.25,
        43.7142857,
        70.4,
        48.7625,
        55.48,
        43.8278571
      ],
      "math_scores": [
        79.45,
        49.28,
        47.2,
        22.15,
        16.67,
        78.17,
        48.64,
        47.6,
        22.06,
        10.0,
        78.62,
        49.92,
        49.0,
        22.02,
        6.67
      ],
      "code_scores": [
        79.27,
        68.09,
        12.19,
        16.08,
        41.86,
        78.66,
        68.09,
        13.26,
        24.43,
        42.99,
        78.66,
        68.48,
        13.98,
        27.77,
        43.21
      ],
      "reasoning_scores": [
        30.51,
        66.6,
        0.39576087,
        36.0,
        34.58,
        66.02,
        0.38217391,
        33.36,
        37.63,
        66.63,
        0.38217391,
        33.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.75
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.68,
        "math_avg": -8.64,
        "code_avg": 6.42,
        "reasoning_avg": -1.07,
        "overall_avg": 2.1,
        "general_task_scores": [
          3.54,
          14.72,
          10.63,
          17.83
        ],
        "math_task_scores": [
          -8.59,
          -15.6,
          -19.47,
          -3.96,
          4.44
        ],
        "code_task_scores": [
          3.25,
          -3.38,
          4.9,
          21.72,
          5.59
        ],
        "reasoning_task_scores": [
          -2.36,
          -3.04,
          -0.0,
          1.15
        ]
      },
      "affiliation": "cognitivecomputations",
      "year": "2024",
      "size": "289k",
      "link": "https://huggingface.co/datasets/cognitivecomputations/Code-290k-ShareGPT-Vicuna"
    },
    {
      "id": 69,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 50.38,
      "math_avg": 46.63,
      "code_avg": 49.69,
      "reasoning_avg": 33.88,
      "overall_avg": 45.15,
      "general_scores": [
        57.28,
        52.2125,
        57.91,
        40.0221429,
        47.53,
        50.24,
        58.4,
        37.8521429,
        52.59,
        51.9275,
        58.37,
        40.2492857
      ],
      "math_scores": [
        82.03,
        57.7,
        57.0,
        26.63,
        13.33,
        81.58,
        55.22,
        53.6,
        25.61,
        10.0,
        79.61,
        58.72,
        59.2,
        25.95,
        13.33
      ],
      "code_scores": [
        71.95,
        73.93,
        11.11,
        46.14,
        49.1,
        70.73,
        71.21,
        11.83,
        47.39,
        44.34,
        71.95,
        70.04,
        10.39,
        46.14,
        49.1
      ],
      "reasoning_scores": [
        32.2,
        69.56,
        0.43152174,
        33.28,
        30.85,
        68.61,
        0.42391304,
        32.08,
        37.29,
        68.85,
        0.4375,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.46
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.56
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.07,
        "math_avg": -3.83,
        "code_avg": 10.97,
        "reasoning_avg": -1.05,
        "overall_avg": 3.29,
        "general_task_scores": [
          -14.52,
          15.97,
          13.37,
          13.46
        ],
        "math_task_scores": [
          -6.27,
          -7.67,
          -10.8,
          0.02,
          5.55
        ],
        "code_task_scores": [
          -4.07,
          0.13,
          2.87,
          45.52,
          10.41
        ],
        "reasoning_task_scores": [
          -3.15,
          -0.45,
          0.04,
          -0.64
        ]
      },
      "affiliation": "b-mc2",
      "year": "2023",
      "size": "78.6k",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context/viewer/default/train?row=0"
    },
    {
      "id": 70,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 43.07,
      "math_avg": 36.92,
      "code_avg": 34.05,
      "reasoning_avg": 26.42,
      "overall_avg": 35.12,
      "general_scores": [
        58.99,
        35.775,
        56.5,
        21.8142857,
        53.6,
        38.29,
        56.49,
        23.0985714
      ],
      "math_scores": [
        75.59,
        45.92,
        47.6,
        19.11,
        10.0,
        65.58,
        36.9,
        36.6,
        18.59,
        13.33
      ],
      "code_scores": [
        71.34,
        71.6,
        12.9,
        11.9,
        5.66,
        70.73,
        71.6,
        12.54,
        11.06,
        1.13
      ],
      "reasoning_scores": [
        36.27,
        69.09,
        0.2898913,
        5.44,
        30.17,
        68.78,
        0.29869565,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.46
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.1
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.48
              },
              {
                "metric": "lcb_test_output",
                "score": 3.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.24,
        "math_avg": -13.54,
        "code_avg": -4.67,
        "reasoning_avg": -8.51,
        "overall_avg": -6.74,
        "general_task_scores": [
          -10.69,
          1.54,
          11.64,
          -3.45
        ],
        "math_task_scores": [
          -16.75,
          -23.47,
          -25.3,
          -7.19,
          4.99
        ],
        "code_task_scores": [
          -4.57,
          0.0,
          4.48,
          10.44,
          -33.7
        ],
        "reasoning_task_scores": [
          -3.38,
          -0.52,
          -0.1,
          -30.04
        ]
      },
      "affiliation": "argilla",
      "year": "2024",
      "size": "109k",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling"
    },
    {
      "id": 71,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 54.99,
      "math_avg": 41.83,
      "code_avg": 45.13,
      "reasoning_avg": 33.87,
      "overall_avg": 43.96,
      "general_scores": [
        70.47,
        51.635,
        54.75,
        43.6728571,
        70.72,
        50.245,
        56.25,
        43.7142857,
        70.4,
        48.7625,
        55.48,
        43.8278571
      ],
      "math_scores": [
        79.45,
        49.28,
        47.2,
        22.15,
        16.67,
        78.17,
        48.64,
        47.6,
        22.06,
        10.0,
        78.62,
        49.92,
        49.0,
        22.02,
        6.67
      ],
      "code_scores": [
        79.27,
        68.09,
        12.19,
        16.08,
        41.86,
        78.66,
        68.09,
        13.26,
        24.43,
        42.99,
        78.66,
        68.48,
        13.98,
        27.77,
        43.21
      ],
      "reasoning_scores": [
        30.51,
        66.6,
        0.39576087,
        36.0,
        34.58,
        66.02,
        0.38217391,
        33.36,
        37.63,
        66.63,
        0.38217391,
        33.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.75
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.68,
        "math_avg": -8.64,
        "code_avg": 6.42,
        "reasoning_avg": -1.07,
        "overall_avg": 2.1,
        "general_task_scores": [
          3.54,
          14.72,
          10.63,
          17.83
        ],
        "math_task_scores": [
          -8.59,
          -15.6,
          -19.47,
          -3.96,
          4.44
        ],
        "code_task_scores": [
          3.25,
          -3.38,
          4.9,
          21.72,
          5.59
        ],
        "reasoning_task_scores": [
          -2.36,
          -3.04,
          -0.0,
          1.15
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "290k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT"
    },
    {
      "id": 72,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 56.77,
      "math_avg": 45.65,
      "code_avg": 43.44,
      "reasoning_avg": 33.93,
      "overall_avg": 44.95,
      "general_scores": [
        74.16,
        50.8675,
        56.01,
        44.1557143,
        75.57,
        50.395,
        56.34,
        45.1885714,
        75.91,
        50.9425,
        56.66,
        45.0992857
      ],
      "math_scores": [
        81.73,
        53.48,
        56.0,
        23.94,
        19.16625,
        80.36,
        53.5,
        53.6,
        23.55,
        10.0,
        82.03,
        53.52,
        53.4,
        23.87,
        16.67
      ],
      "code_scores": [
        76.22,
        71.6,
        12.54,
        13.15,
        44.12,
        76.83,
        70.82,
        11.11,
        13.78,
        44.8,
        77.44,
        70.82,
        12.19,
        12.53,
        43.67
      ],
      "reasoning_scores": [
        35.25,
        67.64,
        0.39271739,
        32.96,
        32.2,
        66.84,
        0.39521739,
        34.16,
        35.93,
        67.66,
        0.38826087,
        33.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.81
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.79
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.28
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.95
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.15
              },
              {
                "metric": "lcb_test_output",
                "score": 44.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.46,
        "math_avg": -4.81,
        "code_avg": 4.72,
        "reasoning_avg": -1.0,
        "overall_avg": 3.09,
        "general_task_scores": [
          8.22,
          15.25,
          11.48,
          18.9
        ],
        "math_task_scores": [
          -5.97,
          -11.38,
          -13.07,
          -2.25,
          8.61
        ],
        "code_task_scores": [
          1.22,
          -0.52,
          3.71,
          12.11,
          7.1
        ],
        "reasoning_task_scores": [
          -2.14,
          -2.08,
          -0.0,
          0.21
        ]
      },
      "affiliation": "ajibawa-2023",
      "year": "2024",
      "size": "74k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT"
    },
    {
      "id": 73,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 43.35,
      "math_avg": 48.75,
      "code_avg": 43.32,
      "reasoning_avg": 30.31,
      "overall_avg": 41.43,
      "general_scores": [
        65.7,
        44.0975,
        58.56,
        11.3207143,
        63.63,
        44.235,
        58.26,
        11.3207143,
        62.16,
        42.895,
        58.07,
        0.0
      ],
      "math_scores": [
        82.71,
        60.1,
        62.0,
        23.19,
        10.0,
        82.87,
        62.6,
        66.2,
        23.53,
        16.67,
        83.17,
        61.14,
        63.4,
        23.64,
        10.0
      ],
      "code_scores": [
        67.07,
        71.6,
        12.54,
        23.17,
        41.86,
        65.85,
        71.98,
        14.34,
        24.22,
        41.4,
        67.07,
        70.43,
        13.26,
        23.38,
        41.63
      ],
      "reasoning_scores": [
        33.22,
        68.61,
        0.36380435,
        20.88,
        28.81,
        68.52,
        0.36141304,
        20.24,
        32.54,
        68.17,
        0.36597826,
        21.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.3
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.59
              },
              {
                "metric": "lcb_test_output",
                "score": 41.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.04,
        "math_avg": -1.72,
        "code_avg": 4.6,
        "reasoning_avg": -4.62,
        "overall_avg": -0.42,
        "general_task_scores": [
          -3.16,
          8.25,
          13.44,
          -18.36
        ],
        "math_task_scores": [
          -4.42,
          -3.6,
          -3.53,
          -2.59,
          5.55
        ],
        "code_task_scores": [
          -8.95,
          -0.26,
          5.14,
          22.55,
          4.53
        ],
        "reasoning_task_scores": [
          -5.08,
          -1.03,
          -0.03,
          -12.35
        ]
      },
      "affiliation": "Tensoic",
      "year": "2023",
      "size": "45k",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook"
    },
    {
      "id": 74,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 44.32,
      "math_avg": 52.42,
      "code_avg": 40.99,
      "reasoning_avg": 31.92,
      "overall_avg": 42.41,
      "general_scores": [
        73.5,
        47.275,
        58.2,
        0.0,
        71.98,
        46.285,
        57.46,
        0.0,
        72.55,
        47.0825,
        57.48,
        0.0
      ],
      "math_scores": [
        77.63,
        63.22,
        65.2,
        10.0,
        73.77,
        62.36,
        64.8,
        3.33,
        77.33,
        62.08,
        62.6,
        6.67
      ],
      "code_scores": [
        50.61,
        66.93,
        9.32,
        42.17,
        35.97,
        51.22,
        66.54,
        9.68,
        40.92,
        31.0,
        53.05,
        68.48,
        9.32,
        43.01,
        36.65
      ],
      "reasoning_scores": [
        37.63,
        67.82,
        0.39728261,
        23.36,
        35.25,
        67.91,
        0.40619565,
        25.92,
        29.83,
        67.61,
        0.42152174,
        26.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.55
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 34.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.01,
        "math_avg": 1.95,
        "code_avg": 2.27,
        "reasoning_avg": -3.01,
        "overall_avg": 0.55,
        "general_task_scores": [
          5.69,
          11.39,
          12.85,
          -25.91
        ],
        "math_task_scores": [
          -11.1,
          -2.33,
          -3.2,
          0.0
        ],
        "code_task_scores": [
          -23.98,
          -4.28,
          1.2,
          40.99,
          -2.56
        ],
        "reasoning_task_scores": [
          -2.36,
          -1.68,
          0.02,
          -8.03
        ]
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python"
    },
    {
      "id": 75,
      "name": "OpenR1-Math-220k(default)",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 61.46,
      "code_avg": 24.86,
      "reasoning_avg": 0,
      "overall_avg": 43.16,
      "general_scores": [],
      "math_scores": [
        91.05,
        79.24,
        79.4,
        16.67,
        90.45,
        79.4,
        77.6,
        29.81,
        13.33,
        91.28,
        79.62,
        79.4,
        29.9,
        23.33
      ],
      "code_scores": [
        1.83,
        74.71,
        8.24,
        27.14,
        16.29,
        0.61,
        74.32,
        6.45,
        26.93,
        12.67,
        1.83,
        74.32,
        9.32,
        24.63,
        13.57
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.42
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.23
              },
              {
                "metric": "lcb_test_output",
                "score": 14.18
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 11.0,
        "code_avg": -13.86,
        "reasoning_avg": -34.93,
        "overall_avg": 1.3,
        "general_task_scores": [],
        "math_task_scores": [
          3.59,
          14.54,
          11.4,
          3.81,
          11.11
        ],
        "code_task_scores": [
          -74.19,
          2.85,
          -0.24,
          25.19,
          -22.92
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k"
    },
    {
      "id": 76,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 65.18,
      "code_avg": 36.49,
      "reasoning_avg": 0,
      "overall_avg": 50.83,
      "general_scores": [],
      "math_scores": [
        92.19,
        74.84,
        77.0,
        16.67
      ],
      "code_scores": [
        48.78,
        73.15,
        11.47,
        44.05,
        4.98
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.05
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 14.71,
        "code_avg": -2.23,
        "reasoning_avg": -34.93,
        "overall_avg": 8.97,
        "general_task_scores": [],
        "math_task_scores": [
          4.85,
          9.96,
          9.6,
          10.0
        ],
        "code_task_scores": [
          -26.83,
          1.55,
          3.23,
          43.01,
          -32.12
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Yonsei University",
      "year": "2025",
      "size": "130k",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K"
    },
    {
      "id": 77,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 65.18,
      "code_avg": 36.49,
      "reasoning_avg": 0,
      "overall_avg": 50.83,
      "general_scores": [],
      "math_scores": [
        92.19,
        74.84,
        77.0,
        16.67
      ],
      "code_scores": [
        48.78,
        73.15,
        11.47,
        44.05,
        4.98
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.05
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 14.71,
        "code_avg": -2.23,
        "reasoning_avg": -34.93,
        "overall_avg": 8.97,
        "general_task_scores": [],
        "math_task_scores": [
          4.85,
          9.96,
          9.6,
          10.0
        ],
        "code_task_scores": [
          -26.83,
          1.55,
          3.23,
          43.01,
          -32.12
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Yonsei University",
      "year": "2025",
      "size": "138k",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2"
    },
    {
      "id": 78,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 43.79,
      "math_avg": 37.24,
      "code_avg": 35.79,
      "reasoning_avg": 29.07,
      "overall_avg": 36.47,
      "general_scores": [
        88.92,
        38.335,
        47.35,
        1.76714286,
        88.5,
        36.485,
        48.11,
        1.29428571,
        87.68,
        37.5275,
        47.76,
        1.70928571
      ],
      "math_scores": [
        7.81,
        68.06,
        68.0,
        24.12,
        26.67,
        9.02,
        68.58,
        68.8,
        24.37,
        10.0,
        9.1,
        68.72,
        68.2,
        23.89,
        13.33
      ],
      "code_scores": [
        58.54,
        71.98,
        9.68,
        31.52,
        1.81,
        64.63,
        71.98,
        11.11,
        33.4,
        2.04,
        63.41,
        75.49,
        10.39,
        29.23,
        1.58
      ],
      "reasoning_scores": [
        35.93,
        69.97,
        0.26184783,
        11.2,
        31.86,
        68.24,
        0.26043478,
        11.92,
        36.95,
        68.83,
        0.2675,
        13.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.64
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.38
              },
              {
                "metric": "lcb_test_output",
                "score": 1.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.47,
        "math_avg": -13.22,
        "code_avg": -2.93,
        "reasoning_avg": -5.86,
        "overall_avg": -5.38,
        "general_task_scores": [
          21.38,
          1.96,
          2.88,
          -24.32
        ],
        "math_task_scores": [
          -78.7,
          3.57,
          0.93,
          -1.91,
          10.0
        ],
        "code_task_scores": [
          -13.42,
          1.55,
          2.15,
          30.34,
          -35.29
        ],
        "reasoning_task_scores": [
          -1.69,
          -0.45,
          -0.13,
          -21.17
        ]
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k"
    },
    {
      "id": 79,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 69.03,
      "code_avg": 40.46,
      "reasoning_avg": 0,
      "overall_avg": 54.75,
      "general_scores": [],
      "math_scores": [
        91.21,
        81.52,
        83.4,
        20.0
      ],
      "code_scores": [
        59.15,
        73.93,
        17.2,
        32.36,
        19.68
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 17.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.36
              },
              {
                "metric": "lcb_test_output",
                "score": 19.68
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 18.57,
        "code_avg": 1.75,
        "reasoning_avg": -34.93,
        "overall_avg": 12.89,
        "general_task_scores": [],
        "math_task_scores": [
          3.87,
          16.64,
          16.0,
          13.33
        ],
        "code_task_scores": [
          -16.46,
          2.33,
          8.96,
          31.32,
          -17.42
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Open Thoughts",
      "year": "2025",
      "size": "114k",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"
    },
    {
      "id": 80,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 53.02,
      "code_avg": 45.34,
      "reasoning_avg": 0,
      "overall_avg": 49.18,
      "general_scores": [],
      "math_scores": [
        85.37,
        59.86,
        60.2,
        6.67
      ],
      "code_scores": [
        65.24,
        68.48,
        11.11,
        38.41,
        43.44
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 43.44
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 2.56,
        "code_avg": 6.62,
        "reasoning_avg": -34.93,
        "overall_avg": 7.32,
        "general_task_scores": [],
        "math_task_scores": [
          -1.97,
          -5.02,
          -7.2,
          0.0
        ],
        "code_task_scores": [
          -10.37,
          -3.12,
          2.87,
          37.37,
          6.34
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT"
    },
    {
      "id": 81,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 61.27,
      "code_avg": 25.11,
      "reasoning_avg": 0,
      "overall_avg": 43.19,
      "general_scores": [],
      "math_scores": [
        90.3,
        70.98,
        73.8,
        10.0
      ],
      "code_scores": [
        1.22,
        74.71,
        12.54,
        11.27,
        25.79
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.3
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.22
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.27
              },
              {
                "metric": "lcb_test_output",
                "score": 25.79
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 10.8,
        "code_avg": -13.61,
        "reasoning_avg": -34.93,
        "overall_avg": 1.33,
        "general_task_scores": [],
        "math_task_scores": [
          2.96,
          6.1,
          6.4,
          3.33
        ],
        "code_task_scores": [
          -74.39,
          3.11,
          4.3,
          10.23,
          -11.31
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "O1-OPEN",
      "year": "2025",
      "size": "77.7k",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT"
    },
    {
      "id": 82,
      "name": "GAIR/o1-journey",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 54.95,
      "code_avg": 35.26,
      "reasoning_avg": 0,
      "overall_avg": 45.11,
      "general_scores": [],
      "math_scores": [
        78.62,
        64.78,
        66.4,
        10.0
      ],
      "code_scores": [
        0.61,
        72.76,
        12.19,
        43.01,
        47.74
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.19
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.01
              },
              {
                "metric": "lcb_test_output",
                "score": 47.74
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 4.48,
        "code_avg": -3.46,
        "reasoning_avg": -34.93,
        "overall_avg": 3.25,
        "general_task_scores": [],
        "math_task_scores": [
          -8.72,
          -0.1,
          -1.0,
          3.33
        ],
        "code_task_scores": [
          -75.0,
          1.16,
          3.95,
          41.97,
          10.64
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey"
    },
    {
      "id": 83,
      "name": "zwhe99/DeepMath-103K",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 77.44,
      "code_avg": 17.68,
      "reasoning_avg": 0,
      "overall_avg": 47.56,
      "general_scores": [],
      "math_scores": [
        89.92,
        90.22,
        89.6,
        40.0
      ],
      "code_scores": [
        0.0,
        49.42,
        5.73,
        21.71,
        11.54
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.22
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.73
              },
              {
                "metric": "lcb_code_execution",
                "score": 21.71
              },
              {
                "metric": "lcb_test_output",
                "score": 11.54
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 26.97,
        "code_avg": -21.04,
        "reasoning_avg": -34.93,
        "overall_avg": 5.7,
        "general_task_scores": [],
        "math_task_scores": [
          2.58,
          25.34,
          22.2,
          33.33
        ],
        "code_task_scores": [
          -75.61,
          -22.18,
          -2.51,
          20.67,
          -25.56
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "zwhe99",
      "year": "2025",
      "size": "309k",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K"
    },
    {
      "id": 84,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 62.06,
      "code_avg": 21.59,
      "reasoning_avg": 0,
      "overall_avg": 41.82,
      "general_scores": [],
      "math_scores": [
        90.52,
        73.3,
        74.4,
        10.0
      ],
      "code_scores": [
        0.0,
        72.37,
        10.04,
        16.49,
        9.05
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.49
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 11.59,
        "code_avg": -17.13,
        "reasoning_avg": -34.93,
        "overall_avg": -0.03,
        "general_task_scores": [],
        "math_task_scores": [
          3.18,
          8.42,
          7.0,
          3.33
        ],
        "code_task_scores": [
          -75.61,
          0.77,
          1.8,
          15.45,
          -28.05
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1/viewer/default/train"
    },
    {
      "id": 85,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 59.62,
      "math_avg": 39.25,
      "code_avg": 43.41,
      "reasoning_avg": 39.61,
      "overall_avg": 45.47,
      "general_scores": [
        78.37,
        55.5525,
        57.7,
        46.9135714,
        78.18,
        54.8,
        57.8,
        47.03,
        78.33,
        54.2675,
        58.49,
        48.0242857
      ],
      "math_scores": [
        85.37,
        46.6,
        46.0,
        19.42,
        0.0,
        84.15,
        46.9,
        45.4,
        19.67,
        0.0,
        83.62,
        46.3,
        45.8,
        19.51,
        0.0
      ],
      "code_scores": [
        71.95,
        74.71,
        15.05,
        26.1,
        28.28,
        68.9,
        74.32,
        14.7,
        30.48,
        27.83,
        70.73,
        75.88,
        15.05,
        25.89,
        31.22
      ],
      "reasoning_scores": [
        58.31,
        67.03,
        0.38565217,
        39.12,
        47.12,
        66.11,
        0.37728261,
        39.36,
        50.85,
        67.07,
        0.38880435,
        39.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.87
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.38
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.49
              },
              {
                "metric": "lcb_test_output",
                "score": 29.11
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.09
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.31,
        "math_avg": -11.22,
        "code_avg": 4.69,
        "reasoning_avg": 4.68,
        "overall_avg": 3.61,
        "general_task_scores": [
          11.3,
          19.38,
          13.14,
          21.41
        ],
        "math_task_scores": [
          -2.96,
          -18.28,
          -21.67,
          -6.51,
          -6.67
        ],
        "code_task_scores": [
          -5.08,
          3.37,
          6.69,
          26.45,
          -7.99
        ],
        "reasoning_task_scores": [
          15.49,
          -2.72,
          -0.01,
          5.95
        ]
      },
      "affiliation": "Magpie-Align",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K"
    },
    {
      "id": 86,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 56.06,
      "code_avg": 23.65,
      "reasoning_avg": 0,
      "overall_avg": 39.85,
      "general_scores": [],
      "math_scores": [
        83.85,
        63.32,
        60.4,
        16.67
      ],
      "code_scores": [
        3.05,
        74.32,
        6.09,
        30.48,
        4.3
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.85
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 4.3
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": 5.59,
        "code_avg": -15.07,
        "reasoning_avg": -34.93,
        "overall_avg": -2.0,
        "general_task_scores": [],
        "math_task_scores": [
          -3.49,
          -1.56,
          -7.0,
          10.0
        ],
        "code_task_scores": [
          -72.56,
          2.72,
          -2.15,
          29.44,
          -32.8
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Magpie-Align",
      "year": "2025",
      "size": "250k",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ"
    },
    {
      "id": 87,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 24.99,
      "code_avg": 29.23,
      "reasoning_avg": 0,
      "overall_avg": 27.11,
      "general_scores": [],
      "math_scores": [
        30.63,
        34.72,
        34.6,
        0.0
      ],
      "code_scores": [
        0.0,
        71.6,
        9.68,
        27.77,
        37.1
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.63
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.77
              },
              {
                "metric": "lcb_test_output",
                "score": 37.1
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -43.31,
        "math_avg": -25.48,
        "code_avg": -9.49,
        "reasoning_avg": -34.93,
        "overall_avg": -14.75,
        "general_task_scores": [],
        "math_task_scores": [
          -56.71,
          -30.16,
          -32.8,
          -6.67
        ],
        "code_task_scores": [
          -75.61,
          0.0,
          1.44,
          26.73,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "KingNish",
      "year": "2024",
      "size": "19.9k",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k"
    },
    {
      "id": 88,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 45.57,
      "math_avg": 29.2,
      "code_avg": 36.02,
      "reasoning_avg": 34.68,
      "overall_avg": 36.37,
      "general_scores": [
        77.08,
        41.81,
        56.82,
        0.0,
        77.08,
        42.0225,
        56.4,
        0.0,
        77.09,
        42.1,
        56.59,
        19.8664286
      ],
      "math_scores": [
        19.41,
        40.04,
        44.0,
        27.3,
        13.33,
        22.74,
        42.14,
        42.8,
        27.42,
        16.67,
        21.61,
        38.42,
        41.2,
        27.53,
        13.33
      ],
      "code_scores": [
        38.41,
        71.98,
        10.39,
        44.47,
        11.09,
        43.9,
        73.15,
        9.32,
        45.09,
        15.61,
        36.59,
        71.98,
        11.83,
        44.47,
        11.99
      ],
      "reasoning_scores": [
        35.93,
        69.92,
        0.38347826,
        32.08,
        38.31,
        69.86,
        0.38391304,
        33.84,
        33.22,
        69.79,
        0.38576087,
        32.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.25
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.68
              },
              {
                "metric": "lcb_test_output",
                "score": 12.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.26,
        "math_avg": -21.27,
        "code_avg": -2.7,
        "reasoning_avg": -0.26,
        "overall_avg": -5.49,
        "general_task_scores": [
          10.09,
          6.49,
          11.74,
          -19.29
        ],
        "math_task_scores": [
          -66.09,
          -24.68,
          -24.73,
          1.38,
          7.77
        ],
        "code_task_scores": [
          -35.98,
          0.77,
          2.27,
          43.64,
          -24.2
        ],
        "reasoning_task_scores": [
          -0.78,
          0.4,
          -0.01,
          -0.64
        ]
      },
      "affiliation": "FreedomIntelligence",
      "year": "2024",
      "size": "19.7k",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT"
    }
  ]
}