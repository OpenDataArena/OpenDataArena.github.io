{
  "llama": [
    {
      "id": 1,
      "name": "meta-llama/Llama-3.1-8B-Instruct",
      "domain": "instruct",
      "general_avg": 56.54,
      "math_avg": 40.91,
      "code_avg": 39.84,
      "reasoning_avg": 42.87,
      "overall_avg": 45.04,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        80.9,
        60.325,
        40.98,
        43.96214
      ],
      "math_task_scores": [
        85.67,
        49.72,
        48.2,
        12.62,
        8.335
      ],
      "code_task_scores": [
        67.68,
        71.21,
        9.68,
        5.01,
        25.11,
        60.37
      ],
      "reasoning_task_scores": [
        79.32,
        62.85,
        24.24,
        0.42163,
        47.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.9
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 60.325
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96214
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.335
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.21
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.01
              },
              {
                "metric": "lcb_test_output",
                "score": 25.11
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 60.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42163
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.52
              }
            ]
          }
        ]
      }
    },
    {
      "id": 0,
      "name": "meta-llama/Llama-3.1-8B",
      "domain": "base",
      "general_avg": 39.05,
      "math_avg": 19.93,
      "code_avg": 19.47,
      "reasoning_avg": 39.02,
      "overall_avg": 29.37,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        64.37,
        20.3375,
        36.65,
        34.8271429
      ],
      "math_task_scores": [
        56.41,
        19.7,
        19.2,
        4.34,
        0.0
      ],
      "code_task_scores": [
        27.44,
        54.47,
        3.58,
        0.21,
        0.0,
        31.1
      ],
      "reasoning_task_scores": [
        80.34,
        62.56,
        29.8,
        0.312,
        22.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.37
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.3375
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.8271429
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.7
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 31.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.56
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.312
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 27.88,
      "math_avg": 13.79,
      "code_avg": 26.31,
      "reasoning_avg": 31.94,
      "overall_avg": 24.98,
      "overall_efficiency": -0.005418,
      "general_efficiency": -0.013802,
      "math_efficiency": -0.007586,
      "code_efficiency": 0.008465,
      "reasoning_efficiency": -0.008749,
      "general_scores": [
        45.88,
        30.76,
        29.51,
        5.28642857,
        42.43,
        34.1625,
        29.15,
        6.52857143,
        43.37,
        31.745,
        30.01,
        5.76714286
      ],
      "math_scores": [
        43.06,
        9.2,
        11.0,
        7.66,
        3.33,
        36.54,
        9.76,
        10.4,
        7.7,
        0.0,
        42.53,
        8.78,
        9.6,
        7.36,
        0.0
      ],
      "code_scores": [
        32.32,
        45.53,
        4.3,
        27.56,
        9.73,
        39.02,
        50.58,
        1.79,
        29.23,
        17.65,
        35.98,
        49.03,
        3.58,
        29.85,
        18.55
      ],
      "reasoning_scores": [
        66.44,
        49.22,
        0.28815217,
        13.68,
        61.69,
        46.13,
        0.29815217,
        18.24,
        63.39,
        48.34,
        0.31217391,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.56
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.88
              },
              {
                "metric": "lcb_test_output",
                "score": 15.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -11.16,
        "math_avg": -6.14,
        "code_avg": 6.85,
        "reasoning_avg": -7.08,
        "overall_avg": -4.38,
        "overall_efficiency": -0.005418,
        "general_efficiency": -0.013802,
        "math_efficiency": -0.007586,
        "code_efficiency": 0.008465,
        "reasoning_efficiency": -0.008749,
        "general_task_scores": [
          -20.48,
          11.88,
          -7.09,
          -28.97
        ],
        "math_task_scores": [
          -15.7,
          -10.45,
          -8.87,
          3.23,
          1.11
        ],
        "code_task_scores": [
          8.33,
          -6.09,
          -0.36,
          28.67,
          15.31
        ],
        "reasoning_task_scores": [
          -16.5,
          -14.66,
          -0.01,
          -6.35
        ]
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 34.07,
      "math_avg": 4.07,
      "code_avg": 23.98,
      "reasoning_avg": 32.65,
      "overall_avg": 23.69,
      "overall_efficiency": -0.109117,
      "general_efficiency": -0.095745,
      "math_efficiency": -0.305001,
      "code_efficiency": 0.08674,
      "reasoning_efficiency": -0.122462,
      "general_scores": [
        38.3,
        42.8125,
        33.56,
        16.8542857,
        48.65,
        44.035,
        35.03,
        12.1478571,
        49.07,
        36.945,
        35.66,
        15.7421429
      ],
      "math_scores": [
        1.82,
        4.62,
        4.6,
        6.05,
        0.0,
        2.58,
        4.74,
        4.8,
        6.01,
        3.33,
        4.17,
        4.64,
        4.0,
        6.35,
        3.33
      ],
      "code_scores": [
        32.32,
        45.91,
        3.23,
        24.84,
        5.43,
        30.49,
        50.19,
        2.51,
        27.56,
        18.1,
        29.27,
        48.64,
        3.94,
        28.18,
        9.05
      ],
      "reasoning_scores": [
        69.83,
        37.81,
        0.3026087,
        20.0,
        70.85,
        43.84,
        0.29945652,
        20.64,
        69.15,
        38.37,
        0.30956522,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.86
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.35
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.98,
        "math_avg": -15.86,
        "code_avg": 4.51,
        "reasoning_avg": -6.37,
        "overall_avg": -5.67,
        "overall_efficiency": -0.109117,
        "general_efficiency": -0.095745,
        "math_efficiency": -0.305001,
        "code_efficiency": 0.08674,
        "reasoning_efficiency": -0.122462,
        "general_task_scores": [
          -19.03,
          20.92,
          -1.9,
          -19.92
        ],
        "math_task_scores": [
          -53.55,
          -15.03,
          -14.73,
          1.8,
          2.22
        ],
        "code_task_scores": [
          3.25,
          -6.22,
          -0.35,
          26.65,
          10.86
        ],
        "reasoning_task_scores": [
          -10.4,
          -22.55,
          -0.01,
          -1.73
        ]
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 32.52,
      "math_avg": 7.57,
      "code_avg": 28.28,
      "reasoning_avg": 28.52,
      "overall_avg": 24.23,
      "overall_efficiency": -0.098839,
      "general_efficiency": -0.12544,
      "math_efficiency": -0.237657,
      "code_efficiency": 0.169557,
      "reasoning_efficiency": -0.201817,
      "general_scores": [
        25.39,
        41.765,
        33.13,
        12.5321429,
        47.94,
        41.5825,
        33.19,
        13.2785714,
        50.1,
        42.3275,
        34.28,
        14.7607143
      ],
      "math_scores": [
        18.04,
        8.54,
        8.8,
        9.35,
        0.0,
        14.86,
        7.06,
        7.4,
        9.98,
        0.0,
        8.72,
        6.76,
        6.0,
        8.06,
        0.0
      ],
      "code_scores": [
        37.2,
        52.14,
        2.51,
        23.8,
        20.36,
        41.46,
        53.7,
        3.58,
        21.92,
        20.36,
        42.68,
        56.81,
        6.09,
        21.29,
        20.36
      ],
      "reasoning_scores": [
        54.58,
        37.05,
        0.32521739,
        23.36,
        55.93,
        29.86,
        0.3526087,
        22.0,
        56.27,
        39.27,
        0.32434783,
        22.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.45
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.34
              },
              {
                "metric": "lcb_test_output",
                "score": 20.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.52,
        "math_avg": -12.36,
        "code_avg": 8.82,
        "reasoning_avg": -10.49,
        "overall_avg": -5.14,
        "overall_efficiency": -0.098839,
        "general_efficiency": -0.12544,
        "math_efficiency": -0.237657,
        "code_efficiency": 0.169557,
        "reasoning_efficiency": -0.201817,
        "general_task_scores": [
          -23.23,
          21.55,
          -3.12,
          -21.31
        ],
        "math_task_scores": [
          -42.54,
          -12.25,
          -11.8,
          4.79,
          0.0
        ],
        "code_task_scores": [
          13.01,
          -0.25,
          0.48,
          22.13,
          20.36
        ],
        "reasoning_task_scores": [
          -24.75,
          -27.17,
          0.02,
          0.69
        ]
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 33.77,
      "math_avg": 6.33,
      "code_avg": 16.73,
      "reasoning_avg": 22.82,
      "overall_avg": 19.91,
      "overall_efficiency": -0.629794,
      "general_efficiency": -0.351623,
      "math_efficiency": -0.906135,
      "code_efficiency": -0.182178,
      "reasoning_efficiency": -1.079237,
      "general_scores": [
        67.24,
        38.7275,
        23.84,
        3.29928571,
        68.3,
        37.7775,
        27.33,
        2.54714286,
        66.8,
        42.2075,
        24.01,
        3.13642857
      ],
      "math_scores": [
        11.52,
        6.46,
        7.6,
        8.69,
        0.0,
        10.92,
        6.98,
        6.4,
        7.9,
        0.0,
        5.53,
        6.5,
        8.0,
        8.42,
        0.0
      ],
      "code_scores": [
        6.71,
        54.86,
        2.87,
        6.47,
        4.75,
        1.22,
        55.64,
        2.87,
        17.95,
        2.26,
        21.95,
        56.03,
        1.43,
        14.61,
        1.36
      ],
      "reasoning_scores": [
        20.34,
        52.1,
        0.33684783,
        19.36,
        30.51,
        49.95,
        0.3623913,
        18.4,
        19.32,
        43.9,
        0.35630435,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.99
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.96
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.01
              },
              {
                "metric": "lcb_test_output",
                "score": 2.79
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.28,
        "math_avg": -13.6,
        "code_avg": -2.73,
        "reasoning_avg": -16.2,
        "overall_avg": -9.45,
        "overall_efficiency": -0.629794,
        "general_efficiency": -0.351623,
        "math_efficiency": -0.906135,
        "code_efficiency": -0.182178,
        "reasoning_efficiency": -1.079237,
        "general_task_scores": [
          3.08,
          19.23,
          -11.59,
          -31.84
        ],
        "math_task_scores": [
          -47.09,
          -13.05,
          -11.87,
          4.0,
          0.0
        ],
        "code_task_scores": [
          -17.48,
          1.04,
          -1.19,
          12.8,
          2.79
        ],
        "reasoning_task_scores": [
          -56.95,
          -13.91,
          0.04,
          -3.2
        ]
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 42.24,
      "math_avg": 21.85,
      "code_avg": 30.9,
      "reasoning_avg": 35.27,
      "overall_avg": 32.57,
      "overall_efficiency": 0.045752,
      "general_efficiency": 0.045671,
      "math_efficiency": 0.027445,
      "code_efficiency": 0.163391,
      "reasoning_efficiency": -0.053501,
      "general_scores": [
        61.23,
        49.03,
        33.62,
        22.8685714,
        60.34,
        49.6325,
        34.8,
        23.5921429,
        62.71,
        48.0125,
        35.05,
        26.0314286
      ],
      "math_scores": [
        38.59,
        13.14,
        13.8,
        42.99,
        13.18,
        13.8,
        34.42,
        12.74,
        14.0
      ],
      "code_scores": [
        42.68,
        57.98,
        7.89,
        30.06,
        21.27,
        44.51,
        53.7,
        6.45,
        31.52,
        12.67,
        39.63,
        55.64,
        6.09,
        28.81,
        24.66
      ],
      "reasoning_scores": [
        60.68,
        59.9,
        0.33619565,
        24.4,
        55.93,
        57.8,
        0.3301087,
        23.04,
        58.31,
        58.85,
        0.34380435,
        23.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.13
              },
              {
                "metric": "lcb_test_output",
                "score": 19.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.2,
        "math_avg": 1.92,
        "code_avg": 11.44,
        "reasoning_avg": -3.75,
        "overall_avg": 3.2,
        "overall_efficiency": 0.045752,
        "general_efficiency": 0.045671,
        "math_efficiency": 0.027445,
        "code_efficiency": 0.163391,
        "reasoning_efficiency": -0.053501,
        "general_task_scores": [
          -2.94,
          28.55,
          -2.16,
          -10.67
        ],
        "math_task_scores": [
          -17.74,
          -6.68,
          -5.33
        ],
        "code_task_scores": [
          14.83,
          1.3,
          3.23,
          29.92,
          19.53
        ],
        "reasoning_task_scores": [
          -22.03,
          -3.71,
          0.03,
          1.52
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 38.04,
      "math_avg": 15.47,
      "code_avg": 28.16,
      "reasoning_avg": 37.96,
      "overall_avg": 29.91,
      "overall_efficiency": 0.003797,
      "general_efficiency": -0.007018,
      "math_efficiency": -0.031189,
      "code_efficiency": 0.060801,
      "reasoning_efficiency": -0.007407,
      "general_scores": [
        41.16,
        47.9275,
        35.71,
        26.5814286,
        49.22,
        49.1075,
        33.58,
        20.6507143,
        45.08,
        49.3525,
        34.68,
        23.4607143
      ],
      "math_scores": [
        45.11,
        12.7,
        13.8,
        8.11,
        3.33,
        41.09,
        12.92,
        11.8,
        7.72,
        0.0,
        40.41,
        12.96,
        14.4,
        7.7,
        0.0
      ],
      "code_scores": [
        43.9,
        55.64,
        5.02,
        12.32,
        18.33,
        50.61,
        52.53,
        3.23,
        18.16,
        17.42,
        45.12,
        57.59,
        5.73,
        22.34,
        14.48
      ],
      "reasoning_scores": [
        71.86,
        54.41,
        0.31336957,
        24.72,
        69.83,
        55.51,
        0.35206522,
        25.28,
        70.51,
        56.47,
        0.33565217,
        25.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.61
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.0,
        "math_avg": -4.46,
        "code_avg": 8.69,
        "reasoning_avg": -1.06,
        "overall_avg": 0.54,
        "overall_efficiency": 0.003797,
        "general_efficiency": -0.007018,
        "math_efficiency": -0.031189,
        "code_efficiency": 0.060801,
        "reasoning_efficiency": -0.007407,
        "general_task_scores": [
          -19.22,
          28.46,
          -1.99,
          -11.27
        ],
        "math_task_scores": [
          -14.21,
          -6.84,
          -5.87,
          3.5,
          1.11
        ],
        "code_task_scores": [
          19.1,
          0.78,
          1.08,
          17.4,
          16.74
        ],
        "reasoning_task_scores": [
          -9.61,
          -7.1,
          0.02,
          3.23
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 30.01,
      "math_avg": 5.61,
      "code_avg": 23.54,
      "reasoning_avg": 27.38,
      "overall_avg": 21.64,
      "overall_efficiency": -7.129951,
      "general_efficiency": -8.33336,
      "math_efficiency": -13.207872,
      "code_efficiency": 3.755842,
      "reasoning_efficiency": -10.734417,
      "general_scores": [
        66.09,
        29.2525,
        24.29,
        0.24857143,
        66.03,
        30.36,
        23.85,
        0.17142857,
        65.83,
        29.9575,
        23.84,
        0.23357143
      ],
      "math_scores": [
        7.73,
        5.6,
        4.0,
        10.52,
        0.0,
        7.73,
        5.6,
        4.6,
        10.46,
        0.0,
        7.51,
        5.76,
        4.2,
        10.48,
        0.0
      ],
      "code_scores": [
        40.24,
        59.14,
        0.36,
        17.95,
        0.45,
        40.85,
        58.75,
        0.36,
        18.16,
        0.23,
        38.41,
        58.37,
        0.36,
        19.21,
        0.23
      ],
      "reasoning_scores": [
        43.05,
        61.43,
        0.27391304,
        5.68,
        42.37,
        61.26,
        0.27641304,
        5.6,
        42.03,
        61.3,
        0.27717391,
        5.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.44
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.03,
        "math_avg": -14.32,
        "code_avg": 4.07,
        "reasoning_avg": -11.64,
        "overall_avg": -7.73,
        "overall_efficiency": -7.129951,
        "general_efficiency": -8.33336,
        "math_efficiency": -13.207872,
        "code_efficiency": 3.755842,
        "reasoning_efficiency": -10.734417,
        "general_task_scores": [
          1.61,
          9.52,
          -12.66,
          -34.61
        ],
        "math_task_scores": [
          -48.75,
          -14.05,
          -14.93,
          6.15,
          0.0
        ],
        "code_task_scores": [
          12.39,
          4.28,
          -3.22,
          18.23,
          0.3
        ],
        "reasoning_task_scores": [
          -37.86,
          -1.23,
          -0.03,
          -16.64
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 8,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 47.13,
      "math_avg": 23.98,
      "code_avg": 30.67,
      "reasoning_avg": 41.66,
      "overall_avg": 35.86,
      "overall_efficiency": 0.006533,
      "general_efficiency": 0.008129,
      "math_efficiency": 0.004076,
      "code_efficiency": 0.01127,
      "reasoning_efficiency": 0.002657,
      "general_scores": [
        70.08,
        54.355,
        37.42,
        28.1542857,
        68.42,
        53.94,
        38.16,
        25.3285714,
        69.38,
        55.04,
        39.08,
        28.6621429,
        67.95,
        54.7825,
        38.46,
        24.82
      ],
      "math_scores": [
        47.84,
        30.1,
        34.2,
        13.37,
        0.0,
        12.92,
        51.78,
        31.4,
        30.6,
        14.23,
        0.0,
        47.46,
        28.44,
        28.6,
        12.76,
        0.0
      ],
      "code_scores": [
        45.73,
        48.64,
        7.53,
        18.37,
        29.64,
        31.1,
        46.34,
        45.73,
        50.58,
        6.45,
        13.15,
        26.7,
        40.85,
        49.42,
        6.81,
        29.44,
        24.89
      ],
      "reasoning_scores": [
        75.93,
        62.69,
        0.34293478,
        31.28,
        76.95,
        62.39,
        25.25,
        0.35608696,
        32.96,
        77.97,
        60.88,
        0.36043478,
        31.28,
        74.92,
        62.07,
        0.3451087,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.32
              },
              {
                "metric": "lcb_test_output",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 31.1
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.01
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.94
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.08,
        "math_avg": 4.05,
        "code_avg": 11.2,
        "reasoning_avg": 2.64,
        "overall_avg": 6.49,
        "overall_efficiency": 0.006533,
        "general_efficiency": 0.008129,
        "math_efficiency": 0.004076,
        "code_efficiency": 0.01127,
        "reasoning_efficiency": 0.002657,
        "general_task_scores": [
          4.59,
          34.19,
          1.63,
          -8.09
        ],
        "math_task_scores": [
          -7.38,
          10.28,
          11.93,
          8.98,
          0.0
        ],
        "code_task_scores": [
          17.22,
          -4.92,
          3.35,
          20.11,
          27.08,
          0.0
        ],
        "reasoning_task_scores": [
          -3.9,
          -0.55,
          -4.55,
          0.04,
          9.86
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 44.34,
      "math_avg": 20.21,
      "code_avg": 30.31,
      "reasoning_avg": 36.93,
      "overall_avg": 32.95,
      "overall_efficiency": 0.003577,
      "general_efficiency": 0.005287,
      "math_efficiency": 0.000279,
      "code_efficiency": 0.010827,
      "reasoning_efficiency": -0.002083,
      "general_scores": [
        64.06,
        50.9275,
        39.9,
        25.0585714,
        58.97,
        52.215,
        39.98,
        24.3621429,
        60.95,
        50.3825,
        39.81,
        25.4792857
      ],
      "math_scores": [
        60.2,
        17.3,
        19.8,
        11.72,
        0.0,
        60.5,
        13.56,
        11.0,
        10.28,
        0.0,
        60.12,
        14.62,
        14.4,
        9.64,
        0.0
      ],
      "code_scores": [
        46.95,
        52.53,
        6.09,
        29.65,
        15.16,
        49.39,
        52.53,
        3.58,
        27.56,
        16.29,
        50.0,
        56.42,
        4.66,
        27.56,
        16.29
      ],
      "reasoning_scores": [
        76.61,
        54.92,
        0.38608696,
        15.52,
        79.32,
        54.08,
        0.3676087,
        17.76,
        78.98,
        54.66,
        0.34847826,
        10.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.26
              },
              {
                "metric": "lcb_test_output",
                "score": 15.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.3,
        "math_avg": 0.28,
        "code_avg": 10.84,
        "reasoning_avg": -2.09,
        "overall_avg": 3.58,
        "overall_efficiency": 0.003577,
        "general_efficiency": 0.005287,
        "math_efficiency": 0.000279,
        "code_efficiency": 0.010827,
        "reasoning_efficiency": -0.002083,
        "general_task_scores": [
          -3.04,
          30.84,
          3.25,
          -9.86
        ],
        "math_task_scores": [
          3.86,
          -4.54,
          -4.13,
          6.21,
          0.0
        ],
        "code_task_scores": [
          21.34,
          -0.64,
          1.2,
          28.05,
          15.91
        ],
        "reasoning_task_scores": [
          -2.04,
          -8.01,
          0.06,
          -7.57
        ]
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 9.56,
      "math_avg": 0.72,
      "code_avg": 5.23,
      "reasoning_avg": 10.96,
      "overall_avg": 6.62,
      "overall_efficiency": -0.275914,
      "general_efficiency": -0.357663,
      "math_efficiency": -0.232971,
      "code_efficiency": -0.172637,
      "reasoning_efficiency": -0.340388,
      "general_scores": [
        17.68,
        3.62,
        7.94071429,
        5.56,
        8.61,
        12.9621429,
        17.73,
        3.9,
        8.04428571
      ],
      "math_scores": [
        0.61,
        0.92,
        0.6,
        0.0,
        0.38,
        1.84,
        1.6,
        0.0,
        0.38,
        1.36,
        1.0,
        0.0
      ],
      "code_scores": [
        7.93,
        14.79,
        0.0,
        3.76,
        0.0,
        9.15,
        19.46,
        0.0,
        0.21,
        0.0,
        6.1,
        17.12,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        12.2,
        22.01,
        0.25108696,
        7.68,
        15.59,
        22.93,
        0.25021739,
        7.36,
        22.71,
        16.11,
        0.23391304,
        4.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.32
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.83
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -29.49,
        "math_avg": -19.21,
        "code_avg": -14.23,
        "reasoning_avg": -28.06,
        "overall_avg": -22.75,
        "overall_efficiency": -0.275914,
        "general_efficiency": -0.357663,
        "math_efficiency": -0.232971,
        "code_efficiency": -0.172637,
        "reasoning_efficiency": -0.340388,
        "general_task_scores": [
          -50.71,
          -31.27,
          -25.18
        ],
        "math_task_scores": [
          -55.95,
          -18.33,
          -18.13,
          0.0
        ],
        "code_task_scores": [
          -19.71,
          -37.35,
          -3.58,
          1.11,
          0.0
        ],
        "reasoning_task_scores": [
          -63.51,
          -42.21,
          -0.06,
          -15.68
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 47.31,
      "math_avg": 29.07,
      "code_avg": 35.33,
      "reasoning_avg": 36.08,
      "overall_avg": 36.95,
      "overall_efficiency": 0.008072,
      "general_efficiency": 0.008795,
      "math_efficiency": 0.009727,
      "code_efficiency": 0.01689,
      "reasoning_efficiency": -0.003125,
      "general_scores": [
        65.4,
        72.9025,
        36.45,
        18.58,
        59.28,
        71.025,
        37.68,
        28.5221429,
        48.68,
        69.4225,
        34.94,
        24.8178571
      ],
      "math_scores": [
        73.39,
        30.14,
        31.0,
        13.98,
        0.0,
        73.77,
        28.76,
        29.4,
        13.89,
        0.0,
        73.16,
        27.5,
        27.8,
        13.21,
        0.0
      ],
      "code_scores": [
        64.02,
        57.59,
        5.02,
        36.53,
        18.33,
        60.98,
        57.98,
        7.53,
        29.02,
        18.1,
        61.59,
        53.31,
        3.23,
        32.78,
        23.98
      ],
      "reasoning_scores": [
        53.56,
        56.83,
        0.39130435,
        24.72,
        64.07,
        56.42,
        0.36663043,
        27.6,
        64.07,
        57.98,
        0.35043478,
        26.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 71.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 20.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.26,
        "math_avg": 9.14,
        "code_avg": 15.87,
        "reasoning_avg": -2.94,
        "overall_avg": 7.58,
        "overall_efficiency": 0.008072,
        "general_efficiency": 0.008795,
        "math_efficiency": 0.009727,
        "code_efficiency": 0.01689,
        "reasoning_efficiency": -0.003125,
        "general_task_scores": [
          -6.58,
          50.78,
          -0.29,
          -10.86
        ],
        "math_task_scores": [
          17.03,
          9.1,
          10.2,
          9.35,
          0.0
        ],
        "code_task_scores": [
          34.76,
          1.82,
          1.68,
          32.57,
          20.14
        ],
        "reasoning_task_scores": [
          -19.77,
          -5.48,
          0.06,
          4.24
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 28.38,
      "math_avg": 3.6,
      "code_avg": 6.79,
      "reasoning_avg": 27.09,
      "overall_avg": 16.47,
      "overall_efficiency": -2.217984,
      "general_efficiency": -1.834276,
      "math_efficiency": -2.807887,
      "code_efficiency": -2.17916,
      "reasoning_efficiency": -2.050611,
      "general_scores": [
        67.91,
        14.0225,
        30.84,
        0.02928571,
        67.22,
        14.905,
        32.86,
        0.00857143,
        67.01,
        13.04,
        32.64,
        0.05071429
      ],
      "math_scores": [
        9.86,
        4.52,
        1.8,
        6.71,
        0.0,
        2.43,
        2.68,
        3.4,
        6.82,
        0.0,
        3.94,
        3.1,
        2.2,
        6.53,
        0.0
      ],
      "code_scores": [
        0.0,
        49.03,
        0.0,
        1.25,
        0.0,
        0.0,
        9.73,
        0.0,
        0.21,
        0.0,
        0.0,
        41.25,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        65.08,
        40.3,
        0.22956522,
        3.12,
        66.44,
        37.15,
        0.24478261,
        3.12,
        64.41,
        41.17,
        0.24021739,
        3.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -10.67,
        "math_avg": -16.33,
        "code_avg": -12.67,
        "reasoning_avg": -11.93,
        "overall_avg": -12.9,
        "overall_efficiency": -2.217984,
        "general_efficiency": -1.834276,
        "math_efficiency": -2.807887,
        "code_efficiency": -2.17916,
        "reasoning_efficiency": -2.050611,
        "general_task_scores": [
          3.01,
          -6.35,
          -4.54,
          -34.8
        ],
        "math_task_scores": [
          -51.0,
          -16.27,
          -16.73,
          2.35,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -21.13,
          -3.58,
          0.42,
          0.0
        ],
        "reasoning_task_scores": [
          -15.03,
          -23.02,
          -0.07,
          -18.8
        ]
      },
      "affiliation": "eddie",
      "year": "2023",
      "size": "5816",
      "size_precise": "5816",
      "link": "https://huggingface.co/datasets/causal-lm/auto_cot",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 35.36,
      "math_avg": 3.85,
      "code_avg": 23.01,
      "reasoning_avg": 33.28,
      "overall_avg": 23.87,
      "overall_efficiency": -0.006159,
      "general_efficiency": -0.004138,
      "math_efficiency": -0.018032,
      "code_efficiency": 0.003968,
      "reasoning_efficiency": -0.006436,
      "general_scores": [
        62.76,
        36.08,
        35.07,
        0.0,
        69.32,
        34.5625,
        36.44,
        11.3207143,
        66.29,
        36.8025,
        35.63,
        0.0
      ],
      "math_scores": [
        3.94,
        4.02,
        4.0,
        8.36,
        0.0,
        4.17,
        3.8,
        3.2,
        7.68,
        0.0,
        2.65,
        4.5,
        3.8,
        7.61,
        0.0
      ],
      "code_scores": [
        22.56,
        41.63,
        2.51,
        26.93,
        19.23,
        25.0,
        43.58,
        2.15,
        29.02,
        22.85,
        25.0,
        41.25,
        2.15,
        26.51,
        14.71
      ],
      "reasoning_scores": [
        76.27,
        50.26,
        0.33782609,
        5.92,
        74.24,
        50.79,
        0.36076087,
        5.36,
        76.95,
        51.53,
        0.36576087,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.88
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.27
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.49
              },
              {
                "metric": "lcb_test_output",
                "score": 18.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.69,
        "math_avg": -16.08,
        "code_avg": 3.54,
        "reasoning_avg": -5.74,
        "overall_avg": -5.49,
        "overall_efficiency": -0.006159,
        "general_efficiency": -0.004138,
        "math_efficiency": -0.018032,
        "code_efficiency": 0.003968,
        "reasoning_efficiency": -0.006436,
        "general_task_scores": [
          1.75,
          15.48,
          -0.94,
          -31.06
        ],
        "math_task_scores": [
          -52.82,
          -15.59,
          -15.53,
          3.54,
          0.0
        ],
        "code_task_scores": [
          -3.25,
          -12.32,
          -1.31,
          27.28,
          18.93
        ],
        "reasoning_task_scores": [
          -4.52,
          -11.7,
          0.04,
          -16.0
        ]
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 24.29,
      "math_avg": 21.25,
      "code_avg": 30.15,
      "reasoning_avg": 36.58,
      "overall_avg": 28.07,
      "overall_efficiency": -0.129695,
      "general_efficiency": -1.475616,
      "math_efficiency": 0.131733,
      "code_efficiency": 1.0686,
      "reasoning_efficiency": -0.243496,
      "general_scores": [
        26.03,
        34.355,
        38.69,
        0.0,
        35.82,
        35.99,
        38.33,
        0.0,
        8.33,
        35.845,
        38.09,
        0.0
      ],
      "math_scores": [
        52.31,
        18.02,
        18.8,
        9.55,
        3.33,
        56.1,
        19.98,
        24.4,
        9.71,
        0.0,
        52.39,
        19.14,
        25.0,
        9.98,
        0.0
      ],
      "code_scores": [
        48.17,
        57.59,
        6.81,
        19.21,
        26.24,
        48.17,
        57.59,
        8.96,
        11.06,
        27.6,
        47.56,
        57.59,
        7.89,
        2.51,
        25.34
      ],
      "reasoning_scores": [
        52.88,
        59.83,
        0.26684783,
        30.0,
        60.0,
        58.97,
        0.27586957,
        29.52,
        57.29,
        60.5,
        0.26858696,
        29.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.37
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.05
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.97
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.59
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 10.93
              },
              {
                "metric": "lcb_test_output",
                "score": 26.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.72
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.57
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -14.76,
        "math_avg": 1.32,
        "code_avg": 10.69,
        "reasoning_avg": -2.43,
        "overall_avg": -1.3,
        "overall_efficiency": -0.129695,
        "general_efficiency": -1.475616,
        "math_efficiency": 0.131733,
        "code_efficiency": 1.0686,
        "reasoning_efficiency": -0.243496,
        "general_task_scores": [
          -40.98,
          15.06,
          1.72,
          -34.83
        ],
        "math_task_scores": [
          -2.81,
          -0.65,
          3.53,
          5.41,
          1.11
        ],
        "code_task_scores": [
          20.53,
          3.12,
          4.31,
          10.72,
          26.39
        ],
        "reasoning_task_scores": [
          -23.62,
          -2.79,
          -0.04,
          7.49
        ]
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 31.99,
      "math_avg": 4.2,
      "code_avg": 21.3,
      "reasoning_avg": 25.02,
      "overall_avg": 20.63,
      "overall_efficiency": -0.01752,
      "general_efficiency": -0.014155,
      "math_efficiency": -0.031531,
      "code_efficiency": 0.003671,
      "reasoning_efficiency": -0.028066,
      "general_scores": [
        43.88,
        40.2925,
        31.15,
        15.5421429,
        43.7,
        40.24,
        31.13,
        15.1221429,
        36.21,
        41.0375,
        30.46,
        15.0628571
      ],
      "math_scores": [
        9.86,
        3.86,
        2.6,
        5.06,
        0.0,
        6.82,
        3.76,
        5.0,
        5.69,
        0.0,
        10.16,
        3.7,
        1.6,
        4.92,
        0.0
      ],
      "code_scores": [
        25.61,
        38.52,
        3.23,
        22.76,
        13.12,
        29.88,
        28.79,
        3.23,
        25.68,
        19.68,
        31.71,
        31.13,
        3.23,
        22.76,
        20.14
      ],
      "reasoning_scores": [
        60.34,
        25.85,
        0.32641304,
        19.84,
        59.66,
        27.84,
        0.32673913,
        7.28,
        58.31,
        22.61,
        0.32184783,
        17.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.95
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 29.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.81
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.73
              },
              {
                "metric": "lcb_test_output",
                "score": 17.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.06,
        "math_avg": -15.73,
        "code_avg": 1.83,
        "reasoning_avg": -14.0,
        "overall_avg": -8.74,
        "overall_efficiency": -0.01752,
        "general_efficiency": -0.014155,
        "math_efficiency": -0.031531,
        "code_efficiency": 0.003671,
        "reasoning_efficiency": -0.028066,
        "general_task_scores": [
          -23.11,
          20.18,
          -5.74,
          -19.59
        ],
        "math_task_scores": [
          -47.46,
          -15.93,
          -16.13,
          0.88,
          0.0
        ],
        "code_task_scores": [
          1.63,
          -21.66,
          -0.35,
          23.52,
          17.65
        ],
        "reasoning_task_scores": [
          -20.9,
          -37.13,
          0.01,
          -7.2
        ]
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 28.98,
      "math_avg": 10.1,
      "code_avg": 9.22,
      "reasoning_avg": 14.28,
      "overall_avg": 15.65,
      "overall_efficiency": -0.101901,
      "general_efficiency": -0.074756,
      "math_efficiency": -0.072996,
      "code_efficiency": -0.076084,
      "reasoning_efficiency": -0.183767,
      "general_scores": [
        62.3,
        28.2,
        24.45,
        1.09357143,
        61.21,
        29.2975,
        23.76,
        1.555
      ],
      "math_scores": [
        20.09,
        12.0,
        10.4,
        6.53,
        0.0,
        21.76,
        12.42,
        11.2,
        6.64,
        0.0
      ],
      "code_scores": [
        9.76,
        38.91,
        0.0,
        0.0,
        0.0,
        13.41,
        29.96,
        0.0,
        0.21,
        0.0
      ],
      "reasoning_scores": [
        29.15,
        24.08,
        0.2076087,
        3.68,
        29.15,
        24.5,
        0.20478261,
        3.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.75
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.58
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 34.44
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.1
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.29
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -10.06,
        "math_avg": -9.83,
        "code_avg": -10.24,
        "reasoning_avg": -24.74,
        "overall_avg": -13.72,
        "overall_efficiency": -0.101901,
        "general_efficiency": -0.074756,
        "math_efficiency": -0.072996,
        "code_efficiency": -0.076084,
        "reasoning_efficiency": -0.183767,
        "general_task_scores": [
          -2.61,
          8.41,
          -12.55,
          -33.51
        ],
        "math_task_scores": [
          -35.49,
          -7.49,
          -8.4,
          2.24,
          0.0
        ],
        "code_task_scores": [
          -15.86,
          -20.03,
          -3.58,
          -0.11,
          0.0
        ],
        "reasoning_task_scores": [
          -51.19,
          -38.27,
          -0.1,
          -18.6
        ]
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 24.15,
      "math_avg": 1.18,
      "code_avg": 4.68,
      "reasoning_avg": 15.67,
      "overall_avg": 11.42,
      "overall_efficiency": -2.401512,
      "general_efficiency": -1.99269,
      "math_efficiency": -2.509389,
      "code_efficiency": -1.979035,
      "reasoning_efficiency": -3.124932,
      "general_scores": [
        68.66,
        19.3875,
        9.05,
        0.09285714,
        70.04,
        18.1275,
        8.25,
        0.13,
        70.38,
        16.5275,
        9.12,
        0.09214286
      ],
      "math_scores": [
        0.0,
        0.2,
        0.0,
        5.67,
        0.0,
        0.0,
        0.16,
        0.0,
        5.94,
        0.0,
        0.0,
        0.2,
        0.0,
        5.49,
        0.0
      ],
      "code_scores": [
        0.0,
        24.51,
        0.0,
        0.42,
        0.0,
        0.0,
        14.79,
        0.0,
        0.42,
        0.0,
        0.0,
        29.18,
        0.0,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        12.88,
        50.39,
        0.12402174,
        0.24,
        12.2,
        49.48,
        0.11141304,
        0.08,
        12.54,
        49.66,
        0.12402174,
        0.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 8.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.7
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.56
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -14.89,
        "math_avg": -18.75,
        "code_avg": -14.79,
        "reasoning_avg": -23.35,
        "overall_avg": -17.95,
        "overall_efficiency": -2.401512,
        "general_efficiency": -1.99269,
        "math_efficiency": -2.509389,
        "code_efficiency": -1.979035,
        "reasoning_efficiency": -3.124932,
        "general_task_scores": [
          5.32,
          -2.33,
          -27.84,
          -34.73
        ],
        "math_task_scores": [
          -56.41,
          -19.51,
          -19.2,
          1.36,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -31.64,
          -3.58,
          0.35,
          0.0
        ],
        "reasoning_task_scores": [
          -67.8,
          -12.72,
          -0.19,
          -21.92
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 29.85,
      "math_avg": 17.08,
      "code_avg": 12.59,
      "reasoning_avg": 24.55,
      "overall_avg": 21.02,
      "overall_efficiency": -1.113327,
      "general_efficiency": -1.226746,
      "math_efficiency": -0.379822,
      "code_efficiency": -0.917334,
      "reasoning_efficiency": -1.929408,
      "general_scores": [
        74.19,
        24.5925,
        23.55,
        0.11428571,
        71.15,
        24.1725,
        21.53,
        0.165,
        73.29,
        22.2625,
        23.13,
        0.0
      ],
      "math_scores": [
        41.24,
        18.72,
        19.0,
        9.67,
        0.0,
        39.42,
        16.32,
        18.6,
        8.85,
        0.0,
        38.97,
        18.58,
        18.0,
        8.85,
        0.0
      ],
      "code_scores": [
        0.61,
        54.86,
        0.72,
        7.72,
        2.26,
        1.22,
        52.92,
        1.43,
        2.09,
        2.04,
        1.83,
        56.03,
        1.43,
        2.51,
        1.13
      ],
      "reasoning_scores": [
        42.37,
        56.37,
        0.17293478,
        3.44,
        38.31,
        52.85,
        0.1675,
        6.88,
        35.93,
        54.25,
        0.15358696,
        3.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.88
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.22
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.19
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.11
              },
              {
                "metric": "lcb_test_output",
                "score": 1.81
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.87
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.2,
        "math_avg": -2.85,
        "code_avg": -6.88,
        "reasoning_avg": -14.47,
        "overall_avg": -8.35,
        "overall_efficiency": -1.113327,
        "general_efficiency": -1.226746,
        "math_efficiency": -0.379822,
        "code_efficiency": -0.917334,
        "reasoning_efficiency": -1.929408,
        "general_task_scores": [
          8.51,
          3.34,
          -13.91,
          -34.74
        ],
        "math_task_scores": [
          -16.53,
          -1.83,
          -0.67,
          4.78,
          0.0
        ],
        "code_task_scores": [
          -26.22,
          0.13,
          -2.39,
          3.9,
          1.81
        ],
        "reasoning_task_scores": [
          -41.47,
          -8.07,
          -0.15,
          -17.41
        ]
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 32.19,
      "math_avg": 42.14,
      "code_avg": 11.81,
      "reasoning_avg": 21.93,
      "overall_avg": 27.01,
      "overall_efficiency": -0.00235,
      "general_efficiency": -0.006858,
      "math_efficiency": 0.022205,
      "code_efficiency": -0.007659,
      "reasoning_efficiency": -0.017091,
      "general_scores": [
        70.44,
        24.0,
        31.77,
        0.05285714,
        74.18,
        28.025,
        30.4,
        0.04642857,
        69.91,
        26.545,
        30.68,
        0.20142857
      ],
      "math_scores": [
        73.54,
        56.88,
        54.6,
        18.27,
        3.33,
        74.75,
        57.16,
        56.0,
        18.9,
        3.33,
        81.35,
        58.18,
        57.4,
        18.34,
        0.0
      ],
      "code_scores": [
        9.76,
        40.86,
        0.72,
        0.21,
        4.98,
        9.15,
        37.74,
        1.79,
        0.0,
        9.5,
        14.63,
        36.58,
        2.15,
        0.0,
        9.05
      ],
      "reasoning_scores": [
        50.51,
        24.09,
        0.18771739,
        7.12,
        54.24,
        32.11,
        0.20554348,
        7.52,
        48.47,
        32.63,
        0.20445652,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.86,
        "math_avg": 22.21,
        "code_avg": -7.66,
        "reasoning_avg": -17.09,
        "overall_avg": -2.35,
        "overall_efficiency": -0.00235,
        "general_efficiency": -0.006858,
        "math_efficiency": 0.022205,
        "code_efficiency": -0.007659,
        "reasoning_efficiency": -0.017091,
        "general_task_scores": [
          7.14,
          5.85,
          -5.7,
          -34.73
        ],
        "math_task_scores": [
          20.14,
          37.71,
          36.8,
          14.16,
          2.22
        ],
        "code_task_scores": [
          -16.26,
          -16.08,
          -2.03,
          -0.14,
          7.84
        ],
        "reasoning_task_scores": [
          -29.27,
          -32.95,
          -0.11,
          -15.25
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 26.68,
      "math_avg": 40.16,
      "code_avg": 9.46,
      "reasoning_avg": 20.09,
      "overall_avg": 24.1,
      "overall_efficiency": -0.006128,
      "general_efficiency": -0.014383,
      "math_efficiency": 0.023532,
      "code_efficiency": -0.011639,
      "reasoning_efficiency": -0.022021,
      "general_scores": [
        56.61,
        24.8925,
        29.94,
        0.21571429,
        55.38,
        25.495,
        29.78,
        0.11142857,
        44.78,
        23.0425,
        29.69,
        0.27
      ],
      "math_scores": [
        80.52,
        48.78,
        49.2,
        19.76,
        0.0,
        78.7,
        48.74,
        51.8,
        19.2,
        3.33,
        80.14,
        50.44,
        48.4,
        19.99,
        3.33
      ],
      "code_scores": [
        12.2,
        40.86,
        0.36,
        0.21,
        2.26,
        1.22,
        40.08,
        0.36,
        0.0,
        0.23,
        6.71,
        36.19,
        0.36,
        0.0,
        0.9
      ],
      "reasoning_scores": [
        41.69,
        24.77,
        0.16913043,
        15.36,
        39.66,
        30.17,
        0.16945652,
        14.4,
        41.02,
        20.88,
        0.16804348,
        12.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.26
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.48
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.79
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.71
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -12.36,
        "math_avg": 20.23,
        "code_avg": -10.0,
        "reasoning_avg": -18.93,
        "overall_avg": -5.27,
        "overall_efficiency": -0.006128,
        "general_efficiency": -0.014383,
        "math_efficiency": 0.023532,
        "code_efficiency": -0.011639,
        "reasoning_efficiency": -0.022021,
        "general_task_scores": [
          -12.11,
          4.14,
          -6.85,
          -34.63
        ],
        "math_task_scores": [
          23.38,
          29.62,
          30.6,
          15.31,
          2.22
        ],
        "code_task_scores": [
          -20.73,
          -15.43,
          -3.22,
          -0.14,
          1.13
        ],
        "reasoning_task_scores": [
          -39.55,
          -37.29,
          -0.14,
          -7.95
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 33.08,
      "math_avg": 20.0,
      "code_avg": 23.14,
      "reasoning_avg": 17.35,
      "overall_avg": 23.39,
      "overall_efficiency": -0.082428,
      "general_efficiency": -0.082303,
      "math_efficiency": 0.001031,
      "code_efficiency": 0.050672,
      "reasoning_efficiency": -0.29911,
      "general_scores": [
        55.39,
        31.35,
        31.26,
        19.7371429,
        45.26,
        31.675,
        31.11,
        21.6564286,
        47.03,
        31.505,
        31.24,
        19.7942857
      ],
      "math_scores": [
        34.34,
        24.28,
        25.0,
        14.86,
        0.0,
        36.09,
        24.58,
        26.2,
        15.15,
        0.0,
        34.8,
        24.76,
        25.2,
        14.81,
        0.0
      ],
      "code_scores": [
        53.66,
        52.53,
        7.17,
        2.3,
        0.23,
        51.83,
        55.25,
        5.02,
        2.3,
        0.0,
        51.22,
        55.25,
        6.09,
        3.76,
        0.45
      ],
      "reasoning_scores": [
        52.2,
        16.99,
        0.17597826,
        3.52,
        49.15,
        13.46,
        0.17934783,
        4.8,
        49.15,
        15.13,
        0.17119565,
        3.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.79
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.96,
        "math_avg": 0.07,
        "code_avg": 3.67,
        "reasoning_avg": -21.67,
        "overall_avg": -5.97,
        "overall_efficiency": -0.082428,
        "general_efficiency": -0.082303,
        "math_efficiency": 0.001031,
        "code_efficiency": 0.050672,
        "reasoning_efficiency": -0.29911,
        "general_task_scores": [
          -15.14,
          11.17,
          -5.45,
          -14.43
        ],
        "math_task_scores": [
          -21.33,
          4.84,
          6.27,
          10.6,
          0.0
        ],
        "code_task_scores": [
          24.8,
          -0.13,
          2.51,
          2.58,
          0.23
        ],
        "reasoning_task_scores": [
          -30.17,
          -47.37,
          -0.13,
          -18.21
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 25.67,
      "math_avg": 29.41,
      "code_avg": 4.59,
      "reasoning_avg": 20.47,
      "overall_avg": 20.04,
      "overall_efficiency": -0.023621,
      "general_efficiency": -0.033864,
      "math_efficiency": 0.023993,
      "code_efficiency": -0.037654,
      "reasoning_efficiency": -0.046959,
      "general_scores": [
        63.63,
        22.3325,
        14.36,
        0.11928571,
        66.62,
        23.03,
        16.5,
        0.09071429,
        64.79,
        22.7125,
        13.7,
        0.15571429
      ],
      "math_scores": [
        73.54,
        30.94,
        31.6,
        10.34,
        0.0,
        75.74,
        31.74,
        30.6,
        9.96,
        0.0,
        75.97,
        31.5,
        29.4,
        9.78,
        0.0
      ],
      "code_scores": [
        0.0,
        19.84,
        0.0,
        0.84,
        0.68,
        0.0,
        17.9,
        0.0,
        4.59,
        0.23,
        0.0,
        20.62,
        0.0,
        3.97,
        0.23
      ],
      "reasoning_scores": [
        39.32,
        35.66,
        0.15347826,
        12.56,
        35.59,
        35.94,
        0.14304348,
        5.2,
        31.86,
        35.44,
        0.16956522,
        13.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 19.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.13
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.38,
        "math_avg": 9.48,
        "code_avg": -14.87,
        "reasoning_avg": -18.55,
        "overall_avg": -9.33,
        "overall_efficiency": -0.023621,
        "general_efficiency": -0.033864,
        "math_efficiency": 0.023993,
        "code_efficiency": -0.037654,
        "reasoning_efficiency": -0.046959,
        "general_task_scores": [
          0.64,
          2.35,
          -21.8,
          -34.71
        ],
        "math_task_scores": [
          18.67,
          11.69,
          11.33,
          5.69,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -35.02,
          -3.58,
          2.92,
          0.38
        ],
        "reasoning_task_scores": [
          -44.75,
          -26.88,
          -0.15,
          -11.63
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 29.84,
      "math_avg": 21.07,
      "code_avg": 17.6,
      "reasoning_avg": 30.49,
      "overall_avg": 24.75,
      "overall_efficiency": -0.01761,
      "general_efficiency": -0.035144,
      "math_efficiency": 0.004343,
      "code_efficiency": -0.007108,
      "reasoning_efficiency": -0.032528,
      "general_scores": [
        43.69,
        34.2625,
        32.41,
        2.02428571,
        45.54,
        35.44,
        34.22,
        0.54357143,
        63.45,
        32.9325,
        32.27,
        1.26071429
      ],
      "math_scores": [
        59.21,
        19.58,
        21.2,
        9.08,
        0.0,
        57.01,
        18.0,
        16.6,
        8.81,
        0.0,
        58.76,
        18.98,
        19.8,
        8.99,
        0.0
      ],
      "code_scores": [
        29.88,
        43.97,
        0.72,
        6.05,
        7.01,
        31.1,
        43.58,
        0.0,
        5.85,
        4.07,
        30.49,
        42.41,
        2.51,
        11.9,
        4.52
      ],
      "reasoning_scores": [
        58.64,
        49.76,
        0.25086957,
        9.52,
        61.69,
        50.93,
        0.2573913,
        8.56,
        62.71,
        49.5,
        0.27836957,
        13.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.08
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.93
              },
              {
                "metric": "lcb_test_output",
                "score": 5.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.21,
        "math_avg": 1.14,
        "code_avg": -1.86,
        "reasoning_avg": -8.52,
        "overall_avg": -4.61,
        "overall_efficiency": -0.01761,
        "general_efficiency": -0.035144,
        "math_efficiency": 0.004343,
        "code_efficiency": -0.007108,
        "reasoning_efficiency": -0.032528,
        "general_task_scores": [
          -13.48,
          13.87,
          -3.68,
          -33.55
        ],
        "math_task_scores": [
          1.92,
          -0.85,
          0.0,
          4.62,
          0.0
        ],
        "code_task_scores": [
          3.05,
          -11.15,
          -2.5,
          7.72,
          5.2
        ],
        "reasoning_task_scores": [
          -19.33,
          -12.5,
          -0.05,
          -11.44
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 17.66,
      "math_avg": 2.27,
      "code_avg": 5.65,
      "reasoning_avg": 1.8,
      "overall_avg": 6.85,
      "overall_efficiency": -0.754736,
      "general_efficiency": -0.716648,
      "math_efficiency": -0.591928,
      "code_efficiency": -0.462938,
      "reasoning_efficiency": -1.247431,
      "general_scores": [
        41.08,
        26.495,
        0.92,
        0.0,
        42.03,
        25.9,
        1.58,
        0.0,
        46.17,
        26.5475,
        1.24,
        0.0
      ],
      "math_scores": [
        2.88,
        0.56,
        0.0,
        5.85,
        0.0,
        4.93,
        1.66,
        0.4,
        6.12,
        0.0,
        4.4,
        1.06,
        0.0,
        6.17,
        0.0
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        0.42,
        0.0,
        0.0,
        26.46,
        0.0,
        1.25,
        0.0,
        0.61,
        27.63,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        0.34,
        3.9,
        0.13880435,
        2.32,
        0.34,
        3.64,
        0.16217391,
        2.64,
        0.68,
        4.71,
        0.1548913,
        2.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 1.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -21.38,
        "math_avg": -17.66,
        "code_avg": -13.81,
        "reasoning_avg": -37.22,
        "overall_avg": -22.52,
        "overall_efficiency": -0.754736,
        "general_efficiency": -0.716648,
        "math_efficiency": -0.591928,
        "code_efficiency": -0.462938,
        "reasoning_efficiency": -1.247431,
        "general_task_scores": [
          -21.28,
          5.97,
          -35.4,
          -34.83
        ],
        "math_task_scores": [
          -52.34,
          -18.61,
          -19.07,
          1.71,
          0.0
        ],
        "code_task_scores": [
          -27.24,
          -27.1,
          -3.58,
          0.49,
          0.0
        ],
        "reasoning_task_scores": [
          -79.89,
          -58.48,
          -0.16,
          -19.57
        ]
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 30.01,
      "math_avg": 17.26,
      "code_avg": 12.64,
      "reasoning_avg": 25.92,
      "overall_avg": 21.46,
      "overall_efficiency": -0.158187,
      "general_efficiency": -0.180804,
      "math_efficiency": -0.053373,
      "code_efficiency": -0.1366,
      "reasoning_efficiency": -0.26197,
      "general_scores": [
        71.36,
        21.0,
        25.47,
        0.02071429,
        72.35,
        24.7925,
        25.27,
        0.08857143,
        72.93,
        21.8575,
        24.91,
        0.02214286
      ],
      "math_scores": [
        40.86,
        17.18,
        18.4,
        8.79,
        0.0,
        41.24,
        17.9,
        17.6,
        9.55,
        0.0,
        38.67,
        18.1,
        18.4,
        8.9,
        3.33
      ],
      "code_scores": [
        0.0,
        54.09,
        0.36,
        5.22,
        0.9,
        1.22,
        55.25,
        1.43,
        3.34,
        2.26,
        1.22,
        56.81,
        1.08,
        5.01,
        1.36
      ],
      "reasoning_scores": [
        48.14,
        54.08,
        0.16869565,
        3.84,
        37.63,
        55.61,
        0.16869565,
        12.08,
        41.36,
        52.05,
        0.15163043,
        5.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.52
              },
              {
                "metric": "lcb_test_output",
                "score": 1.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.04,
        "math_avg": -2.67,
        "code_avg": -6.83,
        "reasoning_avg": -13.1,
        "overall_avg": -7.91,
        "overall_efficiency": -0.158187,
        "general_efficiency": -0.180804,
        "math_efficiency": -0.053373,
        "code_efficiency": -0.1366,
        "reasoning_efficiency": -0.26197,
        "general_task_scores": [
          7.84,
          2.21,
          -11.43,
          -34.79
        ],
        "math_task_scores": [
          -16.15,
          -1.97,
          -1.07,
          4.74,
          1.11
        ],
        "code_task_scores": [
          -26.63,
          0.91,
          -2.62,
          4.31,
          1.51
        ],
        "reasoning_task_scores": [
          -37.96,
          -8.65,
          -0.15,
          -14.85
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 26,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 32.93,
      "math_avg": 10.81,
      "code_avg": 13.31,
      "reasoning_avg": 28.4,
      "overall_avg": 21.36,
      "overall_efficiency": -0.268034,
      "general_efficiency": -0.204786,
      "math_efficiency": -0.305389,
      "code_efficiency": -0.206228,
      "reasoning_efficiency": -0.355732,
      "general_scores": [
        55.66,
        33.68,
        35.43,
        6.08071429,
        58.42,
        32.125,
        36.03,
        3.90071429,
        61.88,
        31.85,
        35.99,
        4.13571429
      ],
      "math_scores": [
        21.3,
        11.1,
        10.8,
        11.36,
        0.0,
        21.68,
        12.36,
        9.8,
        11.36,
        0.0,
        19.94,
        11.28,
        10.2,
        11.0,
        0.0
      ],
      "code_scores": [
        8.54,
        55.64,
        2.51,
        3.13,
        1.36,
        2.44,
        52.14,
        2.87,
        4.59,
        2.26,
        1.22,
        54.47,
        2.51,
        4.38,
        1.58
      ],
      "reasoning_scores": [
        55.25,
        39.81,
        0.28271739,
        21.2,
        53.22,
        39.74,
        0.28728261,
        19.76,
        53.56,
        38.89,
        0.28793478,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.48
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.11,
        "math_avg": -9.12,
        "code_avg": -6.16,
        "reasoning_avg": -10.62,
        "overall_avg": -8.0,
        "overall_efficiency": -0.268034,
        "general_efficiency": -0.204786,
        "math_efficiency": -0.305389,
        "code_efficiency": -0.206228,
        "reasoning_efficiency": -0.355732,
        "general_task_scores": [
          -5.72,
          12.21,
          -0.83,
          -30.12
        ],
        "math_task_scores": [
          -35.44,
          -8.12,
          -8.93,
          6.9,
          0.0
        ],
        "code_task_scores": [
          -23.37,
          -0.39,
          -0.95,
          3.82,
          1.73
        ],
        "reasoning_task_scores": [
          -26.33,
          -23.08,
          -0.02,
          -2.27
        ]
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 22.58,
      "reasoning_avg": 20.09,
      "overall_avg": 24.06,
      "overall_efficiency": -0.035355,
      "general_efficiency": -0.086776,
      "math_efficiency": 0.050814,
      "code_efficiency": 0.020757,
      "reasoning_efficiency": -0.126215,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.6875,
        21.54,
        0.43785714,
        45.71,
        33.075,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.21,
        0.0,
        57.7,
        27.26,
        39.4,
        15.15,
        0.0
      ],
      "code_scores": [
        26.22,
        55.64,
        6.45,
        0.42,
        16.74,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68
      ],
      "reasoning_scores": [
        25.42,
        34.29,
        0.27228261,
        21.36,
        24.07,
        31.44,
        0.25282609,
        21.92,
        28.81,
        32.99,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.01,
        "math_avg": 7.62,
        "code_avg": 3.11,
        "reasoning_avg": -18.93,
        "overall_avg": -5.3,
        "overall_efficiency": -0.035355,
        "general_efficiency": -0.086776,
        "math_efficiency": 0.050814,
        "code_efficiency": 0.020757,
        "reasoning_efficiency": -0.126215,
        "general_task_scores": [
          -15.35,
          11.69,
          -14.0,
          -34.39
        ],
        "math_task_scores": [
          -0.74,
          7.31,
          20.13,
          10.29,
          1.11
        ],
        "code_task_scores": [
          3.86,
          1.3,
          3.83,
          0.49,
          17.72
        ],
        "reasoning_task_scores": [
          -54.24,
          -29.65,
          -0.05,
          -0.99
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 25.29,
      "math_avg": 23.78,
      "code_avg": 8.05,
      "reasoning_avg": 19.05,
      "overall_avg": 19.04,
      "overall_efficiency": -0.516246,
      "general_efficiency": -0.687987,
      "math_efficiency": 0.192633,
      "code_efficiency": -0.571,
      "reasoning_efficiency": -0.99863,
      "general_scores": [
        56.04,
        26.94,
        21.86,
        1.06928571,
        52.56,
        25.765,
        21.19,
        1.61928571,
        48.93,
        25.1,
        21.35,
        1.01357143
      ],
      "math_scores": [
        68.69,
        19.14,
        24.0,
        11.5,
        0.0,
        63.99,
        17.28,
        22.2,
        10.84,
        0.0,
        65.96,
        18.84,
        23.4,
        10.9,
        0.0
      ],
      "code_scores": [
        0.61,
        48.64,
        0.0,
        0.0,
        0.68,
        0.0,
        35.8,
        0.0,
        0.0,
        0.0,
        1.22,
        33.07,
        0.0,
        0.0,
        0.68
      ],
      "reasoning_scores": [
        21.36,
        34.07,
        0.2425,
        18.88,
        22.37,
        38.59,
        0.23369565,
        17.84,
        21.69,
        33.76,
        0.23336957,
        19.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.76,
        "math_avg": 3.85,
        "code_avg": -11.42,
        "reasoning_avg": -19.97,
        "overall_avg": -10.32,
        "overall_efficiency": -0.516246,
        "general_efficiency": -0.687987,
        "math_efficiency": 0.192633,
        "code_efficiency": -0.571,
        "reasoning_efficiency": -0.99863,
        "general_task_scores": [
          -11.86,
          5.6,
          -15.18,
          -33.6
        ],
        "math_task_scores": [
          9.8,
          -1.28,
          4.0,
          6.74,
          0.0
        ],
        "code_task_scores": [
          -26.83,
          -15.3,
          -3.58,
          -0.21,
          0.45
        ],
        "reasoning_task_scores": [
          -58.53,
          -27.09,
          -0.07,
          -3.41
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 34.8,
      "math_avg": 48.35,
      "code_avg": 11.81,
      "reasoning_avg": 16.92,
      "overall_avg": 27.97,
      "overall_efficiency": -0.001391,
      "general_efficiency": -0.004228,
      "math_efficiency": 0.028321,
      "code_efficiency": -0.007632,
      "reasoning_efficiency": -0.022025,
      "general_scores": [
        81.28,
        23.6725,
        32.08,
        0.0,
        83.17,
        24.2225,
        30.46,
        2.06785714,
        81.97,
        23.88,
        0.03357143
      ],
      "math_scores": [
        86.66,
        63.18,
        64.2,
        21.0,
        6.67,
        89.08,
        63.24,
        64.8,
        20.57,
        0.0,
        87.41,
        63.06,
        67.8,
        20.89,
        6.67
      ],
      "code_scores": [
        16.46,
        30.74,
        2.51,
        0.0,
        7.47,
        21.34,
        35.02,
        2.15,
        0.0,
        1.81,
        22.56,
        31.91,
        0.36,
        2.3,
        2.49
      ],
      "reasoning_scores": [
        45.76,
        20.33,
        0.18402174,
        0.16,
        47.8,
        22.91,
        0.18641304,
        0.4,
        44.41,
        20.61,
        0.18130435,
        0.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.72
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.67
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.77
              },
              {
                "metric": "lcb_test_output",
                "score": 3.92
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.24,
        "math_avg": 28.42,
        "code_avg": -7.66,
        "reasoning_avg": -22.1,
        "overall_avg": -1.4,
        "overall_efficiency": -0.001391,
        "general_efficiency": -0.004228,
        "math_efficiency": 0.028321,
        "code_efficiency": -0.007632,
        "reasoning_efficiency": -0.022025,
        "general_task_scores": [
          17.77,
          3.58,
          -5.38,
          -34.13
        ],
        "math_task_scores": [
          31.31,
          43.46,
          46.4,
          16.48,
          4.45
        ],
        "code_task_scores": [
          -7.32,
          -21.91,
          -1.91,
          0.56,
          3.92
        ],
        "reasoning_task_scores": [
          -34.35,
          -41.28,
          -0.13,
          -21.87
        ]
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 25.35,
      "math_avg": 49.58,
      "code_avg": 6.63,
      "reasoning_avg": 27.13,
      "overall_avg": 27.17,
      "overall_efficiency": -0.007092,
      "general_efficiency": -0.044299,
      "math_efficiency": 0.095947,
      "code_efficiency": -0.041542,
      "reasoning_efficiency": -0.038473,
      "general_scores": [
        7.64,
        29.9,
        29.33,
        8.25857143,
        65.16,
        25.4725,
        33.58,
        3.49785714
      ],
      "math_scores": [
        83.24,
        85.02,
        83.2,
        40.67,
        20.0,
        66.64,
        55.84,
        45.4,
        15.83,
        0.0
      ],
      "code_scores": [
        6.1,
        1.95,
        0.72,
        2.71,
        1.58,
        10.37,
        0.0,
        43.19,
        0.0,
        6.05,
        0.23
      ],
      "reasoning_scores": [
        69.15,
        19.22,
        29.8,
        0.29923913,
        10.4,
        66.1,
        37.13,
        0.20913043,
        11.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.88
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.94
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.3
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.05
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.38
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 10.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.18
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.69,
        "math_avg": 29.65,
        "code_avg": -12.84,
        "reasoning_avg": -11.89,
        "overall_avg": -2.19,
        "overall_efficiency": -0.007092,
        "general_efficiency": -0.044299,
        "math_efficiency": 0.095947,
        "code_efficiency": -0.041542,
        "reasoning_efficiency": -0.038473,
        "general_task_scores": [
          -27.97,
          7.35,
          -5.19,
          -28.95
        ],
        "math_task_scores": [
          18.53,
          50.73,
          45.1,
          23.91,
          10.0
        ],
        "code_task_scores": [
          -24.39,
          -31.9,
          -3.22,
          4.17,
          0.9,
          -20.73
        ],
        "reasoning_task_scores": [
          -12.72,
          -34.38,
          0.0,
          -0.06,
          -10.96
        ]
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 34.26,
      "math_avg": 3.92,
      "code_avg": 17.17,
      "reasoning_avg": 24.35,
      "overall_avg": 19.92,
      "overall_efficiency": -0.047202,
      "general_efficiency": -0.023952,
      "math_efficiency": -0.08004,
      "code_efficiency": -0.011498,
      "reasoning_efficiency": -0.073318,
      "general_scores": [
        72.08,
        29.815,
        31.69,
        1.43785714,
        70.87,
        29.4225,
        32.93,
        3.26928571,
        74.2,
        31.1675,
        31.81,
        2.36857143
      ],
      "math_scores": [
        1.74,
        3.2,
        4.4,
        11.18,
        0.0,
        0.53,
        3.16,
        3.8,
        10.98,
        0.0,
        0.91,
        3.58,
        4.2,
        11.11,
        0.0
      ],
      "code_scores": [
        23.78,
        36.19,
        1.08,
        5.43,
        11.31,
        28.66,
        40.86,
        1.79,
        1.46,
        14.25,
        25.0,
        39.3,
        1.08,
        14.41,
        12.9
      ],
      "reasoning_scores": [
        34.24,
        43.53,
        0.22565217,
        16.72,
        44.75,
        44.5,
        0.21793478,
        13.44,
        35.25,
        44.49,
        0.2223913,
        14.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.36
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.06
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.31
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.1
              },
              {
                "metric": "lcb_test_output",
                "score": 12.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.79,
        "math_avg": -16.01,
        "code_avg": -2.3,
        "reasoning_avg": -14.67,
        "overall_avg": -9.44,
        "overall_efficiency": -0.047202,
        "general_efficiency": -0.023952,
        "math_efficiency": -0.08004,
        "code_efficiency": -0.011498,
        "reasoning_efficiency": -0.073318,
        "general_task_scores": [
          8.01,
          9.8,
          -4.51,
          -32.47
        ],
        "math_task_scores": [
          -55.35,
          -16.39,
          -15.07,
          6.75,
          0.0
        ],
        "code_task_scores": [
          -1.63,
          -15.69,
          -2.26,
          6.89,
          12.82
        ],
        "reasoning_task_scores": [
          -42.26,
          -18.39,
          -0.09,
          -7.15
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 32.57,
      "math_avg": 38.19,
      "code_avg": 15.65,
      "reasoning_avg": 30.01,
      "overall_avg": 29.11,
      "overall_efficiency": -0.000444,
      "general_efficiency": -0.011064,
      "math_efficiency": 0.031192,
      "code_efficiency": -0.006516,
      "reasoning_efficiency": -0.015388,
      "general_scores": [
        75.02,
        24.375,
        30.52,
        0.23571429,
        74.78,
        25.16,
        30.61,
        0.19571429,
        74.98,
        23.74,
        31.02,
        0.19714286
      ],
      "math_scores": [
        83.24,
        45.36,
        46.0,
        14.61,
        3.33,
        83.17,
        44.8,
        48.4,
        14.84,
        0.0,
        82.34,
        44.44,
        44.4,
        14.59,
        3.33
      ],
      "code_scores": [
        26.83,
        40.86,
        0.36,
        3.13,
        4.75,
        21.95,
        47.86,
        1.08,
        2.51,
        5.66,
        23.78,
        47.47,
        2.15,
        2.09,
        4.3
      ],
      "reasoning_scores": [
        58.31,
        44.06,
        0.19586957,
        19.12,
        54.92,
        42.44,
        0.21054348,
        18.96,
        57.97,
        43.34,
        0.19315217,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.93
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.72
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.68
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.58
              },
              {
                "metric": "lcb_test_output",
                "score": 4.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 43.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.48,
        "math_avg": 18.26,
        "code_avg": -3.81,
        "reasoning_avg": -9.01,
        "overall_avg": -0.26,
        "overall_efficiency": -0.000444,
        "general_efficiency": -0.011064,
        "math_efficiency": 0.031192,
        "code_efficiency": -0.006516,
        "reasoning_efficiency": -0.015388,
        "general_task_scores": [
          10.56,
          4.08,
          -5.93,
          -34.62
        ],
        "math_task_scores": [
          26.51,
          25.17,
          27.07,
          10.34,
          2.22
        ],
        "code_task_scores": [
          -3.25,
          -9.07,
          -2.38,
          2.37,
          4.9
        ],
        "reasoning_task_scores": [
          -23.27,
          -19.28,
          -0.11,
          -2.59
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 29.85,
      "math_avg": 1.89,
      "code_avg": 9.28,
      "reasoning_avg": 15.17,
      "overall_avg": 14.05,
      "overall_efficiency": -0.649711,
      "general_efficiency": -0.39013,
      "math_efficiency": -0.765204,
      "code_efficiency": -0.432239,
      "reasoning_efficiency": -1.011271,
      "general_scores": [
        73.71,
        22.8025,
        23.56,
        0.60428571,
        73.18,
        21.865,
        24.41,
        0.49428571,
        71.82,
        21.5775,
        23.45,
        0.69857143
      ],
      "math_scores": [
        0.0,
        0.34,
        0.2,
        7.48,
        0.0,
        0.0,
        0.4,
        0.6,
        7.32,
        0.0,
        0.0,
        0.28,
        0.6,
        7.77,
        3.33
      ],
      "code_scores": [
        0.0,
        41.63,
        0.0,
        0.63,
        0.0,
        0.0,
        47.08,
        0.0,
        1.25,
        0.0,
        0.0,
        47.08,
        0.0,
        1.46,
        0.0
      ],
      "reasoning_scores": [
        29.49,
        25.66,
        0.18434783,
        2.0,
        31.86,
        29.84,
        0.17978261,
        2.48,
        30.85,
        27.13,
        0.18173913,
        2.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.9
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.6
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.2,
        "math_avg": -18.04,
        "code_avg": -10.19,
        "reasoning_avg": -23.84,
        "overall_avg": -15.32,
        "overall_efficiency": -0.649711,
        "general_efficiency": -0.39013,
        "math_efficiency": -0.765204,
        "code_efficiency": -0.432239,
        "reasoning_efficiency": -1.011271,
        "general_task_scores": [
          8.53,
          1.74,
          -12.84,
          -34.23
        ],
        "math_task_scores": [
          -56.41,
          -19.36,
          -18.73,
          3.18,
          1.11
        ],
        "code_task_scores": [
          -27.44,
          -9.21,
          -3.58,
          0.9,
          0.0
        ],
        "reasoning_task_scores": [
          -49.61,
          -35.02,
          -0.13,
          -19.84
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 41.4,
      "math_avg": 14.36,
      "code_avg": 36.85,
      "reasoning_avg": 38.24,
      "overall_avg": 32.71,
      "overall_efficiency": 0.003451,
      "general_efficiency": 0.002429,
      "math_efficiency": -0.005746,
      "code_efficiency": 0.01792,
      "reasoning_efficiency": -0.000798,
      "general_scores": [
        66.61,
        42.195,
        36.15,
        17.8557143,
        65.46,
        42.1725,
        35.81,
        25.2014286,
        65.67,
        41.9,
        35.6,
        22.2078571
      ],
      "math_scores": [
        30.1,
        14.06,
        15.0,
        10.89,
        3.33,
        31.01,
        16.04,
        14.8,
        11.11,
        3.33,
        27.52,
        14.46,
        12.0,
        11.7,
        0.0
      ],
      "code_scores": [
        69.51,
        66.93,
        6.81,
        20.88,
        20.59,
        65.85,
        64.98,
        5.02,
        16.08,
        29.19,
        67.68,
        64.2,
        5.02,
        21.71,
        28.28
      ],
      "reasoning_scores": [
        64.41,
        58.95,
        0.3301087,
        28.72,
        66.1,
        58.69,
        0.34315217,
        30.4,
        61.69,
        60.49,
        0.32434783,
        28.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.09
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.56
              },
              {
                "metric": "lcb_test_output",
                "score": 26.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.36,
        "math_avg": -5.57,
        "code_avg": 17.38,
        "reasoning_avg": -0.77,
        "overall_avg": 3.35,
        "overall_efficiency": 0.003451,
        "general_efficiency": 0.002429,
        "math_efficiency": -0.005746,
        "code_efficiency": 0.01792,
        "reasoning_efficiency": -0.000798,
        "general_task_scores": [
          1.54,
          21.75,
          -0.8,
          -13.07
        ],
        "math_task_scores": [
          -26.87,
          -4.85,
          -5.27,
          6.89,
          2.22
        ],
        "code_task_scores": [
          40.24,
          10.9,
          2.04,
          19.35,
          26.02
        ],
        "reasoning_task_scores": [
          -16.27,
          -3.18,
          0.02,
          7.12
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 23.34,
      "math_avg": 11.95,
      "code_avg": 21.97,
      "reasoning_avg": 20.15,
      "overall_avg": 19.35,
      "overall_efficiency": -0.010216,
      "general_efficiency": -0.016025,
      "math_efficiency": -0.00814,
      "code_efficiency": 0.002559,
      "reasoning_efficiency": -0.019257,
      "general_scores": [
        38.16,
        32.3675,
        25.72,
        0.0,
        19.57,
        28.535,
        25.06,
        0.0,
        56.67,
        28.1425,
        25.88,
        0.0
      ],
      "math_scores": [
        32.07,
        11.9,
        9.6,
        9.85,
        0.0,
        22.29,
        11.14,
        12.6,
        10.34,
        0.0,
        25.63,
        11.36,
        11.0,
        11.52,
        0.0
      ],
      "code_scores": [
        36.59,
        45.53,
        2.87,
        20.67,
        11.31,
        31.1,
        45.53,
        3.23,
        14.2,
        14.03,
        31.71,
        46.3,
        2.87,
        6.26,
        17.42
      ],
      "reasoning_scores": [
        42.03,
        18.38,
        0.22717391,
        23.04,
        40.34,
        15.63,
        0.21304348,
        21.52,
        36.95,
        19.87,
        0.2198913,
        23.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.13
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.71
              },
              {
                "metric": "lcb_test_output",
                "score": 14.25
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -15.7,
        "math_avg": -7.98,
        "code_avg": 2.51,
        "reasoning_avg": -18.87,
        "overall_avg": -10.01,
        "overall_efficiency": -0.010216,
        "general_efficiency": -0.016025,
        "math_efficiency": -0.00814,
        "code_efficiency": 0.002559,
        "reasoning_efficiency": -0.019257,
        "general_task_scores": [
          -26.24,
          9.34,
          -11.1,
          -34.83
        ],
        "math_task_scores": [
          -29.75,
          -8.23,
          -8.13,
          6.23,
          0.0
        ],
        "code_task_scores": [
          5.69,
          -8.68,
          -0.59,
          13.5,
          14.25
        ],
        "reasoning_task_scores": [
          -40.57,
          -44.6,
          -0.09,
          0.56
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 31.86,
      "math_avg": 15.55,
      "code_avg": 22.01,
      "reasoning_avg": 17.06,
      "overall_avg": 21.62,
      "overall_efficiency": -0.154915,
      "general_efficiency": -0.143712,
      "math_efficiency": -0.087693,
      "code_efficiency": 0.050854,
      "reasoning_efficiency": -0.439109,
      "general_scores": [
        71.26,
        27.6075,
        23.9,
        3.22285714,
        71.34,
        28.75,
        24.11,
        5.53928571,
        70.42,
        28.1825,
        23.5,
        4.49428571
      ],
      "math_scores": [
        11.75,
        19.32,
        23.6,
        16.78,
        3.33,
        16.0,
        22.28,
        27.8,
        17.1,
        0.0,
        11.3,
        19.86,
        27.8,
        16.26,
        0.0
      ],
      "code_scores": [
        35.98,
        60.7,
        3.58,
        0.0,
        9.5,
        36.59,
        61.87,
        2.87,
        0.0,
        9.73,
        35.98,
        61.48,
        3.94,
        0.0,
        7.92
      ],
      "reasoning_scores": [
        25.42,
        24.23,
        0.235,
        17.76,
        26.1,
        22.58,
        0.24684783,
        16.72,
        28.47,
        26.43,
        0.24369565,
        16.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.42
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.71
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 61.35
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.41
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.19,
        "math_avg": -4.38,
        "code_avg": 2.54,
        "reasoning_avg": -21.96,
        "overall_avg": -7.75,
        "overall_efficiency": -0.154915,
        "general_efficiency": -0.143712,
        "math_efficiency": -0.087693,
        "code_efficiency": 0.050854,
        "reasoning_efficiency": -0.439109,
        "general_task_scores": [
          6.64,
          7.84,
          -12.81,
          -30.41
        ],
        "math_task_scores": [
          -43.39,
          0.79,
          7.2,
          12.37,
          1.11
        ],
        "code_task_scores": [
          8.74,
          6.88,
          -0.12,
          -0.21,
          9.05
        ],
        "reasoning_task_scores": [
          -53.68,
          -38.15,
          -0.07,
          -5.15
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 37,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 36.41,
      "math_avg": 34.4,
      "code_avg": 22.8,
      "reasoning_avg": 29.78,
      "overall_avg": 30.85,
      "overall_efficiency": 0.001658,
      "general_efficiency": -0.002946,
      "math_efficiency": 0.016182,
      "code_efficiency": 0.003731,
      "reasoning_efficiency": -0.010333,
      "general_scores": [
        81.04,
        33.9675,
        31.7,
        0.56642857,
        80.91,
        31.5525,
        26.8,
        3.80857143,
        81.72,
        34.055,
        27.9,
        2.92928571
      ],
      "math_scores": [
        83.24,
        36.94,
        35.4,
        13.14,
        0.0,
        82.03,
        39.58,
        38.8,
        13.71,
        0.0,
        81.96,
        39.06,
        38.6,
        13.48,
        0.0
      ],
      "code_scores": [
        35.98,
        60.7,
        3.58,
        0.63,
        9.73,
        38.41,
        56.03,
        4.3,
        1.04,
        11.99,
        42.07,
        60.31,
        3.58,
        1.46,
        12.22
      ],
      "reasoning_scores": [
        50.17,
        55.0,
        0.28184783,
        11.52,
        53.22,
        51.36,
        0.27869565,
        12.96,
        57.29,
        50.53,
        0.28467391,
        14.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 38.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.82
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.04
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.99
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -2.63,
        "math_avg": 14.47,
        "code_avg": 3.34,
        "reasoning_avg": -9.24,
        "overall_avg": 1.48,
        "overall_efficiency": 0.001658,
        "general_efficiency": -0.002946,
        "math_efficiency": 0.016182,
        "code_efficiency": 0.003731,
        "reasoning_efficiency": -0.010333,
        "general_task_scores": [
          16.85,
          12.85,
          -7.85,
          -32.4
        ],
        "math_task_scores": [
          26.0,
          18.83,
          18.4,
          9.1,
          0.0
        ],
        "code_task_scores": [
          11.38,
          4.54,
          0.24,
          0.83,
          11.31
        ],
        "reasoning_task_scores": [
          -26.78,
          -10.26,
          -0.03,
          -9.09
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 38,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 38.29,
      "math_avg": 31.43,
      "code_avg": 24.86,
      "reasoning_avg": 18.74,
      "overall_avg": 28.33,
      "overall_efficiency": -0.005172,
      "general_efficiency": -0.003776,
      "math_efficiency": 0.0575,
      "code_efficiency": 0.026976,
      "reasoning_efficiency": -0.101386,
      "general_scores": [
        79.58,
        33.4775,
        31.77,
        9.49,
        79.67,
        33.3025,
        32.28,
        6.36857143,
        80.48,
        33.2825,
        30.88,
        8.90928571
      ],
      "math_scores": [
        59.14,
        37.92,
        39.4,
        14.68,
        6.67,
        64.82,
        37.88,
        38.8,
        15.2,
        3.33,
        57.16,
        37.08,
        37.8,
        14.93,
        6.67
      ],
      "code_scores": [
        44.51,
        55.25,
        5.38,
        0.84,
        17.87,
        46.34,
        55.64,
        6.81,
        0.21,
        14.71,
        45.12,
        55.64,
        7.89,
        0.21,
        16.52
      ],
      "reasoning_scores": [
        31.86,
        21.41,
        0.27847826,
        23.36,
        33.56,
        20.92,
        0.27195652,
        21.12,
        29.83,
        21.02,
        0.26173913,
        20.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.64
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.26
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 16.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.76,
        "math_avg": 11.5,
        "code_avg": 5.4,
        "reasoning_avg": -20.28,
        "overall_avg": -1.03,
        "overall_efficiency": -0.005172,
        "general_efficiency": -0.003776,
        "math_efficiency": 0.0575,
        "code_efficiency": 0.026976,
        "reasoning_efficiency": -0.101386,
        "general_task_scores": [
          15.54,
          13.01,
          -5.01,
          -26.57
        ],
        "math_task_scores": [
          3.96,
          17.93,
          19.47,
          10.6,
          5.56
        ],
        "code_task_scores": [
          17.88,
          1.04,
          3.11,
          0.21,
          16.37
        ],
        "reasoning_task_scores": [
          -48.59,
          -41.44,
          -0.04,
          -0.27
        ]
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 28.14,
      "math_avg": 42.11,
      "code_avg": 10.28,
      "reasoning_avg": 24.86,
      "overall_avg": 26.35,
      "overall_efficiency": -0.003367,
      "general_efficiency": -0.012164,
      "math_efficiency": 0.024746,
      "code_efficiency": -0.010246,
      "reasoning_efficiency": -0.015803,
      "general_scores": [
        59.7,
        25.6175,
        28.99,
        0.17714286,
        57.06,
        25.19,
        30.52,
        0.48285714,
        53.75,
        26.26,
        29.74,
        0.245
      ],
      "math_scores": [
        82.64,
        53.2,
        55.0,
        19.49,
        10.0,
        81.73,
        51.56,
        52.4,
        19.83,
        0.0,
        81.5,
        52.16,
        49.2,
        19.58,
        3.33
      ],
      "code_scores": [
        12.2,
        36.96,
        0.36,
        0.21,
        0.23,
        10.98,
        33.46,
        1.43,
        0.21,
        0.68,
        14.02,
        40.86,
        1.08,
        0.0,
        1.58
      ],
      "reasoning_scores": [
        52.2,
        39.04,
        0.21836957,
        10.64,
        48.47,
        32.48,
        0.20858696,
        8.96,
        57.97,
        35.3,
        0.21782609,
        12.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.96
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 12.4
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 37.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 0.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -10.9,
        "math_avg": 22.18,
        "code_avg": -9.18,
        "reasoning_avg": -14.16,
        "overall_avg": -3.02,
        "overall_efficiency": -0.003367,
        "general_efficiency": -0.012164,
        "math_efficiency": 0.024746,
        "code_efficiency": -0.010246,
        "reasoning_efficiency": -0.015803,
        "general_task_scores": [
          -7.53,
          5.35,
          -6.9,
          -34.53
        ],
        "math_task_scores": [
          25.55,
          32.61,
          33.0,
          15.29,
          4.44
        ],
        "code_task_scores": [
          -15.04,
          -17.38,
          -2.62,
          -0.07,
          0.83
        ],
        "reasoning_task_scores": [
          -27.46,
          -26.95,
          -0.1,
          -11.36
        ]
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 32.46,
      "math_avg": 34.75,
      "code_avg": 18.21,
      "reasoning_avg": 28.31,
      "overall_avg": 28.43,
      "overall_efficiency": -0.015637,
      "general_efficiency": -0.110014,
      "math_efficiency": 0.247403,
      "code_efficiency": -0.021065,
      "reasoning_efficiency": -0.178873,
      "general_scores": [
        68.26,
        34.855,
        25.73,
        0.0,
        69.26,
        34.1825,
        27.37,
        0.0
      ],
      "math_scores": [
        59.74,
        36.16,
        34.0,
        3.33,
        66.26,
        36.56,
        38.6,
        3.33
      ],
      "code_scores": [
        22.56,
        51.36,
        2.51,
        0.21,
        17.19,
        18.9,
        53.31,
        1.08,
        0.0,
        14.93
      ],
      "reasoning_scores": [
        33.9,
        51.66,
        0.27717391,
        25.68,
        35.25,
        54.27,
        0.28543478,
        25.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.1
              },
              {
                "metric": "lcb_test_output",
                "score": 16.06
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.59,
        "math_avg": 14.82,
        "code_avg": -1.26,
        "reasoning_avg": -10.71,
        "overall_avg": -0.94,
        "overall_efficiency": -0.015637,
        "general_efficiency": -0.110014,
        "math_efficiency": 0.247403,
        "code_efficiency": -0.021065,
        "reasoning_efficiency": -0.178873,
        "general_task_scores": [
          4.39,
          14.18,
          -10.1,
          -34.83
        ],
        "math_task_scores": [
          6.59,
          16.66,
          17.1,
          3.33
        ],
        "code_task_scores": [
          -6.71,
          -2.13,
          -1.78,
          -0.11,
          16.06
        ],
        "reasoning_task_scores": [
          -45.76,
          -9.6,
          -0.03,
          3.32
        ]
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 45.83,
      "math_avg": 20.9,
      "code_avg": 37.12,
      "reasoning_avg": 43.32,
      "overall_avg": 36.79,
      "overall_efficiency": 0.016038,
      "general_efficiency": 0.014641,
      "math_efficiency": 0.002091,
      "code_efficiency": 0.038125,
      "reasoning_efficiency": 0.009296,
      "general_scores": [
        59.32,
        54.035,
        39.36,
        31.23,
        62.96,
        53.8975,
        38.69,
        24.98,
        61.03,
        54.4775,
        39.14,
        30.785
      ],
      "math_scores": [
        44.35,
        20.88,
        23.4,
        11.99,
        3.33,
        46.93,
        20.76,
        21.4,
        11.09,
        0.0,
        54.28,
        21.66,
        22.2,
        11.2,
        0.0
      ],
      "code_scores": [
        67.68,
        65.37,
        7.17,
        19.42,
        25.79,
        65.85,
        65.37,
        7.53,
        19.62,
        25.57,
        68.9,
        66.93,
        7.17,
        17.95,
        26.47
      ],
      "reasoning_scores": [
        77.97,
        64.32,
        0.34152174,
        29.28,
        80.0,
        62.75,
        0.38271739,
        31.6,
        77.97,
        62.72,
        0.37793478,
        32.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.14
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.43
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.48
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.0
              },
              {
                "metric": "lcb_test_output",
                "score": 25.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.26
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.01
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.78,
        "math_avg": 0.97,
        "code_avg": 17.65,
        "reasoning_avg": 4.3,
        "overall_avg": 7.43,
        "overall_efficiency": 0.016038,
        "general_efficiency": 0.014641,
        "math_efficiency": 0.002091,
        "code_efficiency": 0.038125,
        "reasoning_efficiency": 0.009296,
        "general_task_scores": [
          -3.27,
          33.8,
          2.41,
          -5.83
        ],
        "math_task_scores": [
          -7.89,
          1.4,
          3.13,
          7.09,
          1.11
        ],
        "code_task_scores": [
          40.04,
          11.42,
          3.71,
          18.79,
          25.94
        ],
        "reasoning_task_scores": [
          -1.69,
          0.7,
          0.06,
          8.93
        ]
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 42,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 35.7,
      "math_avg": 4.41,
      "code_avg": 25.61,
      "reasoning_avg": 30.92,
      "overall_avg": 24.16,
      "overall_efficiency": -0.26005,
      "general_efficiency": -0.167148,
      "math_efficiency": -0.77528,
      "code_efficiency": 0.306829,
      "reasoning_efficiency": -0.4046,
      "general_scores": [
        55.24,
        45.455,
        33.64,
        9.75142857,
        57.68,
        44.5575,
        35.0,
        7.50071429,
        56.06,
        44.5625,
        31.89,
        7.05714286
      ],
      "math_scores": [
        3.34,
        4.52,
        5.4,
        6.39,
        0.0,
        3.49,
        5.0,
        6.6,
        6.28,
        3.33,
        3.18,
        4.42,
        4.6,
        6.23,
        3.33
      ],
      "code_scores": [
        34.15,
        46.69,
        0.0,
        26.1,
        23.08,
        32.93,
        42.41,
        0.0,
        25.05,
        23.08,
        31.1,
        47.08,
        0.0,
        27.14,
        25.34
      ],
      "reasoning_scores": [
        63.05,
        45.78,
        0.29,
        16.56,
        62.03,
        49.66,
        0.29076087,
        18.72,
        52.88,
        45.28,
        0.30923913,
        16.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 32.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.1
              },
              {
                "metric": "lcb_test_output",
                "score": 23.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.35,
        "math_avg": -15.52,
        "code_avg": 6.14,
        "reasoning_avg": -8.1,
        "overall_avg": -5.21,
        "overall_efficiency": -0.26005,
        "general_efficiency": -0.167148,
        "math_efficiency": -0.77528,
        "code_efficiency": 0.306829,
        "reasoning_efficiency": -0.4046,
        "general_task_scores": [
          -8.04,
          24.52,
          -3.14,
          -26.73
        ],
        "math_task_scores": [
          -53.07,
          -15.05,
          -13.67,
          1.96,
          2.22
        ],
        "code_task_scores": [
          5.29,
          -9.08,
          -3.58,
          25.89,
          23.83
        ],
        "reasoning_task_scores": [
          -21.02,
          -15.65,
          -0.01,
          -4.93
        ]
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 43,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 29.52,
      "math_avg": 13.17,
      "code_avg": 38.17,
      "reasoning_avg": 31.9,
      "overall_avg": 28.19,
      "overall_efficiency": -0.002694,
      "general_efficiency": -0.021821,
      "math_efficiency": -0.015497,
      "code_efficiency": 0.042856,
      "reasoning_efficiency": -0.016315,
      "general_scores": [
        32.19,
        37.875,
        30.06,
        12.8085714,
        30.96,
        41.305,
        33.27,
        9.16071429,
        46.25,
        39.6725,
        33.54,
        7.20142857
      ],
      "math_scores": [
        29.57,
        9.48,
        10.4,
        14.77,
        0.0,
        31.16,
        10.12,
        9.2,
        15.45,
        6.67,
        23.96,
        10.72,
        11.0,
        15.02,
        0.0
      ],
      "code_scores": [
        78.05,
        63.81,
        7.53,
        18.79,
        26.7,
        76.22,
        67.7,
        7.53,
        17.12,
        22.4,
        75.61,
        65.76,
        6.81,
        18.79,
        19.68
      ],
      "reasoning_scores": [
        52.88,
        48.99,
        0.27619565,
        20.08,
        61.02,
        48.14,
        0.30434783,
        24.24,
        57.97,
        46.37,
        0.28576087,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.23
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.23
              },
              {
                "metric": "lcb_test_output",
                "score": 22.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.52,
        "math_avg": -6.76,
        "code_avg": 18.7,
        "reasoning_avg": -7.12,
        "overall_avg": -1.18,
        "overall_efficiency": -0.002694,
        "general_efficiency": -0.021821,
        "math_efficiency": -0.015497,
        "code_efficiency": 0.042856,
        "reasoning_efficiency": -0.016315,
        "general_task_scores": [
          -27.9,
          19.28,
          -4.36,
          -25.11
        ],
        "math_task_scores": [
          -28.18,
          -9.59,
          -9.0,
          10.74,
          2.22
        ],
        "code_task_scores": [
          49.19,
          11.29,
          3.71,
          18.02,
          22.93
        ],
        "reasoning_task_scores": [
          -23.05,
          -14.73,
          -0.02,
          0.11
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 37.4,
      "math_avg": 13.97,
      "code_avg": 33.46,
      "reasoning_avg": 37.58,
      "overall_avg": 30.6,
      "overall_efficiency": 0.007899,
      "general_efficiency": -0.010488,
      "math_efficiency": -0.038106,
      "code_efficiency": 0.089395,
      "reasoning_efficiency": -0.009204,
      "general_scores": [
        53.9,
        45.0025,
        35.89,
        12.9128571,
        54.34,
        45.9125,
        33.82,
        17.3835714,
        54.99,
        44.7925,
        34.61,
        15.3
      ],
      "math_scores": [
        33.06,
        10.14,
        8.6,
        11.4,
        0.0,
        37.38,
        11.94,
        13.8,
        10.23,
        3.33,
        37.0,
        10.44,
        11.0,
        11.16,
        0.0
      ],
      "code_scores": [
        66.46,
        57.59,
        6.45,
        19.42,
        20.14,
        67.68,
        53.31,
        7.53,
        17.12,
        20.81,
        67.07,
        57.59,
        5.38,
        15.66,
        19.68
      ],
      "reasoning_scores": [
        73.22,
        54.55,
        0.26608696,
        25.84,
        70.85,
        53.7,
        0.24445652,
        26.96,
        70.51,
        50.86,
        0.25206522,
        23.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.4
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.64,
        "math_avg": -5.96,
        "code_avg": 13.99,
        "reasoning_avg": -1.44,
        "overall_avg": 1.24,
        "overall_efficiency": 0.007899,
        "general_efficiency": -0.010488,
        "math_efficiency": -0.038106,
        "code_efficiency": 0.089395,
        "reasoning_efficiency": -0.009204,
        "general_task_scores": [
          -9.96,
          24.9,
          -1.88,
          -19.63
        ],
        "math_task_scores": [
          -20.6,
          -8.86,
          -8.07,
          6.59,
          1.11
        ],
        "code_task_scores": [
          39.63,
          1.69,
          2.87,
          17.19,
          20.21
        ],
        "reasoning_task_scores": [
          -8.81,
          -9.52,
          -0.06,
          3.41
        ]
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 34.16,
      "math_avg": 12.4,
      "code_avg": 37.19,
      "reasoning_avg": 36.84,
      "overall_avg": 30.15,
      "overall_efficiency": 0.007015,
      "general_efficiency": -0.043934,
      "math_efficiency": -0.06769,
      "code_efficiency": 0.159291,
      "reasoning_efficiency": -0.019605,
      "general_scores": [
        47.04,
        39.87,
        34.75,
        12.6614286,
        49.14,
        41.98,
        35.69,
        12.2407143,
        47.41,
        42.24,
        35.69,
        11.1778571
      ],
      "math_scores": [
        22.06,
        10.0,
        9.2,
        16.06,
        0.0,
        24.41,
        9.92,
        8.2,
        15.29,
        6.67,
        29.04,
        9.88,
        10.4,
        14.84,
        0.0
      ],
      "code_scores": [
        69.51,
        58.37,
        5.02,
        31.11,
        18.78,
        69.51,
        59.14,
        5.38,
        36.95,
        16.74,
        69.51,
        59.14,
        6.81,
        28.6,
        23.3
      ],
      "reasoning_scores": [
        64.07,
        53.03,
        0.25380435,
        26.0,
        68.47,
        53.36,
        0.27521739,
        26.08,
        69.15,
        54.76,
        0.27380435,
        26.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.74
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.22
              },
              {
                "metric": "lcb_test_output",
                "score": 19.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.89,
        "math_avg": -7.53,
        "code_avg": 17.72,
        "reasoning_avg": -2.18,
        "overall_avg": 0.78,
        "overall_efficiency": 0.007015,
        "general_efficiency": -0.043934,
        "math_efficiency": -0.06769,
        "code_efficiency": 0.159291,
        "reasoning_efficiency": -0.019605,
        "general_task_scores": [
          -16.51,
          21.02,
          -1.27,
          -22.8
        ],
        "math_task_scores": [
          -31.24,
          -9.77,
          -9.93,
          11.06,
          2.22
        ],
        "code_task_scores": [
          42.07,
          4.41,
          2.16,
          32.01,
          19.61
        ],
        "reasoning_task_scores": [
          -13.11,
          -8.84,
          -0.04,
          4.05
        ]
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 33.9,
      "math_avg": 11.25,
      "code_avg": 35.88,
      "reasoning_avg": 36.22,
      "overall_avg": 29.31,
      "overall_efficiency": -0.000469,
      "general_efficiency": -0.046251,
      "math_efficiency": -0.078045,
      "code_efficiency": 0.147588,
      "reasoning_efficiency": -0.025167,
      "general_scores": [
        50.66,
        41.49,
        34.39,
        13.7185714,
        38.57,
        44.045,
        34.82,
        13.0278571,
        47.54,
        42.6375,
        36.22,
        9.72642857
      ],
      "math_scores": [
        22.67,
        9.46,
        7.2,
        14.7,
        0.0,
        28.35,
        9.62,
        8.2,
        15.99,
        0.0,
        22.67,
        8.22,
        7.8,
        13.91,
        0.0
      ],
      "code_scores": [
        66.46,
        59.92,
        3.58,
        27.14,
        19.68,
        70.73,
        58.37,
        4.3,
        26.1,
        24.21,
        72.56,
        56.81,
        4.3,
        23.17,
        20.81
      ],
      "reasoning_scores": [
        68.47,
        53.95,
        0.27586957,
        26.4,
        62.71,
        53.95,
        0.28597826,
        26.56,
        60.34,
        54.85,
        0.29130435,
        26.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.72
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.47
              },
              {
                "metric": "lcb_test_output",
                "score": 21.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.14,
        "math_avg": -8.68,
        "code_avg": 16.41,
        "reasoning_avg": -2.8,
        "overall_avg": -0.05,
        "overall_efficiency": -0.000469,
        "general_efficiency": -0.046251,
        "math_efficiency": -0.078045,
        "code_efficiency": 0.147588,
        "reasoning_efficiency": -0.025167,
        "general_task_scores": [
          -18.78,
          22.38,
          -1.51,
          -22.67
        ],
        "math_task_scores": [
          -31.85,
          -10.6,
          -11.47,
          10.53,
          0.0
        ],
        "code_task_scores": [
          42.48,
          3.9,
          0.48,
          25.26,
          21.57
        ],
        "reasoning_task_scores": [
          -16.5,
          -8.31,
          -0.03,
          4.43
        ]
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "110k",
      "size_precise": "111183",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 33.31,
      "math_avg": 7.77,
      "code_avg": 20.72,
      "reasoning_avg": 28.51,
      "overall_avg": 22.58,
      "overall_efficiency": -0.09027,
      "general_efficiency": -0.076289,
      "math_efficiency": -0.161762,
      "code_efficiency": 0.016685,
      "reasoning_efficiency": -0.139715,
      "general_scores": [
        63.06,
        35.24,
        33.08,
        3.01714286,
        60.64,
        35.8975,
        33.1,
        2.31428571,
        61.74,
        34.9925,
        32.61,
        4.02285714
      ],
      "math_scores": [
        16.91,
        7.08,
        6.0,
        8.15,
        0.0,
        14.18,
        7.44,
        8.0,
        8.58,
        0.0,
        14.56,
        7.94,
        9.2,
        8.45,
        0.0
      ],
      "code_scores": [
        47.56,
        52.53,
        2.51,
        0.21,
        0.23,
        45.73,
        52.53,
        3.94,
        0.0,
        0.0,
        48.17,
        53.7,
        2.87,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        64.41,
        27.74,
        0.22836957,
        18.24,
        65.08,
        28.16,
        0.2373913,
        17.6,
        66.78,
        37.2,
        0.23119565,
        16.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.74,
        "math_avg": -12.16,
        "code_avg": 1.25,
        "reasoning_avg": -10.51,
        "overall_avg": -6.79,
        "overall_efficiency": -0.09027,
        "general_efficiency": -0.076289,
        "math_efficiency": -0.161762,
        "code_efficiency": 0.016685,
        "reasoning_efficiency": -0.139715,
        "general_task_scores": [
          -2.56,
          15.04,
          -3.72,
          -31.71
        ],
        "math_task_scores": [
          -41.19,
          -12.21,
          -11.47,
          4.05,
          0.0
        ],
        "code_task_scores": [
          19.71,
          -1.55,
          -0.47,
          0.14,
          0.08
        ],
        "reasoning_task_scores": [
          -14.92,
          -31.53,
          -0.08,
          -4.72
        ]
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 32.25,
      "math_avg": 9.75,
      "code_avg": 29.19,
      "reasoning_avg": 27.02,
      "overall_avg": 24.55,
      "overall_efficiency": -0.072505,
      "general_efficiency": -0.102398,
      "math_efficiency": -0.153372,
      "code_efficiency": 0.146543,
      "reasoning_efficiency": -0.180794,
      "general_scores": [
        45.42,
        42.0775,
        31.85,
        11.7392857,
        54.05,
        37.0,
        33.67,
        10.0478571,
        42.18,
        37.8975,
        31.69,
        9.36214286
      ],
      "math_scores": [
        13.72,
        5.96,
        6.0,
        17.48,
        6.67,
        14.03,
        5.86,
        6.2,
        17.66,
        0.0,
        17.82,
        7.16,
        7.0,
        17.34,
        3.33
      ],
      "code_scores": [
        54.27,
        59.14,
        8.96,
        0.42,
        20.36,
        57.93,
        58.37,
        6.81,
        0.21,
        24.89,
        61.59,
        56.42,
        7.53,
        0.21,
        20.81
      ],
      "reasoning_scores": [
        55.59,
        29.31,
        0.22,
        18.08,
        61.69,
        32.06,
        0.23684783,
        15.68,
        60.34,
        32.29,
        0.22423913,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.77
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 22.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.21
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.8,
        "math_avg": -10.18,
        "code_avg": 9.73,
        "reasoning_avg": -12.0,
        "overall_avg": -4.81,
        "overall_efficiency": -0.072505,
        "general_efficiency": -0.102398,
        "math_efficiency": -0.153372,
        "code_efficiency": 0.146543,
        "reasoning_efficiency": -0.180794,
        "general_task_scores": [
          -17.15,
          18.65,
          -4.25,
          -24.45
        ],
        "math_task_scores": [
          -41.22,
          -13.37,
          -12.8,
          13.15,
          3.33
        ],
        "code_task_scores": [
          30.49,
          3.51,
          4.19,
          0.07,
          22.02
        ],
        "reasoning_task_scores": [
          -21.13,
          -31.34,
          -0.08,
          -4.67
        ]
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 18.3,
      "math_avg": 2.27,
      "code_avg": 13.69,
      "reasoning_avg": 24.1,
      "overall_avg": 14.59,
      "overall_efficiency": -0.268072,
      "general_efficiency": -0.376352,
      "math_efficiency": -0.320433,
      "code_efficiency": -0.104831,
      "reasoning_efficiency": -0.270672,
      "general_scores": [
        17.96,
        24.61,
        26.21,
        3.15428571,
        15.06,
        23.915,
        25.4,
        6.20571429,
        21.4,
        23.7825,
        24.58,
        7.35571429
      ],
      "math_scores": [
        1.82,
        2.14,
        3.4,
        2.76,
        0.0,
        2.81,
        2.5,
        2.6,
        3.21,
        0.0,
        2.96,
        3.12,
        4.2,
        2.51,
        0.0
      ],
      "code_scores": [
        14.02,
        47.47,
        0.36,
        12.94,
        0.45,
        8.54,
        46.3,
        0.72,
        10.44,
        4.3,
        6.71,
        46.69,
        1.08,
        4.18,
        1.13
      ],
      "reasoning_scores": [
        40.68,
        48.76,
        0.15815217,
        16.64,
        34.24,
        37.61,
        0.14782609,
        15.04,
        37.97,
        40.61,
        0.14130435,
        17.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.19
              },
              {
                "metric": "lcb_test_output",
                "score": 1.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -20.74,
        "math_avg": -17.66,
        "code_avg": -5.78,
        "reasoning_avg": -14.92,
        "overall_avg": -14.78,
        "overall_efficiency": -0.268072,
        "general_efficiency": -0.376352,
        "math_efficiency": -0.320433,
        "code_efficiency": -0.104831,
        "reasoning_efficiency": -0.270672,
        "general_task_scores": [
          -46.23,
          3.76,
          -11.25,
          -29.26
        ],
        "math_task_scores": [
          -53.88,
          -17.11,
          -15.8,
          -1.51,
          0.0
        ],
        "code_task_scores": [
          -17.68,
          -7.65,
          -2.86,
          8.98,
          1.96
        ],
        "reasoning_task_scores": [
          -42.71,
          -20.23,
          -0.16,
          -5.79
        ]
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 22.02,
      "math_avg": 7.57,
      "code_avg": 25.3,
      "reasoning_avg": 17.45,
      "overall_avg": 18.09,
      "overall_efficiency": -0.222656,
      "general_efficiency": -0.336171,
      "math_efficiency": -0.243882,
      "code_efficiency": 0.115183,
      "reasoning_efficiency": -0.425754,
      "general_scores": [
        32.95,
        30.26,
        20.24,
        4.34785714,
        31.67,
        34.065,
        22.19,
        8.03071429,
        24.39,
        29.7375,
        21.23,
        5.07357143
      ],
      "math_scores": [
        8.64,
        4.24,
        4.0,
        15.65,
        0.0,
        15.85,
        4.66,
        6.0,
        16.46,
        0.0,
        11.6,
        4.98,
        5.8,
        15.74,
        0.0
      ],
      "code_scores": [
        48.17,
        55.64,
        3.58,
        0.21,
        18.78,
        46.34,
        55.25,
        3.94,
        0.21,
        19.91,
        45.12,
        57.2,
        3.23,
        0.0,
        21.95
      ],
      "reasoning_scores": [
        30.51,
        26.58,
        0.14956522,
        7.6,
        32.54,
        33.66,
        0.15380435,
        9.92,
        31.19,
        29.33,
        0.15793478,
        7.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.03
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -17.03,
        "math_avg": -12.36,
        "code_avg": 5.84,
        "reasoning_avg": -21.57,
        "overall_avg": -11.28,
        "overall_efficiency": -0.222656,
        "general_efficiency": -0.336171,
        "math_efficiency": -0.243882,
        "code_efficiency": 0.115183,
        "reasoning_efficiency": -0.425754,
        "general_task_scores": [
          -34.7,
          11.01,
          -15.43,
          -29.01
        ],
        "math_task_scores": [
          -44.38,
          -15.07,
          -13.93,
          11.61,
          0.0
        ],
        "code_task_scores": [
          19.1,
          1.56,
          0.0,
          -0.07,
          20.21
        ],
        "reasoning_task_scores": [
          -48.93,
          -32.7,
          -0.16,
          -13.71
        ]
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 35.27,
      "math_avg": 15.37,
      "code_avg": 33.27,
      "reasoning_avg": 35.26,
      "overall_avg": 29.79,
      "overall_efficiency": 0.000415,
      "general_efficiency": -0.003673,
      "math_efficiency": -0.004444,
      "code_efficiency": 0.01344,
      "reasoning_efficiency": -0.00366,
      "general_scores": [
        41.75,
        46.4425,
        31.8,
        13.5342857,
        49.1,
        47.25,
        32.37,
        19.9864286,
        50.57,
        46.66,
        32.78,
        11.0335714
      ],
      "math_scores": [
        50.19,
        12.72,
        8.6,
        7.0,
        0.0,
        50.04,
        12.3,
        8.0,
        7.45,
        0.0,
        50.8,
        12.6,
        3.2,
        7.59,
        0.0
      ],
      "code_scores": [
        62.2,
        32.3,
        3.23,
        29.23,
        19.23,
        65.24,
        59.92,
        2.87,
        30.48,
        24.66,
        64.02,
        54.47,
        2.51,
        31.73,
        16.97
      ],
      "reasoning_scores": [
        66.44,
        51.65,
        0.39184783,
        21.84,
        63.39,
        52.43,
        0.40597826,
        23.52,
        67.8,
        50.81,
        0.35032609,
        24.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 20.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.77,
        "math_avg": -4.56,
        "code_avg": 13.8,
        "reasoning_avg": -3.76,
        "overall_avg": 0.43,
        "overall_efficiency": 0.000415,
        "general_efficiency": -0.003673,
        "math_efficiency": -0.004444,
        "code_efficiency": 0.01344,
        "reasoning_efficiency": -0.00366,
        "general_task_scores": [
          -17.23,
          26.44,
          -4.33,
          -19.98
        ],
        "math_task_scores": [
          -6.07,
          -7.16,
          -12.6,
          3.01,
          0.0
        ],
        "code_task_scores": [
          36.38,
          -5.57,
          -0.71,
          30.27,
          20.29
        ],
        "reasoning_task_scores": [
          -14.46,
          -10.93,
          0.07,
          1.07
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 21.49,
      "math_avg": 4.36,
      "code_avg": 18.88,
      "reasoning_avg": 20.0,
      "overall_avg": 16.18,
      "overall_efficiency": -0.376697,
      "general_efficiency": -0.501708,
      "math_efficiency": -0.444756,
      "code_efficiency": -0.016819,
      "reasoning_efficiency": -0.543504,
      "general_scores": [
        9.02,
        37.9425,
        17.48,
        8.68857143,
        16.57,
        35.685,
        19.77,
        13.1114286,
        29.23,
        39.87,
        17.6,
        12.875
      ],
      "math_scores": [
        5.46,
        1.94,
        2.0,
        14.54,
        0.0,
        1.52,
        1.72,
        1.6,
        16.62,
        0.0,
        2.65,
        1.88,
        3.2,
        12.33,
        0.0
      ],
      "code_scores": [
        41.46,
        50.97,
        1.43,
        1.25,
        0.23,
        41.46,
        47.47,
        1.43,
        4.59,
        0.9,
        42.68,
        47.86,
        0.36,
        0.63,
        0.45
      ],
      "reasoning_scores": [
        16.61,
        46.75,
        0.10391304,
        12.08,
        22.71,
        44.4,
        0.10847826,
        10.4,
        22.37,
        47.29,
        0.09326087,
        17.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -17.56,
        "math_avg": -15.57,
        "code_avg": -0.59,
        "reasoning_avg": -19.02,
        "overall_avg": -13.18,
        "overall_efficiency": -0.376697,
        "general_efficiency": -0.501708,
        "math_efficiency": -0.444756,
        "code_efficiency": -0.016819,
        "reasoning_efficiency": -0.543504,
        "general_task_scores": [
          -46.1,
          17.49,
          -18.37,
          -23.27
        ],
        "math_task_scores": [
          -53.2,
          -17.85,
          -16.93,
          10.16,
          0.0
        ],
        "code_task_scores": [
          14.43,
          -5.7,
          -2.51,
          1.95,
          0.53
        ],
        "reasoning_task_scores": [
          -59.78,
          -16.41,
          -0.21,
          -8.91
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 20.84,
      "math_avg": 3.44,
      "code_avg": 24.78,
      "reasoning_avg": 5.94,
      "overall_avg": 13.75,
      "overall_efficiency": -0.032261,
      "general_efficiency": -0.037618,
      "math_efficiency": -0.03407,
      "code_efficiency": 0.010976,
      "reasoning_efficiency": -0.068333,
      "general_scores": [
        20.64,
        30.5625,
        21.16,
        12.3721429,
        23.19,
        29.95,
        22.1,
        12.3792857,
        22.08,
        31.2475,
        16.43,
        7.91285714
      ],
      "math_scores": [
        0.23,
        0.08,
        0.2,
        16.73,
        0.0,
        0.0,
        0.02,
        0.0,
        18.34,
        0.0,
        0.08,
        0.02,
        0.0,
        15.85,
        0.0
      ],
      "code_scores": [
        60.98,
        60.7,
        0.0,
        0.42,
        0.0,
        65.24,
        57.2,
        0.0,
        0.21,
        0.23,
        65.24,
        61.48,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        13.9,
        3.87,
        0.14717391,
        8.0,
        13.9,
        3.95,
        0.14663043,
        5.84,
        14.24,
        4.39,
        0.15869565,
        2.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.97
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -18.21,
        "math_avg": -16.49,
        "code_avg": 5.31,
        "reasoning_avg": -33.08,
        "overall_avg": -15.62,
        "overall_efficiency": -0.032261,
        "general_efficiency": -0.037618,
        "math_efficiency": -0.03407,
        "code_efficiency": 0.010976,
        "reasoning_efficiency": -0.068333,
        "general_task_scores": [
          -42.4,
          10.25,
          -16.75,
          -23.94
        ],
        "math_task_scores": [
          -56.31,
          -19.66,
          -19.13,
          12.63,
          0.0
        ],
        "code_task_scores": [
          36.38,
          5.32,
          -3.58,
          0.0,
          0.08
        ],
        "reasoning_task_scores": [
          -66.33,
          -58.49,
          -0.16,
          -16.56
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 27.09,
      "math_avg": 4.74,
      "code_avg": 14.94,
      "reasoning_avg": 27.81,
      "overall_avg": 18.65,
      "overall_efficiency": -0.130005,
      "general_efficiency": -0.145005,
      "math_efficiency": -0.184201,
      "code_efficiency": -0.054852,
      "reasoning_efficiency": -0.135963,
      "general_scores": [
        45.84,
        37.3625,
        18.36,
        7.40285714,
        41.66,
        39.5775,
        15.6,
        5.87214286,
        51.92,
        38.46,
        19.07,
        3.98
      ],
      "math_scores": [
        5.08,
        3.94,
        5.2,
        13.53,
        0.0,
        3.64,
        4.1,
        4.8,
        8.42,
        0.0,
        4.85,
        3.4,
        4.0,
        10.21,
        0.0
      ],
      "code_scores": [
        21.95,
        37.74,
        0.0,
        11.06,
        1.13,
        20.12,
        39.69,
        0.0,
        19.21,
        0.9,
        20.73,
        36.96,
        0.0,
        13.78,
        0.9
      ],
      "reasoning_scores": [
        37.29,
        50.86,
        0.23043478,
        17.76,
        43.73,
        49.59,
        0.26793478,
        18.96,
        44.07,
        51.84,
        0.23804348,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.68
              },
              {
                "metric": "lcb_test_output",
                "score": 0.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.76
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -11.95,
        "math_avg": -15.19,
        "code_avg": -4.52,
        "reasoning_avg": -11.21,
        "overall_avg": -10.72,
        "overall_efficiency": -0.130005,
        "general_efficiency": -0.145005,
        "math_efficiency": -0.184201,
        "code_efficiency": -0.054852,
        "reasoning_efficiency": -0.135963,
        "general_task_scores": [
          -17.9,
          18.13,
          -18.97,
          -29.08
        ],
        "math_task_scores": [
          -51.89,
          -15.89,
          -14.53,
          6.38,
          0.0
        ],
        "code_task_scores": [
          -6.51,
          -16.34,
          -3.58,
          14.47,
          0.98
        ],
        "reasoning_task_scores": [
          -38.64,
          -11.8,
          -0.06,
          -3.55
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 36.01,
      "math_avg": 11.62,
      "code_avg": 29.55,
      "reasoning_avg": 38.52,
      "overall_avg": 28.93,
      "overall_efficiency": -0.005619,
      "general_efficiency": -0.038745,
      "math_efficiency": -0.10623,
      "code_efficiency": 0.128897,
      "reasoning_efficiency": -0.006399,
      "general_scores": [
        43.11,
        48.8325,
        36.7,
        9.22214286,
        47.59,
        47.2175,
        36.09,
        17.1,
        50.64,
        47.5275,
        36.17,
        11.9664286
      ],
      "math_scores": [
        35.33,
        10.36,
        10.0,
        7.0,
        0.0,
        27.6,
        10.16,
        8.6,
        6.71,
        0.0,
        31.39,
        9.84,
        9.8,
        7.45,
        0.0
      ],
      "code_scores": [
        52.44,
        53.7,
        1.08,
        23.8,
        19.68,
        46.95,
        53.7,
        2.51,
        29.02,
        12.9,
        48.78,
        53.31,
        1.08,
        26.72,
        17.65
      ],
      "reasoning_scores": [
        77.97,
        54.67,
        0.35402174,
        21.92,
        77.97,
        53.19,
        0.36195652,
        20.48,
        78.31,
        55.03,
        0.35543478,
        21.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.56
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.51
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.03,
        "math_avg": -8.31,
        "code_avg": 10.09,
        "reasoning_avg": -0.5,
        "overall_avg": -0.44,
        "overall_efficiency": -0.005619,
        "general_efficiency": -0.038745,
        "math_efficiency": -0.10623,
        "code_efficiency": 0.128897,
        "reasoning_efficiency": -0.006399,
        "general_task_scores": [
          -17.26,
          27.52,
          -0.33,
          -22.07
        ],
        "math_task_scores": [
          -24.97,
          -9.58,
          -9.73,
          2.71,
          0.0
        ],
        "code_task_scores": [
          21.95,
          -0.9,
          -2.02,
          26.3,
          16.74
        ],
        "reasoning_task_scores": [
          -2.26,
          -8.26,
          0.05,
          -0.75
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 35.01,
      "math_avg": 5.56,
      "code_avg": 22.51,
      "reasoning_avg": 29.54,
      "overall_avg": 23.15,
      "overall_efficiency": -0.050929,
      "general_efficiency": -0.033134,
      "math_efficiency": -0.117827,
      "code_efficiency": 0.024976,
      "reasoning_efficiency": -0.077731,
      "general_scores": [
        48.8,
        42.0,
        29.99,
        21.575,
        48.26,
        44.71,
        27.82,
        18.9085714,
        48.3,
        43.045,
        29.01,
        17.6428571
      ],
      "math_scores": [
        8.72,
        6.84,
        7.2,
        6.59,
        0.0,
        9.86,
        6.38,
        7.6,
        6.66,
        0.0,
        5.53,
        5.96,
        5.6,
        6.46,
        0.0
      ],
      "code_scores": [
        43.29,
        50.58,
        0.0,
        15.24,
        6.33,
        40.85,
        54.09,
        0.0,
        13.78,
        4.07,
        42.68,
        45.53,
        0.0,
        5.64,
        15.61
      ],
      "reasoning_scores": [
        45.42,
        45.04,
        0.28347826,
        21.6,
        46.1,
        46.26,
        0.275,
        18.96,
        62.37,
        46.6,
        0.27206522,
        21.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.55
              },
              {
                "metric": "lcb_test_output",
                "score": 8.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.61
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.04,
        "math_avg": -14.37,
        "code_avg": 3.05,
        "reasoning_avg": -9.48,
        "overall_avg": -6.21,
        "overall_efficiency": -0.050929,
        "general_efficiency": -0.033134,
        "math_efficiency": -0.117827,
        "code_efficiency": 0.024976,
        "reasoning_efficiency": -0.077731,
        "general_task_scores": [
          -15.92,
          22.91,
          -7.71,
          -15.45
        ],
        "math_task_scores": [
          -48.37,
          -13.31,
          -12.4,
          2.23,
          0.0
        ],
        "code_task_scores": [
          14.83,
          -4.4,
          -3.58,
          11.34,
          8.67
        ],
        "reasoning_task_scores": [
          -29.04,
          -16.59,
          -0.03,
          -1.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 39.97,
      "math_avg": 7.14,
      "code_avg": 23.02,
      "reasoning_avg": 36.11,
      "overall_avg": 26.56,
      "overall_efficiency": -0.150629,
      "general_efficiency": 0.049658,
      "math_efficiency": -0.687192,
      "code_efficiency": 0.191096,
      "reasoning_efficiency": -0.156079,
      "general_scores": [
        46.95,
        46.8975,
        32.6,
        29.5671429,
        51.47,
        48.0325,
        33.12,
        27.4607143,
        53.45,
        47.0875,
        34.54,
        28.4692857
      ],
      "math_scores": [
        7.51,
        8.14,
        11.0,
        7.36,
        0.0,
        10.24,
        8.12,
        9.0,
        7.38,
        3.33,
        8.34,
        7.44,
        9.0,
        6.91,
        3.33
      ],
      "code_scores": [
        37.2,
        54.47,
        0.0,
        19.62,
        9.05,
        36.59,
        52.53,
        0.0,
        10.86,
        11.31,
        39.63,
        54.47,
        0.0,
        6.05,
        13.57
      ],
      "reasoning_scores": [
        70.85,
        53.47,
        0.30804348,
        23.6,
        64.07,
        53.88,
        0.32304348,
        23.44,
        65.76,
        53.76,
        0.30043478,
        23.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.18
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.89
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.92,
        "math_avg": -12.79,
        "code_avg": 3.56,
        "reasoning_avg": -2.9,
        "overall_avg": -2.8,
        "overall_efficiency": -0.150629,
        "general_efficiency": 0.049658,
        "math_efficiency": -0.687192,
        "code_efficiency": 0.191096,
        "reasoning_efficiency": -0.156079,
        "general_task_scores": [
          -13.75,
          27.0,
          -3.23,
          -6.33
        ],
        "math_task_scores": [
          -47.71,
          -11.8,
          -9.53,
          2.88,
          2.22
        ],
        "code_task_scores": [
          10.37,
          -0.65,
          -3.58,
          11.97,
          11.31
        ],
        "reasoning_task_scores": [
          -13.45,
          -8.86,
          -0.0,
          1.47
        ]
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.16,
      "math_avg": 9.34,
      "code_avg": 29.57,
      "reasoning_avg": 34.15,
      "overall_avg": 28.81,
      "overall_efficiency": -0.024666,
      "general_efficiency": 0.137946,
      "math_efficiency": -0.468536,
      "code_efficiency": 0.447099,
      "reasoning_efficiency": -0.215171,
      "general_scores": [
        66.16,
        45.2575,
        34.66,
        21.9871429,
        66.55,
        44.4375,
        35.39,
        23.7114286,
        66.19,
        44.0875,
        34.03,
        23.5171429
      ],
      "math_scores": [
        21.46,
        9.32,
        10.0,
        8.2,
        0.0,
        21.08,
        9.22,
        8.8,
        8.31,
        0.0,
        17.06,
        8.54,
        9.8,
        8.27,
        0.0
      ],
      "code_scores": [
        51.22,
        56.42,
        7.89,
        16.91,
        22.17,
        48.78,
        55.64,
        8.24,
        5.22,
        25.34,
        47.56,
        54.09,
        6.45,
        14.61,
        23.08
      ],
      "reasoning_scores": [
        53.56,
        57.22,
        0.29184783,
        26.0,
        53.22,
        57.96,
        0.29641304,
        26.08,
        51.19,
        57.88,
        0.3075,
        25.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.25
              },
              {
                "metric": "lcb_test_output",
                "score": 23.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.12,
        "math_avg": -10.59,
        "code_avg": 10.11,
        "reasoning_avg": -4.86,
        "overall_avg": -0.56,
        "overall_efficiency": -0.024666,
        "general_efficiency": 0.137946,
        "math_efficiency": -0.468536,
        "code_efficiency": 0.447099,
        "reasoning_efficiency": -0.215171,
        "general_task_scores": [
          1.93,
          24.25,
          -1.96,
          -11.76
        ],
        "math_task_scores": [
          -36.54,
          -10.67,
          -9.67,
          3.92,
          0.0
        ],
        "code_task_scores": [
          21.75,
          0.91,
          3.95,
          12.04,
          23.53
        ],
        "reasoning_task_scores": [
          -27.68,
          -4.87,
          -0.01,
          3.89
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 19.4,
      "math_avg": 3.95,
      "code_avg": 8.24,
      "reasoning_avg": 19.68,
      "overall_avg": 12.82,
      "overall_efficiency": -0.054301,
      "general_efficiency": -0.064468,
      "math_efficiency": -0.052442,
      "code_efficiency": -0.036831,
      "reasoning_efficiency": -0.063463,
      "general_scores": [
        40.6,
        23.7175,
        11.78,
        0.74142857,
        39.93,
        24.685,
        10.92,
        0.19285714,
        40.34,
        27.2425,
        11.89,
        0.8
      ],
      "math_scores": [
        1.59,
        2.86,
        3.2,
        6.35,
        3.33,
        1.67,
        3.0,
        4.8,
        6.75,
        3.33,
        2.73,
        3.2,
        3.4,
        6.39,
        6.67
      ],
      "code_scores": [
        1.83,
        39.3,
        0.0,
        0.0,
        0.0,
        0.61,
        39.3,
        0.0,
        0.0,
        0.0,
        0.61,
        42.02,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        44.41,
        29.34,
        0.20933333,
        4.16,
        33.22,
        28.57,
        0.226,
        6.08,
        51.19,
        32.73,
        0.20533333,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 1.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.21
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -19.64,
        "math_avg": -15.98,
        "code_avg": -11.22,
        "reasoning_avg": -19.34,
        "overall_avg": -16.55,
        "overall_efficiency": -0.054301,
        "general_efficiency": -0.064468,
        "math_efficiency": -0.052442,
        "code_efficiency": -0.036831,
        "reasoning_efficiency": -0.063463,
        "general_task_scores": [
          -24.08,
          4.88,
          -25.12,
          -34.25
        ],
        "math_task_scores": [
          -54.41,
          -16.68,
          -15.4,
          2.16,
          4.44
        ],
        "code_task_scores": [
          -26.42,
          -14.26,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -37.4,
          -32.35,
          -0.1,
          -16.72
        ]
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 17.17,
      "math_avg": 11.25,
      "code_avg": 23.94,
      "reasoning_avg": 10.73,
      "overall_avg": 15.77,
      "overall_efficiency": -0.035768,
      "general_efficiency": -0.057556,
      "math_efficiency": -0.02284,
      "code_efficiency": 0.011779,
      "reasoning_efficiency": -0.074456,
      "general_scores": [
        5.68,
        31.015,
        24.76,
        0.0,
        21.59,
        31.8075,
        23.62,
        0.0,
        13.14,
        30.205,
        24.28,
        0.0
      ],
      "math_scores": [
        17.59,
        6.72,
        6.4,
        31.82,
        0.0,
        14.33,
        6.74,
        6.4,
        30.76,
        0.0,
        8.64,
        4.08,
        2.8,
        32.48,
        0.0
      ],
      "code_scores": [
        60.98,
        21.4,
        8.6,
        0.0,
        6.79,
        58.54,
        54.47,
        10.39,
        0.0,
        10.86,
        58.54,
        54.09,
        8.6,
        0.0,
        5.88
      ],
      "reasoning_scores": [
        28.47,
        6.76,
        0.13891304,
        4.4,
        31.53,
        7.43,
        0.12956522,
        7.04,
        29.49,
        6.15,
        0.12315217,
        7.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 59.35
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.83
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -21.87,
        "math_avg": -8.68,
        "code_avg": 4.48,
        "reasoning_avg": -28.29,
        "overall_avg": -13.59,
        "overall_efficiency": -0.035768,
        "general_efficiency": -0.057556,
        "math_efficiency": -0.02284,
        "code_efficiency": 0.011779,
        "reasoning_efficiency": -0.074456,
        "general_task_scores": [
          -50.9,
          10.67,
          -12.43,
          -34.83
        ],
        "math_task_scores": [
          -42.89,
          -13.85,
          -14.0,
          27.35,
          0.0
        ],
        "code_task_scores": [
          31.91,
          -11.15,
          5.62,
          -0.21,
          7.84
        ],
        "reasoning_task_scores": [
          -50.51,
          -55.78,
          -0.18,
          -15.92
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 43.46,
      "math_avg": 10.98,
      "code_avg": 30.67,
      "reasoning_avg": 37.16,
      "overall_avg": 30.57,
      "overall_efficiency": 0.011113,
      "general_efficiency": 0.040755,
      "math_efficiency": -0.082577,
      "code_efficiency": 0.103385,
      "reasoning_efficiency": -0.017112,
      "general_scores": [
        69.13,
        45.01,
        32.72,
        26.9557143,
        69.82,
        44.8675,
        32.23,
        28.5228571,
        68.85,
        43.4825,
        32.75,
        27.2242857
      ],
      "math_scores": [
        19.26,
        8.9,
        7.2,
        12.08,
        0.0,
        27.52,
        10.66,
        9.6,
        11.34,
        3.33,
        24.64,
        10.48,
        7.8,
        11.88,
        0.0
      ],
      "code_scores": [
        49.39,
        57.59,
        1.08,
        14.82,
        28.51,
        50.61,
        61.09,
        3.94,
        17.33,
        29.64,
        51.83,
        59.53,
        1.08,
        6.05,
        27.6
      ],
      "reasoning_scores": [
        68.14,
        55.67,
        0.17163043,
        23.44,
        73.56,
        56.42,
        0.18043478,
        19.52,
        70.17,
        57.16,
        0.17152174,
        21.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.03
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.73
              },
              {
                "metric": "lcb_test_output",
                "score": 28.58
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.42,
        "math_avg": -8.95,
        "code_avg": 11.21,
        "reasoning_avg": -1.85,
        "overall_avg": 1.2,
        "overall_efficiency": 0.011113,
        "general_efficiency": 0.040755,
        "math_efficiency": -0.082577,
        "code_efficiency": 0.103385,
        "reasoning_efficiency": -0.017112,
        "general_task_scores": [
          4.9,
          24.11,
          -4.08,
          -7.26
        ],
        "math_task_scores": [
          -32.6,
          -9.69,
          -11.0,
          7.43,
          1.11
        ],
        "code_task_scores": [
          23.17,
          4.93,
          -1.55,
          12.52,
          28.58
        ],
        "reasoning_task_scores": [
          -9.72,
          -6.14,
          -0.14,
          -0.64
        ]
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 24.02,
      "math_avg": 4.14,
      "code_avg": 18.0,
      "reasoning_avg": 17.56,
      "overall_avg": 15.93,
      "overall_efficiency": -0.012878,
      "general_efficiency": -0.0144,
      "math_efficiency": -0.015138,
      "code_efficiency": -0.001402,
      "reasoning_efficiency": -0.020571,
      "general_scores": [
        32.32,
        29.495,
        26.9,
        18.985,
        23.31,
        30.8075,
        18.87,
        16.0421429,
        30.22,
        33.365,
        18.76,
        9.20214286
      ],
      "math_scores": [
        0.08,
        0.02,
        0.0,
        20.82,
        0.0,
        0.08,
        0.02,
        0.0,
        21.54,
        0.0,
        0.23,
        0.04,
        0.0,
        19.24,
        0.0
      ],
      "code_scores": [
        37.2,
        47.08,
        0.0,
        0.0,
        0.0,
        37.8,
        55.25,
        0.0,
        0.0,
        0.0,
        38.41,
        53.7,
        0.0,
        0.63,
        0.0
      ],
      "reasoning_scores": [
        60.34,
        5.96,
        0.03206522,
        9.76,
        54.58,
        4.53,
        0.01586957,
        5.12,
        58.31,
        2.91,
        0.01293478,
        9.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.01
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.74
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -15.02,
        "math_avg": -15.79,
        "code_avg": -1.46,
        "reasoning_avg": -21.46,
        "overall_avg": -13.43,
        "overall_efficiency": -0.012878,
        "general_efficiency": -0.0144,
        "math_efficiency": -0.015138,
        "code_efficiency": -0.001402,
        "reasoning_efficiency": -0.020571,
        "general_task_scores": [
          -35.75,
          10.88,
          -15.14,
          -20.09
        ],
        "math_task_scores": [
          -56.28,
          -19.67,
          -19.2,
          16.19,
          0.0
        ],
        "code_task_scores": [
          10.36,
          -2.46,
          -3.58,
          0.0,
          0.0
        ],
        "reasoning_task_scores": [
          -22.6,
          -58.09,
          -0.29,
          -14.08
        ]
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 17.4,
      "math_avg": 0.81,
      "code_avg": 0.08,
      "reasoning_avg": 7.84,
      "overall_avg": 6.53,
      "overall_efficiency": -0.228348,
      "general_efficiency": -0.216488,
      "math_efficiency": -0.19124,
      "code_efficiency": -0.193887,
      "reasoning_efficiency": -0.311779,
      "general_scores": [
        29.71,
        25.09,
        13.99,
        0.67214286,
        29.97,
        24.2225,
        12.53,
        2.99428571
      ],
      "math_scores": [
        0.08,
        0.02,
        1.6,
        3.0,
        0.0,
        0.0,
        0.0,
        0.2,
        3.16,
        0.0
      ],
      "code_scores": [
        0.0,
        0.39,
        0.0,
        0.0,
        0.0,
        0.0,
        0.39,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        25.42,
        6.22,
        0.0401087,
        0.4,
        23.05,
        7.57,
        0.02380435,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.84
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 13.26
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.83
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -21.65,
        "math_avg": -19.12,
        "code_avg": -19.39,
        "reasoning_avg": -31.18,
        "overall_avg": -22.83,
        "overall_efficiency": -0.228348,
        "general_efficiency": -0.216488,
        "math_efficiency": -0.19124,
        "code_efficiency": -0.193887,
        "reasoning_efficiency": -0.311779,
        "general_task_scores": [
          -34.53,
          4.32,
          -23.39,
          -33.0
        ],
        "math_task_scores": [
          -56.37,
          -19.69,
          -18.3,
          -1.26,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -54.08,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -56.1,
          -55.66,
          -0.28,
          -21.88
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 27.71,
      "math_avg": 5.37,
      "code_avg": 18.2,
      "reasoning_avg": 23.89,
      "overall_avg": 18.79,
      "overall_efficiency": -0.469949,
      "general_efficiency": -0.50388,
      "math_efficiency": -0.647259,
      "code_efficiency": -0.056355,
      "reasoning_efficiency": -0.672299,
      "general_scores": [
        33.84,
        41.3275,
        25.41,
        12.4314286,
        29.66,
        45.1325,
        32.35,
        13.9057143,
        25.22,
        40.27,
        24.38,
        8.57928571
      ],
      "math_scores": [
        2.65,
        3.44,
        4.2,
        17.66,
        3.33,
        3.71,
        4.68,
        4.4,
        13.98,
        0.0,
        1.21,
        2.06,
        2.2,
        16.98,
        0.0
      ],
      "code_scores": [
        37.2,
        42.41,
        0.0,
        20.46,
        2.49,
        34.76,
        35.02,
        0.0,
        13.99,
        2.26,
        31.71,
        44.75,
        0.0,
        5.22,
        2.71
      ],
      "reasoning_scores": [
        59.32,
        29.31,
        0.08576087,
        5.04,
        63.73,
        31.61,
        0.08815217,
        10.0,
        64.75,
        20.22,
        0.06619565,
        2.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.57
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 34.56
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 40.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.22
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -11.34,
        "math_avg": -14.56,
        "code_avg": -1.27,
        "reasoning_avg": -15.13,
        "overall_avg": -10.57,
        "overall_efficiency": -0.469949,
        "general_efficiency": -0.50388,
        "math_efficiency": -0.647259,
        "code_efficiency": -0.056355,
        "reasoning_efficiency": -0.672299,
        "general_task_scores": [
          -34.8,
          21.9,
          -9.27,
          -23.19
        ],
        "math_task_scores": [
          -53.89,
          -16.31,
          -15.6,
          11.87,
          1.11
        ],
        "code_task_scores": [
          7.12,
          -13.74,
          -3.58,
          13.01,
          2.49
        ],
        "reasoning_task_scores": [
          -17.74,
          -35.51,
          -0.23,
          -16.24
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "RAG-v1",
      "domain": "code",
      "general_avg": 14.2,
      "math_avg": 3.58,
      "code_avg": 10.52,
      "reasoning_avg": 9.64,
      "overall_avg": 9.49,
      "overall_efficiency": -0.387107,
      "general_efficiency": -0.483788,
      "math_efficiency": -0.318353,
      "code_efficiency": -0.174202,
      "reasoning_efficiency": -0.572084,
      "general_scores": [
        9.15,
        17.085,
        21.15,
        9.11928571,
        9.68,
        17.6125,
        21.47,
        9.11928571,
        9.94,
        15.955,
        21.02,
        9.11928571
      ],
      "math_scores": [
        1.14,
        2.44,
        4.8,
        5.42,
        0.0,
        6.14,
        5.12,
        5.4,
        7.29,
        3.33,
        0.45,
        1.36,
        2.8,
        4.7,
        3.33
      ],
      "code_scores": [
        0.0,
        50.19,
        0.0,
        0.0,
        0.0,
        0.61,
        52.92,
        0.0,
        0.0,
        0.0,
        0.0,
        54.09,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        27.12,
        11.12,
        0.11869565,
        0.96,
        25.76,
        11.09,
        0.11402174,
        0.96,
        25.76,
        11.28,
        0.1125,
        1.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.4
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.21
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -24.84,
        "math_avg": -16.35,
        "code_avg": -8.95,
        "reasoning_avg": -29.38,
        "overall_avg": -19.88,
        "overall_efficiency": -0.387107,
        "general_efficiency": -0.483788,
        "math_efficiency": -0.318353,
        "code_efficiency": -0.174202,
        "reasoning_efficiency": -0.572084,
        "general_task_scores": [
          -54.78,
          -3.46,
          -15.44,
          -25.71
        ],
        "math_task_scores": [
          -53.83,
          -16.73,
          -14.87,
          1.46,
          2.22
        ],
        "code_task_scores": [
          -27.24,
          -2.07,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -54.13,
          -51.4,
          -0.19,
          -21.01
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "51.4k",
      "size_precise": "51354",
      "link": "https://huggingface.co/datasets/glaiveai/RAG-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 14.53,
      "math_avg": 1.24,
      "code_avg": 9.23,
      "reasoning_avg": 7.64,
      "overall_avg": 8.16,
      "overall_efficiency": -0.269878,
      "general_efficiency": -0.312024,
      "math_efficiency": -0.237882,
      "code_efficiency": -0.130327,
      "reasoning_efficiency": -0.399282,
      "general_scores": [
        16.12,
        26.68,
        12.63,
        0.00857143,
        18.69,
        23.56,
        12.6,
        3.35285714,
        23.97,
        21.7775,
        11.67,
        3.28
      ],
      "math_scores": [
        0.0,
        0.18,
        0.4,
        4.04,
        0.0,
        1.74,
        2.32,
        2.2,
        3.84,
        0.0,
        0.08,
        0.58,
        1.2,
        1.99,
        0.0
      ],
      "code_scores": [
        0.0,
        42.41,
        0.0,
        1.04,
        0.0,
        0.0,
        46.3,
        0.0,
        0.0,
        0.0,
        0.0,
        48.64,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        25.76,
        0.09,
        0.11967391,
        3.92,
        30.17,
        2.62,
        0.14521739,
        3.84,
        22.03,
        0.02,
        0.13413043,
        2.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 12.3
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.78
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -24.52,
        "math_avg": -18.69,
        "code_avg": -10.24,
        "reasoning_avg": -31.37,
        "overall_avg": -21.21,
        "overall_efficiency": -0.269878,
        "general_efficiency": -0.312024,
        "math_efficiency": -0.237882,
        "code_efficiency": -0.130327,
        "reasoning_efficiency": -0.399282,
        "general_task_scores": [
          -44.78,
          3.67,
          -24.35,
          -32.62
        ],
        "math_task_scores": [
          -55.8,
          -18.67,
          -17.93,
          -1.05,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -8.69,
          -3.58,
          0.14,
          0.0
        ],
        "reasoning_task_scores": [
          -54.35,
          -61.65,
          -0.18,
          -18.53
        ]
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 7.56,
      "math_avg": 0.02,
      "code_avg": 8.33,
      "reasoning_avg": 3.26,
      "overall_avg": 4.79,
      "overall_efficiency": -0.224633,
      "general_efficiency": -0.287819,
      "math_efficiency": -0.181989,
      "code_efficiency": -0.101823,
      "reasoning_efficiency": -0.326898,
      "general_scores": [
        0.01,
        16.575,
        0.7,
        0.015,
        20.42,
        12.955,
        9.79,
        0.0
      ],
      "math_scores": [
        0.0,
        0.04,
        0.0,
        0.09,
        0.0,
        0.0,
        0.0,
        0.0,
        0.07,
        0.0
      ],
      "code_scores": [
        0.0,
        36.58,
        0.0,
        0.0,
        0.0,
        0.0,
        46.69,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        23.73,
        0.0,
        0.00076087,
        0.0,
        1.36,
        0.95,
        0.0001087,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.24
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 41.64
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.48
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -31.49,
        "math_avg": -19.91,
        "code_avg": -11.14,
        "reasoning_avg": -35.76,
        "overall_avg": -24.58,
        "overall_efficiency": -0.224633,
        "general_efficiency": -0.287819,
        "math_efficiency": -0.181989,
        "code_efficiency": -0.101823,
        "reasoning_efficiency": -0.326898,
        "general_task_scores": [
          -54.15,
          -5.58,
          -31.41,
          -34.82
        ],
        "math_task_scores": [
          -56.41,
          -19.68,
          -19.2,
          -4.26,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -12.83,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -67.8,
          -62.08,
          -0.31,
          -22.08
        ]
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 41.43,
      "math_avg": 15.16,
      "code_avg": 36.63,
      "reasoning_avg": 38.12,
      "overall_avg": 32.84,
      "overall_efficiency": 0.012009,
      "general_efficiency": 0.008252,
      "math_efficiency": -0.016503,
      "code_efficiency": 0.059383,
      "reasoning_efficiency": -0.003096,
      "general_scores": [
        66.26,
        41.69,
        35.11,
        19.05,
        66.63,
        43.42,
        35.55,
        24.8442857,
        64.23,
        41.3525,
        35.48,
        23.5635714
      ],
      "math_scores": [
        29.11,
        14.48,
        14.8,
        11.56,
        6.67,
        32.98,
        16.52,
        16.2,
        11.34,
        3.33,
        29.72,
        14.74,
        14.2,
        11.74,
        0.0
      ],
      "code_scores": [
        66.46,
        65.37,
        7.17,
        25.89,
        23.3,
        65.24,
        64.98,
        3.58,
        13.99,
        25.79,
        67.68,
        65.37,
        6.81,
        19.83,
        28.05
      ],
      "reasoning_scores": [
        64.75,
        60.15,
        0.32402174,
        28.16,
        63.39,
        59.12,
        0.33869565,
        28.96,
        61.36,
        61.17,
        0.31847826,
        29.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.71
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.15
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.49
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.6
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.46
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.85
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.9
              },
              {
                "metric": "lcb_test_output",
                "score": 25.71
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.39,
        "math_avg": -4.77,
        "code_avg": 17.17,
        "reasoning_avg": -0.89,
        "overall_avg": 3.47,
        "overall_efficiency": 0.012009,
        "general_efficiency": 0.008252,
        "math_efficiency": -0.016503,
        "code_efficiency": 0.059383,
        "reasoning_efficiency": -0.003096,
        "general_task_scores": [
          1.34,
          21.81,
          -1.27,
          -12.34
        ],
        "math_task_scores": [
          -25.81,
          -4.45,
          -4.13,
          7.21,
          3.33
        ],
        "code_task_scores": [
          39.02,
          10.77,
          2.27,
          19.69,
          25.71
        ],
        "reasoning_task_scores": [
          -17.17,
          -2.41,
          0.02,
          6.77
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 39.75,
      "math_avg": 13.04,
      "code_avg": 33.03,
      "reasoning_avg": 35.11,
      "overall_avg": 30.23,
      "overall_efficiency": 0.011733,
      "general_efficiency": 0.009542,
      "math_efficiency": -0.093163,
      "code_efficiency": 0.183413,
      "reasoning_efficiency": -0.052859,
      "general_scores": [
        68.75,
        29.31,
        34.56,
        0.69714286,
        71.27,
        48.2975,
        36.37,
        16.58,
        70.78,
        47.68,
        35.97,
        16.7542857
      ],
      "math_scores": [
        8.26,
        6.28,
        5.8,
        11.33,
        0.0,
        35.56,
        17.18,
        15.4,
        11.02,
        0.0,
        38.29,
        16.96,
        18.0,
        11.56,
        0.0
      ],
      "code_scores": [
        50.0,
        59.92,
        3.94,
        8.35,
        0.23,
        65.24,
        65.37,
        7.89,
        26.93,
        24.66,
        67.07,
        62.26,
        8.24,
        23.8,
        21.49
      ],
      "reasoning_scores": [
        55.93,
        43.83,
        0.21619565,
        10.48,
        64.41,
        62.48,
        0.31043478,
        28.64,
        63.73,
        62.85,
        0.29043478,
        28.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 60.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 62.52
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 19.69
              },
              {
                "metric": "lcb_test_output",
                "score": 15.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.71,
        "math_avg": -6.89,
        "code_avg": 13.56,
        "reasoning_avg": -3.91,
        "overall_avg": 0.87,
        "overall_efficiency": 0.011733,
        "general_efficiency": 0.009542,
        "math_efficiency": -0.093163,
        "code_efficiency": 0.183413,
        "reasoning_efficiency": -0.052859,
        "general_task_scores": [
          5.9,
          21.42,
          -1.02,
          -23.49
        ],
        "math_task_scores": [
          -29.04,
          -6.23,
          -6.13,
          6.96,
          0.0
        ],
        "code_task_scores": [
          33.33,
          8.05,
          3.11,
          19.48,
          15.46
        ],
        "reasoning_task_scores": [
          -18.98,
          -6.17,
          -0.04,
          0.35
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 17.81,
      "math_avg": 5.95,
      "code_avg": 25.06,
      "reasoning_avg": 15.93,
      "overall_avg": 16.19,
      "overall_efficiency": -0.289968,
      "general_efficiency": -0.467276,
      "math_efficiency": -0.307575,
      "code_efficiency": 0.123013,
      "reasoning_efficiency": -0.508031,
      "general_scores": [
        20.61,
        30.395,
        24.21,
        0.0,
        15.94,
        28.015,
        25.28,
        0.0,
        19.02,
        26.9925,
        23.25,
        0.0
      ],
      "math_scores": [
        2.2,
        2.1,
        2.2,
        20.3,
        3.33,
        3.03,
        2.5,
        3.2,
        19.47,
        3.33,
        1.59,
        1.92,
        2.4,
        21.7,
        0.0
      ],
      "code_scores": [
        38.41,
        48.64,
        5.02,
        10.02,
        24.21,
        38.41,
        54.09,
        5.38,
        10.02,
        23.08,
        35.98,
        50.58,
        5.02,
        4.38,
        22.62
      ],
      "reasoning_scores": [
        40.34,
        9.88,
        0.17923913,
        3.76,
        59.32,
        14.92,
        0.18532609,
        3.36,
        47.12,
        9.84,
        0.16793478,
        2.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.52
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.17
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.6
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.14
              },
              {
                "metric": "lcb_test_output",
                "score": 23.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 11.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.07
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -21.24,
        "math_avg": -13.98,
        "code_avg": 5.59,
        "reasoning_avg": -23.09,
        "overall_avg": -13.18,
        "overall_efficiency": -0.289968,
        "general_efficiency": -0.467276,
        "math_efficiency": -0.307575,
        "code_efficiency": 0.123013,
        "reasoning_efficiency": -0.508031,
        "general_task_scores": [
          -45.85,
          8.13,
          -12.4,
          -34.83
        ],
        "math_task_scores": [
          -54.14,
          -17.53,
          -16.6,
          16.15,
          2.22
        ],
        "code_task_scores": [
          10.16,
          -3.37,
          1.56,
          7.93,
          23.3
        ],
        "reasoning_task_scores": [
          -31.41,
          -51.01,
          -0.13,
          -19.01
        ]
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 71,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 18.68,
      "math_avg": 1.76,
      "code_avg": 7.4,
      "reasoning_avg": 12.3,
      "overall_avg": 10.04,
      "overall_efficiency": -0.046183,
      "general_efficiency": -0.04866,
      "math_efficiency": -0.043404,
      "code_efficiency": -0.028836,
      "reasoning_efficiency": -0.063834,
      "general_scores": [
        27.15,
        31.1675,
        17.83,
        0.0,
        27.4,
        31.3975,
        16.76,
        0.0,
        24.81,
        30.2225,
        17.42,
        0.0
      ],
      "math_scores": [
        2.2,
        1.68,
        2.0,
        0.0,
        3.18,
        2.56,
        3.2,
        0.0,
        1.29,
        1.12,
        0.6,
        3.33
      ],
      "code_scores": [
        0.0,
        38.13,
        0.0,
        0.84,
        0.0,
        0.0,
        35.41,
        0.0,
        0.0,
        0.0,
        0.0,
        36.58,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        18.98,
        22.24,
        0.18326087,
        3.84,
        26.44,
        23.48,
        0.19141304,
        3.36,
        31.53,
        14.76,
        0.20543478,
        2.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.93
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 36.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.16
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -20.37,
        "math_avg": -18.17,
        "code_avg": -12.07,
        "reasoning_avg": -26.72,
        "overall_avg": -19.33,
        "overall_efficiency": -0.046183,
        "general_efficiency": -0.04866,
        "math_efficiency": -0.043404,
        "code_efficiency": -0.028836,
        "reasoning_efficiency": -0.063834,
        "general_task_scores": [
          -37.92,
          10.59,
          -19.31,
          -34.83
        ],
        "math_task_scores": [
          -54.19,
          -17.91,
          -17.27,
          1.11
        ],
        "code_task_scores": [
          -27.44,
          -17.76,
          -3.58,
          0.07,
          0.0
        ],
        "reasoning_task_scores": [
          -54.69,
          -42.4,
          -0.12,
          -18.88
        ]
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 72,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 18.07,
      "math_avg": 58.15,
      "code_avg": 5.31,
      "reasoning_avg": 24.76,
      "overall_avg": 26.57,
      "overall_efficiency": -0.029779,
      "general_efficiency": -0.223759,
      "math_efficiency": 0.407775,
      "code_efficiency": -0.151014,
      "reasoning_efficiency": -0.152118,
      "general_scores": [
        5.81,
        28.8825,
        29.88,
        7.71785714
      ],
      "math_scores": [
        87.26,
        77.64,
        76.8,
        35.73,
        13.33
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        1.88,
        1.36,
        0.61
      ],
      "reasoning_scores": [
        62.03,
        19.23,
        22.22,
        0.23956522,
        20.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.73
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.88
              },
              {
                "metric": "lcb_test_output",
                "score": 1.36
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -20.97,
        "math_avg": 38.22,
        "code_avg": -14.15,
        "reasoning_avg": -14.26,
        "overall_avg": -2.79,
        "overall_efficiency": -0.029779,
        "general_efficiency": -0.223759,
        "math_efficiency": 0.407775,
        "code_efficiency": -0.151014,
        "reasoning_efficiency": -0.152118,
        "general_task_scores": [
          -58.56,
          8.54,
          -6.77,
          -27.11
        ],
        "math_task_scores": [
          30.85,
          57.94,
          57.6,
          31.39,
          13.33
        ],
        "code_task_scores": [
          -27.44,
          -26.45,
          -3.58,
          1.67,
          1.36,
          -30.49
        ],
        "reasoning_task_scores": [
          -18.31,
          -43.33,
          -7.58,
          -0.07,
          -2.0
        ]
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 73,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 29.74,
      "math_avg": 47.13,
      "code_avg": 16.61,
      "reasoning_avg": 19.92,
      "overall_avg": 28.35,
      "overall_efficiency": -0.00763,
      "general_efficiency": -0.069928,
      "math_efficiency": 0.204369,
      "code_efficiency": -0.02145,
      "reasoning_efficiency": -0.143509,
      "general_scores": [
        53.88,
        29.295,
        27.44,
        8.33928571
      ],
      "math_scores": [
        86.81,
        61.38,
        62.8,
        21.34,
        3.33
      ],
      "code_scores": [
        28.66,
        44.36,
        5.02,
        1.46,
        2.49,
        17.68
      ],
      "reasoning_scores": [
        42.03,
        24.07,
        26.26,
        0.26532609,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.38
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 28.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 44.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 2.49
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 17.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.07
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.26
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.31,
        "math_avg": 27.2,
        "code_avg": -2.86,
        "reasoning_avg": -19.1,
        "overall_avg": -1.02,
        "overall_efficiency": -0.00763,
        "general_efficiency": -0.069928,
        "math_efficiency": 0.204369,
        "code_efficiency": -0.02145,
        "reasoning_efficiency": -0.143509,
        "general_task_scores": [
          -10.49,
          8.96,
          -9.21,
          -26.49
        ],
        "math_task_scores": [
          30.4,
          41.68,
          43.6,
          17.0,
          3.33
        ],
        "code_task_scores": [
          1.22,
          -10.11,
          1.44,
          1.25,
          2.49,
          -13.42
        ],
        "reasoning_task_scores": [
          -38.31,
          -38.49,
          -3.54,
          -0.04,
          -15.12
        ]
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 74,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 27.53,
      "math_avg": 29.85,
      "code_avg": 16.92,
      "reasoning_avg": 21.04,
      "overall_avg": 23.84,
      "overall_efficiency": -0.040069,
      "general_efficiency": -0.083459,
      "math_efficiency": 0.071913,
      "code_efficiency": -0.018454,
      "reasoning_efficiency": -0.130277,
      "general_scores": [
        48.68,
        28.945,
        23.18,
        9.31
      ],
      "math_scores": [
        71.42,
        29.2,
        30.8,
        11.18,
        6.67
      ],
      "code_scores": [
        21.34,
        46.3,
        4.3,
        0.63,
        0.9,
        28.05
      ],
      "reasoning_scores": [
        43.73,
        29.51,
        24.75,
        0.25086957,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 21.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 28.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.96
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -11.52,
        "math_avg": 9.92,
        "code_avg": -2.55,
        "reasoning_avg": -17.98,
        "overall_avg": -5.53,
        "overall_efficiency": -0.040069,
        "general_efficiency": -0.083459,
        "math_efficiency": 0.071913,
        "code_efficiency": -0.018454,
        "reasoning_efficiency": -0.130277,
        "general_task_scores": [
          -15.69,
          8.6,
          -13.47,
          -25.52
        ],
        "math_task_scores": [
          15.01,
          9.5,
          11.6,
          6.84,
          6.67
        ],
        "code_task_scores": [
          -6.1,
          -8.17,
          0.72,
          0.42,
          0.9,
          -3.05
        ],
        "reasoning_task_scores": [
          -36.61,
          -33.05,
          -5.05,
          -0.06,
          -15.12
        ]
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 75,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 15.54,
      "math_avg": 33.72,
      "code_avg": 11.63,
      "reasoning_avg": 17.67,
      "overall_avg": 19.64,
      "overall_efficiency": -0.58207,
      "general_efficiency": -1.406439,
      "math_efficiency": 0.825015,
      "code_efficiency": -0.46918,
      "reasoning_efficiency": -1.277677,
      "general_scores": [
        4.56,
        27.0825,
        23.12,
        7.41571429
      ],
      "math_scores": [
        67.25,
        42.32,
        39.6,
        14.41,
        5.0
      ],
      "code_scores": [
        24.39,
        15.95,
        5.38,
        2.09,
        0.0,
        21.95
      ],
      "reasoning_scores": [
        36.27,
        20.34,
        27.27,
        0.22206522,
        4.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.12
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.42
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.25
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.41
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.09
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 21.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.34
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -23.5,
        "math_avg": 13.79,
        "code_avg": -7.84,
        "reasoning_avg": -21.35,
        "overall_avg": -9.73,
        "overall_efficiency": -0.58207,
        "general_efficiency": -1.406439,
        "math_efficiency": 0.825015,
        "code_efficiency": -0.46918,
        "reasoning_efficiency": -1.277677,
        "general_task_scores": [
          -59.81,
          6.74,
          -13.53,
          -27.41
        ],
        "math_task_scores": [
          10.84,
          22.62,
          20.4,
          10.07,
          5.0
        ],
        "code_task_scores": [
          -3.05,
          -38.52,
          1.8,
          1.88,
          0.0,
          -9.15
        ],
        "reasoning_task_scores": [
          -44.07,
          -42.22,
          -2.53,
          -0.09,
          -17.84
        ]
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "size_precise": "16710",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k",
      "paper_link": "https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation",
      "tag": "general,code,math,science"
    },
    {
      "id": 76,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 21.3,
      "math_avg": 60.37,
      "code_avg": 21.28,
      "reasoning_avg": 0,
      "overall_avg": 34.31,
      "overall_efficiency": 0.043422,
      "general_efficiency": -0.155737,
      "math_efficiency": 0.354835,
      "code_efficiency": 0.015877,
      "reasoning_efficiency": -0.342396,
      "general_scores": [
        15.74,
        31.1125,
        28.91,
        9.43285714
      ],
      "math_scores": [
        84.76,
        75.6,
        70.4,
        37.74,
        33.33
      ],
      "code_scores": [
        37.8,
        49.42,
        10.39,
        8.77,
        0.0
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.43
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.76
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.74
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 8.77
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -17.75,
        "math_avg": 40.44,
        "code_avg": 1.81,
        "reasoning_avg": -39.02,
        "overall_avg": 4.95,
        "overall_efficiency": 0.043422,
        "general_efficiency": -0.155737,
        "math_efficiency": 0.354835,
        "code_efficiency": 0.015877,
        "reasoning_efficiency": -0.342396,
        "general_task_scores": [
          -48.63,
          10.77,
          -7.74,
          -25.4
        ],
        "math_task_scores": [
          28.35,
          55.9,
          51.2,
          33.4,
          33.33
        ],
        "code_task_scores": [
          10.36,
          -5.05,
          6.81,
          8.56,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,code,math,science"
    },
    {
      "id": 77,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 22.08,
      "math_avg": 28.1,
      "code_avg": 2.92,
      "reasoning_avg": 19.87,
      "overall_avg": 18.24,
      "overall_efficiency": -0.064799,
      "general_efficiency": -0.098855,
      "math_efficiency": 0.047621,
      "code_efficiency": -0.09638,
      "reasoning_efficiency": -0.111581,
      "general_scores": [
        28.35,
        22.535,
        25.75,
        11.6764286
      ],
      "math_scores": [
        64.52,
        32.82,
        30.8,
        12.38,
        0.0
      ],
      "code_scores": [
        0.0,
        17.12,
        0.0,
        0.42,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        40.0,
        22.55,
        21.72,
        0.17945652,
        14.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.35
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.82
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.42
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -16.97,
        "math_avg": 8.17,
        "code_avg": -16.54,
        "reasoning_avg": -19.15,
        "overall_avg": -11.12,
        "overall_efficiency": -0.064799,
        "general_efficiency": -0.098855,
        "math_efficiency": 0.047621,
        "code_efficiency": -0.09638,
        "reasoning_efficiency": -0.111581,
        "general_task_scores": [
          -36.02,
          2.2,
          -10.9,
          -23.15
        ],
        "math_task_scores": [
          8.11,
          13.12,
          11.6,
          8.04,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -37.35,
          -3.58,
          0.21,
          0.0,
          -31.1
        ],
        "reasoning_task_scores": [
          -40.34,
          -40.01,
          -8.08,
          -0.13,
          -7.2
        ]
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 78,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 21.66,
      "math_avg": 33.62,
      "code_avg": 32.56,
      "reasoning_avg": 19.01,
      "overall_avg": 26.72,
      "overall_efficiency": -0.034113,
      "general_efficiency": -0.2238,
      "math_efficiency": 0.176276,
      "code_efficiency": 0.168586,
      "reasoning_efficiency": -0.257514,
      "general_scores": [
        22.24,
        34.5225,
        25.73,
        4.14857143
      ],
      "math_scores": [
        77.26,
        37.64,
        40.6,
        12.62,
        0.0
      ],
      "code_scores": [
        51.83,
        51.36,
        4.3,
        16.28,
        24.66,
        46.95
      ],
      "reasoning_scores": [
        22.71,
        20.78,
        26.77,
        0.32717391,
        24.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.73
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.64
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 51.36
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.28
              },
              {
                "metric": "lcb_test_output",
                "score": 24.66
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 46.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -17.39,
        "math_avg": 13.69,
        "code_avg": 13.1,
        "reasoning_avg": -20.0,
        "overall_avg": -2.65,
        "overall_efficiency": -0.034113,
        "general_efficiency": -0.2238,
        "math_efficiency": 0.176276,
        "code_efficiency": 0.168586,
        "reasoning_efficiency": -0.257514,
        "general_task_scores": [
          -42.13,
          14.18,
          -10.92,
          -30.68
        ],
        "math_task_scores": [
          20.85,
          17.94,
          21.4,
          8.28,
          0.0
        ],
        "code_task_scores": [
          24.39,
          -3.11,
          0.72,
          16.07,
          24.66,
          15.85
        ],
        "reasoning_task_scores": [
          -57.63,
          -41.78,
          -3.03,
          0.02,
          2.4
        ]
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 79,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 0.12,
      "code_avg": 10.82,
      "reasoning_avg": 0,
      "overall_avg": 5.47,
      "overall_efficiency": -73.071275,
      "general_efficiency": -119.407219,
      "math_efficiency": -60.568807,
      "code_efficiency": -26.448522,
      "reasoning_efficiency": -119.322324,
      "general_scores": [],
      "math_scores": [
        0.08,
        0.04,
        0.2,
        0.3,
        0.0
      ],
      "code_scores": [
        0.0,
        54.09,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -39.05,
        "math_avg": -19.81,
        "code_avg": -8.65,
        "reasoning_avg": -39.02,
        "overall_avg": -23.89,
        "overall_efficiency": -73.071275,
        "general_efficiency": -119.407219,
        "math_efficiency": -60.568807,
        "code_efficiency": -26.448522,
        "reasoning_efficiency": -119.322324,
        "general_task_scores": [],
        "math_task_scores": [
          -56.33,
          -19.66,
          -19.0,
          -4.04,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -0.38,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 80,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 20.43,
      "math_avg": 30.31,
      "code_avg": 13.44,
      "reasoning_avg": 26.9,
      "overall_avg": 22.77,
      "overall_efficiency": -0.104801,
      "general_efficiency": -0.295894,
      "math_efficiency": 0.164959,
      "code_efficiency": -0.095696,
      "reasoning_efficiency": -0.192571,
      "general_scores": [
        8.32,
        35.7825,
        28.77,
        8.83571429
      ],
      "math_scores": [
        75.28,
        31.24,
        32.0,
        13.03,
        0.0
      ],
      "code_scores": [
        0.0,
        56.42,
        7.17,
        9.39,
        7.69,
        0.0
      ],
      "reasoning_scores": [
        52.88,
        17.73,
        29.8,
        0.33423913,
        33.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.24
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.17
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.39
              },
              {
                "metric": "lcb_test_output",
                "score": 7.69
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -18.62,
        "math_avg": 10.38,
        "code_avg": -6.02,
        "reasoning_avg": -12.12,
        "overall_avg": -6.59,
        "overall_efficiency": -0.104801,
        "general_efficiency": -0.295894,
        "math_efficiency": 0.164959,
        "code_efficiency": -0.095696,
        "reasoning_efficiency": -0.192571,
        "general_task_scores": [
          -56.05,
          15.44,
          -7.88,
          -25.99
        ],
        "math_task_scores": [
          18.87,
          11.54,
          12.8,
          8.69,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          1.95,
          3.59,
          9.18,
          7.69,
          -31.1
        ],
        "reasoning_task_scores": [
          -27.46,
          -44.83,
          0.0,
          0.02,
          11.68
        ]
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 20.02,
      "math_avg": 45.96,
      "code_avg": 17.95,
      "reasoning_avg": 29.75,
      "overall_avg": 28.42,
      "overall_efficiency": -0.00378,
      "general_efficiency": -0.076113,
      "math_efficiency": 0.104168,
      "code_efficiency": -0.006075,
      "reasoning_efficiency": -0.0371,
      "general_scores": [
        8.47,
        30.11,
        34.38,
        7.135
      ],
      "math_scores": [
        83.4,
        58.16,
        57.8,
        20.46,
        10.0
      ],
      "code_scores": [
        35.98,
        24.9,
        4.66,
        15.03,
        0.9,
        26.22
      ],
      "reasoning_scores": [
        77.97,
        14.65,
        36.36,
        0.31163043,
        19.44
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.46
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 24.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 15.03
              },
              {
                "metric": "lcb_test_output",
                "score": 0.9
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 26.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.97
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -19.02,
        "math_avg": 26.03,
        "code_avg": -1.52,
        "reasoning_avg": -9.27,
        "overall_avg": -0.94,
        "overall_efficiency": -0.00378,
        "general_efficiency": -0.076113,
        "math_efficiency": 0.104168,
        "code_efficiency": -0.006075,
        "reasoning_efficiency": -0.0371,
        "general_task_scores": [
          -55.9,
          9.77,
          -2.27,
          -27.69
        ],
        "math_task_scores": [
          26.99,
          38.46,
          38.6,
          16.12,
          10.0
        ],
        "code_task_scores": [
          8.54,
          -29.57,
          1.08,
          14.82,
          0.9,
          -4.88
        ],
        "reasoning_task_scores": [
          -2.37,
          -47.91,
          6.56,
          -0.0,
          -2.64
        ]
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 43.04,
      "math_avg": 28.32,
      "code_avg": 39.39,
      "reasoning_avg": 37.48,
      "overall_avg": 37.06,
      "overall_efficiency": 0.051288,
      "general_efficiency": 0.026636,
      "math_efficiency": 0.055913,
      "code_efficiency": 0.132841,
      "reasoning_efficiency": -0.010237,
      "general_scores": [
        72.79,
        54.23,
        41.44,
        32.9271429,
        71.1,
        0.361429,
        72.22,
        54.27,
        40.8,
        32.2985714,
        34.26,
        54.575,
        41.31,
        0.0
      ],
      "math_scores": [
        74.68,
        27.02,
        27.8,
        11.77,
        3.33,
        74.3,
        25.66,
        26.6,
        12.01,
        0.0
      ],
      "code_scores": [
        55.49,
        64.98,
        10.39,
        5.85,
        19.91,
        59.76,
        53.05,
        57.93,
        64.98,
        8.96,
        20.46,
        20.59,
        57.93,
        51.22
      ],
      "reasoning_scores": [
        81.36,
        46.23,
        0.34467391000000003,
        32.0,
        27.78,
        83.05,
        39.6,
        0.33173913,
        33.44,
        80.34,
        39.52,
        27.78,
        0.34402174,
        32.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.18
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.49
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.34
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.89
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.16
              },
              {
                "metric": "lcb_test_output",
                "score": 20.25
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 52.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.78
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.69
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.0,
        "math_avg": 8.39,
        "code_avg": 19.93,
        "reasoning_avg": -1.54,
        "overall_avg": 7.69,
        "overall_efficiency": 0.051288,
        "general_efficiency": 0.026636,
        "math_efficiency": 0.055913,
        "code_efficiency": 0.132841,
        "reasoning_efficiency": -0.010237,
        "general_task_scores": [
          -1.78,
          34.02,
          4.53,
          -18.43
        ],
        "math_task_scores": [
          18.08,
          6.64,
          8.0,
          7.55,
          1.66
        ],
        "code_task_scores": [
          30.34,
          10.51,
          6.1,
          12.95,
          20.25,
          21.04
        ],
        "reasoning_task_scores": [
          1.24,
          -20.78,
          -2.02,
          0.03,
          10.61
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 18.4,
      "math_avg": 42.47,
      "code_avg": 19.46,
      "reasoning_avg": 24.03,
      "overall_avg": 26.09,
      "overall_efficiency": -0.013104,
      "general_efficiency": -0.082613,
      "math_efficiency": 0.09018,
      "code_efficiency": -2e-05,
      "reasoning_efficiency": -0.059964,
      "general_scores": [
        8.88,
        32.085,
        25.42,
        7.21214286
      ],
      "math_scores": [
        78.62,
        51.74,
        53.6,
        18.38,
        10.0
      ],
      "code_scores": [
        27.44,
        55.25,
        3.23,
        4.38,
        4.52,
        21.95
      ],
      "reasoning_scores": [
        53.56,
        21.8,
        31.31,
        0.2898913,
        13.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.74
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.38
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 27.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.38
              },
              {
                "metric": "lcb_test_output",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 21.95
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.8
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -20.65,
        "math_avg": 22.54,
        "code_avg": -0.0,
        "reasoning_avg": -14.99,
        "overall_avg": -3.28,
        "overall_efficiency": -0.013104,
        "general_efficiency": -0.082613,
        "math_efficiency": 0.09018,
        "code_efficiency": -2e-05,
        "reasoning_efficiency": -0.059964,
        "general_task_scores": [
          -55.49,
          11.74,
          -11.23,
          -27.62
        ],
        "math_task_scores": [
          22.21,
          32.04,
          34.4,
          14.04,
          10.0
        ],
        "code_task_scores": [
          0.0,
          0.78,
          -0.35,
          4.17,
          4.52,
          -9.15
        ],
        "reasoning_task_scores": [
          -26.78,
          -40.76,
          1.51,
          -0.02,
          -8.88
        ]
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 84,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 19.35,
      "math_avg": 10.76,
      "code_avg": 18.64,
      "reasoning_avg": 19.22,
      "overall_avg": 16.99,
      "overall_efficiency": -0.620408,
      "general_efficiency": -0.987627,
      "math_efficiency": -0.459888,
      "code_efficiency": -0.041449,
      "reasoning_efficiency": -0.992667,
      "general_scores": [
        17.93,
        28.35,
        30.01,
        1.10571429
      ],
      "math_scores": [
        14.33,
        13.92,
        15.6,
        9.94,
        0.0
      ],
      "code_scores": [
        18.9,
        59.92,
        2.87,
        0.0,
        2.71,
        27.44
      ],
      "reasoning_scores": [
        47.12,
        12.22,
        29.8,
        0.24326087,
        6.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.93
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.94
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 2.71
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 27.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.12
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 12.22
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.72
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -19.7,
        "math_avg": -9.17,
        "code_avg": -0.83,
        "reasoning_avg": -19.8,
        "overall_avg": -12.37,
        "overall_efficiency": -0.620408,
        "general_efficiency": -0.987627,
        "math_efficiency": -0.459888,
        "code_efficiency": -0.041449,
        "reasoning_efficiency": -0.992667,
        "general_task_scores": [
          -46.44,
          8.01,
          -6.64,
          -33.72
        ],
        "math_task_scores": [
          -42.08,
          -5.78,
          -3.6,
          5.6,
          0.0
        ],
        "code_task_scores": [
          -8.54,
          5.45,
          -0.71,
          -0.21,
          2.71,
          -3.66
        ],
        "reasoning_task_scores": [
          -33.22,
          -50.34,
          0.0,
          -0.07,
          -15.36
        ]
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,code,math,science"
    },
    {
      "id": 85,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 9.51,
      "math_avg": 0.37,
      "code_avg": 10.77,
      "reasoning_avg": 8.31,
      "overall_avg": 7.24,
      "overall_efficiency": -1.122854,
      "general_efficiency": -1.499025,
      "math_efficiency": -0.992489,
      "code_efficiency": -0.441602,
      "reasoning_efficiency": -1.5583,
      "general_scores": [
        7.64,
        10.3225,
        19.25,
        0.0,
        7.85,
        11.29,
        19.47,
        0.0,
        8.34,
        10.71,
        19.24,
        0.0
      ],
      "math_scores": [
        0.0,
        0.0,
        0.0,
        1.24,
        0.0,
        0.0,
        0.26,
        0.0,
        2.96,
        0.0,
        0.0,
        0.0,
        0.0,
        1.15,
        0.0
      ],
      "code_scores": [
        0.0,
        57.2,
        0.0,
        0.0,
        0.0,
        0.0,
        57.98,
        0.0,
        0.0,
        0.0,
        0.0,
        46.3,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        21.36,
        10.62,
        0.1975,
        0.0,
        24.07,
        7.19,
        0.19771739,
        0.0,
        22.37,
        13.56,
        0.19847826,
        0.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.77
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 10.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -29.54,
        "math_avg": -19.56,
        "code_avg": -8.7,
        "reasoning_avg": -30.7,
        "overall_avg": -22.12,
        "overall_efficiency": -1.122854,
        "general_efficiency": -1.499025,
        "math_efficiency": -0.992489,
        "code_efficiency": -0.441602,
        "reasoning_efficiency": -1.5583,
        "general_task_scores": [
          -56.43,
          -9.57,
          -17.33,
          -34.83
        ],
        "math_task_scores": [
          -56.41,
          -19.61,
          -19.2,
          -2.56,
          0.0
        ],
        "code_task_scores": [
          -27.44,
          -0.64,
          -3.58,
          -0.21,
          0.0
        ],
        "reasoning_task_scores": [
          -57.74,
          -52.1,
          -0.11,
          -22.08
        ]
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 86,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 19.13,
      "math_avg": 58.15,
      "code_avg": 7.32,
      "reasoning_avg": 26.69,
      "overall_avg": 27.82,
      "overall_efficiency": -0.01947,
      "general_efficiency": -0.250753,
      "math_efficiency": 0.481099,
      "code_efficiency": -0.152969,
      "reasoning_efficiency": -0.155255,
      "general_scores": [
        7.04,
        29.915,
        30.98,
        8.57142857
      ],
      "math_scores": [
        86.66,
        74.54,
        73.2,
        36.34,
        20.0
      ],
      "code_scores": [
        3.66,
        20.62,
        0.0,
        7.31,
        4.98,
        7.32
      ],
      "reasoning_scores": [
        67.8,
        18.06,
        31.31,
        0.25532609,
        16.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.04
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.31
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.06
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.31
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -19.92,
        "math_avg": 38.22,
        "code_avg": -12.15,
        "reasoning_avg": -12.33,
        "overall_avg": -1.55,
        "overall_efficiency": -0.01947,
        "general_efficiency": -0.250753,
        "math_efficiency": 0.481099,
        "code_efficiency": -0.152969,
        "reasoning_efficiency": -0.155255,
        "general_task_scores": [
          -57.33,
          9.58,
          -5.67,
          -26.26
        ],
        "math_task_scores": [
          30.25,
          54.84,
          54.0,
          32.0,
          20.0
        ],
        "code_task_scores": [
          -23.78,
          -33.85,
          -3.58,
          7.1,
          4.98,
          -23.78
        ],
        "reasoning_task_scores": [
          -12.54,
          -44.5,
          1.51,
          -0.05,
          -6.08
        ]
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,code,math,science"
    },
    {
      "id": 87,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 16.65,
      "math_avg": 39.65,
      "code_avg": 7.36,
      "reasoning_avg": 24.25,
      "overall_avg": 21.98,
      "overall_efficiency": -0.93499,
      "general_efficiency": -2.834821,
      "math_efficiency": 2.495696,
      "code_efficiency": -1.531857,
      "reasoning_efficiency": -1.86898,
      "general_scores": [
        6.48,
        26.995,
        25.24,
        7.88928571
      ],
      "math_scores": [
        77.03,
        46.84,
        44.8,
        21.23,
        8.33
      ],
      "code_scores": [
        9.76,
        22.18,
        1.43,
        1.46,
        2.04,
        7.32
      ],
      "reasoning_scores": [
        66.44,
        16.93,
        31.82,
        0.23728261,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.48
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.24
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.18
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.46
              },
              {
                "metric": "lcb_test_output",
                "score": 2.04
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 7.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 16.93
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -22.4,
        "math_avg": 19.72,
        "code_avg": -12.1,
        "reasoning_avg": -14.76,
        "overall_avg": -7.39,
        "overall_efficiency": -0.93499,
        "general_efficiency": -2.834821,
        "math_efficiency": 2.495696,
        "code_efficiency": -1.531857,
        "reasoning_efficiency": -1.86898,
        "general_task_scores": [
          -57.89,
          6.66,
          -11.41,
          -26.94
        ],
        "math_task_scores": [
          20.62,
          27.14,
          25.6,
          16.89,
          8.33
        ],
        "code_task_scores": [
          -17.68,
          -32.29,
          -2.15,
          1.25,
          2.04,
          -23.78
        ],
        "reasoning_task_scores": [
          -13.9,
          -45.63,
          2.02,
          -0.07,
          -16.24
        ]
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ],
  "qwen": [
    {
      "id": 1,
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "domain": "instruct",
      "general_avg": 66.21,
      "math_avg": 56.81,
      "code_avg": 48.37,
      "reasoning_avg": 34.53,
      "overall_avg": 51.48,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        67.2,
        72.96,
        67.44,
        57.2342857
      ],
      "math_task_scores": [
        92.27,
        75.02,
        77.8,
        29.4,
        9.58375
      ],
      "code_task_scores": [
        64.02,
        74.71,
        11.11,
        43.01,
        41.86,
        55.49
      ],
      "reasoning_task_scores": [
        25.42,
        71.62,
        33.84,
        0.42445652,
        41.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.2
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.96
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.44
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 57.2342857
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.58375
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.02
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.01
              },
              {
                "metric": "lcb_test_output",
                "score": 41.86
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 55.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.62
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42445652
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.36
              }
            ]
          }
        ]
      }
    },
    {
      "id": 0,
      "name": "Qwen/Qwen2.5-7B",
      "domain": "base",
      "general_avg": 51.44,
      "math_avg": 42.8,
      "code_avg": 39.78,
      "reasoning_avg": 34.92,
      "overall_avg": 42.24,
      "overall_efficiency": 0,
      "general_efficiency": 0,
      "math_efficiency": 0,
      "code_efficiency": 0,
      "reasoning_efficiency": 0,
      "general_task_scores": [
        68.31,
        35.49,
        57.74,
        44.2378571
      ],
      "math_task_scores": [
        79.98,
        51.12,
        50.2,
        26.04,
        6.67
      ],
      "code_task_scores": [
        77.44,
        71.6,
        8.24,
        1.04,
        37.1,
        43.29
      ],
      "reasoning_task_scores": [
        36.6,
        69.46,
        34.85,
        0.392,
        33.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.2378571
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.98
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.24
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.04
              },
              {
                "metric": "lcb_test_output",
                "score": 37.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.6
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.46
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.392
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.28
              }
            ]
          }
        ]
      }
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 51.61,
      "math_avg": 39.59,
      "code_avg": 48.03,
      "reasoning_avg": 32.15,
      "overall_avg": 42.85,
      "overall_efficiency": 0.000753,
      "general_efficiency": 0.000206,
      "math_efficiency": -0.003968,
      "code_efficiency": 0.010199,
      "reasoning_efficiency": -0.003425,
      "general_scores": [
        64.81,
        43.28,
        56.03,
        40.0042857,
        64.89,
        45.345,
        56.56,
        39.2021429,
        67.31,
        46.2225,
        56.54,
        39.1364286
      ],
      "math_scores": [
        80.44,
        49.06,
        50.6,
        19.04,
        3.33,
        80.89,
        48.22,
        47.0,
        18.9,
        0.0,
        78.47,
        49.12,
        48.8,
        20.03,
        0.0
      ],
      "code_scores": [
        76.22,
        73.93,
        12.9,
        41.54,
        41.63,
        75.61,
        74.71,
        12.54,
        42.17,
        30.09,
        75.61,
        73.93,
        12.54,
        41.34,
        35.75
      ],
      "reasoning_scores": [
        27.8,
        66.9,
        0.46956522,
        30.08,
        28.47,
        67.1,
        0.44793478,
        29.76,
        35.59,
        67.55,
        0.46315217,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.68
              },
              {
                "metric": "lcb_test_output",
                "score": 35.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.17,
        "math_avg": -3.21,
        "code_avg": 8.25,
        "reasoning_avg": -2.77,
        "overall_avg": 0.61,
        "overall_efficiency": 0.000753,
        "general_efficiency": 0.000206,
        "math_efficiency": -0.003968,
        "code_efficiency": 0.010199,
        "reasoning_efficiency": -0.003425,
        "general_task_scores": [
          -2.64,
          9.46,
          -1.36,
          -4.79
        ],
        "math_task_scores": [
          -0.05,
          -2.32,
          -1.4,
          -6.72,
          -5.56
        ],
        "code_task_scores": [
          -1.63,
          2.59,
          4.42,
          40.64,
          -1.28
        ],
        "reasoning_task_scores": [
          -5.98,
          -2.28,
          0.07,
          -2.96
        ]
      },
      "affiliation": "Nomic AI",
      "year": "2023",
      "size": "809k",
      "size_precise": "808812",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations",
      "paper_link": "https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf",
      "tag": "general"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.35,
      "math_avg": 37.16,
      "code_avg": 46.75,
      "reasoning_avg": 32.24,
      "overall_avg": 42.88,
      "overall_efficiency": 0.012321,
      "general_efficiency": 0.075093,
      "math_efficiency": -0.108406,
      "code_efficiency": 0.134027,
      "reasoning_efficiency": -0.05143,
      "general_scores": [
        65.44,
        54.5275,
        57.92,
        41.5621429,
        65.7,
        54.805,
        58.13,
        43.3885714,
        65.7,
        55.52,
        58.19,
        43.3107143
      ],
      "math_scores": [
        80.59,
        42.94,
        42.6,
        13.28,
        3.33,
        80.74,
        42.5,
        43.2,
        13.62,
        6.67,
        80.29,
        43.42,
        44.0,
        13.62,
        6.67
      ],
      "code_scores": [
        71.34,
        71.98,
        10.75,
        42.38,
        37.1,
        72.56,
        72.76,
        11.11,
        42.59,
        33.26,
        73.17,
        72.37,
        9.68,
        41.13,
        39.14
      ],
      "reasoning_scores": [
        29.49,
        68.37,
        0.38934783,
        28.88,
        29.49,
        68.65,
        0.39,
        29.84,
        32.88,
        68.45,
        0.39347826,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 36.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.91,
        "math_avg": -5.64,
        "code_avg": 6.97,
        "reasoning_avg": -2.67,
        "overall_avg": 0.64,
        "overall_efficiency": 0.012321,
        "general_efficiency": 0.075093,
        "math_efficiency": -0.108406,
        "code_efficiency": 0.134027,
        "reasoning_efficiency": -0.05143,
        "general_task_scores": [
          -2.7,
          19.46,
          0.34,
          -1.49
        ],
        "math_task_scores": [
          0.56,
          -8.17,
          -6.93,
          -12.53,
          -1.11
        ],
        "code_task_scores": [
          -5.08,
          0.77,
          2.27,
          40.99,
          -0.6
        ],
        "reasoning_task_scores": [
          -5.98,
          -0.97,
          -0.0,
          -3.81
        ]
      },
      "affiliation": "Tatsu Lab",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 55.85,
      "math_avg": 44.42,
      "code_avg": 49.88,
      "reasoning_avg": 33.76,
      "overall_avg": 45.98,
      "overall_efficiency": 0.071921,
      "general_efficiency": 0.084692,
      "math_efficiency": 0.031127,
      "code_efficiency": 0.19405,
      "reasoning_efficiency": -0.022187,
      "general_scores": [
        72.8,
        51.095,
        58.83,
        42.3871429,
        73.14,
        50.86,
        58.98,
        39.0971429,
        72.02,
        51.565,
        58.89,
        40.5192857
      ],
      "math_scores": [
        87.95,
        53.36,
        55.2,
        21.3,
        6.67,
        87.19,
        53.06,
        53.2,
        21.48,
        6.67,
        86.88,
        52.8,
        52.4,
        21.48,
        6.67
      ],
      "code_scores": [
        76.22,
        72.76,
        12.54,
        37.16,
        46.61,
        76.83,
        75.1,
        11.47,
        39.04,
        47.51,
        77.44,
        73.15,
        13.26,
        41.54,
        47.51
      ],
      "reasoning_scores": [
        36.27,
        66.0,
        0.45934783,
        33.6,
        34.24,
        65.93,
        0.45108696,
        33.68,
        35.59,
        66.24,
        0.45119565,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.67
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.42
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.25
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.4,
        "math_avg": 1.62,
        "code_avg": 10.09,
        "reasoning_avg": -1.15,
        "overall_avg": 3.74,
        "overall_efficiency": 0.071921,
        "general_efficiency": 0.084692,
        "math_efficiency": 0.031127,
        "code_efficiency": 0.19405,
        "reasoning_efficiency": -0.022187,
        "general_task_scores": [
          4.34,
          15.68,
          1.16,
          -3.57
        ],
        "math_task_scores": [
          7.36,
          1.95,
          3.4,
          -4.62,
          0.0
        ],
        "code_task_scores": [
          -0.61,
          2.07,
          4.18,
          38.21,
          10.11
        ],
        "reasoning_task_scores": [
          -1.23,
          -3.4,
          0.06,
          -0.11
        ]
      },
      "affiliation": "Victor Gallego",
      "year": "2023",
      "size": "52k",
      "size_precise": "52002",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 52.22,
      "math_avg": 39.73,
      "code_avg": 45.41,
      "reasoning_avg": 32.38,
      "overall_avg": 42.44,
      "overall_efficiency": 0.013239,
      "general_efficiency": 0.05171,
      "math_efficiency": -0.204428,
      "code_efficiency": 0.374858,
      "reasoning_efficiency": -0.169185,
      "general_scores": [
        71.97,
        45.8825,
        58.32,
        32.6907143,
        71.08,
        44.0925,
        58.16,
        34.7157143,
        74.04,
        43.6075,
        57.55,
        34.5392857
      ],
      "math_scores": [
        75.59,
        47.64,
        42.8,
        20.91,
        10.0,
        73.69,
        48.24,
        46.0,
        20.75,
        13.33,
        74.68,
        48.18,
        44.0,
        20.19,
        10.0
      ],
      "code_scores": [
        73.78,
        73.15,
        11.11,
        41.96,
        30.77,
        73.78,
        71.21,
        10.04,
        44.05,
        26.92,
        73.17,
        72.37,
        11.83,
        42.38,
        24.66
      ],
      "reasoning_scores": [
        33.9,
        65.45,
        0.4548913,
        27.84,
        34.58,
        65.32,
        0.44652174,
        26.16,
        38.98,
        65.98,
        0.44967391,
        28.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 27.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.58
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.78,
        "math_avg": -3.07,
        "code_avg": 5.63,
        "reasoning_avg": -2.54,
        "overall_avg": 0.2,
        "overall_efficiency": 0.013239,
        "general_efficiency": 0.05171,
        "math_efficiency": -0.204428,
        "code_efficiency": 0.374858,
        "reasoning_efficiency": -0.169185,
        "general_task_scores": [
          4.05,
          9.04,
          0.27,
          -10.26
        ],
        "math_task_scores": [
          -5.33,
          -3.1,
          -5.93,
          -5.42,
          4.44
        ],
        "code_task_scores": [
          -3.86,
          0.64,
          2.75,
          41.76,
          -9.65
        ],
        "reasoning_task_scores": [
          -0.78,
          -3.88,
          0.06,
          -5.63
        ]
      },
      "affiliation": "Databricks",
      "year": "2023",
      "size": "15k",
      "size_precise": "15011",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k",
      "paper_link": "",
      "tag": "general,code"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 55.11,
      "math_avg": 50.21,
      "code_avg": 50.24,
      "reasoning_avg": 34.93,
      "overall_avg": 47.62,
      "overall_efficiency": 0.076956,
      "general_efficiency": 0.052368,
      "math_efficiency": 0.105864,
      "code_efficiency": 0.149414,
      "reasoning_efficiency": 0.000177,
      "general_scores": [
        67.49,
        53.35,
        56.41,
        41.225,
        69.97,
        52.2825,
        56.43,
        41.0078571,
        69.02,
        55.37,
        56.57,
        42.1971429
      ],
      "math_scores": [
        84.53,
        47.66,
        48.4,
        18.54,
        84.31,
        47.94,
        51.2,
        19.11,
        83.32,
        47.98,
        51.0,
        18.56
      ],
      "code_scores": [
        78.66,
        73.54,
        10.75,
        43.01,
        43.21,
        78.66,
        73.15,
        12.19,
        42.59,
        47.29,
        76.22,
        73.93,
        11.47,
        42.38,
        46.61
      ],
      "reasoning_scores": [
        37.29,
        66.88,
        0.45163043,
        36.16,
        37.29,
        67.34,
        0.44543478,
        36.32,
        32.54,
        67.74,
        0.44858696,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.67
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.48
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.74
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.66
              },
              {
                "metric": "lcb_test_output",
                "score": 45.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.67,
        "math_avg": 7.41,
        "code_avg": 10.46,
        "reasoning_avg": 0.01,
        "overall_avg": 5.39,
        "overall_efficiency": 0.076956,
        "general_efficiency": 0.052368,
        "math_efficiency": 0.105864,
        "code_efficiency": 0.149414,
        "reasoning_efficiency": 0.000177,
        "general_task_scores": [
          0.52,
          18.18,
          -1.27,
          -2.76
        ],
        "math_task_scores": [
          4.07,
          -3.26,
          0.0,
          -7.3
        ],
        "code_task_scores": [
          0.41,
          1.94,
          3.23,
          41.62,
          8.6
        ],
        "reasoning_task_scores": [
          -0.89,
          -2.14,
          0.06,
          2.96
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "size_precise": "70000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 55.43,
      "math_avg": 39.17,
      "code_avg": 49.71,
      "reasoning_avg": 34.1,
      "overall_avg": 44.6,
      "overall_efficiency": 0.016555,
      "general_efficiency": 0.027894,
      "math_efficiency": -0.02539,
      "code_efficiency": 0.069429,
      "reasoning_efficiency": -0.005716,
      "general_scores": [
        71.25,
        52.5275,
        58.1,
        40.08,
        72.25,
        53.2075,
        58.15,
        39.6814286,
        71.33,
        51.6175,
        57.95,
        39.0557143
      ],
      "math_scores": [
        82.18,
        47.22,
        47.6,
        19.26,
        0.0,
        81.2,
        48.04,
        48.8,
        19.31,
        0.0,
        80.89,
        47.14,
        46.8,
        19.13,
        0.0
      ],
      "code_scores": [
        80.49,
        74.71,
        11.11,
        40.71,
        42.99,
        78.05,
        74.32,
        10.75,
        41.96,
        42.08,
        78.66,
        72.76,
        11.83,
        40.71,
        44.57
      ],
      "reasoning_scores": [
        32.2,
        67.14,
        0.41554348,
        35.36,
        35.25,
        66.9,
        0.41228261,
        36.4,
        32.2,
        67.14,
        0.41163043,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.61
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.13
              },
              {
                "metric": "lcb_test_output",
                "score": 43.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.99,
        "math_avg": -3.63,
        "code_avg": 9.93,
        "reasoning_avg": -0.82,
        "overall_avg": 2.37,
        "overall_efficiency": 0.016555,
        "general_efficiency": 0.027894,
        "math_efficiency": -0.02539,
        "code_efficiency": 0.069429,
        "reasoning_efficiency": -0.005716,
        "general_task_scores": [
          3.3,
          16.96,
          0.33,
          -4.63
        ],
        "math_task_scores": [
          1.44,
          -3.65,
          -2.47,
          -6.81,
          -6.67
        ],
        "code_task_scores": [
          1.63,
          2.33,
          2.99,
          40.09,
          6.11
        ],
        "reasoning_task_scores": [
          -3.38,
          -2.4,
          0.02,
          2.43
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "size_precise": "143000",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k",
      "paper_link": "https://arxiv.org/abs/2304.12244",
      "tag": "general,math,code"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 52.44,
      "math_avg": 41.23,
      "code_avg": 49.73,
      "reasoning_avg": 32.47,
      "overall_avg": 43.97,
      "overall_efficiency": 1.596301,
      "general_efficiency": 0.916112,
      "math_efficiency": -1.4508,
      "code_efficiency": 9.177429,
      "reasoning_efficiency": -2.257537,
      "general_scores": [
        77.31,
        43.3625,
        57.86,
        29.9021429,
        76.91,
        41.78,
        57.18,
        33.8178571,
        77.2,
        43.195,
        57.96,
        32.7728571
      ],
      "math_scores": [
        72.33,
        50.04,
        49.6,
        22.99,
        10.0,
        71.11,
        50.36,
        48.6,
        22.9,
        16.67,
        71.19,
        50.08,
        46.0,
        23.24,
        13.33
      ],
      "code_scores": [
        74.39,
        70.82,
        11.47,
        45.93,
        42.08,
        72.56,
        69.65,
        10.75,
        46.35,
        51.13,
        73.78,
        69.65,
        12.19,
        45.93,
        49.32
      ],
      "reasoning_scores": [
        34.24,
        67.42,
        0.40869565,
        28.24,
        34.92,
        67.46,
        0.39793478,
        26.32,
        32.88,
        66.94,
        0.40413043,
        30.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.07
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.99,
        "math_avg": -1.57,
        "code_avg": 9.95,
        "reasoning_avg": -2.45,
        "overall_avg": 1.73,
        "overall_efficiency": 1.596301,
        "general_efficiency": 0.916112,
        "math_efficiency": -1.4508,
        "code_efficiency": 9.177429,
        "reasoning_efficiency": -2.257537,
        "general_task_scores": [
          8.83,
          7.29,
          -0.07,
          -12.08
        ],
        "math_task_scores": [
          -8.44,
          -0.96,
          -2.13,
          -3.0,
          6.66
        ],
        "code_task_scores": [
          -3.86,
          -1.56,
          3.23,
          45.03,
          10.41
        ],
        "reasoning_task_scores": [
          -2.59,
          -2.19,
          0.01,
          -5.09
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "size_precise": "1084",
      "link": "https://huggingface.co/datasets/GAIR/lima",
      "paper_link": "https://arxiv.org/abs/2305.11206",
      "tag": "general,math,code,science"
    },
    {
      "id": 8,
      "name": "Orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.36,
      "math_avg": 47.62,
      "code_avg": 48.01,
      "reasoning_avg": 50.09,
      "overall_avg": 51.02,
      "overall_efficiency": 0.008836,
      "general_efficiency": 0.006952,
      "math_efficiency": 0.004849,
      "code_efficiency": 0.008275,
      "reasoning_efficiency": 0.015269,
      "general_scores": [
        76.72,
        57.77,
        55.71,
        42.8821429,
        75.95,
        58.145,
        55.19,
        42.1885714,
        76.05,
        60.0675,
        55.23,
        44.3628571
      ],
      "math_scores": [
        86.88,
        59.16,
        59.4,
        21.27,
        10.0,
        87.57,
        59.84,
        60.2,
        21.18,
        13.33,
        85.67,
        60.6,
        61.2,
        21.36,
        6.67
      ],
      "code_scores": [
        75.0,
        71.98,
        12.54,
        39.67,
        40.95,
        75.0,
        74.32,
        12.9,
        34.86,
        40.95,
        72.56,
        74.32,
        14.34,
        40.71,
        40.05
      ],
      "reasoning_scores": [
        87.46,
        69.34,
        0.37869565,
        41.52,
        87.12,
        70.42,
        0.37706522,
        43.2,
        89.15,
        69.39,
        0.37978261,
        42.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.91,
        "math_avg": 4.82,
        "code_avg": 8.23,
        "reasoning_avg": 15.18,
        "overall_avg": 8.78,
        "overall_efficiency": 0.008836,
        "general_efficiency": 0.006952,
        "math_efficiency": 0.004849,
        "code_efficiency": 0.008275,
        "reasoning_efficiency": 0.015269,
        "general_task_scores": [
          7.93,
          23.17,
          -2.36,
          -1.1
        ],
        "math_task_scores": [
          6.73,
          8.75,
          10.07,
          -4.77,
          3.33
        ],
        "code_task_scores": [
          -3.25,
          1.94,
          5.02,
          37.37,
          3.55
        ],
        "reasoning_task_scores": [
          51.31,
          0.26,
          -0.01,
          9.09
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "size_precise": "994045",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
      "paper_link": "https://arxiv.org/abs/2407.03502",
      "tag": "general,math,code"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 53.96,
      "math_avg": 35.28,
      "code_avg": 46.77,
      "reasoning_avg": 32.76,
      "overall_avg": 42.2,
      "overall_efficiency": -4.2e-05,
      "general_efficiency": 0.002515,
      "math_efficiency": -0.007509,
      "code_efficiency": 0.006977,
      "reasoning_efficiency": -0.002149,
      "general_scores": [
        70.36,
        56.3125,
        57.35,
        34.2271429,
        70.12,
        55.5125,
        57.8,
        35.0807143,
        70.74,
        54.6775,
        58.39,
        26.9914286
      ],
      "math_scores": [
        82.71,
        41.02,
        40.2,
        19.76,
        3.33,
        82.41,
        34.3,
        29.4,
        19.67,
        0.0,
        84.08,
        35.48,
        34.2,
        19.33,
        3.33
      ],
      "code_scores": [
        77.44,
        70.82,
        12.9,
        36.95,
        43.67,
        73.17,
        71.21,
        12.19,
        33.82,
        41.18,
        71.34,
        69.65,
        12.9,
        33.4,
        40.95
      ],
      "reasoning_scores": [
        29.49,
        67.31,
        0.41141304,
        32.8,
        33.22,
        67.85,
        0.41532609,
        27.52,
        37.63,
        66.4,
        0.4398913,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.72
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.52,
        "math_avg": -7.52,
        "code_avg": 6.99,
        "reasoning_avg": -2.15,
        "overall_avg": -0.04,
        "overall_efficiency": -4.2e-05,
        "general_efficiency": 0.002515,
        "math_efficiency": -0.007509,
        "code_efficiency": 0.006977,
        "reasoning_efficiency": -0.002149,
        "general_task_scores": [
          2.1,
          20.01,
          0.11,
          -12.14
        ],
        "math_task_scores": [
          3.09,
          -14.19,
          -15.6,
          -6.45,
          -4.45
        ],
        "code_task_scores": [
          -3.46,
          -1.04,
          4.42,
          33.68,
          4.83
        ],
        "reasoning_task_scores": [
          -3.15,
          -2.27,
          0.03,
          -3.28
        ]
      },
      "affiliation": "NousResearch",
      "year": "2024",
      "size": "1M",
      "size_precise": "1001551",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.52,
      "math_avg": 6.75,
      "code_avg": 14.57,
      "reasoning_avg": 27.38,
      "overall_avg": 22.05,
      "overall_efficiency": -0.24483,
      "general_efficiency": -0.144686,
      "math_efficiency": -0.437348,
      "code_efficiency": -0.30583,
      "reasoning_efficiency": -0.091458,
      "general_scores": [
        49.32,
        39.67,
        30.6535714,
        48.58,
        42.07,
        29.4264286,
        46.43,
        43.67,
        25.8307143
      ],
      "math_scores": [
        5.16,
        7.2,
        7.8,
        6.67,
        4.78,
        6.5,
        5.8,
        6.67,
        10.46,
        7.06,
        6.2,
        6.67
      ],
      "code_scores": [
        13.41,
        28.4,
        0.0,
        31.73,
        0.0,
        14.63,
        28.4,
        0.0,
        30.69,
        0.0,
        12.8,
        27.63,
        0.0,
        30.9,
        0.0
      ],
      "reasoning_scores": [
        32.54,
        54.05,
        0.33826087,
        20.48,
        33.56,
        54.92,
        0.33630435,
        19.28,
        37.63,
        54.57,
        0.33554348,
        20.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -11.93,
        "math_avg": -36.05,
        "code_avg": -25.21,
        "reasoning_avg": -7.54,
        "overall_avg": -20.18,
        "overall_efficiency": -0.24483,
        "general_efficiency": -0.144686,
        "math_efficiency": -0.437348,
        "code_efficiency": -0.30583,
        "reasoning_efficiency": -0.091458,
        "general_task_scores": [
          -20.2,
          -15.94,
          -15.6
        ],
        "math_task_scores": [
          -73.18,
          -44.2,
          -43.6,
          0.0
        ],
        "code_task_scores": [
          -63.83,
          -43.46,
          -8.24,
          30.07,
          -37.1
        ],
        "reasoning_task_scores": [
          -2.02,
          -14.95,
          -0.05,
          -13.2
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "82k",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct",
      "paper_link": "https://arxiv.org/abs/2212.10560",
      "tag": "general"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 62.84,
      "math_avg": 43.62,
      "code_avg": 49.26,
      "reasoning_avg": 46.3,
      "overall_avg": 50.51,
      "overall_efficiency": 0.008802,
      "general_efficiency": 0.012134,
      "math_efficiency": 0.000873,
      "code_efficiency": 0.010088,
      "reasoning_efficiency": 0.012114,
      "general_scores": [
        73.41,
        71.4675,
        58.56,
        50.8528571,
        66.87,
        72.1525,
        57.62,
        49.5364286,
        72.8,
        72.685,
        58.78,
        49.3721429
      ],
      "math_scores": [
        87.41,
        53.8,
        56.4,
        19.49,
        3.33,
        86.43,
        53.42,
        54.8,
        19.99,
        6.67,
        84.61,
        52.82,
        53.0,
        18.83,
        3.33
      ],
      "code_scores": [
        76.83,
        71.21,
        11.83,
        39.04,
        45.93,
        79.88,
        72.76,
        11.47,
        38.62,
        44.34,
        78.05,
        71.6,
        13.62,
        37.58,
        46.15
      ],
      "reasoning_scores": [
        82.71,
        62.09,
        0.44315217,
        40.88,
        83.05,
        61.85,
        0.44576087,
        39.6,
        81.02,
        61.82,
        0.43684783,
        41.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.15
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.86
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 45.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.26
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.4,
        "math_avg": 0.82,
        "code_avg": 9.48,
        "reasoning_avg": 11.38,
        "overall_avg": 8.27,
        "overall_efficiency": 0.008802,
        "general_efficiency": 0.012134,
        "math_efficiency": 0.000873,
        "code_efficiency": 0.010088,
        "reasoning_efficiency": 0.012114,
        "general_task_scores": [
          2.72,
          36.61,
          0.58,
          5.68
        ],
        "math_task_scores": [
          6.17,
          2.23,
          4.53,
          -6.6,
          -2.23
        ],
        "code_task_scores": [
          0.81,
          0.26,
          4.07,
          37.37,
          8.37
        ],
        "reasoning_task_scores": [
          45.66,
          -7.54,
          0.05,
          7.28
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "939k",
      "size_precise": "939343",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "general,math,code,science"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 35.76,
      "math_avg": 7.48,
      "code_avg": 18.52,
      "reasoning_avg": 24.47,
      "overall_avg": 21.56,
      "overall_efficiency": -3.555829,
      "general_efficiency": -2.697536,
      "math_efficiency": -6.072788,
      "code_efficiency": -3.656178,
      "reasoning_efficiency": -1.796816,
      "general_scores": [
        76.37,
        18.9625,
        48.83,
        0.00785714,
        75.92,
        17.9,
        48.41,
        0.00857143,
        76.1,
        19.6425,
        46.9,
        0.01571429
      ],
      "math_scores": [
        0.99,
        8.42,
        14.2,
        13.39,
        0.0,
        0.83,
        7.74,
        12.4,
        13.91,
        0.0,
        1.59,
        9.1,
        15.8,
        13.87,
        0.0
      ],
      "code_scores": [
        9.76,
        72.76,
        4.3,
        4.18,
        0.9,
        9.76,
        72.37,
        3.94,
        4.18,
        0.23,
        15.24,
        71.21,
        4.3,
        3.55,
        1.13
      ],
      "reasoning_scores": [
        42.37,
        51.89,
        0.29358696,
        4.08,
        51.68,
        0.29782609,
        8.56,
        40.34,
        55.03,
        0.31021739,
        8.4,
        40.0,
        55.03,
        0.31021739,
        8.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.59
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.11
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.97
              },
              {
                "metric": "lcb_test_output",
                "score": 0.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.41
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -15.69,
        "math_avg": -35.32,
        "code_avg": -21.26,
        "reasoning_avg": -10.45,
        "overall_avg": -20.68,
        "overall_efficiency": -3.555829,
        "general_efficiency": -2.697536,
        "math_efficiency": -6.072788,
        "code_efficiency": -3.656178,
        "reasoning_efficiency": -1.796816,
        "general_task_scores": [
          7.82,
          -16.66,
          -9.69,
          -44.23
        ],
        "math_task_scores": [
          -78.84,
          -42.7,
          -36.07,
          -12.32,
          -6.67
        ],
        "code_task_scores": [
          -65.85,
          0.51,
          -4.06,
          2.93,
          -36.35
        ],
        "reasoning_task_scores": [
          4.3,
          -16.05,
          -0.09,
          -25.92
        ]
      },
      "affiliation": "eddie",
      "year": "2023",
      "size": "5816",
      "size_precise": "5816",
      "link": "https://huggingface.co/datasets/causal-lm/auto_cot",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 13,
      "name": "Dolphin-flan1m-alpaca-uncensored",
      "domain": "general",
      "general_avg": 46.58,
      "math_avg": 34.84,
      "code_avg": 48.43,
      "reasoning_avg": 35.19,
      "overall_avg": 41.26,
      "overall_efficiency": -0.001094,
      "general_efficiency": -0.005452,
      "math_efficiency": -0.008924,
      "code_efficiency": 0.009691,
      "reasoning_efficiency": 0.000311,
      "general_scores": [
        77.7,
        49.3625,
        58.75,
        0.0,
        77.8,
        49.5325,
        58.6,
        0.0,
        78.1,
        50.65,
        58.48,
        0.0
      ],
      "math_scores": [
        54.74,
        45.52,
        43.6,
        22.0,
        3.33,
        67.02,
        48.24,
        46.4,
        22.36,
        3.33,
        50.42,
        43.74,
        43.0,
        22.27,
        6.67
      ],
      "code_scores": [
        71.34,
        74.71,
        7.17,
        40.71,
        44.34,
        72.56,
        75.1,
        7.17,
        42.8,
        42.76,
        79.27,
        75.1,
        11.47,
        40.29,
        41.63
      ],
      "reasoning_scores": [
        50.17,
        69.87,
        0.42967391,
        25.76,
        48.81,
        69.82,
        0.43793478,
        26.32,
        33.9,
        69.0,
        0.44043478,
        27.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.87
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.85
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.61
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.83
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.6
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.27
              },
              {
                "metric": "lcb_test_output",
                "score": 42.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.56
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.86,
        "math_avg": -7.96,
        "code_avg": 8.64,
        "reasoning_avg": 0.28,
        "overall_avg": -0.98,
        "overall_efficiency": -0.001094,
        "general_efficiency": -0.005452,
        "math_efficiency": -0.008924,
        "code_efficiency": 0.009691,
        "reasoning_efficiency": 0.000311,
        "general_task_scores": [
          9.56,
          14.36,
          0.87,
          -44.24
        ],
        "math_task_scores": [
          -22.59,
          -5.29,
          -5.87,
          -3.83,
          -2.23
        ],
        "code_task_scores": [
          -3.05,
          3.37,
          0.36,
          40.23,
          5.81
        ],
        "reasoning_task_scores": [
          7.69,
          0.1,
          0.05,
          -6.8
        ]
      },
      "affiliation": "QuixiAI",
      "year": "2023",
      "size": "892k",
      "size_precise": "891857",
      "link": "https://huggingface.co/datasets/cognitivecomputations/dolphin",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 14,
      "name": "Magpie-Pro-10K-GPT4o-mini",
      "domain": "general",
      "general_avg": 47.12,
      "math_avg": 57.18,
      "code_avg": 51.64,
      "reasoning_avg": 35.05,
      "overall_avg": 47.75,
      "overall_efficiency": 0.551108,
      "general_efficiency": -0.432029,
      "math_efficiency": 1.438,
      "code_efficiency": 1.1853,
      "reasoning_efficiency": 0.013162,
      "general_scores": [
        76.64,
        51.6575,
        60.23,
        0.0,
        75.95,
        51.7175,
        60.44,
        0.0,
        76.59,
        52.435,
        59.83,
        0.0
      ],
      "math_scores": [
        91.89,
        72.78,
        75.6,
        28.16,
        16.67,
        91.43,
        72.54,
        74.4,
        28.68,
        16.67,
        92.19,
        72.58,
        76.0,
        28.14,
        20.0
      ],
      "code_scores": [
        84.15,
        72.76,
        12.54,
        43.63,
        46.83,
        82.32,
        73.15,
        12.9,
        44.05,
        44.12,
        82.93,
        73.54,
        12.54,
        43.63,
        45.48
      ],
      "reasoning_scores": [
        33.56,
        70.64,
        0.44032609,
        35.36,
        36.27,
        70.41,
        0.43695652,
        35.28,
        31.19,
        71.03,
        0.43891304,
        35.52
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.39
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.13
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 45.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.39
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.32,
        "math_avg": 14.38,
        "code_avg": 11.85,
        "reasoning_avg": 0.13,
        "overall_avg": 5.51,
        "overall_efficiency": 0.551108,
        "general_efficiency": -0.432029,
        "math_efficiency": 1.438,
        "code_efficiency": 1.1853,
        "reasoning_efficiency": 0.013162,
        "general_task_scores": [
          8.08,
          16.45,
          2.43,
          -44.24
        ],
        "math_task_scores": [
          11.86,
          21.51,
          25.13,
          2.29,
          11.11
        ],
        "code_task_scores": [
          5.69,
          1.55,
          4.42,
          42.73,
          8.38
        ],
        "reasoning_task_scores": [
          -2.93,
          1.23,
          0.05,
          2.11
        ]
      },
      "affiliation": "Max Zhang",
      "year": "2024",
      "size": "10k",
      "size_precise": "10000",
      "link": "https://huggingface.co/datasets/Mxode/Magpie-Pro-10K-GPT4o-mini",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 15,
      "name": "open-instruct-v1",
      "domain": "general",
      "general_avg": 48.13,
      "math_avg": 35.7,
      "code_avg": 46.76,
      "reasoning_avg": 33.94,
      "overall_avg": 41.13,
      "overall_efficiency": -0.002214,
      "general_efficiency": -0.006635,
      "math_efficiency": -0.014244,
      "code_efficiency": 0.013978,
      "reasoning_efficiency": -0.001953,
      "general_scores": [
        65.33,
        50.49,
        54.45,
        43.6957143,
        65.16,
        51.0625,
        54.89,
        0.0
      ],
      "math_scores": [
        75.36,
        44.28,
        44.0,
        17.39,
        3.33,
        69.75,
        42.62,
        40.6,
        16.31,
        3.33
      ],
      "code_scores": [
        67.07,
        70.43,
        11.83,
        41.96,
        42.53,
        69.51,
        71.21,
        10.04,
        41.13,
        41.86
      ],
      "reasoning_scores": [
        38.98,
        65.5,
        0.4026087,
        32.72,
        35.93,
        64.31,
        0.41565217,
        33.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.3
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.29
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.54
              },
              {
                "metric": "lcb_test_output",
                "score": 42.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.31,
        "math_avg": -7.1,
        "code_avg": 6.97,
        "reasoning_avg": -0.97,
        "overall_avg": -1.1,
        "overall_efficiency": -0.002214,
        "general_efficiency": -0.006635,
        "math_efficiency": -0.014244,
        "code_efficiency": 0.013978,
        "reasoning_efficiency": -0.001953,
        "general_task_scores": [
          -3.07,
          15.29,
          -3.07,
          -22.39
        ],
        "math_task_scores": [
          -7.42,
          -7.67,
          -7.9,
          -9.19,
          -3.34
        ],
        "code_task_scores": [
          -9.15,
          -0.78,
          2.69,
          40.5,
          5.1
        ],
        "reasoning_task_scores": [
          0.86,
          -4.56,
          0.02,
          -0.28
        ]
      },
      "affiliation": "Reimu Hakurei",
      "year": "2023",
      "size": "499k",
      "size_precise": "498813",
      "link": "https://huggingface.co/datasets/hakurei/open-instruct-v1",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 16,
      "name": "MMOS",
      "domain": "general",
      "general_avg": 38.41,
      "math_avg": 20.1,
      "code_avg": 38.52,
      "reasoning_avg": 28.88,
      "overall_avg": 31.48,
      "overall_efficiency": -0.079931,
      "general_efficiency": -0.096835,
      "math_efficiency": -0.16864,
      "code_efficiency": -0.009413,
      "reasoning_efficiency": -0.044837,
      "general_scores": [
        59.62,
        47.1375,
        42.17,
        0.57571429,
        68.01,
        47.0325,
        41.49,
        0.625,
        63.62,
        46.9625,
        42.76,
        0.91071429
      ],
      "math_scores": [
        37.68,
        26.06,
        25.6,
        10.93,
        0.0,
        37.6,
        25.94,
        25.0,
        10.95,
        0.0,
        36.54,
        25.68,
        25.6,
        10.61,
        3.33
      ],
      "code_scores": [
        57.93,
        66.54,
        7.17,
        36.74,
        28.96,
        59.15,
        64.98,
        7.17,
        29.65,
        31.45,
        56.71,
        65.37,
        5.73,
        31.94,
        28.28
      ],
      "reasoning_scores": [
        35.59,
        63.46,
        0.3225,
        17.04,
        35.59,
        62.6,
        0.31532609,
        16.56,
        37.29,
        62.2,
        0.32271739,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.75
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.04
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.89
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.69
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 29.56
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.16
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.75
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.03,
        "math_avg": -22.7,
        "code_avg": -1.27,
        "reasoning_avg": -6.04,
        "overall_avg": -10.76,
        "overall_efficiency": -0.079931,
        "general_efficiency": -0.096835,
        "math_efficiency": -0.16864,
        "code_efficiency": -0.009413,
        "reasoning_efficiency": -0.044837,
        "general_task_scores": [
          -4.56,
          11.55,
          -15.6,
          -43.54
        ],
        "math_task_scores": [
          -42.71,
          -25.23,
          -24.8,
          -15.21,
          -5.56
        ],
        "code_task_scores": [
          -19.51,
          -5.97,
          -1.55,
          31.74,
          -7.54
        ],
        "reasoning_task_scores": [
          -0.44,
          -6.71,
          -0.07,
          -16.99
        ]
      },
      "affiliation": "ShanghaiTech University",
      "year": "2024",
      "size": "135k",
      "size_precise": "134610",
      "link": "https://huggingface.co/datasets/cyzhh/MMOS",
      "paper_link": "https://arxiv.org/abs/2403.00799",
      "tag": "math,code"
    },
    {
      "id": 17,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 50.0,
      "math_avg": 16.45,
      "code_avg": 42.18,
      "reasoning_avg": 31.75,
      "overall_avg": 35.09,
      "overall_efficiency": -0.955877,
      "general_efficiency": -0.193677,
      "math_efficiency": -3.526206,
      "code_efficiency": 0.320309,
      "reasoning_efficiency": -0.423936,
      "general_scores": [
        67.93,
        47.84,
        56.65,
        28.9,
        66.19,
        48.6625,
        56.56,
        27.7364286,
        68.39,
        46.19,
        56.65,
        28.2664286
      ],
      "math_scores": [
        1.29,
        29.72,
        28.0,
        18.72,
        13.3,
        2.12,
        29.06,
        25.6,
        18.25,
        3.33,
        2.27,
        28.78,
        25.6,
        17.39,
        3.33
      ],
      "code_scores": [
        69.51,
        71.6,
        11.83,
        29.23,
        29.41,
        70.12,
        70.43,
        10.75,
        26.51,
        24.66,
        71.95,
        72.37,
        11.83,
        26.51,
        35.97
      ],
      "reasoning_scores": [
        34.24,
        68.77,
        0.42119565,
        24.16,
        33.56,
        68.88,
        0.40652174,
        24.72,
        31.86,
        68.92,
        0.40217391,
        24.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.12
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.65
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.42
              },
              {
                "metric": "lcb_test_output",
                "score": 30.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.45,
        "math_avg": -26.35,
        "code_avg": 2.39,
        "reasoning_avg": -3.17,
        "overall_avg": -7.14,
        "overall_efficiency": -0.955877,
        "general_efficiency": -0.193677,
        "math_efficiency": -3.526206,
        "code_efficiency": 0.320309,
        "reasoning_efficiency": -0.423936,
        "general_task_scores": [
          -0.81,
          12.07,
          -1.12,
          -15.94
        ],
        "math_task_scores": [
          -78.09,
          -21.93,
          -23.8,
          -7.92,
          -0.02
        ],
        "code_task_scores": [
          -6.91,
          -0.13,
          3.23,
          26.38,
          -7.09
        ],
        "reasoning_task_scores": [
          -3.38,
          -0.6,
          0.02,
          -8.77
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "size_precise": "7473",
      "link": "https://huggingface.co/datasets/openai/gsm8k",
      "paper_link": "https://arxiv.org/abs/2110.14168",
      "tag": "math"
    },
    {
      "id": 18,
      "name": "Competition_Math",
      "domain": "math",
      "general_avg": 48.17,
      "math_avg": 31.32,
      "code_avg": 39.51,
      "reasoning_avg": 31.95,
      "overall_avg": 37.74,
      "overall_efficiency": -0.59978,
      "general_efficiency": -0.436925,
      "math_efficiency": -1.530933,
      "code_efficiency": -0.036311,
      "reasoning_efficiency": -0.394952,
      "general_scores": [
        74.56,
        45.2625,
        52.9,
        21.1664286,
        74.65,
        44.47,
        52.38,
        19.85,
        74.48,
        45.575,
        52.8,
        19.9164286
      ],
      "math_scores": [
        46.63,
        44.52,
        43.8,
        18.04,
        6.67,
        47.38,
        44.56,
        45.0,
        18.07,
        0.0,
        48.82,
        44.8,
        43.4,
        18.11,
        0.0
      ],
      "code_scores": [
        71.95,
        69.65,
        11.11,
        43.22,
        2.26,
        71.95,
        71.6,
        9.68,
        43.84,
        0.9,
        72.56,
        69.65,
        9.32,
        43.42,
        1.58
      ],
      "reasoning_scores": [
        31.86,
        68.47,
        0.42847826,
        27.44,
        29.49,
        68.23,
        0.42815217,
        27.68,
        32.88,
        68.2,
        0.42445652,
        27.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.56
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.31
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.61
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 1.58
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.68
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.28,
        "math_avg": -11.48,
        "code_avg": -0.27,
        "reasoning_avg": -2.96,
        "overall_avg": -4.5,
        "overall_efficiency": -0.59978,
        "general_efficiency": -0.436925,
        "math_efficiency": -1.530933,
        "code_efficiency": -0.036311,
        "reasoning_efficiency": -0.394952,
        "general_task_scores": [
          6.25,
          9.61,
          -5.05,
          -23.93
        ],
        "math_task_scores": [
          -32.37,
          -6.49,
          -6.13,
          -7.97,
          -4.45
        ],
        "code_task_scores": [
          -5.29,
          -1.3,
          1.8,
          42.45,
          -35.52
        ],
        "reasoning_task_scores": [
          -5.19,
          -1.16,
          0.04,
          -5.6
        ]
      },
      "affiliation": "UC Berkeley",
      "year": "2021",
      "size": "7.5k",
      "size_precise": "7500",
      "link": "https://huggingface.co/datasets/hendrycks/competition_math",
      "paper_link": "https://arxiv.org/abs/2103.03874",
      "tag": "math"
    },
    {
      "id": 19,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 46.06,
      "math_avg": 49.65,
      "code_avg": 46.82,
      "reasoning_avg": 29.74,
      "overall_avg": 43.07,
      "overall_efficiency": 0.00083,
      "general_efficiency": -0.00538,
      "math_efficiency": 0.006843,
      "code_efficiency": 0.007038,
      "reasoning_efficiency": -0.00518,
      "general_scores": [
        74.76,
        45.74,
        51.74,
        8.66142857,
        75.4,
        45.3225,
        50.96,
        13.9285714,
        74.32,
        46.115,
        50.35,
        15.465
      ],
      "math_scores": [
        90.9,
        64.28,
        64.2,
        22.56,
        6.67,
        91.21,
        64.56,
        64.6,
        22.47,
        6.67,
        90.67,
        63.28,
        63.4,
        22.54,
        6.67
      ],
      "code_scores": [
        68.9,
        73.15,
        14.7,
        34.66,
        40.05,
        71.34,
        73.54,
        13.26,
        30.06,
        43.21,
        73.17,
        72.37,
        13.62,
        37.79,
        42.53
      ],
      "reasoning_scores": [
        35.59,
        65.67,
        0.3223913,
        18.32,
        32.2,
        66.25,
        0.32119565,
        18.16,
        35.25,
        67.55,
        0.31902174,
        16.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.14
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.17
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.35
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.38,
        "math_avg": 6.84,
        "code_avg": 7.04,
        "reasoning_avg": -5.18,
        "overall_avg": 0.83,
        "overall_efficiency": 0.00083,
        "general_efficiency": -0.00538,
        "math_efficiency": 0.006843,
        "code_efficiency": 0.007038,
        "reasoning_efficiency": -0.00518,
        "general_task_scores": [
          6.52,
          10.24,
          -6.72,
          -31.56
        ],
        "math_task_scores": [
          10.95,
          12.92,
          13.87,
          -3.52,
          0.0
        ],
        "code_task_scores": [
          -6.3,
          1.42,
          5.62,
          33.13,
          4.83
        ],
        "reasoning_task_scores": [
          -2.25,
          -2.97,
          -0.07,
          -15.49
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "size_precise": "1000000",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
      "paper_link": "https://arxiv.org/abs/2410.01560",
      "tag": "math"
    },
    {
      "id": 20,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 45.01,
      "math_avg": 48.64,
      "code_avg": 39.16,
      "reasoning_avg": 36.0,
      "overall_avg": 42.2,
      "overall_efficiency": -4e-05,
      "general_efficiency": -0.007485,
      "math_efficiency": 0.006792,
      "code_efficiency": -0.000724,
      "reasoning_efficiency": 0.00126,
      "general_scores": [
        72.67,
        46.485,
        52.23,
        6.05928571,
        72.99,
        47.7125,
        52.46,
        8.03142857,
        73.75,
        47.54,
        53.36,
        6.83714286
      ],
      "math_scores": [
        85.75,
        61.88,
        61.8,
        26.22,
        10.0,
        84.61,
        62.6,
        63.6,
        25.86,
        13.33,
        84.46,
        62.1,
        60.6,
        26.78,
        0.0
      ],
      "code_scores": [
        56.71,
        68.09,
        10.04,
        20.04,
        40.27,
        54.27,
        68.48,
        5.73,
        23.38,
        39.37,
        55.49,
        67.7,
        4.66,
        30.9,
        42.31
      ],
      "reasoning_scores": [
        43.05,
        59.51,
        0.37402174,
        35.36,
        54.58,
        61.59,
        0.38934783,
        32.96,
        50.51,
        59.03,
        0.39336957,
        34.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.77
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.43,
        "math_avg": 5.84,
        "code_avg": -0.62,
        "reasoning_avg": 1.08,
        "overall_avg": -0.03,
        "overall_efficiency": -4e-05,
        "general_efficiency": -0.007485,
        "math_efficiency": 0.006792,
        "code_efficiency": -0.000724,
        "reasoning_efficiency": 0.00126,
        "general_task_scores": [
          4.83,
          11.76,
          -5.06,
          -37.26
        ],
        "math_task_scores": [
          4.96,
          11.07,
          11.8,
          0.25,
          1.11
        ],
        "code_task_scores": [
          -21.95,
          -3.51,
          -1.43,
          23.73,
          3.55
        ],
        "reasoning_task_scores": [
          12.78,
          -9.42,
          -0.0,
          0.91
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "size_precise": "859494",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 21,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 56.0,
      "math_avg": 45.72,
      "code_avg": 50.18,
      "reasoning_avg": 33.38,
      "overall_avg": 46.32,
      "overall_efficiency": 0.056372,
      "general_efficiency": 0.062829,
      "math_efficiency": 0.04035,
      "code_efficiency": 0.14356,
      "reasoning_efficiency": -0.021254,
      "general_scores": [
        78.27,
        52.5125,
        53.53,
        38.9935714,
        78.53,
        53.2775,
        54.33,
        38.6521429,
        78.27,
        53.16,
        54.19,
        38.235
      ],
      "math_scores": [
        68.61,
        47.22,
        48.0,
        18.95,
        68.54,
        46.5,
        49.2,
        19.63,
        67.85,
        46.92,
        48.2,
        19.08
      ],
      "code_scores": [
        83.54,
        72.76,
        14.34,
        38.41,
        44.12,
        83.54,
        73.54,
        13.62,
        40.29,
        41.63,
        79.88,
        72.76,
        13.62,
        38.41,
        42.31
      ],
      "reasoning_scores": [
        35.59,
        68.96,
        0.37423913,
        28.08,
        38.31,
        68.44,
        0.37815217,
        28.24,
        35.25,
        68.52,
        0.37880435,
        28.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 82.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.04
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.64
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.55,
        "math_avg": 2.92,
        "code_avg": 10.4,
        "reasoning_avg": -1.54,
        "overall_avg": 4.08,
        "overall_efficiency": 0.056372,
        "general_efficiency": 0.062829,
        "math_efficiency": 0.04035,
        "code_efficiency": 0.14356,
        "reasoning_efficiency": -0.021254,
        "general_task_scores": [
          10.05,
          17.49,
          -3.72,
          -5.61
        ],
        "math_task_scores": [
          -11.65,
          -4.24,
          -1.73,
          -6.82
        ],
        "code_task_scores": [
          4.88,
          1.42,
          5.62,
          38.0,
          5.59
        ],
        "reasoning_task_scores": [
          -0.22,
          -0.82,
          -0.01,
          -5.17
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "size_precise": "72441",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 22,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 54.02,
      "math_avg": 40.29,
      "code_avg": 47.14,
      "reasoning_avg": 32.6,
      "overall_avg": 43.51,
      "overall_efficiency": 0.003227,
      "general_efficiency": 0.006513,
      "math_efficiency": -0.006364,
      "code_efficiency": 0.018618,
      "reasoning_efficiency": -0.00586,
      "general_scores": [
        75.29,
        47.59,
        50.75,
        42.155,
        75.05,
        48.8025,
        50.76,
        41.6971429,
        74.84,
        47.615,
        52.0,
        41.6564286
      ],
      "math_scores": [
        83.78,
        49.72,
        50.4,
        19.44,
        0.0,
        82.41,
        49.06,
        48.8,
        18.2,
        3.33,
        83.78,
        48.9,
        47.6,
        18.9,
        0.0
      ],
      "code_scores": [
        73.17,
        71.98,
        9.32,
        44.05,
        38.46,
        73.78,
        69.65,
        9.68,
        42.38,
        35.75,
        78.66,
        68.48,
        10.04,
        40.29,
        41.4
      ],
      "reasoning_scores": [
        38.64,
        67.34,
        0.435,
        28.56,
        35.25,
        68.13,
        0.43043478,
        28.24,
        27.8,
        67.16,
        0.43597826,
        28.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.06
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.24
              },
              {
                "metric": "lcb_test_output",
                "score": 38.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.57,
        "math_avg": -2.51,
        "code_avg": 7.35,
        "reasoning_avg": -2.31,
        "overall_avg": 1.27,
        "overall_efficiency": 0.003227,
        "general_efficiency": 0.006513,
        "math_efficiency": -0.006364,
        "code_efficiency": 0.018618,
        "reasoning_efficiency": -0.00586,
        "general_task_scores": [
          6.75,
          12.51,
          -6.57,
          -2.4
        ],
        "math_task_scores": [
          3.34,
          -1.89,
          -1.27,
          -7.19,
          -5.56
        ],
        "code_task_scores": [
          -2.24,
          -1.56,
          1.44,
          41.2,
          1.44
        ],
        "reasoning_task_scores": [
          -2.7,
          -1.92,
          0.04,
          -4.75
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "size_precise": "395000",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA",
      "paper_link": "https://arxiv.org/abs/2309.12284",
      "tag": "math"
    },
    {
      "id": 23,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 55.72,
      "math_avg": 37.08,
      "code_avg": 49.12,
      "reasoning_avg": 32.49,
      "overall_avg": 43.6,
      "overall_efficiency": 0.005201,
      "general_efficiency": 0.016302,
      "math_efficiency": -0.021834,
      "code_efficiency": 0.035609,
      "reasoning_efficiency": -0.009271,
      "general_scores": [
        77.55,
        50.54,
        56.73,
        38.2635714,
        76.59,
        50.905,
        56.77,
        35.5721429,
        77.55,
        51.535,
        56.75,
        39.8385714
      ],
      "math_scores": [
        79.08,
        45.06,
        41.0,
        18.61,
        0.0,
        78.77,
        46.1,
        42.8,
        19.4,
        0.0,
        76.88,
        46.22,
        43.0,
        19.29,
        0.0
      ],
      "code_scores": [
        78.66,
        71.21,
        13.26,
        41.54,
        41.63,
        76.83,
        73.93,
        11.11,
        41.34,
        42.08,
        76.83,
        72.37,
        10.39,
        42.8,
        42.76
      ],
      "reasoning_scores": [
        33.22,
        67.91,
        0.47076087,
        28.96,
        32.54,
        67.21,
        0.44173913,
        31.28,
        29.49,
        68.22,
        0.42152174,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.59
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.89
              },
              {
                "metric": "lcb_test_output",
                "score": 42.16
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.27,
        "math_avg": -5.72,
        "code_avg": 9.33,
        "reasoning_avg": -2.43,
        "overall_avg": 1.36,
        "overall_efficiency": 0.005201,
        "general_efficiency": 0.016302,
        "math_efficiency": -0.021834,
        "code_efficiency": 0.035609,
        "reasoning_efficiency": -0.009271,
        "general_task_scores": [
          8.92,
          15.5,
          -0.99,
          -6.35
        ],
        "math_task_scores": [
          -1.74,
          -5.33,
          -7.93,
          -6.94,
          -6.67
        ],
        "code_task_scores": [
          0.0,
          0.9,
          3.35,
          40.85,
          5.06
        ],
        "reasoning_task_scores": [
          -4.85,
          -1.68,
          0.05,
          -3.31
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "size_precise": "262039",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
      "paper_link": "https://arxiv.org/abs/2309.05653",
      "tag": "math"
    },
    {
      "id": 24,
      "name": "math_qa",
      "domain": "math",
      "general_avg": 50.77,
      "math_avg": 29.53,
      "code_avg": 47.79,
      "reasoning_avg": 30.73,
      "overall_avg": 39.7,
      "overall_efficiency": -0.084939,
      "general_efficiency": -0.022629,
      "math_efficiency": -0.44484,
      "code_efficiency": 0.268134,
      "reasoning_efficiency": -0.140422,
      "general_scores": [
        69.48,
        50.355,
        51.55,
        28.4207143,
        67.08,
        52.5325,
        52.9,
        31.5907143,
        70.78,
        51.1575,
        53.12,
        30.265
      ],
      "math_scores": [
        48.37,
        37.32,
        30.6,
        18.93,
        13.33,
        48.52,
        37.74,
        31.2,
        20.01,
        10.0,
        49.51,
        38.28,
        33.4,
        19.06,
        6.67
      ],
      "code_scores": [
        67.07,
        69.26,
        11.83,
        44.26,
        45.25,
        73.17,
        69.65,
        10.04,
        42.59,
        38.91,
        73.78,
        68.09,
        12.19,
        43.63,
        47.06
      ],
      "reasoning_scores": [
        33.56,
        66.98,
        0.42391304,
        23.28,
        27.8,
        66.58,
        0.4251087,
        24.88,
        33.22,
        66.67,
        0.42032609,
        24.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.35
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 43.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.68,
        "math_avg": -13.27,
        "code_avg": 8.0,
        "reasoning_avg": -4.19,
        "overall_avg": -2.53,
        "overall_efficiency": -0.084939,
        "general_efficiency": -0.022629,
        "math_efficiency": -0.44484,
        "code_efficiency": 0.268134,
        "reasoning_efficiency": -0.140422,
        "general_task_scores": [
          0.8,
          15.86,
          -5.22,
          -14.15
        ],
        "math_task_scores": [
          -31.18,
          -13.34,
          -18.47,
          -6.71,
          3.33
        ],
        "code_task_scores": [
          -6.1,
          -2.6,
          3.11,
          42.45,
          6.64
        ],
        "reasoning_task_scores": [
          -5.07,
          -2.72,
          0.03,
          -9.07
        ]
      },
      "affiliation": "Allen AI",
      "year": "2019",
      "size": "29.8k",
      "size_precise": "29837",
      "link": "https://huggingface.co/datasets/allenai/math_qa",
      "paper_link": "https://aclanthology.org/N19-1245/",
      "tag": "math"
    },
    {
      "id": 25,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 51.69,
      "math_avg": 34.4,
      "code_avg": 47.28,
      "reasoning_avg": 33.96,
      "overall_avg": 41.83,
      "overall_efficiency": -0.008091,
      "general_efficiency": 0.004918,
      "math_efficiency": -0.16796,
      "code_efficiency": 0.14982,
      "reasoning_efficiency": -0.019143,
      "general_scores": [
        77.61,
        46.9175,
        50.87,
        32.205,
        77.95,
        46.815,
        50.53,
        32.2257143,
        76.95,
        45.7325,
        50.67,
        31.8085714
      ],
      "math_scores": [
        59.21,
        45.0,
        43.8,
        18.68,
        6.67,
        62.85,
        45.44,
        45.8,
        18.97,
        0.0,
        60.96,
        45.54,
        44.8,
        18.34,
        0.0
      ],
      "code_scores": [
        75.61,
        71.6,
        12.19,
        43.84,
        29.64,
        75.0,
        73.15,
        11.83,
        43.42,
        35.29,
        75.61,
        70.43,
        12.19,
        44.05,
        35.29
      ],
      "reasoning_scores": [
        32.2,
        67.88,
        0.4225,
        32.56,
        68.62,
        0.42228261,
        33.52,
        33.56,
        68.98,
        0.41619565,
        33.68,
        35.25
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 33.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.25,
        "math_avg": -8.4,
        "code_avg": 7.49,
        "reasoning_avg": -0.96,
        "overall_avg": -0.4,
        "overall_efficiency": -0.008091,
        "general_efficiency": 0.004918,
        "math_efficiency": -0.16796,
        "code_efficiency": 0.14982,
        "reasoning_efficiency": -0.019143,
        "general_task_scores": [
          9.19,
          11.0,
          -7.05,
          -12.16
        ],
        "math_task_scores": [
          -18.97,
          -5.79,
          -5.4,
          -7.38,
          -4.45
        ],
        "code_task_scores": [
          -2.03,
          0.13,
          3.83,
          42.73,
          -3.69
        ],
        "reasoning_task_scores": [
          -2.93,
          -0.97,
          0.03,
          -0.03
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/camel-ai/math",
      "paper_link": "https://arxiv.org/abs/2303.17760",
      "tag": "math"
    },
    {
      "id": 26,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 38.26,
      "math_avg": 10.48,
      "code_avg": 25.05,
      "reasoning_avg": 30.07,
      "overall_avg": 25.97,
      "overall_efficiency": -0.544995,
      "general_efficiency": -0.441575,
      "math_efficiency": -1.082583,
      "code_efficiency": -0.493496,
      "reasoning_efficiency": -0.162326,
      "general_scores": [
        57.63,
        37.445,
        48.61,
        7.2,
        59.14,
        37.19,
        48.41,
        9.63714286,
        59.3,
        38.0825,
        49.5,
        6.98
      ],
      "math_scores": [
        14.1,
        11.3,
        6.8,
        19.29,
        0.0,
        13.12,
        11.62,
        7.8,
        19.38,
        0.0,
        14.25,
        12.12,
        7.4,
        20.01,
        0.0
      ],
      "code_scores": [
        2.44,
        72.76,
        8.6,
        6.47,
        38.69,
        4.27,
        71.6,
        6.45,
        7.52,
        31.22,
        6.71,
        71.6,
        8.24,
        4.8,
        34.39
      ],
      "reasoning_scores": [
        29.15,
        61.45,
        0.30858696,
        25.84,
        31.19,
        60.35,
        0.30521739,
        25.36,
        36.95,
        61.07,
        0.30413043,
        28.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.76
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.26
              },
              {
                "metric": "lcb_test_output",
                "score": 34.77
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.18,
        "math_avg": -32.32,
        "code_avg": -14.73,
        "reasoning_avg": -4.85,
        "overall_avg": -16.27,
        "overall_efficiency": -0.544995,
        "general_efficiency": -0.441575,
        "math_efficiency": -1.082583,
        "code_efficiency": -0.493496,
        "reasoning_efficiency": -0.162326,
        "general_task_scores": [
          -9.62,
          2.08,
          -8.9,
          -36.3
        ],
        "math_task_scores": [
          -66.16,
          -39.44,
          -42.87,
          -6.48,
          -6.67
        ],
        "code_task_scores": [
          -72.97,
          0.39,
          -0.48,
          5.22,
          -2.33
        ],
        "reasoning_task_scores": [
          -4.17,
          -8.5,
          -0.08,
          -6.69
        ]
      },
      "affiliation": "Skunkworks AI",
      "year": "2025",
      "size": "29.9k",
      "size_precise": "29857",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01",
      "paper_link": "",
      "tag": "general,math,code"
    },
    {
      "id": 27,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 43.67,
      "math_avg": 41.51,
      "code_avg": 41.66,
      "reasoning_avg": 31.99,
      "overall_avg": 39.71,
      "overall_efficiency": -0.016868,
      "general_efficiency": -0.051854,
      "math_efficiency": -0.008634,
      "code_efficiency": 0.012535,
      "reasoning_efficiency": -0.019521,
      "general_scores": [
        75.73,
        47.58,
        48.91,
        0.10928571,
        75.64,
        48.9925,
        48.55,
        0.09428571,
        76.02,
        49.7,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.45,
        6.67,
        69.75,
        29.6,
        67.8,
        27.35,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15
      ],
      "reasoning_scores": [
        31.86,
        68.7,
        0.35880435,
        24.56,
        29.83,
        68.11,
        0.36271739,
        25.84,
        36.61,
        70.3,
        0.37706522,
        26.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.78,
        "math_avg": -1.29,
        "code_avg": 1.88,
        "reasoning_avg": -2.93,
        "overall_avg": -2.53,
        "overall_efficiency": -0.016868,
        "general_efficiency": -0.051854,
        "math_efficiency": -0.008634,
        "code_efficiency": 0.012535,
        "reasoning_efficiency": -0.019521,
        "general_task_scores": [
          7.49,
          13.27,
          -8.03,
          -43.83
        ],
        "math_task_scores": [
          -7.78,
          -22.91,
          17.93,
          0.73,
          5.55
        ],
        "code_task_scores": [
          -34.35,
          0.12,
          4.78,
          34.8,
          7.55
        ],
        "reasoning_task_scores": [
          -3.83,
          -0.42,
          -0.02,
          -7.49
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "149960",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 28,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 48.62,
      "math_avg": 40.04,
      "code_avg": 44.39,
      "reasoning_avg": 29.68,
      "overall_avg": 40.68,
      "overall_efficiency": -0.077645,
      "general_efficiency": -0.141102,
      "math_efficiency": -0.138,
      "code_efficiency": 0.23015,
      "reasoning_efficiency": -0.261629,
      "general_scores": [
        74.88,
        49.64,
        49.5,
        19.2128571,
        75.1,
        48.8,
        49.16,
        19.1157143,
        75.39,
        48.3725,
        48.62,
        25.6778571
      ],
      "math_scores": [
        86.96,
        37.56,
        35.0,
        27.46,
        13.33,
        86.66,
        36.68,
        36.6,
        27.35,
        13.33,
        86.13,
        36.7,
        36.6,
        26.94,
        13.33
      ],
      "code_scores": [
        80.49,
        73.93,
        12.9,
        6.47,
        47.06,
        81.1,
        73.93,
        12.19,
        7.93,
        47.29,
        82.93,
        73.93,
        12.19,
        7.1,
        46.38
      ],
      "reasoning_scores": [
        33.22,
        67.6,
        0.37391304,
        15.92,
        35.59,
        69.85,
        0.37108696,
        16.8,
        31.53,
        68.1,
        0.37076087,
        16.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 81.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.17
              },
              {
                "metric": "lcb_test_output",
                "score": 46.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -2.82,
        "math_avg": -2.76,
        "code_avg": 4.6,
        "reasoning_avg": -5.23,
        "overall_avg": -1.55,
        "overall_efficiency": -0.077645,
        "general_efficiency": -0.141102,
        "math_efficiency": -0.138,
        "code_efficiency": 0.23015,
        "reasoning_efficiency": -0.261629,
        "general_task_scores": [
          6.81,
          13.45,
          -8.65,
          -22.9
        ],
        "math_task_scores": [
          6.6,
          -14.14,
          -14.13,
          1.21,
          6.66
        ],
        "code_task_scores": [
          4.07,
          2.33,
          4.19,
          6.13,
          9.81
        ],
        "reasoning_task_scores": [
          -3.15,
          -0.94,
          -0.02,
          -16.88
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "20k",
      "size_precise": "20000",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra",
      "paper_link": "https://arxiv.org/abs/2411.15124",
      "tag": "math"
    },
    {
      "id": 29,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 42.36,
      "math_avg": 54.56,
      "code_avg": 36.0,
      "reasoning_avg": 25.15,
      "overall_avg": 39.52,
      "overall_efficiency": -0.002712,
      "general_efficiency": -0.009057,
      "math_efficiency": 0.011713,
      "code_efficiency": -0.00377,
      "reasoning_efficiency": -0.009736,
      "general_scores": [
        87.67,
        35.41,
        46.59,
        0.00785714,
        87.29,
        36.31,
        45.59,
        0.0,
        87.32,
        37.18,
        44.91,
        0.0
      ],
      "math_scores": [
        88.1,
        72.64,
        74.0,
        26.78,
        13.33,
        88.17,
        72.22,
        74.0,
        27.12,
        10.0,
        88.1,
        72.2,
        73.8,
        27.87,
        10.0
      ],
      "code_scores": [
        64.63,
        73.54,
        9.68,
        20.88,
        13.57,
        61.59,
        72.76,
        11.47,
        20.46,
        11.76,
        64.02,
        73.54,
        9.32,
        18.79,
        14.03
      ],
      "reasoning_scores": [
        33.9,
        63.97,
        0.22065217,
        1.44,
        37.29,
        64.9,
        0.2201087,
        0.96,
        32.88,
        64.72,
        0.21913043,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.16
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.04
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.09,
        "math_avg": 11.75,
        "code_avg": -3.78,
        "reasoning_avg": -9.77,
        "overall_avg": -2.72,
        "overall_efficiency": -0.002712,
        "general_efficiency": -0.009057,
        "math_efficiency": 0.011713,
        "code_efficiency": -0.00377,
        "reasoning_efficiency": -0.009736,
        "general_task_scores": [
          19.12,
          0.81,
          -12.04,
          -44.24
        ],
        "math_task_scores": [
          8.14,
          21.23,
          23.73,
          1.22,
          4.44
        ],
        "code_task_scores": [
          -14.03,
          1.68,
          1.92,
          19.0,
          -23.98
        ],
        "reasoning_task_scores": [
          -1.91,
          -4.93,
          -0.17,
          -32.13
        ]
      },
      "affiliation": "Soochow University",
      "year": "2024",
      "size": "1M",
      "size_precise": "1003467",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math",
      "paper_link": "https://arxiv.org/abs/2410.18693",
      "tag": "math"
    },
    {
      "id": 30,
      "name": "DeepMath-103K",
      "domain": "math",
      "general_avg": 45.05,
      "math_avg": 72.97,
      "code_avg": 21.44,
      "reasoning_avg": 36.1,
      "overall_avg": 43.89,
      "overall_efficiency": 0.005349,
      "general_efficiency": -0.020692,
      "math_efficiency": 0.097617,
      "code_efficiency": -0.059357,
      "reasoning_efficiency": 0.003827,
      "general_scores": [
        82.23,
        28.82,
        32.77,
        36.3771429
      ],
      "math_scores": [
        90.07,
        88.8,
        88.6,
        45.37,
        34.1675,
        89.92,
        90.22,
        89.6,
        40.0
      ],
      "code_scores": [
        31.71,
        49.42,
        2.87,
        19.83,
        13.12,
        30.49,
        0.0,
        49.42,
        5.73,
        21.71,
        11.54
      ],
      "reasoning_scores": [
        75.93,
        51.61,
        34.34,
        0.29652174,
        18.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.51
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.1
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.08
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 15.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.3
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.77
              },
              {
                "metric": "lcb_test_output",
                "score": 12.33
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 30.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.4,
        "math_avg": 30.17,
        "code_avg": -18.34,
        "reasoning_avg": 1.18,
        "overall_avg": 1.65,
        "overall_efficiency": 0.005349,
        "general_efficiency": -0.020692,
        "math_efficiency": 0.097617,
        "code_efficiency": -0.059357,
        "reasoning_efficiency": 0.003827,
        "general_task_scores": [
          13.92,
          -6.67,
          -24.97,
          -7.86
        ],
        "math_task_scores": [
          10.02,
          38.39,
          38.9,
          19.33,
          30.41
        ],
        "code_task_scores": [
          -61.58,
          -22.18,
          -3.94,
          19.73,
          -24.77,
          -12.8
        ],
        "reasoning_task_scores": [
          39.33,
          -17.85,
          -0.51,
          -0.09,
          -14.96
        ]
      },
      "affiliation": "Tencent & SJTU",
      "year": "2025",
      "size": "103k",
      "size_precise": "309066",
      "link": "https://huggingface.co/datasets/zwhe99/DeepMath-103K",
      "paper_link": "https://arxiv.org/abs/2504.11456",
      "tag": "math"
    },
    {
      "id": 31,
      "name": "GammaCorpus-CoT-Math-170k",
      "domain": "math",
      "general_avg": 46.5,
      "math_avg": 42.49,
      "code_avg": 37.4,
      "reasoning_avg": 30.01,
      "overall_avg": 39.1,
      "overall_efficiency": -0.018499,
      "general_efficiency": -0.02916,
      "math_efficiency": -0.001821,
      "code_efficiency": -0.014092,
      "reasoning_efficiency": -0.028924,
      "general_scores": [
        76.33,
        44.9875,
        53.35,
        13.125,
        76.14,
        42.615,
        54.28,
        12.385,
        76.17,
        43.595,
        53.24,
        11.7964286
      ],
      "math_scores": [
        67.48,
        62.72,
        57.4,
        21.3,
        0.0,
        71.65,
        61.1,
        58.0,
        22.52,
        3.33,
        66.49,
        62.54,
        58.2,
        21.34,
        3.33
      ],
      "code_scores": [
        61.59,
        71.98,
        10.75,
        36.33,
        11.09,
        31.71,
        73.54,
        8.96,
        41.96,
        26.92,
        56.1,
        72.76,
        12.19,
        37.37,
        7.69
      ],
      "reasoning_scores": [
        38.64,
        65.72,
        0.31782609,
        19.12,
        31.86,
        66.5,
        0.34163043,
        18.64,
        31.53,
        66.29,
        0.31782609,
        20.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 15.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.17
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.94,
        "math_avg": -0.31,
        "code_avg": -2.39,
        "reasoning_avg": -4.9,
        "overall_avg": -3.14,
        "overall_efficiency": -0.018499,
        "general_efficiency": -0.02916,
        "math_efficiency": -0.001821,
        "code_efficiency": -0.014092,
        "reasoning_efficiency": -0.028924,
        "general_task_scores": [
          7.9,
          8.24,
          -4.12,
          -31.8
        ],
        "math_task_scores": [
          -11.44,
          11.0,
          7.67,
          -4.32,
          -4.45
        ],
        "code_task_scores": [
          -27.64,
          1.16,
          2.39,
          37.51,
          -21.87
        ],
        "reasoning_task_scores": [
          -2.59,
          -3.29,
          -0.06,
          -13.73
        ]
      },
      "affiliation": "Ruben Roy",
      "year": "2025",
      "size": "170k",
      "size_precise": "169527",
      "link": "https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 32,
      "name": "orca-math-word-problems-200k",
      "domain": "math",
      "general_avg": 58.79,
      "math_avg": 42.67,
      "code_avg": 51.88,
      "reasoning_avg": 34.82,
      "overall_avg": 47.04,
      "overall_efficiency": 0.024007,
      "general_efficiency": 0.036718,
      "math_efficiency": -0.000657,
      "code_efficiency": 0.060448,
      "reasoning_efficiency": -0.000479,
      "general_scores": [
        76.57,
        52.4675,
        58.61,
        46.345,
        76.84,
        53.2375,
        58.49,
        47.52,
        76.92,
        52.4825,
        59.12,
        46.87
      ],
      "math_scores": [
        75.06,
        52.5,
        51.6,
        23.4,
        10.0,
        74.53,
        52.26,
        52.8,
        24.77,
        6.67,
        75.89,
        51.66,
        51.0,
        24.59,
        13.33
      ],
      "code_scores": [
        78.05,
        73.54,
        13.62,
        43.01,
        47.74,
        79.27,
        75.88,
        13.62,
        44.47,
        49.77,
        81.1,
        73.54,
        12.9,
        43.22,
        48.42
      ],
      "reasoning_scores": [
        33.56,
        68.52,
        0.41097826,
        39.12,
        30.85,
        67.72,
        0.40532608999999997,
        38.16,
        31.86,
        67.55,
        0.40902174,
        39.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.78
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.16
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.14
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.25
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.57
              },
              {
                "metric": "lcb_test_output",
                "score": 48.64
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.09
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.34,
        "math_avg": -0.13,
        "code_avg": 12.09,
        "reasoning_avg": -0.1,
        "overall_avg": 4.8,
        "overall_efficiency": 0.024007,
        "general_efficiency": 0.036718,
        "math_efficiency": -0.000657,
        "code_efficiency": 0.060448,
        "reasoning_efficiency": -0.000479,
        "general_task_scores": [
          8.47,
          17.24,
          1.0,
          2.67
        ],
        "math_task_scores": [
          -4.82,
          1.02,
          1.6,
          -1.79,
          3.33
        ],
        "code_task_scores": [
          2.03,
          2.72,
          5.14,
          42.53,
          11.54
        ],
        "reasoning_task_scores": [
          -4.51,
          -1.53,
          0.02,
          5.57
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k",
      "paper_link": "https://arxiv.org/pdf/2402.14830",
      "tag": "math"
    },
    {
      "id": 33,
      "name": "dart-math-hard",
      "domain": "math",
      "general_avg": 45.99,
      "math_avg": 46.67,
      "code_avg": 46.25,
      "reasoning_avg": 27.71,
      "overall_avg": 41.66,
      "overall_efficiency": -0.000993,
      "general_efficiency": -0.009323,
      "math_efficiency": 0.006605,
      "code_efficiency": 0.011051,
      "reasoning_efficiency": -0.012302,
      "general_scores": [
        82.12,
        43.0725,
        57.8,
        0.96571429,
        82.74,
        43.3675,
        57.64,
        0.96714286,
        82.12,
        42.605,
        57.62,
        0.81857143
      ],
      "math_scores": [
        89.23,
        58.26,
        61.4,
        20.51,
        6.67,
        90.45,
        57.96,
        61.2,
        20.17,
        0.0,
        89.39,
        57.68,
        60.0,
        20.44,
        6.67
      ],
      "code_scores": [
        62.8,
        69.26,
        14.34,
        41.34,
        43.44,
        65.85,
        69.26,
        14.7,
        41.13,
        42.76,
        65.24,
        67.32,
        13.98,
        40.08,
        42.31
      ],
      "reasoning_scores": [
        33.22,
        40.46,
        0.4423913,
        36.24,
        29.83,
        40.35,
        0.43956522,
        36.4,
        37.97,
        40.22,
        0.44532609,
        36.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.69
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.97
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 64.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.61
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.34
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.85
              },
              {
                "metric": "lcb_test_output",
                "score": 42.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.46,
        "math_avg": 3.87,
        "code_avg": 6.47,
        "reasoning_avg": -7.2,
        "overall_avg": -0.58,
        "overall_efficiency": -0.000993,
        "general_efficiency": -0.009323,
        "math_efficiency": 0.006605,
        "code_efficiency": 0.011051,
        "reasoning_efficiency": -0.012302,
        "general_task_scores": [
          14.02,
          7.52,
          -0.05,
          -43.32
        ],
        "math_task_scores": [
          9.71,
          6.85,
          10.67,
          -5.67,
          -2.22
        ],
        "code_task_scores": [
          -12.81,
          -2.99,
          6.1,
          39.81,
          5.74
        ],
        "reasoning_task_scores": [
          -2.93,
          -29.12,
          0.05,
          3.12
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "size_precise": "585392",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-hard",
      "paper_link": "https://arxiv.org/abs/2407.13690",
      "tag": "math"
    },
    {
      "id": 34,
      "name": "gretel-math-gsm8k-v1",
      "domain": "math",
      "general_avg": 45.42,
      "math_avg": 5.54,
      "code_avg": 26.26,
      "reasoning_avg": 28.6,
      "overall_avg": 26.46,
      "overall_efficiency": -0.669316,
      "general_efficiency": -0.255449,
      "math_efficiency": -1.580456,
      "code_efficiency": -0.573487,
      "reasoning_efficiency": -0.267873,
      "general_scores": [
        86.43,
        48.0325,
        43.76,
        2.31071429,
        86.5,
        47.8875,
        45.15,
        1.92285714,
        86.65,
        48.885,
        45.65,
        1.87928571
      ],
      "math_scores": [
        0.08,
        2.12,
        1.0,
        19.67,
        6.67,
        0.0,
        1.9,
        0.6,
        18.99,
        6.67,
        0.0,
        2.2,
        0.6,
        19.24,
        3.33
      ],
      "code_scores": [
        3.66,
        72.76,
        13.98,
        21.09,
        21.04,
        3.66,
        73.15,
        12.9,
        20.04,
        22.62,
        0.61,
        73.15,
        11.83,
        21.29,
        22.17
      ],
      "reasoning_scores": [
        35.93,
        70.71,
        0.31369565,
        8.64,
        31.86,
        70.27,
        0.31293478,
        9.52,
        37.29,
        70.68,
        0.31934783,
        7.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.27
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 2.64
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.9
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.81
              },
              {
                "metric": "lcb_test_output",
                "score": 21.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.03
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.02,
        "math_avg": -37.26,
        "code_avg": -13.52,
        "reasoning_avg": -6.32,
        "overall_avg": -15.78,
        "overall_efficiency": -0.669316,
        "general_efficiency": -0.255449,
        "math_efficiency": -1.580456,
        "code_efficiency": -0.573487,
        "reasoning_efficiency": -0.267873,
        "general_task_scores": [
          18.22,
          12.78,
          -12.89,
          -42.2
        ],
        "math_task_scores": [
          -79.95,
          -49.05,
          -49.47,
          -6.74,
          -1.11
        ],
        "code_task_scores": [
          -74.8,
          1.42,
          4.66,
          19.77,
          -15.16
        ],
        "reasoning_task_scores": [
          -1.57,
          1.09,
          -0.07,
          -24.77
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "23.6k",
      "size_precise": "23578",
      "link": "https://huggingface.co/datasets/gretelai/gretel-math-gsm8k-v1",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 35,
      "name": "Maths-College",
      "domain": "math",
      "general_avg": 54.77,
      "math_avg": 41.88,
      "code_avg": 45.31,
      "reasoning_avg": 33.7,
      "overall_avg": 43.91,
      "overall_efficiency": 0.001729,
      "general_efficiency": 0.00343,
      "math_efficiency": -0.000951,
      "code_efficiency": 0.005694,
      "reasoning_efficiency": -0.001257,
      "general_scores": [
        69.51,
        49.795,
        55.2,
        44.2921429,
        70.48,
        50.0625,
        56.78,
        43.7028571,
        69.22,
        48.6925,
        55.65,
        43.8715385
      ],
      "math_scores": [
        80.21,
        49.88,
        46.8,
        21.66,
        13.33,
        76.12,
        48.8,
        48.0,
        22.06,
        10.0,
        77.79,
        49.48,
        46.4,
        21.0,
        16.67
      ],
      "code_scores": [
        78.66,
        68.48,
        12.19,
        12.53,
        42.31,
        78.66,
        67.7,
        13.26,
        32.15,
        41.86,
        79.27,
        69.26,
        12.19,
        29.23,
        41.86
      ],
      "reasoning_scores": [
        37.97,
        66.36,
        0.39804348,
        34.32,
        33.22,
        65.98,
        0.37967391,
        34.4,
        30.17,
        66.31,
        0.37902174,
        34.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.52
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.96
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.64
              },
              {
                "metric": "lcb_test_output",
                "score": 42.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.33,
        "math_avg": -0.92,
        "code_avg": 5.52,
        "reasoning_avg": -1.22,
        "overall_avg": 1.68,
        "overall_efficiency": 0.001729,
        "general_efficiency": 0.00343,
        "math_efficiency": -0.000951,
        "code_efficiency": 0.005694,
        "reasoning_efficiency": -0.001257,
        "general_task_scores": [
          1.43,
          14.03,
          -1.86,
          -0.28
        ],
        "math_task_scores": [
          -1.94,
          -1.73,
          -3.13,
          -4.47,
          6.66
        ],
        "code_task_scores": [
          1.42,
          -3.12,
          4.31,
          23.6,
          4.91
        ],
        "reasoning_task_scores": [
          -2.81,
          -3.24,
          -0.0,
          1.12
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "970k",
      "size_precise": "969980",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-College",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 36,
      "name": "Maths-Grade-School",
      "domain": "math",
      "general_avg": 47.17,
      "math_avg": 35.3,
      "code_avg": 30.64,
      "reasoning_avg": 32.41,
      "overall_avg": 36.38,
      "overall_efficiency": -0.005978,
      "general_efficiency": -0.004359,
      "math_efficiency": -0.007659,
      "code_efficiency": -0.009332,
      "reasoning_efficiency": -0.002558,
      "general_scores": [
        74.96,
        46.36,
        56.09,
        10.9892857,
        75.63,
        46.2175,
        56.19,
        10.9892857,
        75.39,
        44.9275,
        57.01,
        11.3207143
      ],
      "math_scores": [
        76.27,
        37.56,
        43.2,
        17.73,
        0.0,
        75.74,
        38.08,
        42.4,
        16.76,
        3.33,
        77.48,
        38.06,
        41.8,
        17.71,
        3.33
      ],
      "code_scores": [
        1.83,
        66.54,
        9.32,
        37.16,
        35.75,
        3.66,
        67.7,
        11.11,
        36.53,
        34.16,
        4.27,
        66.54,
        10.75,
        36.95,
        37.33
      ],
      "reasoning_scores": [
        33.56,
        60.83,
        0.48076087,
        35.44,
        34.24,
        60.9,
        0.4851087,
        34.08,
        30.51,
        62.62,
        0.48771739,
        35.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.84
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.43
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 3.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 66.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.88
              },
              {
                "metric": "lcb_test_output",
                "score": 35.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.45
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.48
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.27,
        "math_avg": -7.51,
        "code_avg": -9.14,
        "reasoning_avg": -2.51,
        "overall_avg": -5.86,
        "overall_efficiency": -0.005978,
        "general_efficiency": -0.004359,
        "math_efficiency": -0.007659,
        "code_efficiency": -0.009332,
        "reasoning_efficiency": -0.002558,
        "general_task_scores": [
          7.02,
          10.35,
          -1.31,
          -33.14
        ],
        "math_task_scores": [
          -3.48,
          -13.22,
          -7.73,
          -8.64,
          -4.45
        ],
        "code_task_scores": [
          -74.19,
          -4.67,
          2.15,
          35.84,
          -1.35
        ],
        "reasoning_task_scores": [
          -3.83,
          -8.01,
          0.09,
          1.65
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "98k",
      "size_precise": "979915",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 37,
      "name": "WebInstruct-CFT",
      "domain": "math",
      "general_avg": 60.32,
      "math_avg": 54.37,
      "code_avg": 45.21,
      "reasoning_avg": 34.66,
      "overall_avg": 48.64,
      "overall_efficiency": 0.12805,
      "general_efficiency": 0.177427,
      "math_efficiency": 0.231427,
      "code_efficiency": 0.10846,
      "reasoning_efficiency": -0.005114,
      "general_scores": [
        80.37,
        51.715,
        60.66,
        47.0,
        80.14,
        53.0675,
        60.32,
        48.0571429,
        80.43,
        53.955,
        60.41,
        47.665
      ],
      "math_scores": [
        86.66,
        69.92,
        73.8,
        26.74,
        20.0,
        87.87,
        70.18,
        72.6,
        27.01,
        13.33,
        88.32,
        70.26,
        72.4,
        26.51,
        10.0
      ],
      "code_scores": [
        53.05,
        73.15,
        14.34,
        42.38,
        42.99,
        51.22,
        73.54,
        15.05,
        41.75,
        44.12,
        54.27,
        72.76,
        13.26,
        42.8,
        43.44
      ],
      "reasoning_scores": [
        34.92,
        69.49,
        0.42576087,
        34.08,
        32.88,
        69.24,
        0.41304348,
        34.48,
        34.92,
        69.3,
        0.41934783,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.31
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.91
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.46
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 43.52
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.34
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.87,
        "math_avg": 11.57,
        "code_avg": 5.42,
        "reasoning_avg": -0.26,
        "overall_avg": 6.4,
        "overall_efficiency": 0.12805,
        "general_efficiency": 0.177427,
        "math_efficiency": 0.231427,
        "code_efficiency": 0.10846,
        "reasoning_efficiency": -0.005114,
        "general_task_scores": [
          12.0,
          17.42,
          2.72,
          3.33
        ],
        "math_task_scores": [
          7.64,
          19.0,
          22.73,
          0.71,
          7.77
        ],
        "code_task_scores": [
          -24.59,
          1.55,
          5.98,
          41.27,
          6.42
        ],
        "reasoning_task_scores": [
          -2.36,
          -0.12,
          0.03,
          1.36
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2025",
      "size": "50k",
      "size_precise": "50000",
      "link": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
      "paper_link": "https://arxiv.org/abs/2501.17703",
      "tag": "general,math,code,science"
    },
    {
      "id": 38,
      "name": "MATH-plus",
      "domain": "math",
      "general_avg": 55.02,
      "math_avg": 40.7,
      "code_avg": 48.32,
      "reasoning_avg": 32.65,
      "overall_avg": 44.17,
      "overall_efficiency": 0.002166,
      "general_efficiency": 0.004004,
      "math_efficiency": -0.002349,
      "code_efficiency": 0.009549,
      "reasoning_efficiency": -0.002537,
      "general_scores": [
        75.41,
        49.68,
        55.01,
        40.2392857,
        74.43,
        49.875,
        53.68,
        41.8692857
      ],
      "math_scores": [
        82.79,
        49.68,
        48.6,
        18.41,
        3.33,
        85.14,
        49.94,
        51.0,
        18.13,
        0.0
      ],
      "code_scores": [
        72.56,
        68.87,
        12.9,
        42.59,
        41.63,
        73.17,
        74.71,
        12.19,
        40.92,
        43.67
      ],
      "reasoning_scores": [
        29.15,
        65.8,
        0.42076087,
        36.64,
        29.49,
        66.39,
        0.41130435,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.92
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.05
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.96
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.1
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.76
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.58,
        "math_avg": -2.1,
        "code_avg": 8.54,
        "reasoning_avg": -2.27,
        "overall_avg": 1.94,
        "overall_efficiency": 0.002166,
        "general_efficiency": 0.004004,
        "math_efficiency": -0.002349,
        "code_efficiency": 0.009549,
        "reasoning_efficiency": -0.002537,
        "general_task_scores": [
          6.61,
          14.29,
          -3.4,
          -3.19
        ],
        "math_task_scores": [
          3.98,
          -1.31,
          -0.4,
          -7.77,
          -5.01
        ],
        "code_task_scores": [
          -4.57,
          0.19,
          4.3,
          40.72,
          5.55
        ],
        "reasoning_task_scores": [
          -7.28,
          -3.36,
          0.03,
          1.48
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2024",
      "size": "894k",
      "size_precise": "893929",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MATH-plus",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 39,
      "name": "math-gpt-4o-200k",
      "domain": "math",
      "general_avg": 61.04,
      "math_avg": 55.74,
      "code_avg": 51.95,
      "reasoning_avg": 35.07,
      "overall_avg": 50.95,
      "overall_efficiency": 0.04357,
      "general_efficiency": 0.047985,
      "math_efficiency": 0.064672,
      "code_efficiency": 0.060838,
      "reasoning_efficiency": 0.000786,
      "general_scores": [
        79.48,
        51.7625,
        61.1,
        51.39,
        80.36,
        52.635,
        60.93,
        51.0807143,
        79.89,
        52.88,
        60.9,
        50.1085714
      ],
      "math_scores": [
        91.43,
        72.5,
        75.0,
        28.27,
        13.33,
        90.9,
        72.06,
        76.6,
        27.46,
        6.67,
        91.13,
        72.02,
        77.2,
        28.18,
        13.33
      ],
      "code_scores": [
        82.32,
        75.1,
        13.98,
        41.34,
        45.93,
        80.49,
        75.1,
        14.34,
        42.38,
        47.06,
        79.27,
        75.1,
        15.05,
        43.22,
        48.64
      ],
      "reasoning_scores": [
        35.93,
        69.07,
        0.42130435,
        37.28,
        32.54,
        69.06,
        0.4251087,
        37.6,
        30.51,
        68.98,
        0.42630435,
        38.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.91
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.15
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 75.1
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.46
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.84
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.6,
        "math_avg": 12.94,
        "code_avg": 12.17,
        "reasoning_avg": 0.16,
        "overall_avg": 8.72,
        "overall_efficiency": 0.04357,
        "general_efficiency": 0.047985,
        "math_efficiency": 0.064672,
        "code_efficiency": 0.060838,
        "reasoning_efficiency": 0.000786,
        "general_task_scores": [
          11.6,
          16.94,
          3.24,
          6.62
        ],
        "math_task_scores": [
          11.17,
          21.07,
          26.07,
          1.93,
          4.44
        ],
        "code_task_scores": [
          3.25,
          3.5,
          6.22,
          41.27,
          10.11
        ],
        "reasoning_task_scores": [
          -3.61,
          -0.42,
          0.03,
          4.56
        ]
      },
      "affiliation": "PKRD",
      "year": "2024",
      "size": "200k",
      "size_precise": "200035",
      "link": "https://huggingface.co/datasets/PawanKrd/math-gpt-4o-200k",
      "paper_link": "",
      "tag": "math"
    },
    {
      "id": 40,
      "name": "NuminaMath-1.5",
      "domain": "math",
      "general_avg": 42.3,
      "math_avg": 50.32,
      "code_avg": 36.63,
      "reasoning_avg": 32.86,
      "overall_avg": 40.53,
      "overall_efficiency": -0.001906,
      "general_efficiency": -0.010201,
      "math_efficiency": 0.008393,
      "code_efficiency": -0.003517,
      "reasoning_efficiency": -0.002299,
      "general_scores": [
        67.68,
        48.38,
        52.38,
        0.11928571,
        70.7,
        46.18,
        51.79,
        0.28214286,
        69.57,
        47.225,
        52.68,
        0.64428571
      ],
      "math_scores": [
        88.25,
        65.72,
        68.8,
        24.16,
        10.0,
        87.04,
        66.78,
        69.2,
        23.26,
        6.67,
        88.17,
        65.38,
        66.2,
        25.23,
        0.0
      ],
      "code_scores": [
        39.63,
        66.54,
        7.89,
        40.92,
        36.65,
        42.07,
        67.32,
        11.83,
        34.86,
        23.98,
        26.83,
        69.26,
        9.32,
        38.0,
        34.39
      ],
      "reasoning_scores": [
        28.14,
        61.3,
        0.38054348,
        26.48,
        41.69,
        62.08,
        0.37902174,
        29.84,
        51.19,
        59.93,
        0.38913043,
        32.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 36.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.93
              },
              {
                "metric": "lcb_test_output",
                "score": 31.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.1
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -9.14,
        "math_avg": 7.52,
        "code_avg": -3.15,
        "reasoning_avg": -2.06,
        "overall_avg": -1.71,
        "overall_efficiency": -0.001906,
        "general_efficiency": -0.010201,
        "math_efficiency": 0.008393,
        "code_efficiency": -0.003517,
        "reasoning_efficiency": -0.002299,
        "general_task_scores": [
          1.01,
          11.77,
          -5.46,
          -43.89
        ],
        "math_task_scores": [
          7.84,
          14.84,
          17.87,
          -1.82,
          -1.11
        ],
        "code_task_scores": [
          -41.26,
          -3.89,
          1.44,
          36.89,
          -5.43
        ],
        "reasoning_task_scores": [
          3.74,
          -8.36,
          -0.01,
          -3.68
        ]
      },
      "affiliation": "Numina",
      "year": "2025",
      "size": "896k",
      "size_precise": "896215",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
      "paper_link": "http://faculty.bicmr.pku.edu.cn/~dongbin/Publications/numina_dataset.pdf",
      "tag": "math"
    },
    {
      "id": 41,
      "name": "MathFusionQA",
      "domain": "math",
      "general_avg": 46.95,
      "math_avg": 49.31,
      "code_avg": 51.22,
      "reasoning_avg": 35.08,
      "overall_avg": 45.64,
      "overall_efficiency": 0.056843,
      "general_efficiency": -0.075005,
      "math_efficiency": 0.108662,
      "code_efficiency": 0.190916,
      "reasoning_efficiency": 0.002798,
      "general_scores": [
        76.64,
        52.4775,
        58.68,
        0.0,
        76.14,
        52.055,
        58.89,
        0.0,
        77.24,
        52.395,
        58.91,
        0.0
      ],
      "math_scores": [
        84.0,
        64.02,
        64.2,
        24.46,
        13.33,
        77.79,
        64.62,
        65.8,
        24.24,
        10.0,
        81.58,
        64.52,
        67.2,
        23.89,
        10.0
      ],
      "code_scores": [
        76.22,
        72.37,
        14.7,
        43.01,
        48.87,
        78.66,
        71.98,
        12.19,
        44.89,
        47.51,
        76.83,
        73.93,
        13.98,
        44.05,
        49.1
      ],
      "reasoning_scores": [
        32.2,
        70.05,
        0.4301087,
        36.8,
        33.22,
        69.93,
        0.42826087,
        36.48,
        34.92,
        69.87,
        0.43923913,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.83
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.62
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.98
              },
              {
                "metric": "lcb_test_output",
                "score": 48.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.95
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -4.49,
        "math_avg": 6.51,
        "code_avg": 11.43,
        "reasoning_avg": 0.17,
        "overall_avg": 3.4,
        "overall_efficiency": 0.056843,
        "general_efficiency": -0.075005,
        "math_efficiency": 0.108662,
        "code_efficiency": 0.190916,
        "reasoning_efficiency": 0.002798,
        "general_task_scores": [
          8.36,
          16.82,
          1.09,
          -44.24
        ],
        "math_task_scores": [
          1.14,
          13.27,
          15.53,
          -1.84,
          4.44
        ],
        "code_task_scores": [
          -0.2,
          1.16,
          5.38,
          42.94,
          11.39
        ],
        "reasoning_task_scores": [
          -3.15,
          0.49,
          0.04,
          3.23
        ]
      },
      "affiliation": "Shanghai AI Laboratory",
      "year": "2025",
      "size": "5.9k",
      "size_precise": "59892",
      "link": "https://huggingface.co/datasets/QizhiPei/MathFusionQA",
      "paper_link": "https://arxiv.org/abs/2503.16212",
      "tag": "math"
    },
    {
      "id": 42,
      "name": "hercules-v1.0",
      "domain": "math",
      "general_avg": 54.38,
      "math_avg": 40.05,
      "code_avg": 48.66,
      "reasoning_avg": 34.83,
      "overall_avg": 44.48,
      "overall_efficiency": 0.004848,
      "general_efficiency": 0.00634,
      "math_efficiency": -0.005935,
      "code_efficiency": 0.019172,
      "reasoning_efficiency": -0.000186,
      "general_scores": [
        73.09,
        53.1125,
        58.37,
        31.3078571,
        73.29,
        53.95,
        58.2,
        33.72
      ],
      "math_scores": [
        79.98,
        47.88,
        48.0,
        18.88,
        3.33,
        82.11,
        46.62,
        47.4,
        19.67,
        6.67
      ],
      "code_scores": [
        77.44,
        72.76,
        11.83,
        38.83,
        42.08,
        77.44,
        73.54,
        10.75,
        39.87,
        42.08
      ],
      "reasoning_scores": [
        34.58,
        69.31,
        0.43934783,
        34.96,
        36.95,
        68.6,
        0.44413043,
        33.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.51
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.7
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.35
              },
              {
                "metric": "lcb_test_output",
                "score": 42.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.76
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.94,
        "math_avg": -2.75,
        "code_avg": 8.88,
        "reasoning_avg": -0.09,
        "overall_avg": 2.24,
        "overall_efficiency": 0.004848,
        "general_efficiency": 0.00634,
        "math_efficiency": -0.005935,
        "code_efficiency": 0.019172,
        "reasoning_efficiency": -0.000186,
        "general_task_scores": [
          4.88,
          18.04,
          0.54,
          -11.73
        ],
        "math_task_scores": [
          1.06,
          -3.87,
          -2.5,
          -6.77,
          -1.67
        ],
        "code_task_scores": [
          0.0,
          1.55,
          3.05,
          38.31,
          4.98
        ],
        "reasoning_task_scores": [
          -0.84,
          -0.5,
          0.05,
          0.88
        ]
      },
      "affiliation": "Sebastian Gabarain",
      "year": "2024",
      "size": "463k",
      "size_precise": "463022",
      "link": "https://huggingface.co/datasets/Locutusque/hercules-v1.0?not-for-all-audiences=true",
      "paper_link": "",
      "tag": "general,math,code,science"
    },
    {
      "id": 43,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 53.46,
      "math_avg": 41.78,
      "code_avg": 48.27,
      "reasoning_avg": 31.99,
      "overall_avg": 43.88,
      "overall_efficiency": 0.081941,
      "general_efficiency": 0.100913,
      "math_efficiency": -0.051077,
      "code_efficiency": 0.423951,
      "reasoning_efficiency": -0.14602,
      "general_scores": [
        70.76,
        52.305,
        56.6,
        36.5364286,
        69.01,
        53.4925,
        56.83,
        33.3778571,
        67.16,
        53.3425,
        57.03,
        35.135
      ],
      "math_scores": [
        81.05,
        50.6,
        52.2,
        20.48,
        6.67,
        79.98,
        49.42,
        50.0,
        18.61,
        13.33,
        77.63,
        49.1,
        50.0,
        17.62,
        10.0
      ],
      "code_scores": [
        77.44,
        71.98,
        8.96,
        44.05,
        44.8,
        73.17,
        70.82,
        6.45,
        42.59,
        43.67,
        75.61,
        71.6,
        3.94,
        44.68,
        44.34
      ],
      "reasoning_scores": [
        34.24,
        67.9,
        0.38032609,
        29.44,
        27.8,
        68.84,
        0.38195652,
        29.52,
        26.78,
        68.41,
        0.38130435,
        29.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 44.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.02,
        "math_avg": -1.02,
        "code_avg": 8.49,
        "reasoning_avg": -2.92,
        "overall_avg": 1.64,
        "overall_efficiency": 0.081941,
        "general_efficiency": 0.100913,
        "math_efficiency": -0.051077,
        "code_efficiency": 0.423951,
        "reasoning_efficiency": -0.14602,
        "general_task_scores": [
          0.67,
          17.56,
          -0.92,
          -9.22
        ],
        "math_task_scores": [
          -0.43,
          -1.41,
          0.53,
          -7.14,
          3.33
        ],
        "code_task_scores": [
          -2.03,
          -0.13,
          -1.79,
          42.73,
          7.17
        ],
        "reasoning_task_scores": [
          -6.99,
          -1.08,
          -0.01,
          -3.68
        ]
      },
      "affiliation": "Sahil Chaudhary",
      "year": "2023",
      "size": "20k",
      "size_precise": "20022",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
      "paper_link": "https://github.com/sahil280114/codealpaca",
      "tag": "code"
    },
    {
      "id": 44,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 57.83,
      "math_avg": 46.32,
      "code_avg": 45.9,
      "reasoning_avg": 34.08,
      "overall_avg": 46.03,
      "overall_efficiency": 0.008694,
      "general_efficiency": 0.014639,
      "math_efficiency": 0.008056,
      "code_efficiency": 0.014008,
      "reasoning_efficiency": -0.001928,
      "general_scores": [
        73.95,
        54.0475,
        57.61,
        45.75,
        73.28,
        55.2325,
        58.17,
        45.3114286,
        72.35,
        55.42,
        58.46,
        44.4035714
      ],
      "math_scores": [
        86.81,
        49.72,
        51.2,
        21.12,
        26.67,
        86.28,
        49.78,
        51.6,
        21.43,
        20.0,
        84.69,
        48.5,
        52.2,
        21.43,
        23.33
      ],
      "code_scores": [
        78.66,
        75.88,
        13.26,
        40.92,
        16.52,
        79.27,
        72.76,
        13.98,
        41.13,
        17.65,
        82.32,
        75.49,
        14.34,
        38.0,
        28.28
      ],
      "reasoning_scores": [
        33.56,
        67.17,
        0.44717391,
        35.6,
        33.22,
        67.49,
        0.43336957,
        35.76,
        32.54,
        68.17,
        0.43282609,
        34.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.02
              },
              {
                "metric": "lcb_test_output",
                "score": 20.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.39,
        "math_avg": 3.52,
        "code_avg": 6.11,
        "reasoning_avg": -0.84,
        "overall_avg": 3.79,
        "overall_efficiency": 0.008694,
        "general_efficiency": 0.014639,
        "math_efficiency": 0.008056,
        "code_efficiency": 0.014008,
        "reasoning_efficiency": -0.001928,
        "general_task_scores": [
          4.88,
          19.41,
          0.34,
          0.92
        ],
        "math_task_scores": [
          5.95,
          -1.79,
          1.47,
          -4.71,
          16.66
        ],
        "code_task_scores": [
          2.64,
          3.11,
          5.62,
          38.98,
          -16.28
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.85,
          0.05,
          1.87
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "size_precise": "436347",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2",
      "paper_link": "https://arxiv.org/abs/2411.04905",
      "tag": "code"
    },
    {
      "id": 45,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 55.87,
      "math_avg": 42.32,
      "code_avg": 49.43,
      "reasoning_avg": 32.65,
      "overall_avg": 45.07,
      "overall_efficiency": 0.018075,
      "general_efficiency": 0.028266,
      "math_efficiency": -0.003101,
      "code_efficiency": 0.061606,
      "reasoning_efficiency": -0.014472,
      "general_scores": [
        71.55,
        51.1425,
        57.7,
        42.3457143,
        72.22,
        50.4425,
        58.0,
        43.0657143,
        72.94,
        50.2575,
        58.04,
        42.7221429
      ],
      "math_scores": [
        81.96,
        49.72,
        52.6,
        20.57,
        10.0,
        79.76,
        49.92,
        50.8,
        20.37,
        6.67,
        82.11,
        48.64,
        51.4,
        20.23,
        10.0
      ],
      "code_scores": [
        78.66,
        74.32,
        13.62,
        39.04,
        43.67,
        79.27,
        70.04,
        14.7,
        38.41,
        44.34,
        80.49,
        70.82,
        13.98,
        38.2,
        41.86
      ],
      "reasoning_scores": [
        28.47,
        67.62,
        0.41380435,
        33.44,
        28.14,
        67.7,
        0.41195652,
        32.8,
        29.83,
        68.4,
        0.42804348,
        34.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.42,
        "math_avg": -0.49,
        "code_avg": 9.64,
        "reasoning_avg": -2.27,
        "overall_avg": 2.83,
        "overall_efficiency": 0.018075,
        "general_efficiency": 0.028266,
        "math_efficiency": -0.003101,
        "code_efficiency": 0.061606,
        "reasoning_efficiency": -0.014472,
        "general_task_scores": [
          3.93,
          15.12,
          0.17,
          -1.53
        ],
        "math_task_scores": [
          1.3,
          -1.69,
          1.4,
          -5.65,
          2.22
        ],
        "code_task_scores": [
          2.03,
          0.13,
          5.86,
          37.51,
          6.19
        ],
        "reasoning_task_scores": [
          -7.79,
          -1.55,
          0.03,
          0.19
        ]
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "157k",
      "size_precise": "156526",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 46,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 55.97,
      "math_avg": 42.77,
      "code_avg": 48.81,
      "reasoning_avg": 33.63,
      "overall_avg": 45.29,
      "overall_efficiency": 0.027481,
      "general_efficiency": 0.040665,
      "math_efficiency": -0.00027,
      "code_efficiency": 0.081114,
      "reasoning_efficiency": -0.011585,
      "general_scores": [
        73.46,
        50.4625,
        58.39,
        40.425,
        72.79,
        51.2425,
        58.7,
        42.015,
        73.71,
        52.2175,
        58.47,
        39.75
      ],
      "math_scores": [
        85.97,
        48.64,
        51.8,
        21.03,
        6.67,
        85.75,
        50.6,
        52.6,
        20.05,
        3.33,
        85.22,
        49.72,
        53.0,
        20.53,
        6.67
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        37.58,
        42.76,
        78.66,
        71.6,
        12.19,
        36.95,
        43.21,
        78.05,
        73.15,
        11.47,
        38.41,
        43.44
      ],
      "reasoning_scores": [
        32.2,
        67.97,
        0.43880435,
        33.6,
        30.17,
        68.39,
        0.43891304,
        34.64,
        31.86,
        68.34,
        0.4401087,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.52,
        "math_avg": -0.03,
        "code_avg": 9.03,
        "reasoning_avg": -1.29,
        "overall_avg": 3.06,
        "overall_efficiency": 0.027481,
        "general_efficiency": 0.040665,
        "math_efficiency": -0.00027,
        "code_efficiency": 0.081114,
        "reasoning_efficiency": -0.011585,
        "general_task_scores": [
          5.01,
          15.82,
          0.78,
          -3.51
        ],
        "math_task_scores": [
          5.67,
          -1.47,
          2.27,
          -5.5,
          -1.11
        ],
        "code_task_scores": [
          1.63,
          0.77,
          3.59,
          36.61,
          6.04
        ],
        "reasoning_task_scores": [
          -5.19,
          -1.23,
          0.05,
          1.15
        ]
      },
      "affiliation": "theblackcat102",
      "year": "2023",
      "size": "111k",
      "size_precise": "111272",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 47,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 56.18,
      "math_avg": 43.35,
      "code_avg": 48.67,
      "reasoning_avg": 33.49,
      "overall_avg": 45.42,
      "overall_efficiency": 0.028637,
      "general_efficiency": 0.042548,
      "math_efficiency": 0.004923,
      "code_efficiency": 0.079884,
      "reasoning_efficiency": -0.012806,
      "general_scores": [
        72.15,
        52.31,
        59.02,
        40.0664286,
        73.45,
        50.7375,
        58.68,
        41.9592857,
        73.67,
        52.3975,
        58.3,
        41.36
      ],
      "math_scores": [
        84.69,
        50.9,
        53.4,
        20.71,
        10.0,
        85.14,
        49.32,
        50.6,
        19.87,
        6.67,
        85.22,
        49.98,
        52.6,
        21.14,
        10.0
      ],
      "code_scores": [
        78.05,
        73.54,
        11.47,
        38.41,
        45.48,
        79.88,
        71.21,
        12.19,
        35.49,
        43.21,
        78.66,
        72.37,
        10.75,
        36.53,
        42.76
      ],
      "reasoning_scores": [
        33.56,
        67.98,
        0.44086957,
        34.24,
        27.8,
        67.78,
        0.44271739,
        34.48,
        30.17,
        67.71,
        0.42641304,
        36.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.81
              },
              {
                "metric": "lcb_test_output",
                "score": 43.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.51
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.73,
        "math_avg": 0.55,
        "code_avg": 8.88,
        "reasoning_avg": -1.42,
        "overall_avg": 3.18,
        "overall_efficiency": 0.028637,
        "general_efficiency": 0.042548,
        "math_efficiency": 0.004923,
        "code_efficiency": 0.079884,
        "reasoning_efficiency": -0.012806,
        "general_task_scores": [
          4.78,
          16.33,
          0.93,
          -3.11
        ],
        "math_task_scores": [
          5.04,
          -1.05,
          2.0,
          -5.47,
          2.22
        ],
        "code_task_scores": [
          1.42,
          0.77,
          3.23,
          35.77,
          6.72
        ],
        "reasoning_task_scores": [
          -6.09,
          -1.64,
          0.05,
          1.92
        ]
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "110k",
      "size_precise": "111183",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K",
      "paper_link": "https://arxiv.org/abs/2306.08568",
      "tag": "code"
    },
    {
      "id": 48,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 57.03,
      "math_avg": 48.88,
      "code_avg": 48.19,
      "reasoning_avg": 33.52,
      "overall_avg": 46.91,
      "overall_efficiency": 0.062081,
      "general_efficiency": 0.074306,
      "math_efficiency": 0.080828,
      "code_efficiency": 0.111773,
      "reasoning_efficiency": -0.018582,
      "general_scores": [
        74.18,
        50.5175,
        58.27,
        44.5807143,
        75.17,
        49.6875,
        58.73,
        45.1207143
      ],
      "math_scores": [
        77.86,
        61.66,
        61.2,
        26.31,
        16.67,
        81.27,
        61.66,
        60.0,
        25.5,
        16.67
      ],
      "code_scores": [
        79.27,
        72.37,
        11.83,
        42.8,
        35.29,
        79.88,
        71.6,
        12.54,
        42.38,
        33.94
      ],
      "reasoning_scores": [
        35.25,
        69.52,
        0.41336957,
        29.12,
        34.24,
        70.48,
        0.40923913,
        28.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.57
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.59
              },
              {
                "metric": "lcb_test_output",
                "score": 34.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.92
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.59,
        "math_avg": 6.08,
        "code_avg": 8.4,
        "reasoning_avg": -1.4,
        "overall_avg": 4.67,
        "overall_efficiency": 0.062081,
        "general_efficiency": 0.074306,
        "math_efficiency": 0.080828,
        "code_efficiency": 0.111773,
        "reasoning_efficiency": -0.018582,
        "general_task_scores": [
          6.37,
          14.61,
          0.76,
          0.61
        ],
        "math_task_scores": [
          -0.42,
          10.54,
          10.4,
          -0.14,
          10.0
        ],
        "code_task_scores": [
          2.13,
          0.38,
          3.94,
          41.55,
          -2.49
        ],
        "reasoning_task_scores": [
          -1.85,
          0.54,
          0.02,
          -4.36
        ]
      },
      "affiliation": "UIUC",
      "year": "2023",
      "size": "75.2k",
      "size_precise": "75197",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K",
      "paper_link": "https://openai.com/zh-Hans-CN/policies/usage-policies/",
      "tag": "code"
    },
    {
      "id": 49,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 34.24,
      "code_avg": 45.39,
      "reasoning_avg": 32.58,
      "overall_avg": 40.92,
      "overall_efficiency": -0.019818,
      "general_efficiency": 0.000381,
      "math_efficiency": -0.128929,
      "code_efficiency": 0.084495,
      "reasoning_efficiency": -0.035218,
      "general_scores": [
        75.82,
        48.3275,
        55.52,
        27.7192857,
        76.13,
        48.3925,
        54.78,
        25.4685714,
        76.39,
        45.895,
        54.65,
        28.5435714
      ],
      "math_scores": [
        76.8,
        30.94,
        29.0,
        22.34,
        10.0,
        75.44,
        31.74,
        30.2,
        23.06,
        13.33,
        73.92,
        31.56,
        30.4,
        21.59,
        13.33
      ],
      "code_scores": [
        75.0,
        73.15,
        12.19,
        24.01,
        44.34,
        73.17,
        73.54,
        12.9,
        26.1,
        41.4,
        71.34,
        71.98,
        13.26,
        27.35,
        41.18
      ],
      "reasoning_scores": [
        33.22,
        68.02,
        0.32086957,
        28.96,
        30.17,
        67.36,
        0.32782609,
        30.16,
        35.93,
        67.51,
        0.32369565,
        28.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.82
              },
              {
                "metric": "lcb_test_output",
                "score": 42.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.03,
        "math_avg": -8.56,
        "code_avg": 5.61,
        "reasoning_avg": -2.34,
        "overall_avg": -1.32,
        "overall_efficiency": -0.019818,
        "general_efficiency": 0.000381,
        "math_efficiency": -0.128929,
        "code_efficiency": 0.084495,
        "reasoning_efficiency": -0.035218,
        "general_task_scores": [
          7.8,
          12.05,
          -2.76,
          -17.0
        ],
        "math_task_scores": [
          -4.59,
          -19.71,
          -20.33,
          -3.71,
          5.55
        ],
        "code_task_scores": [
          -4.27,
          1.29,
          4.54,
          24.78,
          5.21
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.83,
          -0.07,
          -4.03
        ]
      },
      "affiliation": "M-A-P",
      "year": "2024",
      "size": "66.4k",
      "size_precise": "66383",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback",
      "paper_link": "https://arxiv.org/abs/2402.14658",
      "tag": "code"
    },
    {
      "id": 50,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 49.96,
      "math_avg": 36.48,
      "code_avg": 39.29,
      "reasoning_avg": 30.68,
      "overall_avg": 39.1,
      "overall_efficiency": -0.056879,
      "general_efficiency": -0.026952,
      "math_efficiency": -0.114762,
      "code_efficiency": -0.008957,
      "reasoning_efficiency": -0.076844,
      "general_scores": [
        71.86,
        39.445,
        44.32,
        42.75,
        72.08,
        39.645,
        45.15,
        44.2685714,
        72.69,
        39.16,
        44.39,
        43.7485714
      ],
      "math_scores": [
        68.08,
        40.54,
        36.6,
        25.09,
        6.67,
        69.9,
        42.76,
        39.6,
        25.43,
        10.0,
        66.26,
        39.7,
        34.6,
        25.25,
        16.67
      ],
      "code_scores": [
        69.51,
        68.87,
        9.32,
        40.71,
        0.45,
        70.73,
        66.93,
        11.83,
        46.76,
        3.17,
        69.51,
        66.54,
        8.24,
        48.43,
        8.37
      ],
      "reasoning_scores": [
        33.22,
        68.44,
        0.38967391,
        19.6,
        34.24,
        68.19,
        0.38804348,
        21.92,
        31.53,
        67.94,
        0.39413043,
        21.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 45.3
              },
              {
                "metric": "lcb_test_output",
                "score": 4.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.49,
        "math_avg": -6.33,
        "code_avg": -0.49,
        "reasoning_avg": -4.24,
        "overall_avg": -3.13,
        "overall_efficiency": -0.056879,
        "general_efficiency": -0.026952,
        "math_efficiency": -0.114762,
        "code_efficiency": -0.008957,
        "reasoning_efficiency": -0.076844,
        "general_task_scores": [
          3.9,
          3.93,
          -13.12,
          -0.65
        ],
        "math_task_scores": [
          -11.9,
          -10.12,
          -13.27,
          -0.78,
          4.44
        ],
        "code_task_scores": [
          -7.52,
          -4.15,
          1.56,
          44.26,
          -33.1
        ],
        "reasoning_task_scores": [
          -3.6,
          -1.27,
          -0.0,
          -12.13
        ]
      },
      "affiliation": "Nicolas Mejia-Petit",
      "year": "2024",
      "size": "55.1k",
      "size_precise": "55117",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 51,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 53.39,
      "math_avg": 35.08,
      "code_avg": 42.42,
      "reasoning_avg": 30.13,
      "overall_avg": 40.26,
      "overall_efficiency": -0.039111,
      "general_efficiency": 0.03833,
      "math_efficiency": -0.152359,
      "code_efficiency": 0.052105,
      "reasoning_efficiency": -0.094517,
      "general_scores": [
        75.76,
        45.205,
        55.36,
        37.7707143,
        75.5,
        46.14,
        54.79,
        37.9335714,
        75.76,
        45.295,
        55.3,
        35.8214286
      ],
      "math_scores": [
        61.11,
        45.02,
        42.8,
        23.28,
        6.67,
        55.5,
        43.84,
        42.4,
        23.01,
        6.67,
        56.33,
        44.3,
        41.4,
        23.92,
        10.0
      ],
      "code_scores": [
        72.56,
        73.54,
        13.98,
        24.43,
        28.73,
        71.95,
        71.21,
        13.98,
        25.47,
        26.47,
        71.95,
        71.21,
        14.34,
        27.14,
        29.41
      ],
      "reasoning_scores": [
        31.19,
        68.64,
        0.38173913,
        19.6,
        31.86,
        67.61,
        0.38108696,
        18.56,
        34.24,
        68.38,
        0.37456522,
        20.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.68
              },
              {
                "metric": "lcb_test_output",
                "score": 28.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.94,
        "math_avg": -7.72,
        "code_avg": 2.64,
        "reasoning_avg": -4.79,
        "overall_avg": -1.98,
        "overall_efficiency": -0.039111,
        "general_efficiency": 0.03833,
        "math_efficiency": -0.152359,
        "code_efficiency": 0.052105,
        "reasoning_efficiency": -0.094517,
        "general_task_scores": [
          7.36,
          10.06,
          -2.59,
          -7.06
        ],
        "math_task_scores": [
          -22.33,
          -6.73,
          -8.0,
          -2.64,
          1.11
        ],
        "code_task_scores": [
          -5.29,
          0.39,
          5.86,
          24.64,
          -8.9
        ],
        "reasoning_task_scores": [
          -4.17,
          -1.25,
          -0.01,
          -13.79
        ]
      },
      "affiliation": "BigCode",
      "year": "2024",
      "size": "50.7k",
      "size_precise": "50661",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 52,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.5,
      "math_avg": 34.35,
      "code_avg": 46.65,
      "reasoning_avg": 32.18,
      "overall_avg": 42.42,
      "overall_efficiency": 0.000178,
      "general_efficiency": 0.004924,
      "math_efficiency": -0.008232,
      "code_efficiency": 0.006685,
      "reasoning_efficiency": -0.002667,
      "general_scores": [
        74.85,
        51.775,
        58.8,
        42.7107143,
        74.91,
        50.195,
        58.67,
        41.5028571,
        73.57,
        51.02,
        57.95,
        42.0721429
      ],
      "math_scores": [
        58.68,
        46.02,
        41.6,
        20.53,
        3.33,
        54.06,
        44.66,
        42.4,
        21.5,
        3.33,
        62.77,
        47.18,
        44.2,
        21.61,
        3.33
      ],
      "code_scores": [
        76.83,
        71.6,
        13.26,
        37.16,
        43.21,
        78.05,
        71.6,
        12.19,
        27.97,
        42.76,
        81.71,
        69.26,
        11.47,
        18.58,
        44.12
      ],
      "reasoning_scores": [
        25.76,
        65.39,
        0.43847826,
        33.52,
        32.88,
        65.57,
        0.43271739,
        34.16,
        28.14,
        66.51,
        0.4475,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.9
              },
              {
                "metric": "lcb_test_output",
                "score": 43.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.06,
        "math_avg": -8.46,
        "code_avg": 6.87,
        "reasoning_avg": -2.74,
        "overall_avg": 0.18,
        "overall_efficiency": 0.000178,
        "general_efficiency": 0.004924,
        "math_efficiency": -0.008232,
        "code_efficiency": 0.006685,
        "reasoning_efficiency": -0.002667,
        "general_task_scores": [
          6.13,
          15.51,
          0.73,
          -2.14
        ],
        "math_task_scores": [
          -21.48,
          -5.17,
          -7.47,
          -4.83,
          -3.34
        ],
        "code_task_scores": [
          1.42,
          -0.78,
          4.07,
          26.86,
          6.26
        ],
        "reasoning_task_scores": [
          -7.67,
          -3.64,
          0.05,
          0.24
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "size_precise": "1027064",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1",
      "paper_link": "https://arxiv.org/pdf/2411.04905",
      "tag": "code"
    },
    {
      "id": 53,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 54.71,
      "math_avg": 51.5,
      "code_avg": 40.42,
      "reasoning_avg": 32.58,
      "overall_avg": 44.8,
      "overall_efficiency": 0.073284,
      "general_efficiency": 0.093283,
      "math_efficiency": 0.248617,
      "code_efficiency": 0.018048,
      "reasoning_efficiency": -0.066811,
      "general_scores": [
        72.01,
        51.76,
        59.64,
        37.0535714,
        70.65,
        51.2125,
        58.86,
        37.1528571,
        70.48,
        51.0075,
        58.75,
        37.935
      ],
      "math_scores": [
        83.17,
        65.76,
        66.6,
        27.48,
        13.33,
        81.58,
        66.6,
        70.6,
        27.08,
        13.33,
        81.5,
        66.88,
        67.8,
        27.51,
        13.33
      ],
      "code_scores": [
        77.44,
        73.93,
        10.04,
        43.01,
        0.0,
        75.61,
        73.54,
        11.47,
        40.92,
        0.0,
        78.66,
        73.54,
        9.68,
        38.41,
        0.0
      ],
      "reasoning_scores": [
        28.47,
        69.63,
        0.42565217,
        28.64,
        30.85,
        69.84,
        0.41673913,
        29.76,
        30.85,
        69.71,
        0.42467391,
        31.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.78
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.06
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.26,
        "math_avg": 8.7,
        "code_avg": 0.63,
        "reasoning_avg": -2.34,
        "overall_avg": 2.56,
        "overall_efficiency": 0.073284,
        "general_efficiency": 0.093283,
        "math_efficiency": 0.248617,
        "code_efficiency": 0.018048,
        "reasoning_efficiency": -0.066811,
        "general_task_scores": [
          2.74,
          15.84,
          1.34,
          -6.86
        ],
        "math_task_scores": [
          2.1,
          15.29,
          18.13,
          1.32,
          6.66
        ],
        "code_task_scores": [
          -0.2,
          2.07,
          2.16,
          39.74,
          -37.1
        ],
        "reasoning_task_scores": [
          -6.54,
          0.27,
          0.03,
          -3.17
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "35k",
      "size_precise": "34999",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code",
      "paper_link": "https://arxiv.org/pdf/2411.15124",
      "tag": "code"
    },
    {
      "id": 54,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 53.94,
      "math_avg": 48.18,
      "code_avg": 35.22,
      "reasoning_avg": 25.19,
      "overall_avg": 40.63,
      "overall_efficiency": -0.00332,
      "general_efficiency": 0.005146,
      "math_efficiency": 0.011103,
      "code_efficiency": -0.009433,
      "reasoning_efficiency": -0.020096,
      "general_scores": [
        66.21,
        56.4025,
        51.46,
        42.5707143,
        65.93,
        56.725,
        51.79,
        39.9364286,
        66.4,
        55.995,
        52.15,
        41.6585714
      ],
      "math_scores": [
        74.83,
        63.16,
        63.4,
        26.51,
        6.67,
        75.28,
        61.7,
        65.6,
        25.56,
        20.0,
        73.39,
        60.18,
        60.2,
        26.17,
        20.0
      ],
      "code_scores": [
        77.44,
        76.65,
        0.0,
        22.76,
        0.0,
        81.1,
        74.32,
        0.0,
        19.42,
        0.45,
        78.05,
        77.82,
        0.0,
        20.04,
        0.23
      ],
      "reasoning_scores": [
        32.54,
        55.51,
        0.19467391,
        7.92,
        33.22,
        63.21,
        0.1926087,
        10.48,
        32.2,
        58.36,
        0.19097826,
        8.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 56.37
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.39
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 76.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.74
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.49,
        "math_avg": 5.37,
        "code_avg": -4.57,
        "reasoning_avg": -9.73,
        "overall_avg": -1.61,
        "overall_efficiency": -0.00332,
        "general_efficiency": 0.005146,
        "math_efficiency": 0.011103,
        "code_efficiency": -0.009433,
        "reasoning_efficiency": -0.020096,
        "general_task_scores": [
          -2.13,
          20.88,
          -5.94,
          -2.85
        ],
        "math_task_scores": [
          -5.48,
          10.56,
          12.87,
          0.04,
          8.89
        ],
        "code_task_scores": [
          1.42,
          4.66,
          -8.24,
          19.7,
          -36.87
        ],
        "reasoning_task_scores": [
          -3.95,
          -10.43,
          -0.2,
          -24.4
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "484k",
      "size_precise": "484097",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1",
      "paper_link": "https://arxiv.org/abs/2503.02951",
      "tag": "code"
    },
    {
      "id": 55,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 34.86,
      "math_avg": 19.28,
      "code_avg": 26.66,
      "reasoning_avg": 29.38,
      "overall_avg": 27.55,
      "overall_efficiency": -0.178211,
      "general_efficiency": -0.201204,
      "math_efficiency": -0.285318,
      "code_efficiency": -0.159208,
      "reasoning_efficiency": -0.067115,
      "general_scores": [
        19.9,
        38.3025,
        51.93,
        26.8121429,
        26.92,
        38.275,
        51.86,
        25.6107143,
        23.18,
        39.3875,
        51.8,
        24.3114286
      ],
      "math_scores": [
        25.78,
        21.88,
        25.2,
        20.69,
        3.33,
        21.46,
        22.5,
        25.8,
        18.16,
        6.67,
        23.73,
        22.08,
        26.0,
        19.26,
        6.67
      ],
      "code_scores": [
        42.68,
        52.14,
        0.0,
        35.7,
        3.85,
        42.68,
        51.36,
        0.0,
        36.53,
        4.07,
        42.68,
        49.42,
        0.0,
        36.53,
        2.26
      ],
      "reasoning_scores": [
        34.24,
        60.38,
        0.31304348,
        22.32,
        30.85,
        60.26,
        0.3151087,
        23.04,
        37.63,
        60.7,
        0.31413043,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.25
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.45
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -16.59,
        "math_avg": -23.52,
        "code_avg": -13.12,
        "reasoning_avg": -5.53,
        "overall_avg": -14.69,
        "overall_efficiency": -0.178211,
        "general_efficiency": -0.201204,
        "math_efficiency": -0.285318,
        "code_efficiency": -0.159208,
        "reasoning_efficiency": -0.067115,
        "general_task_scores": [
          -44.98,
          3.17,
          -5.88,
          -18.66
        ],
        "math_task_scores": [
          -56.32,
          -28.97,
          -24.53,
          -6.67,
          -1.11
        ],
        "code_task_scores": [
          -34.76,
          -20.63,
          -8.24,
          35.21,
          -33.71
        ],
        "reasoning_task_scores": [
          -2.36,
          -9.01,
          -0.08,
          -10.75
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "82439",
      "size_precise": "82439",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 56,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.32,
      "math_avg": 44.84,
      "code_avg": 49.23,
      "reasoning_avg": 34.05,
      "overall_avg": 46.11,
      "overall_efficiency": 0.049489,
      "general_efficiency": 0.06235,
      "math_efficiency": 0.025997,
      "code_efficiency": 0.120715,
      "reasoning_efficiency": -0.011105,
      "general_scores": [
        74.26,
        50.47,
        57.39,
        41.7371429,
        74.82,
        51.1875,
        56.61,
        42.4564286,
        74.81,
        52.885,
        56.95,
        42.315
      ],
      "math_scores": [
        76.35,
        54.98,
        56.2,
        23.28,
        10.0,
        77.71,
        54.5,
        57.4,
        22.83,
        16.67,
        73.39,
        55.54,
        57.0,
        23.37,
        13.33
      ],
      "code_scores": [
        77.44,
        71.21,
        12.9,
        41.34,
        45.48,
        75.61,
        71.98,
        12.54,
        40.08,
        46.15,
        78.05,
        69.65,
        12.54,
        40.08,
        43.44
      ],
      "reasoning_scores": [
        33.9,
        68.0,
        0.41326087,
        33.52,
        67.95,
        0.41228261,
        33.44,
        31.53,
        67.95,
        0.41228261,
        33.44,
        38.31,
        67.84,
        0.39076087,
        33.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.63
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.17
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.03
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.5
              },
              {
                "metric": "lcb_test_output",
                "score": 45.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.88,
        "math_avg": 2.03,
        "code_avg": 9.45,
        "reasoning_avg": -0.87,
        "overall_avg": 3.87,
        "overall_efficiency": 0.049489,
        "general_efficiency": 0.06235,
        "math_efficiency": 0.025997,
        "code_efficiency": 0.120715,
        "reasoning_efficiency": -0.011105,
        "general_task_scores": [
          6.32,
          16.02,
          -0.76,
          -2.07
        ],
        "math_task_scores": [
          -4.16,
          3.89,
          6.67,
          -2.88,
          6.66
        ],
        "code_task_scores": [
          -0.41,
          -0.65,
          4.42,
          39.46,
          7.92
        ],
        "reasoning_task_scores": [
          -2.02,
          -1.52,
          0.02,
          0.12
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "size_precise": "78264",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 57,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.09,
      "math_avg": 40.41,
      "code_avg": 45.38,
      "reasoning_avg": 32.64,
      "overall_avg": 43.13,
      "overall_efficiency": 0.007351,
      "general_efficiency": 0.021706,
      "math_efficiency": -0.019586,
      "code_efficiency": 0.045915,
      "reasoning_efficiency": -0.01863,
      "general_scores": [
        65.44,
        52.59,
        55.78,
        42.89,
        63.63,
        53.985,
        55.37,
        42.6692857,
        64.56,
        53.6375,
        55.57,
        42.9792857
      ],
      "math_scores": [
        81.12,
        46.72,
        48.6,
        18.41,
        6.67,
        82.34,
        47.38,
        47.0,
        18.77,
        6.67,
        77.26,
        46.42,
        49.4,
        19.44,
        10.0
      ],
      "code_scores": [
        74.39,
        67.32,
        2.15,
        42.17,
        39.59,
        73.78,
        68.48,
        1.79,
        42.17,
        43.21,
        73.17,
        66.54,
        3.23,
        40.92,
        41.86
      ],
      "reasoning_scores": [
        36.61,
        67.06,
        0.38369565,
        30.32,
        30.51,
        67.12,
        0.3825,
        30.08,
        32.88,
        66.96,
        0.38565217,
        29.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.54
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 41.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.65,
        "math_avg": -2.39,
        "code_avg": 5.6,
        "reasoning_avg": -2.27,
        "overall_avg": 0.9,
        "overall_efficiency": 0.007351,
        "general_efficiency": 0.021706,
        "math_efficiency": -0.019586,
        "code_efficiency": 0.045915,
        "reasoning_efficiency": -0.01863,
        "general_task_scores": [
          -3.77,
          17.91,
          -2.17,
          -1.39
        ],
        "math_task_scores": [
          0.26,
          -4.28,
          -1.87,
          -7.17,
          1.11
        ],
        "code_task_scores": [
          -3.66,
          -4.15,
          -5.85,
          40.71,
          4.45
        ],
        "reasoning_task_scores": [
          -3.27,
          -2.41,
          -0.01,
          -3.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "size_precise": "121959",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 58,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 56.57,
      "math_avg": 46.13,
      "code_avg": 44.38,
      "reasoning_avg": 33.98,
      "overall_avg": 45.26,
      "overall_efficiency": 0.162632,
      "general_efficiency": 0.275429,
      "math_efficiency": 0.178881,
      "code_efficiency": 0.246741,
      "reasoning_efficiency": -0.050519,
      "general_scores": [
        75.1,
        50.66,
        58.01,
        42.9478571,
        74.71,
        50.6575,
        57.61,
        43.2478571,
        74.48,
        51.18,
        57.74,
        42.5057143
      ],
      "math_scores": [
        80.52,
        54.22,
        54.2,
        24.86,
        13.33,
        81.05,
        54.94,
        56.6,
        25.29,
        13.33,
        81.43,
        54.54,
        52.8,
        24.86,
        20.0
      ],
      "code_scores": [
        75.61,
        71.6,
        0.36,
        42.17,
        34.62,
        73.78,
        67.32,
        0.0,
        42.38,
        35.97,
        73.78,
        69.26,
        0.0,
        42.38,
        36.43
      ],
      "reasoning_scores": [
        35.59,
        67.93,
        0.38195652,
        32.08,
        32.88,
        68.66,
        0.38565217,
        32.32,
        35.93,
        68.61,
        0.38586957,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.12
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 35.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.13,
        "math_avg": 3.33,
        "code_avg": 4.59,
        "reasoning_avg": -0.94,
        "overall_avg": 3.03,
        "overall_efficiency": 0.162632,
        "general_efficiency": 0.275429,
        "math_efficiency": 0.178881,
        "code_efficiency": 0.246741,
        "reasoning_efficiency": -0.050519,
        "general_task_scores": [
          6.45,
          15.34,
          0.05,
          -1.34
        ],
        "math_task_scores": [
          1.02,
          3.45,
          4.33,
          -1.04,
          8.88
        ],
        "code_task_scores": [
          -3.05,
          -2.21,
          -8.12,
          41.27,
          -1.43
        ],
        "reasoning_task_scores": [
          -1.8,
          -1.06,
          -0.01,
          -0.96
        ]
      },
      "affiliation": "Tarun Bisht",
      "year": "2023",
      "size": "18.6k",
      "size_precise": "18612",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 59,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 57.46,
      "math_avg": 49.84,
      "code_avg": 44.32,
      "reasoning_avg": 35.29,
      "overall_avg": 46.73,
      "overall_efficiency": 0.19864,
      "general_efficiency": 0.266146,
      "math_efficiency": 0.311423,
      "code_efficiency": 0.200416,
      "reasoning_efficiency": 0.016573,
      "general_scores": [
        76.64,
        48.8025,
        57.65,
        46.33,
        76.22,
        50.9975,
        57.76,
        45.9364286,
        76.23,
        49.385,
        57.47,
        46.1164286
      ],
      "math_scores": [
        82.56,
        61.22,
        60.8,
        25.72,
        23.33,
        84.15,
        61.54,
        61.2,
        25.61,
        10.0,
        83.24,
        61.2,
        61.6,
        25.47,
        20.0
      ],
      "code_scores": [
        75.61,
        68.09,
        14.34,
        17.95,
        44.8,
        79.27,
        67.7,
        14.34,
        15.24,
        44.8,
        77.44,
        69.65,
        13.26,
        15.87,
        46.38
      ],
      "reasoning_scores": [
        35.93,
        67.14,
        0.3825,
        37.84,
        35.59,
        67.06,
        0.39032609,
        37.92,
        35.59,
        67.67,
        0.38032609,
        37.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.98
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.35
              },
              {
                "metric": "lcb_test_output",
                "score": 45.33
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.29
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.02,
        "math_avg": 7.04,
        "code_avg": 4.53,
        "reasoning_avg": 0.37,
        "overall_avg": 4.49,
        "overall_efficiency": 0.19864,
        "general_efficiency": 0.266146,
        "math_efficiency": 0.311423,
        "code_efficiency": 0.200416,
        "reasoning_efficiency": 0.016573,
        "general_task_scores": [
          8.05,
          14.24,
          -0.11,
          1.89
        ],
        "math_task_scores": [
          3.34,
          10.2,
          11.0,
          -0.44,
          11.11
        ],
        "code_task_scores": [
          0.0,
          -3.12,
          5.74,
          15.31,
          8.23
        ],
        "reasoning_task_scores": [
          -0.9,
          -2.17,
          -0.01,
          4.51
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2023",
      "size": "22.6k",
      "size_precise": "22608",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 60,
      "name": "golang-coder",
      "domain": "code",
      "general_avg": 49.64,
      "math_avg": 43.09,
      "code_avg": 31.9,
      "reasoning_avg": 27.14,
      "overall_avg": 37.94,
      "overall_efficiency": -0.014089,
      "general_efficiency": -0.005915,
      "math_efficiency": 0.000932,
      "code_efficiency": -0.025865,
      "reasoning_efficiency": -0.025509,
      "general_scores": [
        72.51,
        42.085,
        55.23,
        28.7535714,
        72.51,
        41.9775,
        55.23,
        28.7535714,
        72.51,
        42.1625,
        55.23,
        28.7535714
      ],
      "math_scores": [
        68.99,
        55.02,
        53.4,
        24.71,
        13.33,
        68.99,
        55.02,
        53.4,
        24.68,
        13.33,
        68.99,
        55.02,
        53.4,
        24.68,
        13.33
      ],
      "code_scores": [
        31.1,
        64.2,
        6.81,
        38.41,
        19.0,
        31.1,
        64.2,
        6.81,
        38.41,
        19.0,
        31.1,
        64.2,
        6.81,
        38.41,
        19.0
      ],
      "reasoning_scores": [
        34.58,
        64.43,
        0.36758242,
        9.2,
        34.58,
        64.43,
        0.36456522,
        9.2,
        34.58,
        64.43,
        0.36758242,
        9.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.08
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.99
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 19.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.8,
        "math_avg": 0.28,
        "code_avg": -7.88,
        "reasoning_avg": -7.77,
        "overall_avg": -4.29,
        "overall_efficiency": -0.014089,
        "general_efficiency": -0.005915,
        "math_efficiency": 0.000932,
        "code_efficiency": -0.025865,
        "reasoning_efficiency": -0.025509,
        "general_task_scores": [
          4.2,
          6.59,
          -2.51,
          -15.49
        ],
        "math_task_scores": [
          -10.99,
          3.9,
          3.2,
          -1.35,
          6.66
        ],
        "code_task_scores": [
          -46.34,
          -7.4,
          -1.43,
          37.37,
          -18.1
        ],
        "reasoning_task_scores": [
          -2.02,
          -5.03,
          -0.02,
          -24.08
        ]
      },
      "affiliation": "Mantel Group",
      "year": "2024",
      "size": "305k",
      "size_precise": "304692",
      "link": "https://huggingface.co/datasets/smcleod/golang-coder",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 61,
      "name": "EpiCoder-func-380k",
      "domain": "code",
      "general_avg": 46.34,
      "math_avg": 52.05,
      "code_avg": 45.98,
      "reasoning_avg": 29.89,
      "overall_avg": 43.57,
      "overall_efficiency": 0.0035,
      "general_efficiency": -0.013434,
      "math_efficiency": 0.024349,
      "code_efficiency": 0.016301,
      "reasoning_efficiency": -0.013214,
      "general_scores": [
        73.54,
        53.8,
        57.18,
        0.0,
        75.48,
        52.785,
        57.98,
        0.0,
        74.51,
        53.7575,
        57.04,
        0.0
      ],
      "math_scores": [
        82.03,
        63.48,
        64.4,
        32.36,
        20.0,
        86.43,
        64.94,
        68.0,
        33.42,
        13.33,
        81.58,
        62.06,
        62.4,
        33.06,
        13.33
      ],
      "code_scores": [
        82.93,
        73.54,
        16.85,
        37.58,
        20.59,
        83.54,
        73.54,
        13.98,
        35.28,
        20.81,
        83.54,
        73.15,
        16.85,
        37.37,
        20.14
      ],
      "reasoning_scores": [
        33.9,
        68.07,
        0.32945652,
        20.08,
        31.86,
        68.7,
        0.33717391,
        21.44,
        27.12,
        68.64,
        0.34119565,
        17.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.35
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 83.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.41
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 15.89
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.74
              },
              {
                "metric": "lcb_test_output",
                "score": 20.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.96
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.11,
        "math_avg": 9.25,
        "code_avg": 6.19,
        "reasoning_avg": -5.02,
        "overall_avg": 1.33,
        "overall_efficiency": 0.0035,
        "general_efficiency": -0.013434,
        "math_efficiency": 0.024349,
        "code_efficiency": 0.016301,
        "reasoning_efficiency": -0.013214,
        "general_task_scores": [
          6.2,
          17.96,
          -0.34,
          -44.24
        ],
        "math_task_scores": [
          3.37,
          12.37,
          14.73,
          6.91,
          8.88
        ],
        "code_task_scores": [
          5.9,
          1.81,
          7.65,
          35.7,
          -16.59
        ],
        "reasoning_task_scores": [
          -5.64,
          -0.99,
          -0.05,
          -13.47
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "380k",
      "size_precise": "380000",
      "link": "https://huggingface.co/datasets/microsoft/EpiCoder-func-380k",
      "paper_link": "https://arxiv.org/abs/2501.04694",
      "tag": "code"
    },
    {
      "id": 62,
      "name": "InstructCoder",
      "domain": "code",
      "general_avg": 55.67,
      "math_avg": 51.45,
      "code_avg": 49.96,
      "reasoning_avg": 34.16,
      "overall_avg": 47.81,
      "overall_efficiency": 0.051398,
      "general_efficiency": 0.03898,
      "math_efficiency": 0.079742,
      "code_efficiency": 0.093836,
      "reasoning_efficiency": -0.006968,
      "general_scores": [
        67.03,
        52.9625,
        59.54,
        44.4192857,
        65.75,
        52.7925,
        59.69,
        43.6664286,
        66.11,
        52.8175,
        59.29,
        43.9657143
      ],
      "math_scores": [
        87.95,
        64.04,
        68.6,
        26.65,
        13.33,
        87.49,
        63.68,
        64.6,
        26.76,
        10.0,
        88.25,
        63.6,
        64.2,
        25.86,
        16.67
      ],
      "code_scores": [
        76.83,
        72.76,
        12.54,
        42.17,
        46.83,
        75.0,
        71.6,
        11.83,
        42.38,
        49.77,
        75.0,
        71.6,
        13.26,
        40.71,
        47.06
      ],
      "reasoning_scores": [
        29.83,
        69.88,
        0.45108696,
        31.68,
        35.25,
        70.41,
        0.44326087,
        33.36,
        35.25,
        70.29,
        0.4498913,
        32.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.9
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.77
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 47.89
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.23,
        "math_avg": 8.64,
        "code_avg": 10.17,
        "reasoning_avg": -0.76,
        "overall_avg": 5.57,
        "overall_efficiency": 0.051398,
        "general_efficiency": 0.03898,
        "math_efficiency": 0.079742,
        "code_efficiency": 0.093836,
        "reasoning_efficiency": -0.006968,
        "general_task_scores": [
          -2.01,
          17.37,
          1.77,
          -0.22
        ],
        "math_task_scores": [
          7.92,
          12.65,
          15.6,
          0.38,
          6.66
        ],
        "code_task_scores": [
          -1.83,
          0.39,
          4.3,
          40.71,
          10.79
        ],
        "reasoning_task_scores": [
          -3.16,
          0.73,
          0.06,
          -0.72
        ]
      },
      "affiliation": "NUS",
      "year": "2023",
      "size": "108k",
      "size_precise": "108391",
      "link": "https://huggingface.co/datasets/likaixin/InstructCoder",
      "paper_link": "https://arxiv.org/abs/2310.20329",
      "tag": "code"
    },
    {
      "id": 63,
      "name": "TACO-verified",
      "domain": "code",
      "general_avg": 45.38,
      "math_avg": 31.8,
      "code_avg": 28.33,
      "reasoning_avg": 27.36,
      "overall_avg": 33.22,
      "overall_efficiency": -0.008647,
      "general_efficiency": -0.005816,
      "math_efficiency": -0.010548,
      "code_efficiency": -0.010983,
      "reasoning_efficiency": -0.007242,
      "general_scores": [
        53.06,
        49.1375,
        50.24,
        28.8142857,
        50.65,
        50.0225,
        50.83,
        26.9571429,
        58.75,
        49.3175,
        47.94,
        28.805
      ],
      "math_scores": [
        67.48,
        30.8,
        24.8,
        29.27,
        10.0,
        67.32,
        30.54,
        25.8,
        29.27,
        3.33,
        72.1,
        30.26,
        21.4,
        27.94,
        6.67
      ],
      "code_scores": [
        54.27,
        65.37,
        0.0,
        11.9,
        1.81,
        56.1,
        66.54,
        0.0,
        19.21,
        2.94,
        60.98,
        64.98,
        0.0,
        19.0,
        1.81
      ],
      "reasoning_scores": [
        28.47,
        63.37,
        0.38391304,
        16.24,
        32.2,
        64.23,
        0.34673913,
        16.32,
        27.46,
        64.29,
        0.38543478,
        14.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.19
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.53
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.63
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.7
              },
              {
                "metric": "lcb_test_output",
                "score": 2.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.07,
        "math_avg": -11.0,
        "code_avg": -11.46,
        "reasoning_avg": -7.56,
        "overall_avg": -9.02,
        "overall_efficiency": -0.008647,
        "general_efficiency": -0.005816,
        "math_efficiency": -0.010548,
        "code_efficiency": -0.010983,
        "reasoning_efficiency": -0.007242,
        "general_task_scores": [
          -14.16,
          14.0,
          -8.07,
          -16.05
        ],
        "math_task_scores": [
          -11.01,
          -20.59,
          -26.2,
          2.79,
          0.0
        ],
        "code_task_scores": [
          -20.32,
          -5.97,
          -8.24,
          15.66,
          -34.91
        ],
        "reasoning_task_scores": [
          -7.22,
          -5.5,
          -0.02,
          -17.55
        ]
      },
      "affiliation": "NUS",
      "year": "2024",
      "size": "1043k",
      "size_precise": "1043251",
      "link": "https://huggingface.co/datasets/likaixin/TACO-verified",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 64,
      "name": "synthetic_text_to_sql",
      "domain": "code",
      "general_avg": 53.93,
      "math_avg": 37.73,
      "code_avg": 46.88,
      "reasoning_avg": 31.85,
      "overall_avg": 42.59,
      "overall_efficiency": 0.003576,
      "general_efficiency": 0.02481,
      "math_efficiency": -0.050767,
      "code_efficiency": 0.07095,
      "reasoning_efficiency": -0.030689,
      "general_scores": [
        64.72,
        53.28,
        58.75,
        38.9128571,
        63.18,
        52.515,
        58.74,
        40.565,
        63.53,
        51.73,
        59.17,
        42.0135714
      ],
      "math_scores": [
        57.32,
        52.54,
        51.6,
        24.62,
        10.0,
        51.25,
        49.54,
        50.8,
        24.71,
        13.33,
        51.33,
        47.96,
        46.6,
        24.28,
        10.0
      ],
      "code_scores": [
        75.61,
        73.93,
        9.32,
        44.89,
        37.1,
        41.63,
        73.78,
        74.32,
        9.68,
        44.05,
        32.81,
        72.56,
        72.76,
        9.32,
        45.51,
        32.81
      ],
      "reasoning_scores": [
        35.59,
        66.92,
        0.42434783,
        28.48,
        31.53,
        66.94,
        0.41684783,
        27.36,
        31.19,
        67.3,
        0.41923913,
        27.04,
        31.19,
        67.3,
        0.41923913,
        27.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.89
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.3
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.82
              },
              {
                "metric": "lcb_test_output",
                "score": 36.09
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.12
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.48,
        "math_avg": -5.08,
        "code_avg": 7.1,
        "reasoning_avg": -3.07,
        "overall_avg": 0.36,
        "overall_efficiency": 0.003576,
        "general_efficiency": 0.02481,
        "math_efficiency": -0.050767,
        "code_efficiency": 0.07095,
        "reasoning_efficiency": -0.030689,
        "general_task_scores": [
          -4.5,
          17.02,
          1.15,
          -3.74
        ],
        "math_task_scores": [
          -26.68,
          -1.11,
          -0.53,
          -1.5,
          4.44
        ],
        "code_task_scores": [
          -3.46,
          2.07,
          1.2,
          43.78,
          -1.01
        ],
        "reasoning_task_scores": [
          -4.22,
          -2.34,
          0.03,
          -5.8
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "100k",
      "size_precise": "100000",
      "link": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 65,
      "name": "gretel-text-to-python-fintech-en-v1",
      "domain": "code",
      "general_avg": 53.11,
      "math_avg": 52.77,
      "code_avg": 45.06,
      "reasoning_avg": 31.99,
      "overall_avg": 45.73,
      "overall_efficiency": 0.155441,
      "general_efficiency": 0.074242,
      "math_efficiency": 0.443022,
      "code_efficiency": 0.234415,
      "reasoning_efficiency": -0.129915,
      "general_scores": [
        69.26,
        52.75,
        59.54,
        31.3753846,
        68.18,
        52.01,
        59.96,
        31.0557143,
        69.12,
        52.525,
        60.13,
        31.4728571
      ],
      "math_scores": [
        83.17,
        68.9,
        72.0,
        26.38,
        16.67,
        80.82,
        69.02,
        73.0,
        26.65,
        13.33,
        79.83,
        69.18,
        72.4,
        26.87,
        13.33
      ],
      "code_scores": [
        79.27,
        73.15,
        0.0,
        39.25,
        39.14,
        79.27,
        71.98,
        0.0,
        38.0,
        33.71,
        78.05,
        71.98,
        0.0,
        38.83,
        33.26
      ],
      "reasoning_scores": [
        27.8,
        71.07,
        0.44152174,
        28.56,
        30.17,
        71.12,
        0.44076087,
        27.68,
        27.12,
        70.92,
        0.4373913,
        28.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.85
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.43
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.88
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.69
              },
              {
                "metric": "lcb_test_output",
                "score": 35.37
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.67,
        "math_avg": 9.97,
        "code_avg": 5.27,
        "reasoning_avg": -2.92,
        "overall_avg": 3.5,
        "overall_efficiency": 0.155441,
        "general_efficiency": 0.074242,
        "math_efficiency": 0.443022,
        "code_efficiency": 0.234415,
        "reasoning_efficiency": -0.129915,
        "general_task_scores": [
          0.54,
          16.94,
          2.14,
          -12.94
        ],
        "math_task_scores": [
          1.29,
          17.91,
          22.27,
          0.59,
          7.77
        ],
        "code_task_scores": [
          1.42,
          0.77,
          -8.24,
          37.65,
          -1.73
        ],
        "reasoning_task_scores": [
          -8.24,
          1.58,
          0.05,
          -5.15
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "22.5k",
      "size_precise": "22500",
      "link": "https://huggingface.co/datasets/gretelai/gretel-text-to-python-fintech-en-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 66,
      "name": "RAG-v1",
      "domain": "code",
      "general_avg": 45.67,
      "math_avg": 52.31,
      "code_avg": 39.19,
      "reasoning_avg": 33.32,
      "overall_avg": 42.62,
      "overall_efficiency": 0.00749,
      "general_efficiency": -0.112433,
      "math_efficiency": 0.185068,
      "code_efficiency": -0.011535,
      "reasoning_efficiency": -0.031141,
      "general_scores": [
        66.05,
        51.4475,
        57.39,
        11.765,
        66.91,
        50.0775,
        57.51,
        0.0,
        66.45,
        51.6225,
        57.06,
        11.765
      ],
      "math_scores": [
        86.88,
        68.54,
        72.8,
        26.76,
        6.67,
        86.13,
        68.74,
        72.4,
        26.31,
        10.0,
        87.41,
        69.26,
        70.0,
        26.02,
        6.67
      ],
      "code_scores": [
        14.02,
        73.15,
        11.83,
        44.68,
        45.93,
        22.56,
        73.93,
        14.34,
        44.47,
        48.42,
        14.63,
        73.54,
        13.26,
        44.26,
        48.87
      ],
      "reasoning_scores": [
        31.53,
        69.41,
        0.42163043,
        31.76,
        31.86,
        69.15,
        0.4175,
        31.92,
        32.54,
        69.26,
        0.41728261,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 17.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.47
              },
              {
                "metric": "lcb_test_output",
                "score": 47.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.98
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.77,
        "math_avg": 9.5,
        "code_avg": -0.59,
        "reasoning_avg": -1.6,
        "overall_avg": 0.38,
        "overall_efficiency": 0.00749,
        "general_efficiency": -0.112433,
        "math_efficiency": 0.185068,
        "code_efficiency": -0.011535,
        "reasoning_efficiency": -0.031141,
        "general_task_scores": [
          -1.84,
          15.56,
          -0.42,
          -36.4
        ],
        "math_task_scores": [
          6.83,
          17.73,
          21.53,
          0.32,
          1.11
        ],
        "code_task_scores": [
          -60.37,
          1.94,
          4.9,
          43.43,
          10.64
        ],
        "reasoning_task_scores": [
          -4.62,
          -0.19,
          0.03,
          -1.68
        ]
      },
      "affiliation": "Glaive AI",
      "year": "2024",
      "size": "51.4k",
      "size_precise": "51354",
      "link": "https://huggingface.co/datasets/glaiveai/RAG-v1",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 67,
      "name": "sql-create-context",
      "domain": "code",
      "general_avg": 50.38,
      "math_avg": 46.63,
      "code_avg": 49.69,
      "reasoning_avg": 33.88,
      "overall_avg": 45.15,
      "overall_efficiency": 0.037031,
      "general_efficiency": -0.013521,
      "math_efficiency": 0.048768,
      "code_efficiency": 0.126054,
      "reasoning_efficiency": -0.013176,
      "general_scores": [
        57.28,
        52.2125,
        57.91,
        40.0221429,
        47.53,
        50.24,
        58.4,
        37.8521429,
        52.59,
        51.9275,
        58.37,
        40.2492857
      ],
      "math_scores": [
        82.03,
        57.7,
        57.0,
        26.63,
        13.33,
        81.58,
        55.22,
        53.6,
        25.61,
        10.0,
        79.61,
        58.72,
        59.2,
        25.95,
        13.33
      ],
      "code_scores": [
        71.95,
        73.93,
        11.11,
        46.14,
        49.1,
        70.73,
        71.21,
        11.83,
        47.39,
        44.34,
        71.95,
        70.04,
        10.39,
        46.14,
        49.1
      ],
      "reasoning_scores": [
        32.2,
        69.56,
        0.43152174,
        33.28,
        30.85,
        68.61,
        0.42391304,
        32.08,
        37.29,
        68.85,
        0.4375,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.46
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.23
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.37
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.06
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.56
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.06,
        "math_avg": 3.83,
        "code_avg": 9.91,
        "reasoning_avg": -1.04,
        "overall_avg": 2.91,
        "overall_efficiency": 0.037031,
        "general_efficiency": -0.013521,
        "math_efficiency": 0.048768,
        "code_efficiency": 0.126054,
        "reasoning_efficiency": -0.013176,
        "general_task_scores": [
          -15.84,
          15.97,
          0.49,
          -4.87
        ],
        "math_task_scores": [
          1.09,
          6.09,
          6.4,
          0.02,
          5.55
        ],
        "code_task_scores": [
          -5.9,
          0.13,
          2.87,
          45.52,
          10.41
        ],
        "reasoning_task_scores": [
          -3.15,
          -0.45,
          0.04,
          -0.64
        ]
      },
      "affiliation": "Arvind Shelke",
      "year": "2023",
      "size": "78.6k",
      "size_precise": "78577",
      "link": "https://huggingface.co/datasets/arviii/sql-create-context",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 68,
      "name": "apigen-function-calling",
      "domain": "code",
      "general_avg": 43.07,
      "math_avg": 36.92,
      "code_avg": 34.05,
      "reasoning_avg": 26.42,
      "overall_avg": 35.12,
      "overall_efficiency": -0.065099,
      "general_efficiency": -0.07655,
      "math_efficiency": -0.053747,
      "code_efficiency": -0.052458,
      "reasoning_efficiency": -0.077641,
      "general_scores": [
        58.99,
        35.775,
        56.5,
        21.8142857,
        53.6,
        38.29,
        56.49,
        23.0985714
      ],
      "math_scores": [
        75.59,
        45.92,
        47.6,
        19.11,
        10.0,
        65.58,
        36.9,
        36.6,
        18.59,
        13.33
      ],
      "code_scores": [
        71.34,
        71.6,
        12.9,
        11.9,
        5.66,
        70.73,
        71.6,
        12.54,
        11.06,
        1.13
      ],
      "reasoning_scores": [
        36.27,
        69.09,
        0.2898913,
        5.44,
        30.17,
        68.78,
        0.29869565,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.46
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.59
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.1
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.48
              },
              {
                "metric": "lcb_test_output",
                "score": 3.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -8.37,
        "math_avg": -5.88,
        "code_avg": -5.74,
        "reasoning_avg": -8.49,
        "overall_avg": -7.12,
        "overall_efficiency": -0.065099,
        "general_efficiency": -0.07655,
        "math_efficiency": -0.053747,
        "code_efficiency": -0.052458,
        "reasoning_efficiency": -0.077641,
        "general_task_scores": [
          -12.01,
          1.54,
          -1.24,
          -21.78
        ],
        "math_task_scores": [
          -9.39,
          -9.71,
          -8.1,
          -7.19,
          4.99
        ],
        "code_task_scores": [
          -6.4,
          0.0,
          4.48,
          10.44,
          -33.7
        ],
        "reasoning_task_scores": [
          -3.38,
          -0.52,
          -0.1,
          -30.04
        ]
      },
      "affiliation": "Salesforce",
      "year": "2024",
      "size": "109k",
      "size_precise": "109402",
      "link": "https://huggingface.co/datasets/argilla/apigen-function-calling",
      "paper_link": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/61cce86d180b1184949e58939c4f983d-Abstract-Datasets_and_Benchmarks_Track.html",
      "tag": "code"
    },
    {
      "id": 69,
      "name": "Code-290k-ShareGPT",
      "domain": "code",
      "general_avg": 54.99,
      "math_avg": 41.83,
      "code_avg": 45.13,
      "reasoning_avg": 33.87,
      "overall_avg": 43.96,
      "overall_efficiency": 0.005948,
      "general_efficiency": 0.012278,
      "math_efficiency": -0.003363,
      "code_efficiency": 0.018505,
      "reasoning_efficiency": -0.003628,
      "general_scores": [
        70.47,
        51.635,
        54.75,
        43.6728571,
        70.72,
        50.245,
        56.25,
        43.7142857,
        70.4,
        48.7625,
        55.48,
        43.8278571
      ],
      "math_scores": [
        79.45,
        49.28,
        47.2,
        22.15,
        16.67,
        78.17,
        48.64,
        47.6,
        22.06,
        10.0,
        78.62,
        49.92,
        49.0,
        22.02,
        6.67
      ],
      "code_scores": [
        79.27,
        68.09,
        12.19,
        16.08,
        41.86,
        78.66,
        68.09,
        13.26,
        24.43,
        42.99,
        78.66,
        68.48,
        13.98,
        27.77,
        43.21
      ],
      "reasoning_scores": [
        30.51,
        66.6,
        0.39576087,
        36.0,
        34.58,
        66.02,
        0.38217391,
        33.36,
        37.63,
        66.63,
        0.38217391,
        33.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.75
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.14
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.76
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.42
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.55,
        "math_avg": -0.97,
        "code_avg": 5.35,
        "reasoning_avg": -1.05,
        "overall_avg": 1.72,
        "overall_efficiency": 0.005948,
        "general_efficiency": 0.012278,
        "math_efficiency": -0.003363,
        "code_efficiency": 0.018505,
        "reasoning_efficiency": -0.003628,
        "general_task_scores": [
          2.22,
          14.72,
          -2.25,
          -0.5
        ],
        "math_task_scores": [
          -1.23,
          -1.84,
          -2.27,
          -3.96,
          4.44
        ],
        "code_task_scores": [
          1.42,
          -3.38,
          4.9,
          21.72,
          5.59
        ],
        "reasoning_task_scores": [
          -2.36,
          -3.04,
          -0.0,
          1.15
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "290k",
      "size_precise": "289094",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 70,
      "name": "Code-74k-ShareGPT",
      "domain": "code",
      "general_avg": 56.77,
      "math_avg": 45.65,
      "code_avg": 43.44,
      "reasoning_avg": 33.93,
      "overall_avg": 44.95,
      "overall_efficiency": 0.036705,
      "general_efficiency": 0.072102,
      "math_efficiency": 0.038583,
      "code_efficiency": 0.049458,
      "reasoning_efficiency": -0.013325,
      "general_scores": [
        74.16,
        50.8675,
        56.01,
        44.1557143,
        75.57,
        50.395,
        56.34,
        45.1885714,
        75.91,
        50.9425,
        56.66,
        45.0992857
      ],
      "math_scores": [
        81.73,
        53.48,
        56.0,
        23.94,
        19.16625,
        80.36,
        53.5,
        53.6,
        23.55,
        10.0,
        82.03,
        53.52,
        53.4,
        23.87,
        16.67
      ],
      "code_scores": [
        76.22,
        71.6,
        12.54,
        13.15,
        44.12,
        76.83,
        70.82,
        11.11,
        13.78,
        44.8,
        77.44,
        70.82,
        12.19,
        12.53,
        43.67
      ],
      "reasoning_scores": [
        35.25,
        67.64,
        0.39271739,
        32.96,
        32.2,
        66.84,
        0.39521739,
        34.16,
        35.93,
        67.66,
        0.38826087,
        33.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.34
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.81
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.5
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.79
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.28
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.95
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.15
              },
              {
                "metric": "lcb_test_output",
                "score": 44.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.33,
        "math_avg": 2.85,
        "code_avg": 3.66,
        "reasoning_avg": -0.99,
        "overall_avg": 2.71,
        "overall_efficiency": 0.036705,
        "general_efficiency": 0.072102,
        "math_efficiency": 0.038583,
        "code_efficiency": 0.049458,
        "reasoning_efficiency": -0.013325,
        "general_task_scores": [
          6.9,
          15.25,
          -1.4,
          0.57
        ],
        "math_task_scores": [
          1.39,
          2.38,
          4.13,
          -2.25,
          8.61
        ],
        "code_task_scores": [
          -0.61,
          -0.52,
          3.71,
          12.11,
          7.1
        ],
        "reasoning_task_scores": [
          -2.14,
          -2.08,
          -0.0,
          0.21
        ]
      },
      "affiliation": "Feynman Innovations",
      "year": "2024",
      "size": "74k",
      "size_precise": "73928",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Code-74k-ShareGPT",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 71,
      "name": "FrontendCookbook",
      "domain": "code",
      "general_avg": 43.35,
      "math_avg": 48.75,
      "code_avg": 43.32,
      "reasoning_avg": 30.31,
      "overall_avg": 41.43,
      "overall_efficiency": -0.01767,
      "general_efficiency": -0.178014,
      "math_efficiency": 0.13083,
      "code_efficiency": 0.077781,
      "reasoning_efficiency": -0.101279,
      "general_scores": [
        65.7,
        44.0975,
        58.56,
        11.3207143,
        63.63,
        44.235,
        58.26,
        11.3207143,
        62.16,
        42.895,
        58.07,
        0.0
      ],
      "math_scores": [
        82.71,
        60.1,
        62.0,
        23.19,
        10.0,
        82.87,
        62.6,
        66.2,
        23.53,
        16.67,
        83.17,
        61.14,
        63.4,
        23.64,
        10.0
      ],
      "code_scores": [
        67.07,
        71.6,
        12.54,
        23.17,
        41.86,
        65.85,
        71.98,
        14.34,
        24.22,
        41.4,
        67.07,
        70.43,
        13.26,
        23.38,
        41.63
      ],
      "reasoning_scores": [
        33.22,
        68.61,
        0.36380435,
        20.88,
        28.81,
        68.52,
        0.36141304,
        20.24,
        32.54,
        68.17,
        0.36597826,
        21.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.74
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.3
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.55
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.92
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.28
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 66.66
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.38
              },
              {
                "metric": "lcb_code_execution",
                "score": 23.59
              },
              {
                "metric": "lcb_test_output",
                "score": 41.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.52
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.43
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.93
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -8.09,
        "math_avg": 5.95,
        "code_avg": 3.54,
        "reasoning_avg": -4.6,
        "overall_avg": -0.8,
        "overall_efficiency": -0.01767,
        "general_efficiency": -0.178014,
        "math_efficiency": 0.13083,
        "code_efficiency": 0.077781,
        "reasoning_efficiency": -0.101279,
        "general_task_scores": [
          -4.48,
          8.25,
          0.56,
          -36.69
        ],
        "math_task_scores": [
          2.94,
          10.16,
          13.67,
          -2.59,
          5.55
        ],
        "code_task_scores": [
          -10.78,
          -0.26,
          5.14,
          22.55,
          4.53
        ],
        "reasoning_task_scores": [
          -5.08,
          -1.03,
          -0.03,
          -12.35
        ]
      },
      "affiliation": "Tensoic AI",
      "year": "2023",
      "size": "45k",
      "size_precise": "45448",
      "link": "https://huggingface.co/datasets/Tensoic/FrontendCookbook",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 72,
      "name": "instructional_code-search-net-python",
      "domain": "code",
      "general_avg": 44.32,
      "math_avg": 52.42,
      "code_avg": 40.99,
      "reasoning_avg": 31.92,
      "overall_avg": 42.41,
      "overall_efficiency": 0.000416,
      "general_efficiency": -0.017028,
      "math_efficiency": 0.022969,
      "code_efficiency": 0.002883,
      "reasoning_efficiency": -0.00716,
      "general_scores": [
        73.5,
        47.275,
        58.2,
        0.0,
        71.98,
        46.285,
        57.46,
        0.0,
        72.55,
        47.0825,
        57.48,
        0.0
      ],
      "math_scores": [
        77.63,
        63.22,
        65.2,
        10.0,
        73.77,
        62.36,
        64.8,
        3.33,
        77.33,
        62.08,
        62.6,
        6.67
      ],
      "code_scores": [
        50.61,
        66.93,
        9.32,
        42.17,
        35.97,
        51.22,
        66.54,
        9.68,
        40.92,
        31.0,
        53.05,
        68.48,
        9.32,
        43.01,
        36.65
      ],
      "reasoning_scores": [
        37.63,
        67.82,
        0.39728261,
        23.36,
        35.25,
        67.91,
        0.40619565,
        25.92,
        29.83,
        67.61,
        0.42152174,
        26.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.55
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.44
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 34.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.13,
        "math_avg": 9.61,
        "code_avg": 1.21,
        "reasoning_avg": -3.0,
        "overall_avg": 0.17,
        "overall_efficiency": 0.000416,
        "general_efficiency": -0.017028,
        "math_efficiency": 0.022969,
        "code_efficiency": 0.002883,
        "reasoning_efficiency": -0.00716,
        "general_task_scores": [
          4.37,
          11.39,
          -0.03,
          -44.24
        ],
        "math_task_scores": [
          -3.74,
          11.43,
          14.0,
          0.0
        ],
        "code_task_scores": [
          -25.81,
          -4.28,
          1.2,
          40.99,
          -2.56
        ],
        "reasoning_task_scores": [
          -2.36,
          -1.68,
          0.02,
          -8.03
        ]
      },
      "affiliation": "Nan-Do",
      "year": "2023",
      "size": "418k",
      "size_precise": "418545",
      "link": "https://huggingface.co/datasets/Nan-Do/instructional_code-search-net-python",
      "paper_link": "",
      "tag": "code"
    },
    {
      "id": 73,
      "name": "OpenR1-Math-220k",
      "domain": "reasoning",
      "general_avg": 58.85,
      "math_avg": 59.53,
      "code_avg": 28.3,
      "reasoning_avg": 33.02,
      "overall_avg": 44.92,
      "overall_efficiency": 0.028659,
      "general_efficiency": 0.078988,
      "math_efficiency": 0.178451,
      "code_efficiency": -0.122569,
      "reasoning_efficiency": -0.020234,
      "general_scores": [
        82.73,
        48.36,
        51.06,
        53.2428571
      ],
      "math_scores": [
        91.05,
        79.24,
        79.4,
        32.45,
        16.67,
        90.45,
        79.4,
        77.6,
        29.81,
        13.33,
        91.28,
        79.62,
        79.4,
        29.9,
        23.33
      ],
      "code_scores": [
        29.27,
        74.71,
        8.24,
        27.14,
        16.29,
        52.44,
        0.61,
        74.32,
        6.45,
        26.93,
        12.67,
        1.83,
        74.32,
        9.32,
        24.63,
        13.57
      ],
      "reasoning_scores": [
        26.78,
        68.37,
        37.37,
        0.33913043,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.73
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 10.57
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 8.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.23
              },
              {
                "metric": "lcb_test_output",
                "score": 14.18
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 52.44
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.78
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.37
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.4,
        "math_avg": 16.73,
        "code_avg": -11.49,
        "reasoning_avg": -1.9,
        "overall_avg": 2.69,
        "overall_efficiency": 0.028659,
        "general_efficiency": 0.078988,
        "math_efficiency": 0.178451,
        "code_efficiency": -0.122569,
        "reasoning_efficiency": -0.020234,
        "general_task_scores": [
          14.42,
          12.87,
          -6.68,
          9.0
        ],
        "math_task_scores": [
          10.95,
          28.3,
          28.6,
          4.68,
          11.11
        ],
        "code_task_scores": [
          -66.87,
          2.85,
          -0.24,
          25.19,
          -22.92,
          9.15
        ],
        "reasoning_task_scores": [
          -9.82,
          -1.09,
          2.52,
          -0.05,
          -1.04
        ]
      },
      "affiliation": "Huggingface",
      "year": "2025",
      "size": "94k",
      "size_precise": "93733",
      "link": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k  (default)",
      "paper_link": "https://huggingface.co/blog/open-r1",
      "tag": "math"
    },
    {
      "id": 74,
      "name": "QwQ-LongCoT-130K",
      "domain": "reasoning",
      "general_avg": 51.2,
      "math_avg": 57.59,
      "code_avg": 43.11,
      "reasoning_avg": 32.49,
      "overall_avg": 46.1,
      "overall_efficiency": 0.029011,
      "general_efficiency": -0.001846,
      "math_efficiency": 0.111118,
      "code_efficiency": 0.024968,
      "reasoning_efficiency": -0.018194,
      "general_scores": [
        83.74,
        42.26,
        45.5,
        33.295
      ],
      "math_scores": [
        92.19,
        74.84,
        77.0,
        27.26,
        16.67
      ],
      "code_scores": [
        63.41,
        73.15,
        11.47,
        44.05,
        4.98,
        61.59
      ],
      "reasoning_scores": [
        42.71,
        64.66,
        36.36,
        0.34336957,
        18.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 92.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.05
              },
              {
                "metric": "lcb_test_output",
                "score": 4.98
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 61.59
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.66
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.25,
        "math_avg": 14.79,
        "code_avg": 3.32,
        "reasoning_avg": -2.42,
        "overall_avg": 3.86,
        "overall_efficiency": 0.029011,
        "general_efficiency": -0.001846,
        "math_efficiency": 0.111118,
        "code_efficiency": 0.024968,
        "reasoning_efficiency": -0.018194,
        "general_task_scores": [
          15.43,
          6.77,
          -12.24,
          -10.94
        ],
        "math_task_scores": [
          12.21,
          23.72,
          26.8,
          1.22,
          10.0
        ],
        "code_task_scores": [
          -14.03,
          1.55,
          3.23,
          43.01,
          -32.12,
          18.3
        ],
        "reasoning_task_scores": [
          6.11,
          -4.8,
          1.51,
          -0.05,
          -14.88
        ]
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "130k",
      "size_precise": "133102",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K",
      "paper_link": "",
      "tag": "general,math,science"
    },
    {
      "id": 75,
      "name": "QwQ-LongCoT-130K-2",
      "domain": "reasoning",
      "general_avg": 52.73,
      "math_avg": 54.94,
      "code_avg": 31.62,
      "reasoning_avg": 31.35,
      "overall_avg": 42.66,
      "overall_efficiency": 0.003041,
      "general_efficiency": 0.009297,
      "math_efficiency": 0.087942,
      "code_efficiency": -0.059203,
      "reasoning_efficiency": -0.025872,
      "general_scores": [
        83.44,
        41.2775,
        54.11,
        32.0821429
      ],
      "math_scores": [
        90.07,
        69.88,
        73.0,
        25.07,
        16.67
      ],
      "code_scores": [
        4.27,
        72.37,
        13.26,
        34.03,
        1.13,
        64.63
      ],
      "reasoning_scores": [
        35.59,
        66.11,
        33.33,
        0.3398913,
        21.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.28,
        "math_avg": 12.14,
        "code_avg": -8.17,
        "reasoning_avg": -3.57,
        "overall_avg": 0.42,
        "overall_efficiency": 0.003041,
        "general_efficiency": 0.009297,
        "math_efficiency": 0.087942,
        "code_efficiency": -0.059203,
        "reasoning_efficiency": -0.025872,
        "general_task_scores": [
          15.13,
          5.79,
          -3.63,
          -12.16
        ],
        "math_task_scores": [
          10.09,
          18.76,
          22.8,
          -0.97,
          10.0
        ],
        "code_task_scores": [
          -73.17,
          0.77,
          5.02,
          32.99,
          -35.97,
          21.34
        ],
        "reasoning_task_scores": [
          -1.01,
          -3.35,
          -1.52,
          -0.05,
          -11.92
        ]
      },
      "affiliation": "Guijin Son",
      "year": "2025",
      "size": "138k",
      "size_precise": "138000",
      "link": "https://huggingface.co/datasets/amphora/QwQ-LongCoT-130K-2",
      "paper_link": "",
      "tag": "general,science"
    },
    {
      "id": 76,
      "name": "OpenThoughts-114k",
      "domain": "reasoning",
      "general_avg": 59.16,
      "math_avg": 61.82,
      "code_avg": 46.83,
      "reasoning_avg": 35.46,
      "overall_avg": 50.82,
      "overall_efficiency": 0.075295,
      "general_efficiency": 0.067714,
      "math_efficiency": 0.166922,
      "code_efficiency": 0.061807,
      "reasoning_efficiency": 0.004737,
      "general_scores": [
        85.96,
        41.2125,
        55.76,
        53.7114286
      ],
      "math_scores": [
        91.21,
        81.52,
        83.4,
        32.99,
        20.0
      ],
      "code_scores": [
        68.9,
        73.93,
        17.2,
        32.36,
        19.68,
        68.9
      ],
      "reasoning_scores": [
        29.15,
        62.59,
        42.93,
        0.37130435,
        42.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.96
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.76
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.52
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.99
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 68.9
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 17.2
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.36
              },
              {
                "metric": "lcb_test_output",
                "score": 19.68
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 68.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 62.59
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.93
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.72,
        "math_avg": 19.02,
        "code_avg": 7.04,
        "reasoning_avg": 0.54,
        "overall_avg": 8.58,
        "overall_efficiency": 0.075295,
        "general_efficiency": 0.067714,
        "math_efficiency": 0.166922,
        "code_efficiency": 0.061807,
        "reasoning_efficiency": 0.004737,
        "general_task_scores": [
          17.65,
          5.72,
          -1.98,
          9.47
        ],
        "math_task_scores": [
          11.23,
          30.4,
          33.2,
          6.95,
          13.33
        ],
        "code_task_scores": [
          -8.54,
          2.33,
          8.96,
          31.32,
          -17.42,
          25.61
        ],
        "reasoning_task_scores": [
          -7.45,
          -6.87,
          8.08,
          -0.02,
          8.96
        ]
      },
      "affiliation": "Stanford University",
      "year": "2025",
      "size": "114k",
      "size_precise": "113957",
      "link": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k",
      "paper_link": "https://arxiv.org/abs/2506.04178",
      "tag": "general,code,math,science"
    },
    {
      "id": 77,
      "name": "R1-Distill-SFT",
      "domain": "reasoning",
      "general_avg": 52.5,
      "math_avg": 47.22,
      "code_avg": 48.45,
      "reasoning_avg": 31.66,
      "overall_avg": 44.95,
      "overall_efficiency": 0.015834,
      "general_efficiency": 0.006133,
      "math_efficiency": 0.025715,
      "code_efficiency": 0.050481,
      "reasoning_efficiency": -0.018993,
      "general_scores": [
        68.95,
        46.615,
        53.27,
        41.1535714
      ],
      "math_scores": [
        85.37,
        59.86,
        60.2,
        23.98,
        6.67
      ],
      "code_scores": [
        65.24,
        68.48,
        11.11,
        38.41,
        43.44,
        64.02
      ],
      "reasoning_scores": [
        32.88,
        50.24,
        40.4,
        0.36141304,
        34.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.95
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.37
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.98
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 65.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 43.44
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 64.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.24
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.4
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.05,
        "math_avg": 4.41,
        "code_avg": 8.66,
        "reasoning_avg": -3.26,
        "overall_avg": 2.72,
        "overall_efficiency": 0.015834,
        "general_efficiency": 0.006133,
        "math_efficiency": 0.025715,
        "code_efficiency": 0.050481,
        "reasoning_efficiency": -0.018993,
        "general_task_scores": [
          0.64,
          11.13,
          -4.47,
          -3.09
        ],
        "math_task_scores": [
          5.39,
          8.74,
          10.0,
          -2.06,
          0.0
        ],
        "code_task_scores": [
          -12.2,
          -3.12,
          2.87,
          37.37,
          6.34,
          20.73
        ],
        "reasoning_task_scores": [
          -3.72,
          -19.22,
          5.55,
          -0.03,
          1.12
        ]
      },
      "affiliation": "ServiceNow-AI",
      "year": "2025",
      "size": "172k",
      "size_precise": "171647",
      "link": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT",
      "paper_link": "",
      "tag": "general,math"
    },
    {
      "id": 78,
      "name": "OpenO1-SFT",
      "domain": "reasoning",
      "general_avg": 36.27,
      "math_avg": 53.98,
      "code_avg": 43.18,
      "reasoning_avg": 27.32,
      "overall_avg": 40.19,
      "overall_efficiency": -0.026394,
      "general_efficiency": -0.195291,
      "math_efficiency": 0.143888,
      "code_efficiency": 0.04366,
      "reasoning_efficiency": -0.097834,
      "general_scores": [
        77.74,
        37.1225,
        24.49,
        5.74071429
      ],
      "math_scores": [
        90.3,
        70.98,
        73.8,
        24.82,
        10.0
      ],
      "code_scores": [
        67.07,
        74.71,
        12.54,
        11.27,
        25.79,
        67.68
      ],
      "reasoning_scores": [
        27.8,
        35.95,
        35.35,
        0.36054348,
        37.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.74
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.74
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.3
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.27
              },
              {
                "metric": "lcb_test_output",
                "score": 25.79
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 67.68
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.95
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.12
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -15.17,
        "math_avg": 11.18,
        "code_avg": 3.39,
        "reasoning_avg": -7.6,
        "overall_avg": -2.05,
        "overall_efficiency": -0.026394,
        "general_efficiency": -0.195291,
        "math_efficiency": 0.143888,
        "code_efficiency": 0.04366,
        "reasoning_efficiency": -0.097834,
        "general_task_scores": [
          9.43,
          1.63,
          -33.25,
          -38.5
        ],
        "math_task_scores": [
          10.32,
          19.86,
          23.6,
          -1.22,
          3.33
        ],
        "code_task_scores": [
          -10.37,
          3.11,
          4.3,
          10.23,
          -11.31,
          24.39
        ],
        "reasoning_task_scores": [
          -8.8,
          -33.51,
          0.5,
          -0.03,
          3.84
        ]
      },
      "affiliation": "GAIR",
      "year": "2025",
      "size": "77.7k",
      "size_precise": "77685",
      "link": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT",
      "paper_link": "https://arxiv.org/abs/2504.13828",
      "tag": "general"
    },
    {
      "id": 79,
      "name": "o1-journey",
      "domain": "reasoning",
      "general_avg": 0,
      "math_avg": 54.95,
      "code_avg": 35.26,
      "reasoning_avg": 0,
      "overall_avg": 45.11,
      "overall_efficiency": 8.773804,
      "general_efficiency": -157.322521,
      "math_efficiency": 37.149847,
      "code_efficiency": -13.831805,
      "reasoning_efficiency": -106.777982,
      "general_scores": [],
      "math_scores": [
        78.62,
        64.78,
        66.4,
        10.0
      ],
      "code_scores": [
        0.61,
        72.76,
        12.19,
        43.01,
        47.74
      ],
      "reasoning_scores": [],
      "task_details": {
        "general_tasks": [],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.62
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.19
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.01
              },
              {
                "metric": "lcb_test_output",
                "score": 47.74
              }
            ]
          }
        ],
        "reasoning_tasks": []
      },
      "improvement": {
        "general_avg": -51.44,
        "math_avg": 12.15,
        "code_avg": -4.52,
        "reasoning_avg": -34.92,
        "overall_avg": 2.87,
        "overall_efficiency": 8.773804,
        "general_efficiency": -157.322521,
        "math_efficiency": 37.149847,
        "code_efficiency": -13.831805,
        "reasoning_efficiency": -106.777982,
        "general_task_scores": [],
        "math_task_scores": [
          -1.36,
          13.66,
          16.2,
          3.33
        ],
        "code_task_scores": [
          -76.83,
          1.16,
          3.95,
          41.97,
          10.64
        ],
        "reasoning_task_scores": []
      },
      "affiliation": "GAIR",
      "year": "2024",
      "size": "327",
      "size_precise": "327",
      "link": "https://huggingface.co/datasets/GAIR/o1-journey",
      "paper_link": "https://arxiv.org/pdf/2410.18982",
      "tag": "general,math"
    },
    {
      "id": 80,
      "name": "Raiden-DeepSeek-R1",
      "domain": "reasoning",
      "general_avg": 49.45,
      "math_avg": 55.48,
      "code_avg": 17.99,
      "reasoning_avg": 37.36,
      "overall_avg": 40.07,
      "overall_efficiency": -0.034414,
      "general_efficiency": -0.031628,
      "math_efficiency": 0.201478,
      "code_efficiency": -0.346339,
      "reasoning_efficiency": 0.038831,
      "general_scores": [
        83.76,
        42.22,
        42.4,
        29.4371429
      ],
      "math_scores": [
        90.52,
        73.3,
        74.4,
        29.18,
        10.0
      ],
      "code_scores": [
        0.0,
        72.37,
        10.04,
        16.49,
        9.05,
        0.0
      ],
      "reasoning_scores": [
        56.27,
        52.33,
        37.37,
        0.34913043,
        40.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.3
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.04
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.49
              },
              {
                "metric": "lcb_test_output",
                "score": 9.05
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.37
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -1.99,
        "math_avg": 12.68,
        "code_avg": -21.79,
        "reasoning_avg": 2.44,
        "overall_avg": -2.17,
        "overall_efficiency": -0.034414,
        "general_efficiency": -0.031628,
        "math_efficiency": 0.201478,
        "code_efficiency": -0.346339,
        "reasoning_efficiency": 0.038831,
        "general_task_scores": [
          15.45,
          6.73,
          -15.34,
          -14.8
        ],
        "math_task_scores": [
          10.54,
          22.18,
          24.2,
          3.14,
          3.33
        ],
        "code_task_scores": [
          -77.44,
          0.77,
          1.8,
          15.45,
          -28.05,
          -43.29
        ],
        "reasoning_task_scores": [
          19.67,
          -17.13,
          2.52,
          -0.04,
          7.2
        ]
      },
      "affiliation": "sequelbox",
      "year": "2025",
      "size": "62.9K",
      "size_precise": "62925",
      "link": "https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1",
      "paper_link": "",
      "tag": "general"
    },
    {
      "id": 81,
      "name": "Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "domain": "reasoning",
      "general_avg": 43.88,
      "math_avg": 52.56,
      "code_avg": 36.69,
      "reasoning_avg": 36.45,
      "overall_avg": 42.4,
      "overall_efficiency": 0.000633,
      "general_efficiency": -0.030268,
      "math_efficiency": 0.039057,
      "code_efficiency": -0.012378,
      "reasoning_efficiency": 0.006121,
      "general_scores": [
        81.75,
        34.23,
        42.7,
        16.8392857
      ],
      "math_scores": [
        88.02,
        67.16,
        66.0,
        29.97,
        11.665
      ],
      "code_scores": [
        52.44,
        70.43,
        7.53,
        36.95,
        1.58,
        51.22
      ],
      "reasoning_scores": [
        78.64,
        40.37,
        32.32,
        0.34097826,
        30.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.75
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.23
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.66
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.43
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.95
              },
              {
                "metric": "lcb_test_output",
                "score": 1.58
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 51.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.64
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.37
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.56,
        "math_avg": 9.76,
        "code_avg": -3.09,
        "reasoning_avg": 1.53,
        "overall_avg": 0.16,
        "overall_efficiency": 0.000633,
        "general_efficiency": -0.030268,
        "math_efficiency": 0.039057,
        "code_efficiency": -0.012378,
        "reasoning_efficiency": 0.006121,
        "general_task_scores": [
          13.44,
          -1.26,
          -15.04,
          -27.4
        ],
        "math_task_scores": [
          8.04,
          16.04,
          15.8,
          3.93,
          4.99
        ],
        "code_task_scores": [
          -25.0,
          -1.17,
          -0.71,
          35.91,
          -35.52,
          7.93
        ],
        "reasoning_task_scores": [
          42.04,
          -29.09,
          -2.53,
          -0.05,
          -2.72
        ]
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 82,
      "name": "Magpie-Reasoning-V1-150K",
      "domain": "reasoning",
      "general_avg": 59.62,
      "math_avg": 39.25,
      "code_avg": 43.41,
      "reasoning_avg": 39.61,
      "overall_avg": 45.47,
      "overall_efficiency": 0.021565,
      "general_efficiency": 0.054514,
      "math_efficiency": -0.023685,
      "code_efficiency": 0.02414,
      "reasoning_efficiency": 0.031292,
      "general_scores": [
        78.37,
        55.5525,
        57.7,
        46.9135714,
        78.18,
        54.8,
        57.8,
        47.03,
        78.33,
        54.2675,
        58.49,
        48.0242857
      ],
      "math_scores": [
        85.37,
        46.6,
        46.0,
        19.42,
        0.0,
        84.15,
        46.9,
        45.4,
        19.67,
        0.0,
        83.62,
        46.3,
        45.8,
        19.51,
        0.0
      ],
      "code_scores": [
        71.95,
        74.71,
        15.05,
        26.1,
        28.28,
        68.9,
        74.32,
        14.7,
        30.48,
        27.83,
        70.73,
        75.88,
        15.05,
        25.89,
        31.22
      ],
      "reasoning_scores": [
        58.31,
        67.03,
        0.38565217,
        39.12,
        47.12,
        66.11,
        0.37728261,
        39.36,
        50.85,
        67.07,
        0.38880435,
        39.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.29
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.87
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.0
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.32
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.38
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.6
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.53
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.49
              },
              {
                "metric": "lcb_test_output",
                "score": 29.11
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.09
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.18,
        "math_avg": -3.55,
        "code_avg": 3.62,
        "reasoning_avg": 4.69,
        "overall_avg": 3.23,
        "overall_efficiency": 0.021565,
        "general_efficiency": 0.054514,
        "math_efficiency": -0.023685,
        "code_efficiency": 0.02414,
        "reasoning_efficiency": 0.031292,
        "general_task_scores": [
          9.98,
          19.38,
          0.26,
          3.08
        ],
        "math_task_scores": [
          4.4,
          -4.52,
          -4.47,
          -6.51,
          -6.67
        ],
        "code_task_scores": [
          -6.91,
          3.37,
          6.69,
          26.45,
          -7.99
        ],
        "reasoning_task_scores": [
          15.49,
          -2.72,
          -0.01,
          5.95
        ]
      },
      "affiliation": "Allen AI",
      "year": "2024",
      "size": "150k",
      "size_precise": "150000",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 83,
      "name": "Magpie-Reasoning-V2-250K-CoT-QwQ",
      "domain": "reasoning",
      "general_avg": 49.02,
      "math_avg": 49.71,
      "code_avg": 37.19,
      "reasoning_avg": 28.21,
      "overall_avg": 41.03,
      "overall_efficiency": -0.004824,
      "general_efficiency": -0.009707,
      "math_efficiency": 0.027633,
      "code_efficiency": -0.010397,
      "reasoning_efficiency": -0.026825,
      "general_scores": [
        88.1,
        41.1025,
        41.1,
        25.7714286
      ],
      "math_scores": [
        83.85,
        63.32,
        60.4,
        24.3,
        16.67
      ],
      "code_scores": [
        33.54,
        74.32,
        6.09,
        30.48,
        4.3,
        74.39
      ],
      "reasoning_scores": [
        36.95,
        55.0,
        30.3,
        0.33086957,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.1
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.77
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.85
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 33.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 4.3
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 74.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.95
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.48
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -2.43,
        "math_avg": 6.91,
        "code_avg": -2.6,
        "reasoning_avg": -6.7,
        "overall_avg": -1.21,
        "overall_efficiency": -0.004824,
        "general_efficiency": -0.009707,
        "math_efficiency": 0.027633,
        "code_efficiency": -0.010397,
        "reasoning_efficiency": -0.026825,
        "general_task_scores": [
          19.79,
          5.61,
          -16.64,
          -18.47
        ],
        "math_task_scores": [
          3.87,
          12.2,
          10.2,
          -1.74,
          10.0
        ],
        "code_task_scores": [
          -43.9,
          2.72,
          -2.15,
          29.44,
          -32.8,
          31.1
        ],
        "reasoning_task_scores": [
          0.35,
          -14.46,
          -4.55,
          -0.06,
          -14.8
        ]
      },
      "affiliation": "Allen AI",
      "year": "2025",
      "size": "250k",
      "size_precise": "249922",
      "link": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ",
      "paper_link": "https://arxiv.org/abs/2406.08464",
      "tag": "general,math,science"
    },
    {
      "id": 84,
      "name": "reasoning-base-20k",
      "domain": "reasoning",
      "general_avg": 37.76,
      "math_avg": 23.35,
      "code_avg": 35.44,
      "reasoning_avg": 28.85,
      "overall_avg": 31.35,
      "overall_efficiency": -0.546035,
      "general_efficiency": -0.686113,
      "math_efficiency": -0.975531,
      "code_efficiency": -0.218111,
      "reasoning_efficiency": -0.304386,
      "general_scores": [
        28.43,
        44.7325,
        52.73,
        25.15
      ],
      "math_scores": [
        30.63,
        34.72,
        34.6,
        16.78,
        0.0
      ],
      "code_scores": [
        0.0,
        71.6,
        9.68,
        27.77,
        37.1,
        66.46
      ],
      "reasoning_scores": [
        29.15,
        63.43,
        33.33,
        0.31858696,
        18.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.73
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.15
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.63
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.78
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.6
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.77
              },
              {
                "metric": "lcb_test_output",
                "score": 37.1
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 66.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 63.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -13.68,
        "math_avg": -19.46,
        "code_avg": -4.35,
        "reasoning_avg": -6.07,
        "overall_avg": -10.89,
        "overall_efficiency": -0.546035,
        "general_efficiency": -0.686113,
        "math_efficiency": -0.975531,
        "code_efficiency": -0.218111,
        "reasoning_efficiency": -0.304386,
        "general_task_scores": [
          -39.88,
          9.24,
          -5.01,
          -19.09
        ],
        "math_task_scores": [
          -49.35,
          -16.4,
          -15.6,
          -9.26,
          -6.67
        ],
        "code_task_scores": [
          -77.44,
          0.0,
          1.44,
          26.73,
          0.0,
          23.17
        ],
        "reasoning_task_scores": [
          -7.45,
          -6.03,
          -1.52,
          -0.07,
          -15.28
        ]
      },
      "affiliation": "Nishith Jain",
      "year": "2024",
      "size": "19.9k",
      "size_precise": "19944",
      "link": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
      "paper_link": "",
      "tag": "general,code,math,science"
    },
    {
      "id": 85,
      "name": "medical-o1-reasoning-SFT",
      "domain": "reasoning",
      "general_avg": 45.57,
      "math_avg": 29.2,
      "code_avg": 36.02,
      "reasoning_avg": 34.68,
      "overall_avg": 36.37,
      "overall_efficiency": -0.297998,
      "general_efficiency": -0.298056,
      "math_efficiency": -0.690519,
      "code_efficiency": -0.191179,
      "reasoning_efficiency": -0.012238,
      "general_scores": [
        77.08,
        41.81,
        56.82,
        0.0,
        77.08,
        42.0225,
        56.4,
        0.0,
        77.09,
        42.1,
        56.59,
        19.8664286
      ],
      "math_scores": [
        19.41,
        40.04,
        44.0,
        27.3,
        13.33,
        22.74,
        42.14,
        42.8,
        27.42,
        16.67,
        21.61,
        38.42,
        41.2,
        27.53,
        13.33
      ],
      "code_scores": [
        38.41,
        71.98,
        10.39,
        44.47,
        11.09,
        43.9,
        73.15,
        9.32,
        45.09,
        15.61,
        36.59,
        71.98,
        11.83,
        44.47,
        11.99
      ],
      "reasoning_scores": [
        35.93,
        69.92,
        0.38347826,
        32.08,
        38.31,
        69.86,
        0.38391304,
        33.84,
        33.22,
        69.79,
        0.38576087,
        32.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.08
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.6
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.62
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.25
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.2
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 44.68
              },
              {
                "metric": "lcb_test_output",
                "score": 12.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.87,
        "math_avg": -13.61,
        "code_avg": -3.77,
        "reasoning_avg": -0.24,
        "overall_avg": -5.87,
        "overall_efficiency": -0.297998,
        "general_efficiency": -0.298056,
        "math_efficiency": -0.690519,
        "code_efficiency": -0.191179,
        "reasoning_efficiency": -0.012238,
        "general_task_scores": [
          8.77,
          6.49,
          -1.14,
          -37.62
        ],
        "math_task_scores": [
          -58.73,
          -10.92,
          -7.53,
          1.38,
          7.77
        ],
        "code_task_scores": [
          -37.81,
          0.77,
          2.27,
          43.64,
          -24.2
        ],
        "reasoning_task_scores": [
          -0.78,
          0.4,
          -0.01,
          -0.64
        ]
      },
      "affiliation": "CUHK",
      "year": "2024",
      "size": "19.7k",
      "size_precise": "19704",
      "link": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT",
      "paper_link": "https://arxiv.org/abs/2412.18925",
      "tag": "science"
    },
    {
      "id": 86,
      "name": "Light-R1-SFTData",
      "domain": "reasoning",
      "general_avg": 50.46,
      "math_avg": 68.65,
      "code_avg": 33.58,
      "reasoning_avg": 39.29,
      "overall_avg": 47.99,
      "overall_efficiency": 0.072464,
      "general_efficiency": -0.012403,
      "math_efficiency": 0.32536,
      "code_efficiency": -0.078132,
      "reasoning_efficiency": 0.055033,
      "general_scores": [
        79.4,
        38.4875,
        39.49,
        44.4592857
      ],
      "math_scores": [
        91.13,
        85.32,
        87.2,
        43.34,
        36.25125
      ],
      "code_scores": [
        45.73,
        66.15,
        3.23,
        30.06,
        16.06,
        40.24
      ],
      "reasoning_scores": [
        80.0,
        50.58,
        38.38,
        0.28054348,
        27.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.4
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.46
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 91.13
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.25
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 66.15
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.06
              },
              {
                "metric": "lcb_test_output",
                "score": 16.06
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 40.24
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.58
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.99,
        "math_avg": 25.85,
        "code_avg": -6.21,
        "reasoning_avg": 4.37,
        "overall_avg": 5.76,
        "overall_efficiency": 0.072464,
        "general_efficiency": -0.012403,
        "math_efficiency": 0.32536,
        "code_efficiency": -0.078132,
        "reasoning_efficiency": 0.055033,
        "general_task_scores": [
          11.09,
          3.0,
          -18.25,
          0.22
        ],
        "math_task_scores": [
          11.15,
          34.2,
          37.0,
          17.3,
          29.58
        ],
        "code_task_scores": [
          -31.71,
          -5.45,
          -5.01,
          29.02,
          -21.04,
          -3.05
        ],
        "reasoning_task_scores": [
          43.4,
          -18.88,
          3.53,
          -0.11,
          -6.08
        ]
      },
      "affiliation": "Qiyuan Tech",
      "year": "2025",
      "size": "79k",
      "size_precise": "79439",
      "link": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData",
      "paper_link": "https://arxiv.org/abs/2503.10460",
      "tag": "general,code,math,science"
    },
    {
      "id": 87,
      "name": "Fast-Math-R1-SFT",
      "domain": "reasoning",
      "general_avg": 55.01,
      "math_avg": 61.08,
      "code_avg": 31.03,
      "reasoning_avg": 28.95,
      "overall_avg": 44.02,
      "overall_efficiency": 0.225808,
      "general_efficiency": 0.451819,
      "math_efficiency": 2.314303,
      "code_efficiency": -1.108017,
      "reasoning_efficiency": -0.754877,
      "general_scores": [
        84.94,
        43.3925,
        47.97,
        43.7528571
      ],
      "math_scores": [
        89.84,
        78.1,
        78.4,
        35.75,
        23.335
      ],
      "code_scores": [
        43.29,
        74.71,
        6.09,
        7.52,
        13.12,
        41.46
      ],
      "reasoning_scores": [
        24.75,
        53.41,
        38.38,
        0.30434783,
        27.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.39
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.84
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.75
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.34
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.52
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          },
          {
            "task_name": "humaneval+",
            "metrics": [
              {
                "metric": "Metric_17",
                "score": 41.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.92
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 3.57,
        "math_avg": 18.28,
        "code_avg": -8.75,
        "reasoning_avg": -5.96,
        "overall_avg": 1.78,
        "overall_efficiency": 0.225808,
        "general_efficiency": 0.451819,
        "math_efficiency": 2.314303,
        "code_efficiency": -1.108017,
        "reasoning_efficiency": -0.754877,
        "general_task_scores": [
          16.63,
          7.9,
          -9.77,
          -0.49
        ],
        "math_task_scores": [
          9.86,
          26.98,
          28.2,
          9.71,
          16.67
        ],
        "code_task_scores": [
          -34.15,
          3.11,
          -2.15,
          6.48,
          -23.98,
          -1.83
        ],
        "reasoning_task_scores": [
          -11.85,
          -16.05,
          3.53,
          -0.09,
          -5.36
        ]
      },
      "affiliation": "University of Tokyo",
      "year": "2025",
      "size": "7.9k",
      "size_precise": "7900",
      "link": "https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT",
      "paper_link": "https://arxiv.org/abs/2507.08267",
      "tag": "general,math"
    }
  ]
}