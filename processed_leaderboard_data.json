{
  "llama": [
    {
      "id": 0,
      "name": "base",
      "domain": "base",
      "general_avg": 16.01,
      "math_avg": 13.45,
      "code_avg": 15.68,
      "reasoning_avg": 41.32,
      "overall_avg": 21.62,
      "general_task_scores": [
        21.83,
        20.3375,
        17.97,
        3.92071429
      ],
      "math_task_scores": [
        55.8,
        5.5,
        1.6,
        4.34,
        0.0
      ],
      "code_task_scores": [
        20.12,
        54.47,
        3.58,
        0.21,
        0.0
      ],
      "reasoning_task_scores": [
        80.34,
        62.56,
        0.312,
        22.08
      ]
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 27.88,
      "math_avg": 13.79,
      "code_avg": 26.31,
      "reasoning_avg": 31.94,
      "overall_avg": 24.98,
      "general_scores": [
        45.88,
        30.76,
        29.51,
        5.28642857,
        42.43,
        34.1625,
        29.15,
        6.52857143,
        43.37,
        31.745,
        30.01,
        5.76714286
      ],
      "math_scores": [
        43.06,
        9.2,
        11.0,
        7.66,
        3.33,
        36.54,
        9.76,
        10.4,
        7.7,
        0.0,
        42.53,
        8.78,
        9.6,
        7.36,
        0.0
      ],
      "code_scores": [
        32.32,
        45.53,
        4.3,
        27.56,
        9.73,
        39.02,
        50.58,
        1.79,
        29.23,
        17.65,
        35.98,
        49.03,
        3.58,
        29.85,
        18.55
      ],
      "reasoning_scores": [
        66.44,
        49.22,
        0.28815217,
        13.68,
        61.69,
        46.13,
        0.29815217,
        18.24,
        63.39,
        48.34,
        0.31217391,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.56
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.88
              },
              {
                "metric": "lcb_test_output",
                "score": 15.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.73
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.87,
        "math_avg": 0.35,
        "code_avg": 10.64,
        "reasoning_avg": -9.38,
        "overall_avg": 3.37,
        "general_task_scores": [
          22.06,
          11.88,
          11.59,
          1.94
        ],
        "math_task_scores": [
          -15.09,
          3.75,
          8.73,
          3.23,
          1.11
        ],
        "code_task_scores": [
          15.65,
          -6.09,
          -0.36,
          28.67,
          15.31
        ],
        "reasoning_task_scores": [
          -16.5,
          -14.66,
          -0.01,
          -6.35
        ]
      },
      "affiliation": "nomic-ai",
      "year": "2023",
      "size": "809k",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 34.07,
      "math_avg": 4.07,
      "code_avg": 23.98,
      "reasoning_avg": 32.65,
      "overall_avg": 23.69,
      "general_scores": [
        38.3,
        42.8125,
        33.56,
        16.8542857,
        48.65,
        44.035,
        35.03,
        12.1478571,
        49.07,
        36.945,
        35.66,
        15.7421429
      ],
      "math_scores": [
        1.82,
        4.62,
        4.6,
        6.05,
        0.0,
        2.58,
        4.74,
        4.8,
        6.01,
        3.33,
        4.17,
        4.64,
        4.0,
        6.35,
        3.33
      ],
      "code_scores": [
        32.32,
        45.91,
        3.23,
        24.84,
        5.43,
        30.49,
        50.19,
        2.51,
        27.56,
        18.1,
        29.27,
        48.64,
        3.94,
        28.18,
        9.05
      ],
      "reasoning_scores": [
        69.83,
        37.81,
        0.3026087,
        20.0,
        70.85,
        43.84,
        0.29945652,
        20.64,
        69.15,
        38.37,
        0.30956522,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.86
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.94
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 40.01
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.35
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.05,
        "math_avg": -9.38,
        "code_avg": 8.3,
        "reasoning_avg": -8.67,
        "overall_avg": 2.08,
        "general_task_scores": [
          23.51,
          20.92,
          16.78,
          10.99
        ],
        "math_task_scores": [
          -52.94,
          -0.83,
          2.87,
          1.8,
          2.22
        ],
        "code_task_scores": [
          10.57,
          -6.22,
          -0.35,
          26.65,
          10.86
        ],
        "reasoning_task_scores": [
          -10.4,
          -22.55,
          -0.01,
          -1.73
        ]
      },
      "affiliation": "tatsu-lab",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 32.52,
      "math_avg": 7.57,
      "code_avg": 28.28,
      "reasoning_avg": 28.52,
      "overall_avg": 24.23,
      "general_scores": [
        25.39,
        41.765,
        33.13,
        12.5321429,
        47.94,
        41.5825,
        33.19,
        13.2785714,
        50.1,
        42.3275,
        34.28,
        14.7607143
      ],
      "math_scores": [
        18.04,
        8.54,
        8.8,
        9.35,
        0.0,
        14.86,
        7.06,
        7.4,
        9.98,
        0.0,
        8.72,
        6.76,
        6.0,
        8.06,
        0.0
      ],
      "code_scores": [
        37.2,
        52.14,
        2.51,
        23.8,
        20.36,
        41.46,
        53.7,
        3.58,
        21.92,
        20.36,
        42.68,
        56.81,
        6.09,
        21.29,
        20.36
      ],
      "reasoning_scores": [
        54.58,
        37.05,
        0.32521739,
        23.36,
        55.93,
        29.86,
        0.3526087,
        22.0,
        56.27,
        39.27,
        0.32434783,
        22.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.45
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.34
              },
              {
                "metric": "lcb_test_output",
                "score": 20.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.39
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.77
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.51,
        "math_avg": -5.88,
        "code_avg": 12.61,
        "reasoning_avg": -12.8,
        "overall_avg": 2.61,
        "general_task_scores": [
          19.31,
          21.55,
          15.56,
          9.6
        ],
        "math_task_scores": [
          -41.93,
          1.95,
          5.8,
          4.79,
          0.0
        ],
        "code_task_scores": [
          20.33,
          -0.25,
          0.48,
          22.13,
          20.36
        ],
        "reasoning_task_scores": [
          -24.75,
          -27.17,
          0.02,
          0.69
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 33.77,
      "math_avg": 6.33,
      "code_avg": 16.73,
      "reasoning_avg": 22.82,
      "overall_avg": 19.91,
      "general_scores": [
        67.24,
        38.7275,
        23.84,
        3.29928571,
        68.3,
        37.7775,
        27.33,
        2.54714286,
        66.8,
        42.2075,
        24.01,
        3.13642857
      ],
      "math_scores": [
        11.52,
        6.46,
        7.6,
        8.69,
        0.0,
        10.92,
        6.98,
        6.4,
        7.9,
        0.0,
        5.53,
        6.5,
        8.0,
        8.42,
        0.0
      ],
      "code_scores": [
        6.71,
        54.86,
        2.87,
        6.47,
        4.75,
        1.22,
        55.64,
        2.87,
        17.95,
        2.26,
        21.95,
        56.03,
        1.43,
        14.61,
        1.36
      ],
      "reasoning_scores": [
        20.34,
        52.1,
        0.33684783,
        19.36,
        30.51,
        49.95,
        0.3623913,
        18.4,
        19.32,
        43.9,
        0.35630435,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.06
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.99
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.34
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.96
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.51
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 13.01
              },
              {
                "metric": "lcb_test_output",
                "score": 2.79
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.65
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.75,
        "math_avg": -7.12,
        "code_avg": 1.06,
        "reasoning_avg": -18.51,
        "overall_avg": -1.7,
        "general_task_scores": [
          45.62,
          19.23,
          7.09,
          -0.93
        ],
        "math_task_scores": [
          -46.48,
          1.15,
          5.73,
          4.0,
          0.0
        ],
        "code_task_scores": [
          -10.16,
          1.04,
          -1.19,
          12.8,
          2.79
        ],
        "reasoning_task_scores": [
          -56.95,
          -13.91,
          0.04,
          -3.2
        ]
      },
      "affiliation": "databricks",
      "year": "2023",
      "size": "15k",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 41.63,
      "math_avg": 21.85,
      "code_avg": 30.9,
      "reasoning_avg": 35.27,
      "overall_avg": 32.41,
      "general_scores": [
        61.23,
        33.62,
        22.8685714,
        60.34,
        49.6325,
        34.8,
        23.5921429,
        62.71,
        48.0125,
        35.05,
        26.0314286
      ],
      "math_scores": [
        38.59,
        13.14,
        13.8,
        42.99,
        13.18,
        13.8,
        34.42,
        12.74,
        14.0
      ],
      "code_scores": [
        42.68,
        57.98,
        7.89,
        30.06,
        21.27,
        44.51,
        53.7,
        6.45,
        31.52,
        12.67,
        39.63,
        55.64,
        6.09,
        28.81,
        24.66
      ],
      "reasoning_scores": [
        60.68,
        59.9,
        0.33619565,
        24.4,
        55.93,
        57.8,
        0.3301087,
        23.04,
        58.31,
        58.85,
        0.34380435,
        23.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.49
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 38.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.13
              },
              {
                "metric": "lcb_test_output",
                "score": 19.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.85
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 25.61,
        "math_avg": 8.4,
        "code_avg": 15.23,
        "reasoning_avg": -6.05,
        "overall_avg": 10.8,
        "general_task_scores": [
          39.6,
          28.48,
          16.52,
          20.24
        ],
        "math_task_scores": [
          -17.13,
          7.52,
          12.27
        ],
        "code_task_scores": [
          22.15,
          1.3,
          3.23,
          29.92,
          19.53
        ],
        "reasoning_task_scores": [
          -22.03,
          -3.71,
          0.03,
          1.52
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 38.04,
      "math_avg": 15.47,
      "code_avg": 28.16,
      "reasoning_avg": 37.96,
      "overall_avg": 29.91,
      "general_scores": [
        41.16,
        47.9275,
        35.71,
        26.5814286,
        49.22,
        49.1075,
        33.58,
        20.6507143,
        45.08,
        49.3525,
        34.68,
        23.4607143
      ],
      "math_scores": [
        45.11,
        12.7,
        13.8,
        8.11,
        3.33,
        41.09,
        12.92,
        11.8,
        7.72,
        0.0,
        40.41,
        12.96,
        14.4,
        7.7,
        0.0
      ],
      "code_scores": [
        43.9,
        55.64,
        5.02,
        12.32,
        18.33,
        50.61,
        52.53,
        3.23,
        18.16,
        17.42,
        45.12,
        57.59,
        5.73,
        22.34,
        14.48
      ],
      "reasoning_scores": [
        71.86,
        54.41,
        0.31336957,
        24.72,
        69.83,
        55.51,
        0.35206522,
        25.28,
        70.51,
        56.47,
        0.33565217,
        25.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.61
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.73
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.46
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.31
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 22.03,
        "math_avg": 2.02,
        "code_avg": 12.49,
        "reasoning_avg": -3.36,
        "overall_avg": 8.29,
        "general_task_scores": [
          23.32,
          28.46,
          16.69,
          19.64
        ],
        "math_task_scores": [
          -13.6,
          7.36,
          11.73,
          3.5,
          1.11
        ],
        "code_task_scores": [
          26.42,
          0.78,
          1.08,
          17.4,
          16.74
        ],
        "reasoning_task_scores": [
          -9.61,
          -7.1,
          0.02,
          3.23
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 30.01,
      "math_avg": 5.61,
      "code_avg": 23.54,
      "reasoning_avg": 27.38,
      "overall_avg": 21.64,
      "general_scores": [
        66.09,
        29.2525,
        24.29,
        0.24857143,
        66.03,
        30.36,
        23.85,
        0.17142857,
        65.83,
        29.9575,
        23.84,
        0.23357143
      ],
      "math_scores": [
        7.73,
        5.6,
        4.0,
        10.52,
        0.0,
        7.73,
        5.6,
        4.6,
        10.46,
        0.0,
        7.51,
        5.76,
        4.2,
        10.48,
        0.0
      ],
      "code_scores": [
        40.24,
        59.14,
        0.36,
        17.95,
        0.45,
        40.85,
        58.75,
        0.36,
        18.16,
        0.23,
        38.41,
        58.37,
        0.36,
        19.21,
        0.23
      ],
      "reasoning_scores": [
        43.05,
        61.43,
        0.27391304,
        5.68,
        42.37,
        61.26,
        0.27641304,
        5.6,
        42.03,
        61.3,
        0.27717391,
        5.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.44
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.48
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.0,
        "math_avg": -7.84,
        "code_avg": 7.86,
        "reasoning_avg": -13.94,
        "overall_avg": 0.02,
        "general_task_scores": [
          44.15,
          9.52,
          6.02,
          -3.7
        ],
        "math_task_scores": [
          -48.14,
          0.15,
          2.67,
          6.15,
          0.0
        ],
        "code_task_scores": [
          19.71,
          4.28,
          -3.22,
          18.23,
          0.3
        ],
        "reasoning_task_scores": [
          -37.86,
          -1.23,
          -0.03,
          -16.64
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "link": "https://huggingface.co/datasets/GAIR/lima"
    },
    {
      "id": 8,
      "name": "orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 47.35,
      "math_avg": 24.72,
      "code_avg": 29.6,
      "reasoning_avg": 42.53,
      "overall_avg": 36.05,
      "general_scores": [
        70.08,
        54.355,
        37.42,
        28.1542857,
        69.38,
        55.04,
        39.08,
        28.6621429,
        67.95,
        54.7825,
        38.46,
        24.82
      ],
      "math_scores": [
        47.84,
        30.1,
        34.2,
        13.37,
        0.0,
        51.78,
        31.4,
        30.6,
        14.23,
        0.0,
        47.46,
        28.44,
        28.6,
        12.76,
        0.0
      ],
      "code_scores": [
        45.73,
        48.64,
        7.53,
        18.37,
        29.64,
        45.73,
        50.58,
        6.45,
        13.15,
        26.7,
        40.85,
        49.42,
        6.81,
        29.44,
        24.89
      ],
      "reasoning_scores": [
        75.93,
        62.69,
        0.34293478,
        31.28,
        77.97,
        60.88,
        0.36043478,
        31.28,
        74.92,
        62.07,
        0.3451087,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.32
              },
              {
                "metric": "lcb_test_output",
                "score": 27.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.88
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 31.33,
        "math_avg": 11.27,
        "code_avg": 13.92,
        "reasoning_avg": 1.2,
        "overall_avg": 14.43,
        "general_task_scores": [
          47.31,
          34.39,
          20.35,
          23.29
        ],
        "math_task_scores": [
          -6.77,
          24.48,
          29.53,
          9.11,
          0.0
        ],
        "code_task_scores": [
          23.98,
          -4.92,
          3.35,
          20.11,
          27.08
        ],
        "reasoning_task_scores": [
          -4.07,
          -0.68,
          0.04,
          9.52
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 44.34,
      "math_avg": 20.21,
      "code_avg": 30.31,
      "reasoning_avg": 36.93,
      "overall_avg": 32.95,
      "general_scores": [
        64.06,
        50.9275,
        39.9,
        25.0585714,
        58.97,
        52.215,
        39.98,
        24.3621429,
        60.95,
        50.3825,
        39.81,
        25.4792857
      ],
      "math_scores": [
        60.2,
        17.3,
        19.8,
        11.72,
        0.0,
        60.5,
        13.56,
        11.0,
        10.28,
        0.0,
        60.12,
        14.62,
        14.4,
        9.64,
        0.0
      ],
      "code_scores": [
        46.95,
        52.53,
        6.09,
        29.65,
        15.16,
        49.39,
        52.53,
        3.58,
        27.56,
        16.29,
        50.0,
        56.42,
        4.66,
        27.56,
        16.29
      ],
      "reasoning_scores": [
        76.61,
        54.92,
        0.38608696,
        15.52,
        79.32,
        54.08,
        0.3676087,
        17.76,
        78.98,
        54.66,
        0.34847826,
        10.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.26
              },
              {
                "metric": "lcb_test_output",
                "score": 15.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.55
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 28.33,
        "math_avg": 6.76,
        "code_avg": 14.63,
        "reasoning_avg": -4.39,
        "overall_avg": 11.33,
        "general_task_scores": [
          39.5,
          30.84,
          21.93,
          21.05
        ],
        "math_task_scores": [
          4.47,
          9.66,
          13.47,
          6.21,
          0.0
        ],
        "code_task_scores": [
          28.66,
          -0.64,
          1.2,
          28.05,
          15.91
        ],
        "reasoning_task_scores": [
          -2.04,
          -8.01,
          0.06,
          -7.57
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 9.56,
      "math_avg": 0.72,
      "code_avg": 5.23,
      "reasoning_avg": 10.96,
      "overall_avg": 6.62,
      "general_scores": [
        17.68,
        3.62,
        7.94071429,
        5.56,
        8.61,
        12.9621429,
        17.73,
        3.9,
        8.04428571
      ],
      "math_scores": [
        0.61,
        0.92,
        0.6,
        0.0,
        0.38,
        1.84,
        1.6,
        0.0,
        0.38,
        1.36,
        1.0,
        0.0
      ],
      "code_scores": [
        7.93,
        14.79,
        0.0,
        3.76,
        0.0,
        9.15,
        19.46,
        0.0,
        0.21,
        0.0,
        6.1,
        17.12,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        12.2,
        22.01,
        0.25108696,
        7.68,
        15.59,
        22.93,
        0.25021739,
        7.36,
        22.71,
        16.11,
        0.23391304,
        4.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.32
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.83
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -6.45,
        "math_avg": -12.72,
        "code_avg": -10.44,
        "reasoning_avg": -30.37,
        "overall_avg": -15.0,
        "general_task_scores": [
          -8.17,
          -12.59,
          5.73
        ],
        "math_task_scores": [
          -55.34,
          -4.13,
          -0.53,
          0.0
        ],
        "code_task_scores": [
          -12.39,
          -37.35,
          -3.58,
          1.11,
          0.0
        ],
        "reasoning_task_scores": [
          -63.51,
          -42.21,
          -0.06,
          -15.68
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "252k",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 47.31,
      "math_avg": 36.33,
      "code_avg": 35.33,
      "reasoning_avg": 36.08,
      "overall_avg": 38.76,
      "general_scores": [
        65.4,
        72.9025,
        36.45,
        18.58,
        59.28,
        71.025,
        37.68,
        28.5221429,
        48.68,
        69.4225,
        34.94,
        24.8178571
      ],
      "math_scores": [
        73.39,
        30.14,
        31.0,
        13.98,
        73.77,
        28.76,
        29.4,
        13.89,
        73.16,
        27.5,
        27.8,
        13.21
      ],
      "code_scores": [
        64.02,
        57.59,
        5.02,
        36.53,
        18.33,
        60.98,
        57.98,
        7.53,
        29.02,
        18.1,
        61.59,
        53.31,
        3.23,
        32.78,
        23.98
      ],
      "reasoning_scores": [
        53.56,
        56.83,
        0.39130435,
        24.72,
        64.07,
        56.42,
        0.36663043,
        27.6,
        64.07,
        57.98,
        0.35043478,
        26.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 71.12
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.69
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 62.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.29
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.78
              },
              {
                "metric": "lcb_test_output",
                "score": 20.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.57
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 31.29,
        "math_avg": 22.89,
        "code_avg": 19.66,
        "reasoning_avg": -5.24,
        "overall_avg": 17.15,
        "general_task_scores": [
          35.96,
          50.78,
          18.39,
          20.05
        ],
        "math_task_scores": [
          17.64,
          23.3,
          27.8,
          9.35
        ],
        "code_task_scores": [
          42.08,
          1.82,
          1.68,
          32.57,
          20.14
        ],
        "reasoning_task_scores": [
          -19.77,
          -5.48,
          0.06,
          4.24
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "939k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 28.38,
      "math_avg": 3.6,
      "code_avg": 6.79,
      "reasoning_avg": 27.09,
      "overall_avg": 16.47,
      "general_scores": [
        67.91,
        14.0225,
        30.84,
        0.02928571,
        67.22,
        14.905,
        32.86,
        0.00857143,
        67.01,
        13.04,
        32.64,
        0.05071429
      ],
      "math_scores": [
        9.86,
        4.52,
        1.8,
        6.71,
        0.0,
        2.43,
        2.68,
        3.4,
        6.82,
        0.0,
        3.94,
        3.1,
        2.2,
        6.53,
        0.0
      ],
      "code_scores": [
        0.0,
        49.03,
        0.0,
        1.25,
        0.0,
        0.0,
        9.73,
        0.0,
        0.21,
        0.0,
        0.0,
        41.25,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        65.08,
        40.3,
        0.22956522,
        3.12,
        66.44,
        37.15,
        0.24478261,
        3.12,
        64.41,
        41.17,
        0.24021739,
        3.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.28
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.36,
        "math_avg": -9.85,
        "code_avg": -8.88,
        "reasoning_avg": -14.23,
        "overall_avg": -5.15,
        "general_task_scores": [
          45.55,
          -6.35,
          14.14,
          -3.89
        ],
        "math_task_scores": [
          -50.39,
          -2.07,
          0.87,
          2.35,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -21.13,
          -3.58,
          0.42,
          0.0
        ],
        "reasoning_task_scores": [
          -15.03,
          -23.02,
          -0.07,
          -18.8
        ]
      },
      "affiliation": "CAS",
      "year": "2023",
      "size": "-",
      "link": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"
    },
    {
      "id": 13,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 24.15,
      "math_avg": 1.47,
      "code_avg": 4.68,
      "reasoning_avg": 15.67,
      "overall_avg": 11.49,
      "general_scores": [
        68.66,
        19.3875,
        9.05,
        0.09285714,
        70.04,
        18.1275,
        8.25,
        0.13,
        70.38,
        16.5275,
        9.12,
        0.09214286
      ],
      "math_scores": [
        0.0,
        0.2,
        0.0,
        5.67,
        0.0,
        0.16,
        0.0,
        5.94,
        0.0,
        0.2,
        0.0,
        5.49
      ],
      "code_scores": [
        0.0,
        24.51,
        0.0,
        0.42,
        0.0,
        0.0,
        14.79,
        0.0,
        0.42,
        0.0,
        0.0,
        29.18,
        0.0,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        12.88,
        50.39,
        0.12402174,
        0.24,
        12.2,
        49.48,
        0.11141304,
        0.08,
        12.54,
        49.66,
        0.12402174,
        0.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 8.81
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.7
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 22.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.56
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.84
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.16
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.14,
        "math_avg": -11.98,
        "code_avg": -11.0,
        "reasoning_avg": -25.66,
        "overall_avg": -10.12,
        "general_task_scores": [
          47.86,
          -2.33,
          -9.16,
          -3.82
        ],
        "math_task_scores": [
          -55.8,
          -5.31,
          -1.6,
          1.36
        ],
        "code_task_scores": [
          -20.12,
          -31.64,
          -3.58,
          0.35,
          0.0
        ],
        "reasoning_task_scores": [
          -67.8,
          -12.72,
          -0.19,
          -21.92
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "link": "https://huggingface.co/datasets/openai/gsm8k"
    },
    {
      "id": 14,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 32.19,
      "math_avg": 42.14,
      "code_avg": 11.81,
      "reasoning_avg": 21.93,
      "overall_avg": 27.01,
      "general_scores": [
        70.44,
        24.0,
        31.77,
        0.05285714,
        74.18,
        28.025,
        30.4,
        0.04642857,
        69.91,
        26.545,
        30.68,
        0.20142857
      ],
      "math_scores": [
        73.54,
        56.88,
        54.6,
        18.27,
        3.33,
        74.75,
        57.16,
        56.0,
        18.9,
        3.33,
        81.35,
        58.18,
        57.4,
        18.34,
        0.0
      ],
      "code_scores": [
        9.76,
        40.86,
        0.72,
        0.21,
        4.98,
        9.15,
        37.74,
        1.79,
        0.0,
        9.5,
        14.63,
        36.58,
        2.15,
        0.0,
        9.05
      ],
      "reasoning_scores": [
        50.51,
        24.09,
        0.18771739,
        7.12,
        54.24,
        32.11,
        0.20554348,
        7.52,
        48.47,
        32.63,
        0.20445652,
        5.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.19
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.95
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.18
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.55
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 7.84
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.07
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.83
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.17,
        "math_avg": 28.69,
        "code_avg": -3.87,
        "reasoning_avg": -19.4,
        "overall_avg": 5.4,
        "general_task_scores": [
          49.68,
          5.85,
          12.98,
          -3.82
        ],
        "math_task_scores": [
          20.75,
          51.91,
          54.4,
          14.16,
          2.22
        ],
        "code_task_scores": [
          -8.94,
          -16.08,
          -2.03,
          -0.14,
          7.84
        ],
        "reasoning_task_scores": [
          -29.27,
          -32.95,
          -0.11,
          -15.25
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"
    },
    {
      "id": 15,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 26.68,
      "math_avg": 40.16,
      "code_avg": 9.46,
      "reasoning_avg": 20.09,
      "overall_avg": 24.1,
      "general_scores": [
        56.61,
        24.8925,
        29.94,
        0.21571429,
        55.38,
        25.495,
        29.78,
        0.11142857,
        44.78,
        23.0425,
        29.69,
        0.27
      ],
      "math_scores": [
        80.52,
        48.78,
        49.2,
        19.76,
        0.0,
        78.7,
        48.74,
        51.8,
        19.2,
        3.33,
        80.14,
        50.44,
        48.4,
        19.99,
        3.33
      ],
      "code_scores": [
        12.2,
        40.86,
        0.36,
        0.21,
        2.26,
        1.22,
        40.08,
        0.36,
        0.0,
        0.23,
        6.71,
        36.19,
        0.36,
        0.0,
        0.9
      ],
      "reasoning_scores": [
        41.69,
        24.77,
        0.16913043,
        15.36,
        39.66,
        30.17,
        0.16945652,
        14.4,
        41.02,
        20.88,
        0.16804348,
        12.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.26
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.48
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.79
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.65
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 6.71
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.07
              },
              {
                "metric": "lcb_test_output",
                "score": 1.13
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.79
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.67,
        "math_avg": 26.71,
        "code_avg": -6.21,
        "reasoning_avg": -21.23,
        "overall_avg": 2.48,
        "general_task_scores": [
          30.43,
          4.14,
          11.83,
          -3.72
        ],
        "math_task_scores": [
          23.99,
          43.82,
          48.2,
          15.31,
          2.22
        ],
        "code_task_scores": [
          -13.41,
          -15.43,
          -3.22,
          -0.14,
          1.13
        ],
        "reasoning_task_scores": [
          -39.55,
          -37.29,
          -0.14,
          -7.95
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT"
    },
    {
      "id": 16,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 33.08,
      "math_avg": 25.01,
      "code_avg": 23.14,
      "reasoning_avg": 17.35,
      "overall_avg": 24.64,
      "general_scores": [
        55.39,
        31.35,
        31.26,
        19.7371429,
        45.26,
        31.675,
        31.11,
        21.6564286,
        47.03,
        31.505,
        31.24,
        19.7942857
      ],
      "math_scores": [
        34.34,
        24.28,
        25.0,
        14.86,
        36.09,
        24.58,
        26.2,
        15.15,
        34.8,
        24.76,
        25.2,
        14.81
      ],
      "code_scores": [
        53.66,
        52.53,
        7.17,
        2.3,
        0.23,
        51.83,
        55.25,
        5.02,
        2.3,
        0.0,
        51.22,
        55.25,
        6.09,
        3.76,
        0.45
      ],
      "reasoning_scores": [
        52.2,
        16.99,
        0.17597826,
        3.52,
        49.15,
        13.46,
        0.17934783,
        4.8,
        49.15,
        15.13,
        0.17119565,
        3.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.2
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 20.4
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.94
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 52.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.09
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.79
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.07,
        "math_avg": 11.56,
        "code_avg": 7.46,
        "reasoning_avg": -23.97,
        "overall_avg": 3.03,
        "general_task_scores": [
          27.4,
          11.17,
          13.23,
          16.48
        ],
        "math_task_scores": [
          -20.72,
          19.04,
          23.87,
          10.6
        ],
        "code_task_scores": [
          32.12,
          -0.13,
          2.51,
          2.58,
          0.23
        ],
        "reasoning_task_scores": [
          -30.17,
          -47.37,
          -0.13,
          -18.21
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR"
    },
    {
      "id": 17,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 25.67,
      "math_avg": 29.41,
      "code_avg": 4.59,
      "reasoning_avg": 20.47,
      "overall_avg": 20.04,
      "general_scores": [
        63.63,
        22.3325,
        14.36,
        0.11928571,
        66.62,
        23.03,
        16.5,
        0.09071429,
        64.79,
        22.7125,
        13.7,
        0.15571429
      ],
      "math_scores": [
        73.54,
        30.94,
        31.6,
        10.34,
        0.0,
        75.74,
        31.74,
        30.6,
        9.96,
        0.0,
        75.97,
        31.5,
        29.4,
        9.78,
        0.0
      ],
      "code_scores": [
        0.0,
        19.84,
        0.0,
        0.84,
        0.68,
        0.0,
        17.9,
        0.0,
        4.59,
        0.23,
        0.0,
        20.62,
        0.0,
        3.97,
        0.23
      ],
      "reasoning_scores": [
        39.32,
        35.66,
        0.15347826,
        12.56,
        35.59,
        35.94,
        0.14304348,
        5.2,
        31.86,
        35.44,
        0.16956522,
        13.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.01
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.69
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.03
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 19.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.13
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.68
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.45
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.66,
        "math_avg": 15.96,
        "code_avg": -11.08,
        "reasoning_avg": -20.85,
        "overall_avg": -1.58,
        "general_task_scores": [
          43.18,
          2.35,
          -3.12,
          -3.8
        ],
        "math_task_scores": [
          19.28,
          25.89,
          28.93,
          5.69,
          0.0
        ],
        "code_task_scores": [
          -20.12,
          -35.02,
          -3.58,
          2.92,
          0.38
        ],
        "reasoning_task_scores": [
          -44.75,
          -26.88,
          -0.15,
          -11.63
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA"
    },
    {
      "id": 18,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 29.84,
      "math_avg": 21.07,
      "code_avg": 17.6,
      "reasoning_avg": 30.49,
      "overall_avg": 24.75,
      "general_scores": [
        43.69,
        34.2625,
        32.41,
        2.02428571,
        45.54,
        35.44,
        34.22,
        0.54357143,
        63.45,
        32.9325,
        32.27,
        1.26071429
      ],
      "math_scores": [
        59.21,
        19.58,
        21.2,
        9.08,
        0.0,
        57.01,
        18.0,
        16.6,
        8.81,
        0.0,
        58.76,
        18.98,
        19.8,
        8.99,
        0.0
      ],
      "code_scores": [
        29.88,
        43.97,
        0.72,
        6.05,
        7.01,
        31.1,
        43.58,
        0.0,
        5.85,
        4.07,
        30.49,
        42.41,
        2.51,
        11.9,
        4.52
      ],
      "reasoning_scores": [
        58.64,
        49.76,
        0.25086957,
        9.52,
        61.69,
        50.93,
        0.2573913,
        8.56,
        62.71,
        49.5,
        0.27836957,
        13.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 34.21
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.97
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.28
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.96
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.08
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.93
              },
              {
                "metric": "lcb_test_output",
                "score": 5.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.64
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.82,
        "math_avg": 7.62,
        "code_avg": 1.93,
        "reasoning_avg": -10.83,
        "overall_avg": 3.14,
        "general_task_scores": [
          29.06,
          13.87,
          15.0,
          -2.64
        ],
        "math_task_scores": [
          2.53,
          13.35,
          17.6,
          4.62,
          0.0
        ],
        "code_task_scores": [
          10.37,
          -11.15,
          -2.5,
          7.72,
          5.2
        ],
        "reasoning_task_scores": [
          -19.33,
          -12.5,
          -0.05,
          -11.44
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct"
    },
    {
      "id": 19,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 35.08,
      "math_avg": 4.32,
      "code_avg": 16.89,
      "reasoning_avg": 23.49,
      "overall_avg": 19.94,
      "general_scores": [
        71.93,
        31.235,
        31.68,
        2.60785714,
        73.08,
        32.4475,
        32.67,
        3.575,
        73.01,
        34.6325,
        31.73,
        2.355
      ],
      "math_scores": [
        2.65,
        5.2,
        5.4,
        10.7,
        3.33,
        0.53,
        3.1,
        3.0,
        11.86,
        0.0,
        0.76,
        3.58,
        3.2,
        11.43,
        0.0
      ],
      "code_scores": [
        22.56,
        38.13,
        2.15,
        2.71,
        13.12,
        27.44,
        40.47,
        2.51,
        6.05,
        10.86,
        23.17,
        39.3,
        0.72,
        8.98,
        15.16
      ],
      "reasoning_scores": [
        32.54,
        41.37,
        0.21934783,
        14.48,
        37.97,
        45.11,
        0.2176087,
        14.64,
        36.27,
        41.01,
        0.22934783,
        17.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.31
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.79
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.91
              },
              {
                "metric": "lcb_test_output",
                "score": 13.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.59
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.5
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.06,
        "math_avg": -9.13,
        "code_avg": 1.21,
        "reasoning_avg": -17.83,
        "overall_avg": -1.67,
        "general_task_scores": [
          50.84,
          12.43,
          14.06,
          -1.07
        ],
        "math_task_scores": [
          -54.49,
          -1.54,
          2.27,
          6.99,
          1.11
        ],
        "code_task_scores": [
          4.27,
          -15.17,
          -1.79,
          5.7,
          13.05
        ],
        "reasoning_task_scores": [
          -44.75,
          -20.06,
          -0.09,
          -6.43
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 20,
      "name": "DART-Math",
      "domain": "math",
      "general_avg": 32.77,
      "math_avg": 41.3,
      "code_avg": 15.45,
      "reasoning_avg": 29.95,
      "overall_avg": 29.87,
      "general_scores": [
        76.17,
        25.66,
        30.71,
        0.25285714,
        76.17,
        25.81,
        30.71,
        0.25285714,
        73.21,
        24.2825,
        29.78,
        0.17785714
      ],
      "math_scores": [
        83.09,
        44.7,
        47.8,
        15.29,
        83.09,
        44.7,
        47.8,
        15.27,
        3.33,
        83.24,
        44.64,
        47.0,
        14.97,
        3.33
      ],
      "code_scores": [
        24.39,
        42.8,
        0.72,
        2.09,
        6.33,
        24.39,
        42.8,
        0.72,
        2.09,
        6.33,
        26.22,
        45.53,
        0.72,
        2.71,
        3.85
      ],
      "reasoning_scores": [
        57.29,
        42.99,
        0.19228261,
        18.88,
        57.29,
        42.99,
        0.19228261,
        18.88,
        56.95,
        42.99,
        0.18543478,
        20.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.18
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 43.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.3
              },
              {
                "metric": "lcb_test_output",
                "score": 5.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.99
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.75,
        "math_avg": 27.86,
        "code_avg": -0.23,
        "reasoning_avg": -11.37,
        "overall_avg": 8.25,
        "general_task_scores": [
          53.35,
          4.91,
          12.43,
          -3.69
        ],
        "math_task_scores": [
          27.34,
          39.18,
          45.93,
          10.84,
          3.33
        ],
        "code_task_scores": [
          4.88,
          -10.76,
          -2.86,
          2.09,
          5.5
        ],
        "reasoning_task_scores": [
          -23.16,
          -19.57,
          -0.12,
          -2.64
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-uniform https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 21,
      "name": "MathQA",
      "domain": "math",
      "general_avg": 17.66,
      "math_avg": 2.27,
      "code_avg": 5.65,
      "reasoning_avg": 1.8,
      "overall_avg": 6.85,
      "general_scores": [
        41.08,
        26.495,
        0.92,
        0.0,
        42.03,
        25.9,
        1.58,
        0.0,
        46.17,
        26.5475,
        1.24,
        0.0
      ],
      "math_scores": [
        2.88,
        0.56,
        0.0,
        5.85,
        0.0,
        4.93,
        1.66,
        0.4,
        6.12,
        0.0,
        4.4,
        1.06,
        0.0,
        6.17,
        0.0
      ],
      "code_scores": [
        0.0,
        28.02,
        0.0,
        0.42,
        0.0,
        0.0,
        26.46,
        0.0,
        1.25,
        0.0,
        0.61,
        27.63,
        0.0,
        0.42,
        0.0
      ],
      "reasoning_scores": [
        0.34,
        3.9,
        0.13880435,
        2.32,
        0.34,
        3.64,
        0.16217391,
        2.64,
        0.68,
        4.71,
        0.1548913,
        2.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 26.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 1.25
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.09
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 27.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.08
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.65,
        "math_avg": -11.18,
        "code_avg": -10.02,
        "reasoning_avg": -39.52,
        "overall_avg": -14.77,
        "general_task_scores": [
          21.26,
          5.97,
          -16.72,
          -3.92
        ],
        "math_task_scores": [
          -51.73,
          -4.41,
          -1.47,
          1.71,
          0.0
        ],
        "code_task_scores": [
          -19.92,
          -27.1,
          -3.58,
          0.49,
          0.0
        ],
        "reasoning_task_scores": [
          -79.89,
          -58.48,
          -0.16,
          -19.57
        ]
      },
      "affiliation": "AllenAI",
      "year": "2019",
      "size": "29.8k",
      "link": "https://huggingface.co/datasets/allenai/math_qa"
    },
    {
      "id": 22,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 30.01,
      "math_avg": 17.26,
      "code_avg": 12.64,
      "reasoning_avg": 25.92,
      "overall_avg": 21.46,
      "general_scores": [
        71.36,
        21.0,
        25.47,
        0.02071429,
        72.35,
        24.7925,
        25.27,
        0.08857143,
        72.93,
        21.8575,
        24.91,
        0.02214286
      ],
      "math_scores": [
        40.86,
        17.18,
        18.4,
        8.79,
        0.0,
        41.24,
        17.9,
        17.6,
        9.55,
        0.0,
        38.67,
        18.1,
        18.4,
        8.9,
        3.33
      ],
      "code_scores": [
        0.0,
        54.09,
        0.36,
        5.22,
        0.9,
        1.22,
        55.25,
        1.43,
        3.34,
        2.26,
        1.22,
        56.81,
        1.08,
        5.01,
        1.36
      ],
      "reasoning_scores": [
        48.14,
        54.08,
        0.16869565,
        3.84,
        37.63,
        55.61,
        0.16869565,
        12.08,
        41.36,
        52.05,
        0.15163043,
        5.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.52
              },
              {
                "metric": "lcb_test_output",
                "score": 1.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.23
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.99,
        "math_avg": 3.81,
        "code_avg": -3.04,
        "reasoning_avg": -15.4,
        "overall_avg": -0.16,
        "general_task_scores": [
          50.38,
          2.21,
          7.25,
          -3.88
        ],
        "math_task_scores": [
          -15.54,
          12.23,
          16.53,
          4.74,
          1.11
        ],
        "code_task_scores": [
          -19.31,
          0.91,
          -2.62,
          4.31,
          1.51
        ],
        "reasoning_task_scores": [
          -37.96,
          -8.65,
          -0.15,
          -14.85
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "link": "https://huggingface.co/datasets/camel-ai/math"
    },
    {
      "id": 23,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 32.93,
      "math_avg": 10.81,
      "code_avg": 13.31,
      "reasoning_avg": 28.4,
      "overall_avg": 21.36,
      "general_scores": [
        55.66,
        33.68,
        35.43,
        6.08071429,
        58.42,
        32.125,
        36.03,
        3.90071429,
        61.88,
        31.85,
        35.99,
        4.13571429
      ],
      "math_scores": [
        21.3,
        11.1,
        10.8,
        11.36,
        0.0,
        21.68,
        12.36,
        9.8,
        11.36,
        0.0,
        19.94,
        11.28,
        10.2,
        11.0,
        0.0
      ],
      "code_scores": [
        8.54,
        55.64,
        2.51,
        3.13,
        1.36,
        2.44,
        52.14,
        2.87,
        4.59,
        2.26,
        1.22,
        54.47,
        2.51,
        4.38,
        1.58
      ],
      "reasoning_scores": [
        55.25,
        39.81,
        0.28271739,
        21.2,
        53.22,
        39.74,
        0.28728261,
        19.76,
        53.56,
        38.89,
        0.28793478,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.48
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.92,
        "math_avg": -2.64,
        "code_avg": -2.37,
        "reasoning_avg": -12.93,
        "overall_avg": -0.25,
        "general_task_scores": [
          36.82,
          12.21,
          17.85,
          0.79
        ],
        "math_task_scores": [
          -34.83,
          6.08,
          8.67,
          6.9,
          0.0
        ],
        "code_task_scores": [
          -16.05,
          -0.39,
          -0.95,
          3.82,
          1.73
        ],
        "reasoning_task_scores": [
          -26.33,
          -23.08,
          -0.02,
          -2.27
        ]
      },
      "affiliation": "SkunkworksAI",
      "year": "2025",
      "size": "29.9k",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01"
    },
    {
      "id": 24,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 22.58,
      "reasoning_avg": 20.09,
      "overall_avg": 24.06,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.6875,
        21.54,
        0.43785714,
        45.71,
        33.075,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.21,
        0.0,
        57.7,
        27.26,
        39.4,
        15.15,
        0.0
      ],
      "code_scores": [
        26.22,
        55.64,
        6.45,
        0.42,
        16.74,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68
      ],
      "reasoning_scores": [
        25.42,
        34.29,
        0.27228261,
        21.36,
        24.07,
        31.44,
        0.25282609,
        21.92,
        28.81,
        32.99,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.02,
        "math_avg": 14.1,
        "code_avg": 6.9,
        "reasoning_avg": -21.23,
        "overall_avg": 2.45,
        "general_task_scores": [
          27.19,
          11.69,
          4.68,
          -3.48
        ],
        "math_task_scores": [
          -0.13,
          21.51,
          37.73,
          10.29,
          1.11
        ],
        "code_task_scores": [
          11.18,
          1.3,
          3.83,
          0.49,
          17.72
        ],
        "reasoning_task_scores": [
          -54.24,
          -29.65,
          -0.05,
          -0.99
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math"
    },
    {
      "id": 25,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 25.29,
      "math_avg": 26.6,
      "code_avg": 8.05,
      "reasoning_avg": 19.05,
      "overall_avg": 19.75,
      "general_scores": [
        56.04,
        26.94,
        21.86,
        1.06928571,
        52.56,
        25.765,
        21.19,
        1.61928571,
        48.93,
        25.1,
        21.35,
        1.01357143
      ],
      "math_scores": [
        68.69,
        19.14,
        24.0,
        11.5,
        0.0,
        63.99,
        17.28,
        22.2,
        10.84,
        0.0,
        65.96,
        18.84,
        23.4
      ],
      "code_scores": [
        0.61,
        48.64,
        0.0,
        0.0,
        0.68,
        0.0,
        35.8,
        0.0,
        0.0,
        0.0,
        1.22,
        33.07,
        0.0,
        0.0,
        0.68
      ],
      "reasoning_scores": [
        21.36,
        34.07,
        0.2425,
        18.88,
        22.37,
        38.59,
        0.23369565,
        17.84,
        21.69,
        33.76,
        0.23336957,
        19.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.51
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.23
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.17
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.17
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.0
              },
              {
                "metric": "lcb_test_output",
                "score": 0.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.67
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.27,
        "math_avg": 13.16,
        "code_avg": -7.63,
        "reasoning_avg": -22.28,
        "overall_avg": -1.87,
        "general_task_scores": [
          30.68,
          5.6,
          3.5,
          -2.69
        ],
        "math_task_scores": [
          10.41,
          12.92,
          21.6,
          6.83,
          0.0
        ],
        "code_task_scores": [
          -19.51,
          -15.3,
          -3.58,
          -0.21,
          0.45
        ],
        "reasoning_task_scores": [
          -58.53,
          -27.09,
          -0.07,
          -3.41
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "20k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra"
    },
    {
      "id": 26,
      "name": "tulu-3-sft-personas-math-grade",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 22.55,
      "reasoning_avg": 20.09,
      "overall_avg": 24.06,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.67,
        21.54,
        0.43785714,
        45.71,
        33.0275,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.23,
        0.0,
        57.7,
        27.26,
        39.4,
        15.13,
        0.0
      ],
      "code_scores": [
        26.22,
        55.25,
        6.45,
        0.42,
        16.74,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68
      ],
      "reasoning_scores": [
        25.42,
        34.29,
        0.27228261,
        21.36,
        24.07,
        31.44,
        0.25282609,
        21.92,
        28.81,
        32.99,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.01
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.64
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.09
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.01,
        "math_avg": 14.1,
        "code_avg": 6.88,
        "reasoning_avg": -21.23,
        "overall_avg": 2.44,
        "general_task_scores": [
          27.19,
          11.67,
          4.68,
          -3.48
        ],
        "math_task_scores": [
          -0.13,
          21.51,
          37.73,
          10.29,
          1.11
        ],
        "code_task_scores": [
          11.18,
          1.17,
          3.83,
          0.49,
          17.72
        ],
        "reasoning_task_scores": [
          -54.24,
          -29.65,
          -0.05,
          -0.99
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "50k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade"
    },
    {
      "id": 27,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 34.8,
      "math_avg": 48.35,
      "code_avg": 11.81,
      "reasoning_avg": 16.92,
      "overall_avg": 27.97,
      "general_scores": [
        81.28,
        23.6725,
        32.08,
        0.0,
        83.17,
        24.2225,
        30.46,
        2.06785714,
        81.97,
        23.88,
        0.03357143
      ],
      "math_scores": [
        86.66,
        63.18,
        64.2,
        21.0,
        6.67,
        89.08,
        63.24,
        64.8,
        20.57,
        0.0,
        87.41,
        63.06,
        67.8,
        20.89,
        6.67
      ],
      "code_scores": [
        16.46,
        30.74,
        2.51,
        0.0,
        7.47,
        21.34,
        35.02,
        2.15,
        0.0,
        1.81,
        22.56,
        31.91,
        0.36,
        2.3,
        2.49
      ],
      "reasoning_scores": [
        45.76,
        20.33,
        0.18402174,
        0.16,
        47.8,
        22.91,
        0.18641304,
        0.4,
        44.41,
        20.61,
        0.18130435,
        0.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.72
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.67
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.77
              },
              {
                "metric": "lcb_test_output",
                "score": 3.92
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.99
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.28
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.79,
        "math_avg": 34.9,
        "code_avg": -3.87,
        "reasoning_avg": -24.41,
        "overall_avg": 6.35,
        "general_task_scores": [
          60.31,
          3.58,
          13.3,
          -3.22
        ],
        "math_task_scores": [
          31.92,
          57.66,
          64.0,
          16.48,
          4.45
        ],
        "code_task_scores": [
          0.0,
          -21.91,
          -1.91,
          0.56,
          3.92
        ],
        "reasoning_task_scores": [
          -34.35,
          -41.28,
          -0.13,
          -21.87
        ]
      },
      "affiliation": "Soochow Univ",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math"
    },
    {
      "id": 28,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 35.7,
      "math_avg": 4.41,
      "code_avg": 25.61,
      "reasoning_avg": 30.92,
      "overall_avg": 24.16,
      "general_scores": [
        55.24,
        45.455,
        33.64,
        9.75142857,
        57.68,
        44.5575,
        35.0,
        7.50071429,
        56.06,
        44.5625,
        31.89,
        7.05714286
      ],
      "math_scores": [
        3.34,
        4.52,
        5.4,
        6.39,
        0.0,
        3.49,
        5.0,
        6.6,
        6.28,
        3.33,
        3.18,
        4.42,
        4.6,
        6.23,
        3.33
      ],
      "code_scores": [
        34.15,
        46.69,
        0.0,
        26.1,
        23.08,
        32.93,
        42.41,
        0.0,
        25.05,
        23.08,
        31.1,
        47.08,
        0.0,
        27.14,
        25.34
      ],
      "reasoning_scores": [
        63.05,
        45.78,
        0.29,
        16.56,
        62.03,
        49.66,
        0.29076087,
        18.72,
        52.88,
        45.28,
        0.30923913,
        16.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.51
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.3
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 32.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 45.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.1
              },
              {
                "metric": "lcb_test_output",
                "score": 23.83
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.68,
        "math_avg": -9.04,
        "code_avg": 9.93,
        "reasoning_avg": -10.41,
        "overall_avg": 2.54,
        "general_task_scores": [
          34.5,
          24.52,
          15.54,
          4.18
        ],
        "math_task_scores": [
          -52.46,
          -0.85,
          3.93,
          1.96,
          2.22
        ],
        "code_task_scores": [
          12.61,
          -9.08,
          -3.58,
          25.89,
          23.83
        ],
        "reasoning_task_scores": [
          -21.02,
          -15.65,
          -0.01,
          -4.93
        ]
      },
      "affiliation": "glaiveAI",
      "year": "2023",
      "size": "20k",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"
    },
    {
      "id": 29,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 29.52,
      "math_avg": 13.17,
      "code_avg": 38.17,
      "reasoning_avg": 31.9,
      "overall_avg": 28.19,
      "general_scores": [
        32.19,
        37.875,
        30.06,
        12.8085714,
        30.96,
        41.305,
        33.27,
        9.16071429,
        46.25,
        39.6725,
        33.54,
        7.20142857
      ],
      "math_scores": [
        29.57,
        9.48,
        10.4,
        14.77,
        0.0,
        31.16,
        10.12,
        9.2,
        15.45,
        6.67,
        23.96,
        10.72,
        11.0,
        15.02,
        0.0
      ],
      "code_scores": [
        78.05,
        63.81,
        7.53,
        18.79,
        26.7,
        76.22,
        67.7,
        7.53,
        17.12,
        22.4,
        75.61,
        65.76,
        6.81,
        18.79,
        19.68
      ],
      "reasoning_scores": [
        52.88,
        48.99,
        0.27619565,
        20.08,
        61.02,
        48.14,
        0.30434783,
        24.24,
        57.97,
        46.37,
        0.28576087,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.62
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.29
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.72
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.23
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.11
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.63
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 65.76
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.29
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.23
              },
              {
                "metric": "lcb_test_output",
                "score": 22.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 47.83
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.51,
        "math_avg": -0.28,
        "code_avg": 22.49,
        "reasoning_avg": -9.42,
        "overall_avg": 6.57,
        "general_task_scores": [
          14.64,
          19.28,
          14.32,
          5.8
        ],
        "math_task_scores": [
          -27.57,
          4.61,
          8.6,
          10.74,
          2.22
        ],
        "code_task_scores": [
          56.51,
          11.29,
          3.71,
          18.02,
          22.93
        ],
        "reasoning_task_scores": [
          -23.05,
          -14.73,
          -0.02,
          0.11
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2"
    },
    {
      "id": 30,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 37.4,
      "math_avg": 13.97,
      "code_avg": 33.46,
      "reasoning_avg": 37.58,
      "overall_avg": 30.6,
      "general_scores": [
        53.9,
        45.0025,
        35.89,
        12.9128571,
        54.34,
        45.9125,
        33.82,
        17.3835714,
        54.99,
        44.7925,
        34.61,
        15.3
      ],
      "math_scores": [
        33.06,
        10.14,
        8.6,
        11.4,
        0.0,
        37.38,
        11.94,
        13.8,
        10.23,
        3.33,
        37.0,
        10.44,
        11.0,
        11.16,
        0.0
      ],
      "code_scores": [
        66.46,
        57.59,
        6.45,
        19.42,
        20.14,
        67.68,
        53.31,
        7.53,
        17.12,
        20.81,
        67.07,
        57.59,
        5.38,
        15.66,
        19.68
      ],
      "reasoning_scores": [
        73.22,
        54.55,
        0.26608696,
        25.84,
        70.85,
        53.7,
        0.24445652,
        26.96,
        70.51,
        50.86,
        0.25206522,
        23.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.4
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 21.39,
        "math_avg": 0.52,
        "code_avg": 17.78,
        "reasoning_avg": -3.75,
        "overall_avg": 8.99,
        "general_task_scores": [
          32.58,
          24.9,
          16.8,
          11.28
        ],
        "math_task_scores": [
          -19.99,
          5.34,
          9.53,
          6.59,
          1.11
        ],
        "code_task_scores": [
          46.95,
          1.69,
          2.87,
          17.19,
          20.21
        ],
        "reasoning_task_scores": [
          -8.81,
          -9.52,
          -0.06,
          3.41
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "157k",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction"
    },
    {
      "id": 31,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 34.16,
      "math_avg": 12.4,
      "code_avg": 37.19,
      "reasoning_avg": 36.84,
      "overall_avg": 30.15,
      "general_scores": [
        47.04,
        39.87,
        34.75,
        12.6614286,
        49.14,
        41.98,
        35.69,
        12.2407143,
        47.41,
        42.24,
        35.69,
        11.1778571
      ],
      "math_scores": [
        22.06,
        10.0,
        9.2,
        16.06,
        0.0,
        24.41,
        9.92,
        8.2,
        15.29,
        6.67,
        29.04,
        9.88,
        10.4,
        14.84,
        0.0
      ],
      "code_scores": [
        69.51,
        58.37,
        5.02,
        31.11,
        18.78,
        69.51,
        59.14,
        5.38,
        36.95,
        16.74,
        69.51,
        59.14,
        6.81,
        28.6,
        23.3
      ],
      "reasoning_scores": [
        64.07,
        53.03,
        0.25380435,
        26.0,
        68.47,
        53.36,
        0.27521739,
        26.08,
        69.15,
        54.76,
        0.27380435,
        26.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.74
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.22
              },
              {
                "metric": "lcb_test_output",
                "score": 19.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.13
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.14,
        "math_avg": -1.05,
        "code_avg": 21.52,
        "reasoning_avg": -4.49,
        "overall_avg": 8.53,
        "general_task_scores": [
          26.03,
          21.02,
          17.41,
          8.11
        ],
        "math_task_scores": [
          -30.63,
          4.43,
          7.67,
          11.06,
          2.22
        ],
        "code_task_scores": [
          49.39,
          4.41,
          2.16,
          32.01,
          19.61
        ],
        "reasoning_task_scores": [
          -13.11,
          -8.84,
          -0.04,
          4.05
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "111k",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1"
    },
    {
      "id": 32,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 33.9,
      "math_avg": 11.25,
      "code_avg": 35.88,
      "reasoning_avg": 36.22,
      "overall_avg": 29.31,
      "general_scores": [
        50.66,
        41.49,
        34.39,
        13.7185714,
        38.57,
        44.045,
        34.82,
        13.0278571,
        47.54,
        42.6375,
        36.22,
        9.72642857
      ],
      "math_scores": [
        22.67,
        9.46,
        7.2,
        14.7,
        0.0,
        28.35,
        9.62,
        8.2,
        15.99,
        0.0,
        22.67,
        8.22,
        7.8,
        13.91,
        0.0
      ],
      "code_scores": [
        66.46,
        59.92,
        3.58,
        27.14,
        19.68,
        70.73,
        58.37,
        4.3,
        26.1,
        24.21,
        72.56,
        56.81,
        4.3,
        23.17,
        20.81
      ],
      "reasoning_scores": [
        68.47,
        53.95,
        0.27586957,
        26.4,
        62.71,
        53.95,
        0.28597826,
        26.56,
        60.34,
        54.85,
        0.29130435,
        26.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.72
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.47
              },
              {
                "metric": "lcb_test_output",
                "score": 21.57
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.84
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.25
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.89,
        "math_avg": -2.2,
        "code_avg": 20.2,
        "reasoning_avg": -5.1,
        "overall_avg": 7.7,
        "general_task_scores": [
          23.76,
          22.38,
          17.17,
          8.24
        ],
        "math_task_scores": [
          -31.24,
          3.6,
          6.13,
          10.53,
          0.0
        ],
        "code_task_scores": [
          49.8,
          3.9,
          0.48,
          25.26,
          21.57
        ],
        "reasoning_task_scores": [
          -16.5,
          -8.31,
          -0.03,
          4.43
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "110k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K"
    },
    {
      "id": 33,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 33.31,
      "math_avg": 7.77,
      "code_avg": 20.72,
      "reasoning_avg": 28.51,
      "overall_avg": 22.58,
      "general_scores": [
        63.06,
        35.24,
        33.08,
        3.01714286,
        60.64,
        35.8975,
        33.1,
        2.31428571,
        61.74,
        34.9925,
        32.61,
        4.02285714
      ],
      "math_scores": [
        16.91,
        7.08,
        6.0,
        8.15,
        0.0,
        14.18,
        7.44,
        8.0,
        8.58,
        0.0,
        14.56,
        7.94,
        9.2,
        8.45,
        0.0
      ],
      "code_scores": [
        47.56,
        52.53,
        2.51,
        0.21,
        0.23,
        45.73,
        52.53,
        3.94,
        0.0,
        0.0,
        48.17,
        53.7,
        2.87,
        0.84,
        0.0
      ],
      "reasoning_scores": [
        64.41,
        27.74,
        0.22836957,
        18.24,
        65.08,
        28.16,
        0.2373913,
        17.6,
        66.78,
        37.2,
        0.23119565,
        16.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 17.29,
        "math_avg": -5.68,
        "code_avg": 5.05,
        "reasoning_avg": -12.81,
        "overall_avg": 0.96,
        "general_task_scores": [
          39.98,
          15.04,
          14.96,
          -0.8
        ],
        "math_task_scores": [
          -40.58,
          1.99,
          6.13,
          4.05,
          0.0
        ],
        "code_task_scores": [
          27.03,
          -1.55,
          -0.47,
          0.14,
          0.08
        ],
        "reasoning_task_scores": [
          -14.92,
          -31.53,
          -0.08,
          -4.72
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "75.2k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K"
    },
    {
      "id": 34,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 32.25,
      "math_avg": 9.75,
      "code_avg": 29.19,
      "reasoning_avg": 27.02,
      "overall_avg": 24.55,
      "general_scores": [
        45.42,
        42.0775,
        31.85,
        11.7392857,
        54.05,
        37.0,
        33.67,
        10.0478571,
        42.18,
        37.8975,
        31.69,
        9.36214286
      ],
      "math_scores": [
        13.72,
        5.96,
        6.0,
        17.48,
        6.67,
        14.03,
        5.86,
        6.2,
        17.66,
        0.0,
        17.82,
        7.16,
        7.0,
        17.34,
        3.33
      ],
      "code_scores": [
        54.27,
        59.14,
        8.96,
        0.42,
        20.36,
        57.93,
        58.37,
        6.81,
        0.21,
        24.89,
        61.59,
        56.42,
        7.53,
        0.21,
        20.81
      ],
      "reasoning_scores": [
        55.59,
        29.31,
        0.22,
        18.08,
        61.69,
        32.06,
        0.23684783,
        15.68,
        60.34,
        32.29,
        0.22423913,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.77
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 22.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.21
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.22
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.41
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 16.23,
        "math_avg": -3.7,
        "code_avg": 13.52,
        "reasoning_avg": -14.31,
        "overall_avg": 2.94,
        "general_task_scores": [
          25.39,
          18.65,
          14.43,
          6.46
        ],
        "math_task_scores": [
          -40.61,
          0.83,
          4.8,
          13.15,
          3.33
        ],
        "code_task_scores": [
          37.81,
          3.51,
          4.19,
          0.07,
          22.02
        ],
        "reasoning_task_scores": [
          -21.13,
          -31.34,
          -0.08,
          -4.67
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "66.4k",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback"
    },
    {
      "id": 35,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 18.3,
      "math_avg": 2.27,
      "code_avg": 13.69,
      "reasoning_avg": 24.1,
      "overall_avg": 14.59,
      "general_scores": [
        17.96,
        24.61,
        26.21,
        3.15428571,
        15.06,
        23.915,
        25.4,
        6.20571429,
        21.4,
        23.7825,
        24.58,
        7.35571429
      ],
      "math_scores": [
        1.82,
        2.14,
        3.4,
        2.76,
        0.0,
        2.81,
        2.5,
        2.6,
        3.21,
        0.0,
        2.96,
        3.12,
        4.2,
        2.51,
        0.0
      ],
      "code_scores": [
        14.02,
        47.47,
        0.36,
        12.94,
        0.45,
        8.54,
        46.3,
        0.72,
        10.44,
        4.3,
        6.71,
        46.69,
        1.08,
        4.18,
        1.13
      ],
      "reasoning_scores": [
        40.68,
        48.76,
        0.15815217,
        16.64,
        34.24,
        37.61,
        0.14782609,
        15.04,
        37.97,
        40.61,
        0.14130435,
        17.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.19
              },
              {
                "metric": "lcb_test_output",
                "score": 1.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.63
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.33
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.29
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.29,
        "math_avg": -11.18,
        "code_avg": -1.99,
        "reasoning_avg": -17.22,
        "overall_avg": -7.03,
        "general_task_scores": [
          -3.69,
          3.76,
          7.43,
          1.65
        ],
        "math_task_scores": [
          -53.27,
          -2.91,
          1.8,
          -1.51,
          0.0
        ],
        "code_task_scores": [
          -10.36,
          -7.65,
          -2.86,
          8.98,
          1.96
        ],
        "reasoning_task_scores": [
          -42.71,
          -20.23,
          -0.16,
          -5.79
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "55.1k",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT"
    },
    {
      "id": 36,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 22.02,
      "math_avg": 7.57,
      "code_avg": 25.3,
      "reasoning_avg": 17.45,
      "overall_avg": 18.09,
      "general_scores": [
        32.95,
        30.26,
        20.24,
        4.34785714,
        31.67,
        34.065,
        22.19,
        8.03071429,
        24.39,
        29.7375,
        21.23,
        5.07357143
      ],
      "math_scores": [
        8.64,
        4.24,
        4.0,
        15.65,
        0.0,
        15.85,
        4.66,
        6.0,
        16.46,
        0.0,
        11.6,
        4.98,
        5.8,
        15.74,
        0.0
      ],
      "code_scores": [
        48.17,
        55.64,
        3.58,
        0.21,
        18.78,
        46.34,
        55.25,
        3.94,
        0.21,
        19.91,
        45.12,
        57.2,
        3.23,
        0.0,
        21.95
      ],
      "reasoning_scores": [
        30.51,
        26.58,
        0.14956522,
        7.6,
        32.54,
        33.66,
        0.15380435,
        9.92,
        31.19,
        29.33,
        0.15793478,
        7.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 31.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.82
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.63
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.95
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.03
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.14
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.0,
        "math_avg": -5.87,
        "code_avg": 9.63,
        "reasoning_avg": -23.87,
        "overall_avg": -3.53,
        "general_task_scores": [
          7.84,
          11.01,
          3.25,
          1.9
        ],
        "math_task_scores": [
          -43.77,
          -0.87,
          3.67,
          11.61,
          0.0
        ],
        "code_task_scores": [
          26.42,
          1.56,
          0.0,
          -0.07,
          20.21
        ],
        "reasoning_task_scores": [
          -48.93,
          -32.7,
          -0.16,
          -13.71
        ]
      },
      "affiliation": "bigcode (Huggingface)",
      "year": "2024",
      "size": "50.7k",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k"
    },
    {
      "id": 37,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 35.27,
      "math_avg": 15.37,
      "code_avg": 33.27,
      "reasoning_avg": 35.26,
      "overall_avg": 29.79,
      "general_scores": [
        41.75,
        46.4425,
        31.8,
        13.5342857,
        49.1,
        47.25,
        32.37,
        19.9864286,
        50.57,
        46.66,
        32.78,
        11.0335714
      ],
      "math_scores": [
        50.19,
        12.72,
        8.6,
        7.0,
        0.0,
        50.04,
        12.3,
        8.0,
        7.45,
        0.0,
        50.8,
        12.6,
        3.2,
        7.59,
        0.0
      ],
      "code_scores": [
        62.2,
        32.3,
        3.23,
        29.23,
        19.23,
        65.24,
        59.92,
        2.87,
        30.48,
        24.66,
        64.02,
        54.47,
        2.51,
        31.73,
        16.97
      ],
      "reasoning_scores": [
        66.44,
        51.65,
        0.39184783,
        21.84,
        63.39,
        52.43,
        0.40597826,
        23.52,
        67.8,
        50.81,
        0.35032609,
        24.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 20.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.88
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.26,
        "math_avg": 1.92,
        "code_avg": 17.59,
        "reasoning_avg": -6.06,
        "overall_avg": 8.18,
        "general_task_scores": [
          25.31,
          26.44,
          14.35,
          10.93
        ],
        "math_task_scores": [
          -5.46,
          7.04,
          5.0,
          3.01,
          0.0
        ],
        "code_task_scores": [
          43.7,
          -5.57,
          -0.71,
          30.27,
          20.29
        ],
        "reasoning_task_scores": [
          -14.46,
          -10.93,
          0.07,
          1.07
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1"
    },
    {
      "id": 38,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 21.49,
      "math_avg": 4.36,
      "code_avg": 18.88,
      "reasoning_avg": 20.0,
      "overall_avg": 16.18,
      "general_scores": [
        9.02,
        37.9425,
        17.48,
        8.68857143,
        16.57,
        35.685,
        19.77,
        13.1114286,
        29.23,
        39.87,
        17.6,
        12.875
      ],
      "math_scores": [
        5.46,
        1.94,
        2.0,
        14.54,
        0.0,
        1.52,
        1.72,
        1.6,
        16.62,
        0.0,
        2.65,
        1.88,
        3.2,
        12.33,
        0.0
      ],
      "code_scores": [
        41.46,
        50.97,
        1.43,
        1.25,
        0.23,
        41.46,
        47.47,
        1.43,
        4.59,
        0.9,
        42.68,
        47.86,
        0.36,
        0.63,
        0.45
      ],
      "reasoning_scores": [
        16.61,
        46.75,
        0.10391304,
        12.08,
        22.71,
        44.4,
        0.10847826,
        10.4,
        22.37,
        47.29,
        0.09326087,
        17.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.56
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 46.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.47,
        "math_avg": -9.08,
        "code_avg": 3.2,
        "reasoning_avg": -21.33,
        "overall_avg": -5.43,
        "general_task_scores": [
          -3.56,
          17.49,
          0.31,
          7.64
        ],
        "math_task_scores": [
          -52.59,
          -3.65,
          0.67,
          10.16,
          0.0
        ],
        "code_task_scores": [
          21.75,
          -5.7,
          -2.51,
          1.95,
          0.53
        ],
        "reasoning_task_scores": [
          -59.78,
          -16.41,
          -0.21,
          -8.91
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "35k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code"
    },
    {
      "id": 39,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 20.84,
      "math_avg": 3.44,
      "code_avg": 24.78,
      "reasoning_avg": 5.94,
      "overall_avg": 13.75,
      "general_scores": [
        20.64,
        30.5625,
        21.16,
        12.3721429,
        23.19,
        29.95,
        22.1,
        12.3792857,
        22.08,
        31.2475,
        16.43,
        7.91285714
      ],
      "math_scores": [
        0.23,
        0.08,
        0.2,
        16.73,
        0.0,
        0.0,
        0.02,
        0.0,
        18.34,
        0.0,
        0.08,
        0.02,
        0.0,
        15.85,
        0.0
      ],
      "code_scores": [
        60.98,
        60.7,
        0.0,
        0.42,
        0.0,
        65.24,
        57.2,
        0.0,
        0.21,
        0.23,
        65.24,
        61.48,
        0.0,
        0.0,
        0.0
      ],
      "reasoning_scores": [
        13.9,
        3.87,
        0.14717391,
        8.0,
        13.9,
        3.95,
        0.14663043,
        5.84,
        14.24,
        4.39,
        0.15869565,
        2.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.97
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 59.79
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.21
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 4.82,
        "math_avg": -10.01,
        "code_avg": 9.1,
        "reasoning_avg": -35.38,
        "overall_avg": -7.87,
        "general_task_scores": [
          0.14,
          10.25,
          1.93,
          6.97
        ],
        "math_task_scores": [
          -55.7,
          -5.46,
          -1.53,
          12.63,
          0.0
        ],
        "code_task_scores": [
          43.7,
          5.32,
          -3.58,
          0.0,
          0.08
        ],
        "reasoning_task_scores": [
          -66.33,
          -58.49,
          -0.16,
          -16.56
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "444k",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1"
    },
    {
      "id": 40,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 27.09,
      "math_avg": 4.74,
      "code_avg": 14.94,
      "reasoning_avg": 27.81,
      "overall_avg": 18.65,
      "general_scores": [
        45.84,
        37.3625,
        18.36,
        7.40285714,
        41.66,
        39.5775,
        15.6,
        5.87214286,
        51.92,
        38.46,
        19.07,
        3.98
      ],
      "math_scores": [
        5.08,
        3.94,
        5.2,
        13.53,
        0.0,
        3.64,
        4.1,
        4.8,
        8.42,
        0.0,
        4.85,
        3.4,
        4.0,
        10.21,
        0.0
      ],
      "code_scores": [
        21.95,
        37.74,
        0.0,
        11.06,
        1.13,
        20.12,
        39.69,
        0.0,
        19.21,
        0.9,
        20.73,
        36.96,
        0.0,
        13.78,
        0.9
      ],
      "reasoning_scores": [
        37.29,
        50.86,
        0.23043478,
        17.76,
        43.73,
        49.59,
        0.26793478,
        18.96,
        44.07,
        51.84,
        0.23804348,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.68
              },
              {
                "metric": "lcb_test_output",
                "score": 0.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.76
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.08,
        "math_avg": -8.7,
        "code_avg": -0.73,
        "reasoning_avg": -13.51,
        "overall_avg": -2.97,
        "general_task_scores": [
          24.64,
          18.13,
          -0.29,
          1.83
        ],
        "math_task_scores": [
          -51.28,
          -1.69,
          3.07,
          6.38,
          0.0
        ],
        "code_task_scores": [
          0.81,
          -16.34,
          -3.58,
          14.47,
          0.98
        ],
        "reasoning_task_scores": [
          -38.64,
          -11.8,
          -0.06,
          -3.55
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "5k",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder"
    },
    {
      "id": 41,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 36.01,
      "math_avg": 11.62,
      "code_avg": 29.55,
      "reasoning_avg": 38.52,
      "overall_avg": 28.93,
      "general_scores": [
        43.11,
        48.8325,
        36.7,
        9.22214286,
        47.59,
        47.2175,
        36.09,
        17.1,
        50.64,
        47.5275,
        36.17,
        11.9664286
      ],
      "math_scores": [
        35.33,
        10.36,
        10.0,
        7.0,
        0.0,
        27.6,
        10.16,
        8.6,
        6.71,
        0.0,
        31.39,
        9.84,
        9.8,
        7.45,
        0.0
      ],
      "code_scores": [
        52.44,
        53.7,
        1.08,
        23.8,
        19.68,
        46.95,
        53.7,
        2.51,
        29.02,
        12.9,
        48.78,
        53.31,
        1.08,
        26.72,
        17.65
      ],
      "reasoning_scores": [
        77.97,
        54.67,
        0.35402174,
        21.92,
        77.97,
        53.19,
        0.36195652,
        20.48,
        78.31,
        55.03,
        0.35543478,
        21.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.56
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.51
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.08
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.3
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 20.0,
        "math_avg": -1.83,
        "code_avg": 13.88,
        "reasoning_avg": -2.81,
        "overall_avg": 7.31,
        "general_task_scores": [
          25.28,
          27.52,
          18.35,
          8.84
        ],
        "math_task_scores": [
          -24.36,
          4.62,
          7.87,
          2.71,
          0.0
        ],
        "code_task_scores": [
          29.27,
          -0.9,
          -2.02,
          26.3,
          16.74
        ],
        "reasoning_task_scores": [
          -2.26,
          -8.26,
          0.05,
          -0.75
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1"
    },
    {
      "id": 42,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 35.01,
      "math_avg": 5.56,
      "code_avg": 22.51,
      "reasoning_avg": 29.54,
      "overall_avg": 23.15,
      "general_scores": [
        48.8,
        42.0,
        29.99,
        21.575,
        48.26,
        44.71,
        27.82,
        18.9085714,
        48.3,
        43.045,
        29.01,
        17.6428571
      ],
      "math_scores": [
        8.72,
        6.84,
        7.2,
        6.59,
        0.0,
        9.86,
        6.38,
        7.6,
        6.66,
        0.0,
        5.53,
        5.96,
        5.6,
        6.46,
        0.0
      ],
      "code_scores": [
        43.29,
        50.58,
        0.0,
        15.24,
        6.33,
        40.85,
        54.09,
        0.0,
        13.78,
        4.07,
        42.68,
        45.53,
        0.0,
        5.64,
        15.61
      ],
      "reasoning_scores": [
        45.42,
        45.04,
        0.28347826,
        21.6,
        46.1,
        46.26,
        0.275,
        18.96,
        62.37,
        46.6,
        0.27206522,
        21.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.55
              },
              {
                "metric": "lcb_test_output",
                "score": 8.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.97
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.61
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 18.99,
        "math_avg": -7.89,
        "code_avg": 6.84,
        "reasoning_avg": -11.78,
        "overall_avg": 1.54,
        "general_task_scores": [
          26.62,
          22.91,
          10.97,
          15.46
        ],
        "math_task_scores": [
          -47.76,
          0.89,
          5.2,
          2.23,
          0.0
        ],
        "code_task_scores": [
          22.15,
          -4.4,
          -3.58,
          11.34,
          8.67
        ],
        "reasoning_task_scores": [
          -29.04,
          -16.59,
          -0.03,
          -1.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca"
    },
    {
      "id": 43,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 39.97,
      "math_avg": 7.14,
      "code_avg": 23.02,
      "reasoning_avg": 36.11,
      "overall_avg": 26.56,
      "general_scores": [
        46.95,
        46.8975,
        32.6,
        29.5671429,
        51.47,
        48.0325,
        33.12,
        27.4607143,
        53.45,
        47.0875,
        34.54,
        28.4692857
      ],
      "math_scores": [
        7.51,
        8.14,
        11.0,
        7.36,
        0.0,
        10.24,
        8.12,
        9.0,
        7.38,
        3.33,
        8.34,
        7.44,
        9.0,
        6.91,
        3.33
      ],
      "code_scores": [
        37.2,
        54.47,
        0.0,
        19.62,
        9.05,
        36.59,
        52.53,
        0.0,
        10.86,
        11.31,
        39.63,
        54.47,
        0.0,
        6.05,
        13.57
      ],
      "reasoning_scores": [
        70.85,
        53.47,
        0.30804348,
        23.6,
        64.07,
        53.88,
        0.32304348,
        23.44,
        65.76,
        53.76,
        0.30043478,
        23.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.18
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.89
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.7
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.55
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 23.96,
        "math_avg": -6.31,
        "code_avg": 7.35,
        "reasoning_avg": -5.21,
        "overall_avg": 4.95,
        "general_task_scores": [
          28.79,
          27.0,
          15.45,
          24.58
        ],
        "math_task_scores": [
          -47.1,
          2.4,
          8.07,
          2.88,
          2.22
        ],
        "code_task_scores": [
          17.69,
          -0.65,
          -3.58,
          11.97,
          11.31
        ],
        "reasoning_task_scores": [
          -13.45,
          -8.86,
          -0.0,
          1.47
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "18.6k",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca"
    },
    {
      "id": 44,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.16,
      "math_avg": 9.34,
      "code_avg": 29.57,
      "reasoning_avg": 34.15,
      "overall_avg": 28.81,
      "general_scores": [
        66.16,
        45.2575,
        34.66,
        21.9871429,
        66.55,
        44.4375,
        35.39,
        23.7114286,
        66.19,
        44.0875,
        34.03,
        23.5171429
      ],
      "math_scores": [
        21.46,
        9.32,
        10.0,
        8.2,
        0.0,
        21.08,
        9.22,
        8.8,
        8.31,
        0.0,
        17.06,
        8.54,
        9.8,
        8.27,
        0.0
      ],
      "code_scores": [
        51.22,
        56.42,
        7.89,
        16.91,
        22.17,
        48.78,
        55.64,
        8.24,
        5.22,
        25.34,
        47.56,
        54.09,
        6.45,
        14.61,
        23.08
      ],
      "reasoning_scores": [
        53.56,
        57.22,
        0.29184783,
        26.0,
        53.22,
        57.96,
        0.29641304,
        26.08,
        51.19,
        57.88,
        0.3075,
        25.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.25
              },
              {
                "metric": "lcb_test_output",
                "score": 23.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.66
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 26.15,
        "math_avg": -4.11,
        "code_avg": 13.9,
        "reasoning_avg": -7.17,
        "overall_avg": 7.19,
        "general_task_scores": [
          44.47,
          24.25,
          16.72,
          19.15
        ],
        "math_task_scores": [
          -35.93,
          3.53,
          7.93,
          3.92,
          0.0
        ],
        "code_task_scores": [
          29.07,
          0.91,
          3.95,
          12.04,
          23.53
        ],
        "reasoning_task_scores": [
          -27.68,
          -4.87,
          -0.01,
          3.89
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "22.6k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT"
    },
    {
      "id": 45,
      "name": "Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 21.46,
      "math_avg": 20.94,
      "code_avg": 21.8,
      "reasoning_avg": 18.47,
      "overall_avg": 20.67,
      "general_scores": [
        31.42,
        28.3025,
        25.66,
        0.12142857,
        38.8,
        27.275,
        24.39,
        0.105,
        29.52,
        27.4475,
        24.46,
        0.065
      ],
      "math_scores": [
        4.4,
        42.86,
        40.8,
        15.22,
        0.0,
        3.49,
        42.54,
        40.0,
        15.06,
        0.0,
        5.08,
        44.16,
        42.2,
        15.0,
        3.33
      ],
      "code_scores": [
        38.41,
        56.03,
        9.68,
        9.6,
        0.45,
        42.68,
        52.53,
        9.32,
        2.51,
        0.0,
        39.02,
        52.92,
        8.96,
        4.18,
        0.68
      ],
      "reasoning_scores": [
        47.12,
        22.95,
        0.17032609,
        7.12,
        45.42,
        22.43,
        0.16684783,
        6.48,
        40.68,
        22.03,
        0.16467391,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.43
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.47
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.85
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.45,
        "math_avg": 7.49,
        "code_avg": 6.12,
        "reasoning_avg": -22.85,
        "overall_avg": -0.95,
        "general_task_scores": [
          11.42,
          7.34,
          6.87,
          -3.82
        ],
        "math_task_scores": [
          -51.48,
          37.69,
          39.4,
          10.75,
          1.11
        ],
        "code_task_scores": [
          19.92,
          -0.64,
          5.74,
          5.22,
          0.38
        ],
        "reasoning_task_scores": [
          -35.93,
          -40.09,
          -0.14,
          -15.23
        ]
      },
      "affiliation": "Bespoke Labs",
      "year": "2025",
      "size": "17k",
      "link": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k"
    }
  ],
  "qwen": [
    {
      "id": 0,
      "name": "base",
      "domain": "base",
      "general_avg": 43.31,
      "math_avg": 50.47,
      "code_avg": 38.72,
      "reasoning_avg": 34.93,
      "overall_avg": 41.86,
      "general_task_scores": [
        66.99,
        35.49,
        44.86,
        25.91
      ],
      "math_task_scores": [
        87.34,
        64.88,
        67.4,
        26.04,
        6.67
      ],
      "code_task_scores": [
        75.61,
        71.6,
        8.24,
        1.04,
        37.1
      ],
      "reasoning_task_scores": [
        36.6,
        69.46,
        0.392,
        33.28
      ]
    },
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 51.61,
      "math_avg": 39.59,
      "code_avg": 48.03,
      "reasoning_avg": 32.15,
      "overall_avg": 42.85,
      "general_scores": [
        64.81,
        43.28,
        56.03,
        40.0042857,
        64.89,
        45.345,
        56.56,
        39.2021429,
        67.31,
        46.2225,
        56.54,
        39.1364286
      ],
      "math_scores": [
        80.44,
        49.06,
        50.6,
        19.04,
        3.33,
        80.89,
        48.22,
        47.0,
        18.9,
        0.0,
        78.47,
        49.12,
        48.8,
        20.03,
        0.0
      ],
      "code_scores": [
        76.22,
        73.93,
        12.9,
        41.54,
        41.63,
        75.61,
        74.71,
        12.54,
        42.17,
        30.09,
        75.61,
        73.93,
        12.54,
        41.34,
        35.75
      ],
      "reasoning_scores": [
        27.8,
        66.9,
        0.46956522,
        30.08,
        28.47,
        67.1,
        0.44793478,
        29.76,
        35.59,
        67.55,
        0.46315217,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.32
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.68
              },
              {
                "metric": "lcb_test_output",
                "score": 35.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.18
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.3,
        "math_avg": -10.87,
        "code_avg": 9.32,
        "reasoning_avg": -2.79,
        "overall_avg": 0.99,
        "general_task_scores": [
          -1.32,
          9.46,
          11.52,
          13.54
        ],
        "math_task_scores": [
          -7.41,
          -16.08,
          -18.6,
          -6.72,
          -5.56
        ],
        "code_task_scores": [
          0.2,
          2.59,
          4.42,
          40.64,
          -1.28
        ],
        "reasoning_task_scores": [
          -5.98,
          -2.28,
          0.07,
          -2.96
        ]
      },
      "affiliation": "nomic-ai",
      "year": "2023",
      "size": "809k",
      "link": "https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations"
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.35,
      "math_avg": 37.16,
      "code_avg": 46.75,
      "reasoning_avg": 32.24,
      "overall_avg": 42.88,
      "general_scores": [
        65.44,
        54.5275,
        57.92,
        41.5621429,
        65.7,
        54.805,
        58.13,
        43.3885714,
        65.7,
        55.52,
        58.19,
        43.3107143
      ],
      "math_scores": [
        80.59,
        42.94,
        42.6,
        13.28,
        3.33,
        80.74,
        42.5,
        43.2,
        13.62,
        6.67,
        80.29,
        43.42,
        44.0,
        13.62,
        6.67
      ],
      "code_scores": [
        71.34,
        71.98,
        10.75,
        42.38,
        37.1,
        72.56,
        72.76,
        11.11,
        42.59,
        33.26,
        73.17,
        72.37,
        9.68,
        41.13,
        39.14
      ],
      "reasoning_scores": [
        29.49,
        68.37,
        0.38934783,
        28.88,
        29.49,
        68.65,
        0.39,
        29.84,
        32.88,
        68.45,
        0.39347826,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 36.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.62
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.04,
        "math_avg": -13.3,
        "code_avg": 8.04,
        "reasoning_avg": -2.69,
        "overall_avg": 1.02,
        "general_task_scores": [
          -1.38,
          19.46,
          13.22,
          16.84
        ],
        "math_task_scores": [
          -6.8,
          -21.93,
          -24.13,
          -12.53,
          -1.11
        ],
        "code_task_scores": [
          -3.25,
          0.77,
          2.27,
          40.99,
          -0.6
        ],
        "reasoning_task_scores": [
          -5.98,
          -0.97,
          -0.0,
          -3.81
        ]
      },
      "affiliation": "tatsu-lab",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca"
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 55.85,
      "math_avg": 44.42,
      "code_avg": 49.88,
      "reasoning_avg": 33.76,
      "overall_avg": 45.98,
      "general_scores": [
        72.8,
        51.095,
        58.83,
        42.3871429,
        73.14,
        50.86,
        58.98,
        39.0971429,
        72.02,
        51.565,
        58.89,
        40.5192857
      ],
      "math_scores": [
        87.95,
        53.36,
        55.2,
        21.3,
        6.67,
        87.19,
        53.06,
        53.2,
        21.48,
        6.67,
        86.88,
        52.8,
        52.4,
        21.48,
        6.67
      ],
      "code_scores": [
        76.22,
        72.76,
        12.54,
        37.16,
        46.61,
        76.83,
        75.1,
        11.47,
        39.04,
        47.51,
        77.44,
        73.15,
        13.26,
        41.54,
        47.51
      ],
      "reasoning_scores": [
        36.27,
        66.0,
        0.45934783,
        33.6,
        34.24,
        65.93,
        0.45108696,
        33.68,
        35.59,
        66.24,
        0.45119565,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.67
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.42
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.42
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.25
              },
              {
                "metric": "lcb_test_output",
                "score": 47.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.17
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.54,
        "math_avg": -6.05,
        "code_avg": 11.16,
        "reasoning_avg": -1.17,
        "overall_avg": 4.12,
        "general_task_scores": [
          5.66,
          15.68,
          14.04,
          14.76
        ],
        "math_task_scores": [
          0.0,
          -11.81,
          -13.8,
          -4.62,
          0.0
        ],
        "code_task_scores": [
          1.22,
          2.07,
          4.18,
          38.21,
          10.11
        ],
        "reasoning_task_scores": [
          -1.23,
          -3.4,
          0.06,
          -0.11
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "52k",
      "link": "https://huggingface.co/datasets/vicgalle/alpaca-gpt4"
    },
    {
      "id": 4,
      "name": "Dolly",
      "domain": "general",
      "general_avg": 52.22,
      "math_avg": 39.73,
      "code_avg": 45.41,
      "reasoning_avg": 32.38,
      "overall_avg": 42.44,
      "general_scores": [
        71.97,
        45.8825,
        58.32,
        32.6907143,
        71.08,
        44.0925,
        58.16,
        34.7157143,
        74.04,
        43.6075,
        57.55,
        34.5392857
      ],
      "math_scores": [
        75.59,
        47.64,
        42.8,
        20.91,
        10.0,
        73.69,
        48.24,
        46.0,
        20.75,
        13.33,
        74.68,
        48.18,
        44.0,
        20.19,
        10.0
      ],
      "code_scores": [
        73.78,
        73.15,
        11.11,
        41.96,
        30.77,
        73.78,
        71.21,
        10.04,
        44.05,
        26.92,
        73.17,
        72.37,
        11.83,
        42.38,
        24.66
      ],
      "reasoning_scores": [
        33.9,
        65.45,
        0.4548913,
        27.84,
        34.58,
        65.32,
        0.44652174,
        26.16,
        38.98,
        65.98,
        0.44967391,
        28.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.53
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.01
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 33.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.02
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.62
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.99
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 27.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.58
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.65
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.91,
        "math_avg": -10.73,
        "code_avg": 6.69,
        "reasoning_avg": -2.56,
        "overall_avg": 0.58,
        "general_task_scores": [
          5.37,
          9.04,
          13.15,
          8.07
        ],
        "math_task_scores": [
          -12.69,
          -16.86,
          -23.13,
          -5.42,
          4.44
        ],
        "code_task_scores": [
          -2.03,
          0.64,
          2.75,
          41.76,
          -9.65
        ],
        "reasoning_task_scores": [
          -0.78,
          -3.88,
          0.06,
          -5.63
        ]
      },
      "affiliation": "databricks",
      "year": "2023",
      "size": "15k",
      "link": "https://huggingface.co/datasets/databricks/databricks-dolly-15k"
    },
    {
      "id": 5,
      "name": "WizardLM_evol_instruct_70k",
      "domain": "general",
      "general_avg": 55.11,
      "math_avg": 50.21,
      "code_avg": 50.24,
      "reasoning_avg": 34.93,
      "overall_avg": 47.62,
      "general_scores": [
        67.49,
        53.35,
        56.41,
        41.225,
        69.97,
        52.2825,
        56.43,
        41.0078571,
        69.02,
        55.37,
        56.57,
        42.1971429
      ],
      "math_scores": [
        84.53,
        47.66,
        48.4,
        18.54,
        84.31,
        47.94,
        51.2,
        19.11,
        83.32,
        47.98,
        51.0,
        18.56
      ],
      "code_scores": [
        78.66,
        73.54,
        10.75,
        43.01,
        43.21,
        78.66,
        73.15,
        12.19,
        42.59,
        47.29,
        76.22,
        73.93,
        11.47,
        42.38,
        46.61
      ],
      "reasoning_scores": [
        37.29,
        66.88,
        0.45163043,
        36.16,
        37.29,
        67.34,
        0.44543478,
        36.32,
        32.54,
        67.74,
        0.44858696,
        36.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.67
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.48
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.05
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.74
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.85
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.66
              },
              {
                "metric": "lcb_test_output",
                "score": 45.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.32
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.45
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.24
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.8,
        "math_avg": -0.25,
        "code_avg": 11.53,
        "reasoning_avg": -0.0,
        "overall_avg": 5.77,
        "general_task_scores": [
          1.84,
          18.18,
          11.61,
          15.57
        ],
        "math_task_scores": [
          -3.29,
          -17.02,
          -17.2,
          -7.3
        ],
        "code_task_scores": [
          2.24,
          1.94,
          3.23,
          41.62,
          8.6
        ],
        "reasoning_task_scores": [
          -0.89,
          -2.14,
          0.06,
          2.96
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "70k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k"
    },
    {
      "id": 6,
      "name": "WizardLM_evol_instruct_V2_196k",
      "domain": "general",
      "general_avg": 55.43,
      "math_avg": 39.17,
      "code_avg": 49.71,
      "reasoning_avg": 34.1,
      "overall_avg": 44.6,
      "general_scores": [
        71.25,
        52.5275,
        58.1,
        40.08,
        72.25,
        53.2075,
        58.15,
        39.6814286,
        71.33,
        51.6175,
        57.95,
        39.0557143
      ],
      "math_scores": [
        82.18,
        47.22,
        47.6,
        19.26,
        0.0,
        81.2,
        48.04,
        48.8,
        19.31,
        0.0,
        80.89,
        47.14,
        46.8,
        19.13,
        0.0
      ],
      "code_scores": [
        80.49,
        74.71,
        11.11,
        40.71,
        42.99,
        78.05,
        74.32,
        10.75,
        41.96,
        42.08,
        78.66,
        72.76,
        11.83,
        40.71,
        44.57
      ],
      "reasoning_scores": [
        32.2,
        67.14,
        0.41554348,
        35.36,
        35.25,
        66.9,
        0.41228261,
        36.4,
        32.2,
        67.14,
        0.41163043,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.61
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.13
              },
              {
                "metric": "lcb_test_output",
                "score": 43.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.06
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.71
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.12,
        "math_avg": -11.29,
        "code_avg": 11.0,
        "reasoning_avg": -0.83,
        "overall_avg": 2.75,
        "general_task_scores": [
          4.62,
          16.96,
          13.21,
          13.7
        ],
        "math_task_scores": [
          -5.92,
          -17.41,
          -19.67,
          -6.81,
          -6.67
        ],
        "code_task_scores": [
          3.46,
          2.33,
          2.99,
          40.09,
          6.11
        ],
        "reasoning_task_scores": [
          -3.38,
          -2.4,
          0.02,
          2.43
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "143k",
      "link": "https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_V2_196k"
    },
    {
      "id": 7,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 52.44,
      "math_avg": 41.23,
      "code_avg": 49.73,
      "reasoning_avg": 32.47,
      "overall_avg": 43.97,
      "general_scores": [
        77.31,
        43.3625,
        57.86,
        29.9021429,
        76.91,
        41.78,
        57.18,
        33.8178571,
        77.2,
        43.195,
        57.96,
        32.7728571
      ],
      "math_scores": [
        72.33,
        50.04,
        49.6,
        22.99,
        10.0,
        71.11,
        50.36,
        48.6,
        22.9,
        16.67,
        71.19,
        50.08,
        46.0,
        23.24,
        13.33
      ],
      "code_scores": [
        74.39,
        70.82,
        11.47,
        45.93,
        42.08,
        72.56,
        69.65,
        10.75,
        46.35,
        51.13,
        73.78,
        69.65,
        12.19,
        45.93,
        49.32
      ],
      "reasoning_scores": [
        34.24,
        67.42,
        0.40869565,
        28.24,
        34.92,
        67.46,
        0.39793478,
        26.32,
        32.88,
        66.94,
        0.40413043,
        30.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.07
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.01
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.27
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 9.13,
        "math_avg": -9.24,
        "code_avg": 11.02,
        "reasoning_avg": -2.46,
        "overall_avg": 2.11,
        "general_task_scores": [
          10.15,
          7.29,
          12.81,
          6.25
        ],
        "math_task_scores": [
          -15.8,
          -14.72,
          -19.33,
          -3.0,
          6.66
        ],
        "code_task_scores": [
          -2.03,
          -1.56,
          3.23,
          45.03,
          10.41
        ],
        "reasoning_task_scores": [
          -2.59,
          -2.19,
          0.01,
          -5.09
        ]
      },
      "affiliation": "GAIR",
      "year": "2023",
      "size": "1k",
      "link": "https://huggingface.co/datasets/GAIR/lima"
    },
    {
      "id": 8,
      "name": "orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.36,
      "math_avg": 47.62,
      "code_avg": 48.01,
      "reasoning_avg": 50.09,
      "overall_avg": 51.02,
      "general_scores": [
        76.72,
        57.77,
        55.71,
        42.8821429,
        75.95,
        58.145,
        55.19,
        42.1885714,
        76.05,
        60.0675,
        55.23,
        44.3628571
      ],
      "math_scores": [
        86.88,
        59.16,
        59.4,
        21.27,
        10.0,
        87.57,
        59.84,
        60.2,
        21.18,
        13.33,
        85.67,
        60.6,
        61.2,
        21.36,
        6.67
      ],
      "code_scores": [
        75.0,
        71.98,
        12.54,
        39.67,
        40.95,
        75.0,
        74.32,
        12.9,
        34.86,
        40.95,
        72.56,
        74.32,
        14.34,
        40.71,
        40.05
      ],
      "reasoning_scores": [
        87.46,
        69.34,
        0.37869565,
        41.52,
        87.12,
        70.42,
        0.37706522,
        43.2,
        89.15,
        69.39,
        0.37978261,
        42.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.91
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.72
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.37
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 15.04,
        "math_avg": -2.84,
        "code_avg": 9.29,
        "reasoning_avg": 15.16,
        "overall_avg": 9.16,
        "general_task_scores": [
          9.25,
          23.17,
          10.52,
          17.23
        ],
        "math_task_scores": [
          -0.63,
          -5.01,
          -7.13,
          -4.77,
          3.33
        ],
        "code_task_scores": [
          -1.42,
          1.94,
          5.02,
          37.37,
          3.55
        ],
        "reasoning_task_scores": [
          51.31,
          0.26,
          -0.01,
          9.09
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1"
    },
    {
      "id": 9,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 53.96,
      "math_avg": 35.28,
      "code_avg": 46.77,
      "reasoning_avg": 32.76,
      "overall_avg": 42.2,
      "general_scores": [
        70.36,
        56.3125,
        57.35,
        34.2271429,
        70.12,
        55.5125,
        57.8,
        35.0807143,
        70.74,
        54.6775,
        58.39,
        26.9914286
      ],
      "math_scores": [
        82.71,
        41.02,
        40.2,
        19.76,
        3.33,
        82.41,
        34.3,
        29.4,
        19.67,
        0.0,
        84.08,
        35.48,
        34.2,
        19.33,
        3.33
      ],
      "code_scores": [
        77.44,
        70.82,
        12.9,
        36.95,
        43.67,
        73.17,
        71.21,
        12.19,
        33.82,
        41.18,
        71.34,
        69.65,
        12.9,
        33.4,
        40.95
      ],
      "reasoning_scores": [
        29.49,
        67.31,
        0.41141304,
        32.8,
        33.22,
        67.85,
        0.41532609,
        27.52,
        37.63,
        66.4,
        0.4398913,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.72
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.0
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.65,
        "math_avg": -15.18,
        "code_avg": 8.05,
        "reasoning_avg": -2.17,
        "overall_avg": 0.34,
        "general_task_scores": [
          3.42,
          20.01,
          12.99,
          6.19
        ],
        "math_task_scores": [
          -4.27,
          -27.95,
          -32.8,
          -6.45,
          -4.45
        ],
        "code_task_scores": [
          -1.63,
          -1.04,
          4.42,
          33.68,
          4.83
        ],
        "reasoning_task_scores": [
          -3.15,
          -2.27,
          0.03,
          -3.28
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/teknium/OpenHermes-2.5"
    },
    {
      "id": 10,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.52,
      "math_avg": 6.75,
      "code_avg": 14.57,
      "reasoning_avg": 27.38,
      "overall_avg": 22.05,
      "general_scores": [
        49.32,
        39.67,
        30.6535714,
        48.58,
        42.07,
        29.4264286,
        46.43,
        43.67,
        25.8307143
      ],
      "math_scores": [
        5.16,
        7.2,
        7.8,
        6.67,
        4.78,
        6.5,
        5.8,
        6.67,
        10.46,
        7.06,
        6.2,
        6.67
      ],
      "code_scores": [
        13.41,
        28.4,
        0.0,
        31.73,
        0.0,
        14.63,
        28.4,
        0.0,
        30.69,
        0.0,
        12.8,
        27.63,
        0.0,
        30.9,
        0.0
      ],
      "reasoning_scores": [
        32.54,
        54.05,
        0.33826087,
        20.48,
        33.56,
        54.92,
        0.33630435,
        19.28,
        37.63,
        54.57,
        0.33554348,
        20.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.11
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.51
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.08
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -3.8,
        "math_avg": -43.72,
        "code_avg": -24.15,
        "reasoning_avg": -7.56,
        "overall_avg": -19.8,
        "general_task_scores": [
          -18.88,
          -3.06,
          2.73
        ],
        "math_task_scores": [
          -80.54,
          -57.96,
          -60.8,
          0.0
        ],
        "code_task_scores": [
          -62.0,
          -43.46,
          -8.24,
          30.07,
          -37.1
        ],
        "reasoning_task_scores": [
          -2.02,
          -14.95,
          -0.05,
          -13.2
        ]
      },
      "affiliation": "University of Washington",
      "year": "2023",
      "size": "252k",
      "link": "https://huggingface.co/datasets/yizhongw/self_instruct"
    },
    {
      "id": 11,
      "name": "tulu3-sft-mixture",
      "domain": "general",
      "general_avg": 62.84,
      "math_avg": 43.62,
      "code_avg": 49.26,
      "reasoning_avg": 46.3,
      "overall_avg": 50.51,
      "general_scores": [
        73.41,
        71.4675,
        58.56,
        50.8528571,
        66.87,
        72.1525,
        57.62,
        49.5364286,
        72.8,
        72.685,
        58.78,
        49.3721429
      ],
      "math_scores": [
        87.41,
        53.8,
        56.4,
        19.49,
        3.33,
        86.43,
        53.42,
        54.8,
        19.99,
        6.67,
        84.61,
        52.82,
        53.0,
        18.83,
        3.33
      ],
      "code_scores": [
        76.83,
        71.21,
        11.83,
        39.04,
        45.93,
        79.88,
        72.76,
        11.47,
        38.62,
        44.34,
        78.05,
        71.6,
        13.62,
        37.58,
        46.15
      ],
      "reasoning_scores": [
        82.71,
        62.09,
        0.44315217,
        40.88,
        83.05,
        61.85,
        0.44576087,
        39.6,
        81.02,
        61.82,
        0.43684783,
        41.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.03
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 72.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.92
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.15
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.44
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.44
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.25
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.86
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 45.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.26
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 61.92
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.56
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 19.53,
        "math_avg": -6.84,
        "code_avg": 10.54,
        "reasoning_avg": 11.36,
        "overall_avg": 8.65,
        "general_task_scores": [
          4.04,
          36.61,
          13.46,
          24.01
        ],
        "math_task_scores": [
          -1.19,
          -11.53,
          -12.67,
          -6.6,
          -2.23
        ],
        "code_task_scores": [
          2.64,
          0.26,
          4.07,
          37.37,
          8.37
        ],
        "reasoning_task_scores": [
          45.66,
          -7.54,
          0.05,
          7.28
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "939k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"
    },
    {
      "id": 12,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 35.76,
      "math_avg": 7.48,
      "code_avg": 18.52,
      "reasoning_avg": 24.47,
      "overall_avg": 21.56,
      "general_scores": [
        76.37,
        18.9625,
        48.83,
        0.00785714,
        75.92,
        17.9,
        48.41,
        0.00857143,
        76.1,
        19.6425,
        46.9,
        0.01571429
      ],
      "math_scores": [
        0.99,
        8.42,
        14.2,
        13.39,
        0.0,
        0.83,
        7.74,
        12.4,
        13.91,
        0.0,
        1.59,
        9.1,
        15.8,
        13.87,
        0.0
      ],
      "code_scores": [
        9.76,
        72.76,
        4.3,
        4.18,
        0.9,
        9.76,
        72.37,
        3.94,
        4.18,
        0.23,
        15.24,
        71.21,
        4.3,
        3.55,
        1.13
      ],
      "reasoning_scores": [
        42.37,
        51.89,
        0.29358696,
        4.08,
        51.68,
        0.29782609,
        8.56,
        40.34,
        55.03,
        0.31021739,
        8.4,
        40.0,
        55.03,
        0.31021739,
        8.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.59
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.11
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.97
              },
              {
                "metric": "lcb_test_output",
                "score": 0.75
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 53.41
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.36
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -7.56,
        "math_avg": -42.98,
        "code_avg": -20.2,
        "reasoning_avg": -10.47,
        "overall_avg": -20.3,
        "general_task_scores": [
          9.14,
          -16.66,
          3.19,
          -25.9
        ],
        "math_task_scores": [
          -86.2,
          -56.46,
          -53.27,
          -12.32,
          -6.67
        ],
        "code_task_scores": [
          -64.02,
          0.51,
          -4.06,
          2.93,
          -36.35
        ],
        "reasoning_task_scores": [
          4.3,
          -16.05,
          -0.09,
          -25.92
        ]
      },
      "affiliation": "CAS",
      "year": "2023",
      "size": "-",
      "link": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT"
    },
    {
      "id": 13,
      "name": "GSM8K",
      "domain": "math",
      "general_avg": 50.0,
      "math_avg": 18.9,
      "code_avg": 42.18,
      "reasoning_avg": 31.75,
      "overall_avg": 35.71,
      "general_scores": [
        67.93,
        47.84,
        56.65,
        28.9,
        66.19,
        48.6625,
        56.56,
        27.7364286,
        68.39,
        46.19,
        56.65,
        28.2664286
      ],
      "math_scores": [
        1.29,
        29.72,
        28.0,
        18.72,
        2.12,
        29.06,
        25.6,
        18.25,
        2.27,
        28.78,
        25.6,
        17.39
      ],
      "code_scores": [
        69.51,
        71.6,
        11.83,
        29.23,
        29.41,
        70.12,
        70.43,
        10.75,
        26.51,
        24.66,
        71.95,
        72.37,
        11.83,
        26.51,
        35.97
      ],
      "reasoning_scores": [
        34.24,
        68.77,
        0.42119565,
        24.16,
        33.56,
        68.88,
        0.40652174,
        24.72,
        31.86,
        68.92,
        0.40217391,
        24.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.56
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.3
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.89
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.12
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 70.53
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.42
              },
              {
                "metric": "lcb_test_output",
                "score": 30.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.86
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.51
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.68,
        "math_avg": -31.57,
        "code_avg": 3.46,
        "reasoning_avg": -3.18,
        "overall_avg": -6.15,
        "general_task_scores": [
          0.51,
          12.07,
          11.76,
          2.39
        ],
        "math_task_scores": [
          -85.45,
          -35.69,
          -41.0,
          -7.92
        ],
        "code_task_scores": [
          -5.08,
          -0.13,
          3.23,
          26.38,
          -7.09
        ],
        "reasoning_task_scores": [
          -3.38,
          -0.6,
          0.02,
          -8.77
        ]
      },
      "affiliation": "OpenAI",
      "year": "2021",
      "size": "8.5k",
      "link": "https://huggingface.co/datasets/openai/gsm8k"
    },
    {
      "id": 14,
      "name": "OpenMathInstruct-2",
      "domain": "math",
      "general_avg": 46.06,
      "math_avg": 49.65,
      "code_avg": 46.82,
      "reasoning_avg": 29.74,
      "overall_avg": 43.07,
      "general_scores": [
        74.76,
        45.74,
        51.74,
        8.66142857,
        75.4,
        45.3225,
        50.96,
        13.9285714,
        74.32,
        46.115,
        50.35,
        15.465
      ],
      "math_scores": [
        90.9,
        64.28,
        64.2,
        22.56,
        6.67,
        91.21,
        64.56,
        64.6,
        22.47,
        6.67,
        90.67,
        63.28,
        63.4,
        22.54,
        6.67
      ],
      "code_scores": [
        68.9,
        73.15,
        14.7,
        34.66,
        40.05,
        71.34,
        73.54,
        13.26,
        30.06,
        43.21,
        73.17,
        72.37,
        13.62,
        37.79,
        42.53
      ],
      "reasoning_scores": [
        35.59,
        65.67,
        0.3223913,
        18.32,
        32.2,
        66.25,
        0.32119565,
        18.16,
        35.25,
        67.55,
        0.31902174,
        16.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.83
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.68
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 90.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.04
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.52
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.14
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.17
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.35
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.75,
        "math_avg": -0.82,
        "code_avg": 8.11,
        "reasoning_avg": -5.2,
        "overall_avg": 1.21,
        "general_task_scores": [
          7.84,
          10.24,
          6.16,
          -13.23
        ],
        "math_task_scores": [
          3.59,
          -0.84,
          -3.33,
          -3.52,
          0.0
        ],
        "code_task_scores": [
          -4.47,
          1.42,
          5.62,
          33.13,
          4.83
        ],
        "reasoning_task_scores": [
          -2.25,
          -2.97,
          -0.07,
          -15.49
        ]
      },
      "affiliation": "Nvidia",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"
    },
    {
      "id": 15,
      "name": "NuminaMath-CoT",
      "domain": "math",
      "general_avg": 45.01,
      "math_avg": 48.64,
      "code_avg": 39.16,
      "reasoning_avg": 36.0,
      "overall_avg": 42.2,
      "general_scores": [
        72.67,
        46.485,
        52.23,
        6.05928571,
        72.99,
        47.7125,
        52.46,
        8.03142857,
        73.75,
        47.54,
        53.36,
        6.83714286
      ],
      "math_scores": [
        85.75,
        61.88,
        61.8,
        26.22,
        10.0,
        84.61,
        62.6,
        63.6,
        25.86,
        13.33,
        84.46,
        62.1,
        60.6,
        26.78,
        0.0
      ],
      "code_scores": [
        56.71,
        68.09,
        10.04,
        20.04,
        40.27,
        54.27,
        68.48,
        5.73,
        23.38,
        39.37,
        55.49,
        67.7,
        4.66,
        30.9,
        42.31
      ],
      "reasoning_scores": [
        43.05,
        59.51,
        0.37402174,
        35.36,
        54.58,
        61.59,
        0.38934783,
        32.96,
        50.51,
        59.03,
        0.39336957,
        34.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.98
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 84.94
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 62.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.29
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 55.49
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.09
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.81
              },
              {
                "metric": "lcb_code_execution",
                "score": 24.77
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.19
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 1.7,
        "math_avg": -1.83,
        "code_avg": 0.44,
        "reasoning_avg": 1.07,
        "overall_avg": 0.35,
        "general_task_scores": [
          6.15,
          11.76,
          7.82,
          -18.93
        ],
        "math_task_scores": [
          -2.4,
          -2.69,
          -5.4,
          0.25,
          1.11
        ],
        "code_task_scores": [
          -20.12,
          -3.51,
          -1.43,
          23.73,
          3.55
        ],
        "reasoning_task_scores": [
          12.78,
          -9.42,
          -0.0,
          0.91
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "859k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT"
    },
    {
      "id": 16,
      "name": "NuminaMath-TIR",
      "domain": "math",
      "general_avg": 56.0,
      "math_avg": 45.72,
      "code_avg": 50.18,
      "reasoning_avg": 33.38,
      "overall_avg": 46.32,
      "general_scores": [
        78.27,
        52.5125,
        53.53,
        38.9935714,
        78.53,
        53.2775,
        54.33,
        38.6521429,
        78.27,
        53.16,
        54.19,
        38.235
      ],
      "math_scores": [
        68.61,
        47.22,
        48.0,
        18.95,
        68.54,
        46.5,
        49.2,
        19.63,
        67.85,
        46.92,
        48.2,
        19.08
      ],
      "code_scores": [
        83.54,
        72.76,
        14.34,
        38.41,
        44.12,
        83.54,
        73.54,
        13.62,
        40.29,
        41.63,
        79.88,
        72.76,
        13.62,
        38.41,
        42.31
      ],
      "reasoning_scores": [
        35.59,
        68.96,
        0.37423913,
        28.08,
        38.31,
        68.44,
        0.37815217,
        28.24,
        35.25,
        68.52,
        0.37880435,
        28.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.98
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.02
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.63
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.88
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 82.32
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.02
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.04
              },
              {
                "metric": "lcb_test_output",
                "score": 42.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.64
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.68,
        "math_avg": -4.74,
        "code_avg": 11.47,
        "reasoning_avg": -1.56,
        "overall_avg": 4.46,
        "general_task_scores": [
          11.37,
          17.49,
          9.16,
          12.72
        ],
        "math_task_scores": [
          -19.01,
          -18.0,
          -18.93,
          -6.82
        ],
        "code_task_scores": [
          6.71,
          1.42,
          5.62,
          38.0,
          5.59
        ],
        "reasoning_task_scores": [
          -0.22,
          -0.82,
          -0.01,
          -5.17
        ]
      },
      "affiliation": "Numina",
      "year": "2024",
      "size": "72.4k",
      "link": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR"
    },
    {
      "id": 17,
      "name": "MetaMathQA",
      "domain": "math",
      "general_avg": 54.02,
      "math_avg": 40.29,
      "code_avg": 47.14,
      "reasoning_avg": 32.6,
      "overall_avg": 43.51,
      "general_scores": [
        75.29,
        47.59,
        50.75,
        42.155,
        75.05,
        48.8025,
        50.76,
        41.6971429,
        74.84,
        47.615,
        52.0,
        41.6564286
      ],
      "math_scores": [
        83.78,
        49.72,
        50.4,
        19.44,
        0.0,
        82.41,
        49.06,
        48.8,
        18.2,
        3.33,
        83.78,
        48.9,
        47.6,
        18.9,
        0.0
      ],
      "code_scores": [
        73.17,
        71.98,
        9.32,
        44.05,
        38.46,
        73.78,
        69.65,
        9.68,
        42.38,
        35.75,
        78.66,
        68.48,
        10.04,
        40.29,
        41.4
      ],
      "reasoning_scores": [
        38.64,
        67.34,
        0.435,
        28.56,
        35.25,
        68.13,
        0.43043478,
        28.24,
        27.8,
        67.16,
        0.43597826,
        28.8
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.06
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.17
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.84
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.23
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.85
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.2
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.68
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.24
              },
              {
                "metric": "lcb_test_output",
                "score": 38.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.9
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.54
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.7,
        "math_avg": -10.18,
        "code_avg": 8.42,
        "reasoning_avg": -2.33,
        "overall_avg": 1.65,
        "general_task_scores": [
          8.07,
          12.51,
          6.31,
          15.93
        ],
        "math_task_scores": [
          -4.02,
          -15.65,
          -18.47,
          -7.19,
          -5.56
        ],
        "code_task_scores": [
          -0.41,
          -1.56,
          1.44,
          41.2,
          1.44
        ],
        "reasoning_task_scores": [
          -2.7,
          -1.92,
          0.04,
          -4.75
        ]
      },
      "affiliation": "Meta",
      "year": "2023",
      "size": "395k",
      "link": "https://huggingface.co/datasets/meta-math/MetaMathQA"
    },
    {
      "id": 18,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 55.72,
      "math_avg": 37.08,
      "code_avg": 49.12,
      "reasoning_avg": 32.49,
      "overall_avg": 43.6,
      "general_scores": [
        77.55,
        50.54,
        56.73,
        38.2635714,
        76.59,
        50.905,
        56.77,
        35.5721429,
        77.55,
        51.535,
        56.75,
        39.8385714
      ],
      "math_scores": [
        79.08,
        45.06,
        41.0,
        18.61,
        0.0,
        78.77,
        46.1,
        42.8,
        19.4,
        0.0,
        76.88,
        46.22,
        43.0,
        19.29,
        0.0
      ],
      "code_scores": [
        78.66,
        71.21,
        13.26,
        41.54,
        41.63,
        76.83,
        73.93,
        11.11,
        41.34,
        42.08,
        76.83,
        72.37,
        10.39,
        42.8,
        42.76
      ],
      "reasoning_scores": [
        33.22,
        67.91,
        0.47076087,
        28.96,
        32.54,
        67.21,
        0.44173913,
        31.28,
        29.49,
        68.22,
        0.42152174,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.59
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.89
              },
              {
                "metric": "lcb_test_output",
                "score": 42.16
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.78
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.97
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.4,
        "math_avg": -13.39,
        "code_avg": 10.4,
        "reasoning_avg": -2.45,
        "overall_avg": 1.74,
        "general_task_scores": [
          10.24,
          15.5,
          11.89,
          11.98
        ],
        "math_task_scores": [
          -9.1,
          -19.09,
          -25.13,
          -6.94,
          -6.67
        ],
        "code_task_scores": [
          1.83,
          0.9,
          3.35,
          40.85,
          5.06
        ],
        "reasoning_task_scores": [
          -4.85,
          -1.68,
          0.05,
          -3.31
        ]
      },
      "affiliation": "TIGER-Lab",
      "year": "2023",
      "size": "262k",
      "link": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct"
    },
    {
      "id": 19,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 56.93,
      "math_avg": 39.59,
      "code_avg": 50.26,
      "reasoning_avg": 35.26,
      "overall_avg": 45.51,
      "general_scores": [
        76.89,
        53.39,
        58.6,
        46.8271429,
        76.06,
        52.1325,
        58.76,
        47.5414286,
        75.05,
        50.765,
        57.84,
        29.2514286
      ],
      "math_scores": [
        75.36,
        52.54,
        51.2,
        24.44,
        6.67,
        73.92,
        51.36,
        49.6,
        24.07,
        13.33,
        62.4,
        42.26,
        43.0,
        20.39,
        3.33
      ],
      "code_scores": [
        78.05,
        75.1,
        13.98,
        42.38,
        49.32,
        79.27,
        75.1,
        13.26,
        43.84,
        49.1,
        65.24,
        71.98,
        12.54,
        42.17,
        42.53
      ],
      "reasoning_scores": [
        39.66,
        68.32,
        0.41423913,
        39.44,
        33.9,
        68.11,
        0.40315217,
        38.08,
        26.44,
        66.65,
        0.39,
        41.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.06
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 46.98
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.69
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.61,
        "math_avg": -10.87,
        "code_avg": 11.54,
        "reasoning_avg": 0.32,
        "overall_avg": 3.65,
        "general_task_scores": [
          9.01,
          16.61,
          13.54,
          15.3
        ],
        "math_task_scores": [
          -16.78,
          -16.16,
          -19.47,
          -3.07,
          1.11
        ],
        "code_task_scores": [
          -1.42,
          2.46,
          5.02,
          41.76,
          9.88
        ],
        "reasoning_task_scores": [
          -3.27,
          -1.77,
          0.01,
          6.32
        ]
      },
      "affiliation": "Microsoft",
      "year": "2024",
      "size": "200k",
      "link": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
    },
    {
      "id": 20,
      "name": "DART-Math",
      "domain": "math",
      "general_avg": 46.13,
      "math_avg": 46.04,
      "code_avg": 45.9,
      "reasoning_avg": 28.13,
      "overall_avg": 41.55,
      "general_scores": [
        81.6,
        44.02,
        57.16,
        1.0,
        81.91,
        43.745,
        57.85,
        0.99785714,
        82.12,
        45.09,
        57.07,
        1.03357143
      ],
      "math_scores": [
        89.23,
        58.68,
        60.4,
        20.33,
        0.0,
        90.14,
        57.58,
        58.2,
        20.28,
        3.33,
        90.22,
        57.44,
        60.6,
        20.8,
        3.33
      ],
      "code_scores": [
        67.07,
        68.09,
        14.7,
        41.34,
        41.4,
        67.68,
        67.7,
        15.05,
        39.46,
        38.69,
        66.46,
        67.32,
        13.98,
        39.46,
        40.05
      ],
      "reasoning_scores": [
        32.88,
        41.41,
        0.42532609,
        36.0,
        33.56,
        40.69,
        0.42467391,
        36.4,
        38.31,
        41.34,
        0.43315217,
        35.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.88
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.28
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.36
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 89.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.47
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.7
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.58
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.09
              },
              {
                "metric": "lcb_test_output",
                "score": 40.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.92
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.15
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.43
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.03
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 2.82,
        "math_avg": -4.43,
        "code_avg": 7.18,
        "reasoning_avg": -6.8,
        "overall_avg": -0.31,
        "general_task_scores": [
          14.89,
          8.79,
          12.5,
          -24.9
        ],
        "math_task_scores": [
          2.52,
          -6.98,
          -7.67,
          -5.57,
          -4.45
        ],
        "code_task_scores": [
          -8.54,
          -3.9,
          6.34,
          39.05,
          2.95
        ],
        "reasoning_task_scores": [
          -1.68,
          -28.31,
          0.04,
          2.75
        ]
      },
      "affiliation": "HKUST-NLP",
      "year": "2024",
      "size": "585k",
      "link": "https://huggingface.co/datasets/hkust-nlp/dart-math-uniform https://huggingface.co/datasets/hkust-nlp/dart-math-hard"
    },
    {
      "id": 21,
      "name": "MathQA",
      "domain": "math",
      "general_avg": 50.77,
      "math_avg": 29.53,
      "code_avg": 47.79,
      "reasoning_avg": 30.73,
      "overall_avg": 39.7,
      "general_scores": [
        69.48,
        50.355,
        51.55,
        28.4207143,
        67.08,
        52.5325,
        52.9,
        31.5907143,
        70.78,
        51.1575,
        53.12,
        30.265
      ],
      "math_scores": [
        48.37,
        37.32,
        30.6,
        18.93,
        13.33,
        48.52,
        37.74,
        31.2,
        20.01,
        10.0,
        49.51,
        38.28,
        33.4,
        19.06,
        6.67
      ],
      "code_scores": [
        67.07,
        69.26,
        11.83,
        44.26,
        45.25,
        73.17,
        69.65,
        10.04,
        42.59,
        38.91,
        73.78,
        68.09,
        12.19,
        43.63,
        47.06
      ],
      "reasoning_scores": [
        33.56,
        66.98,
        0.42391304,
        23.28,
        27.8,
        66.58,
        0.4251087,
        24.88,
        33.22,
        66.67,
        0.42032609,
        24.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.35
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 52.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.09
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.78
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 71.34
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.0
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.35
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.49
              },
              {
                "metric": "lcb_test_output",
                "score": 43.74
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.53
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 66.74
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.21
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 7.46,
        "math_avg": -20.94,
        "code_avg": 9.07,
        "reasoning_avg": -4.21,
        "overall_avg": -2.15,
        "general_task_scores": [
          2.12,
          15.86,
          7.66,
          4.18
        ],
        "math_task_scores": [
          -38.54,
          -27.1,
          -35.67,
          -6.71,
          3.33
        ],
        "code_task_scores": [
          -4.27,
          -2.6,
          3.11,
          42.45,
          6.64
        ],
        "reasoning_task_scores": [
          -5.07,
          -2.72,
          0.03,
          -9.07
        ]
      },
      "affiliation": "AllenAI",
      "year": "2019",
      "size": "29.8k",
      "link": "https://huggingface.co/datasets/allenai/math_qa"
    },
    {
      "id": 22,
      "name": "CAMEL-Math",
      "domain": "math",
      "general_avg": 51.69,
      "math_avg": 34.4,
      "code_avg": 47.28,
      "reasoning_avg": 33.96,
      "overall_avg": 41.83,
      "general_scores": [
        77.61,
        46.9175,
        50.87,
        32.205,
        77.95,
        46.815,
        50.53,
        32.2257143,
        76.95,
        45.7325,
        50.67,
        31.8085714
      ],
      "math_scores": [
        59.21,
        45.0,
        43.8,
        18.68,
        6.67,
        62.85,
        45.44,
        45.8,
        18.97,
        0.0,
        60.96,
        45.54,
        44.8,
        18.34,
        0.0
      ],
      "code_scores": [
        75.61,
        71.6,
        12.19,
        43.84,
        29.64,
        75.0,
        73.15,
        11.83,
        43.42,
        35.29,
        75.61,
        70.43,
        12.19,
        44.05,
        35.29
      ],
      "reasoning_scores": [
        32.2,
        67.88,
        0.4225,
        32.56,
        68.62,
        0.42228261,
        33.52,
        33.56,
        68.98,
        0.41619565,
        33.68,
        35.25
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 33.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.67
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.49
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.38,
        "math_avg": -16.06,
        "code_avg": 8.56,
        "reasoning_avg": -0.97,
        "overall_avg": -0.02,
        "general_task_scores": [
          10.51,
          11.0,
          5.83,
          6.17
        ],
        "math_task_scores": [
          -26.33,
          -19.55,
          -22.6,
          -7.38,
          -4.45
        ],
        "code_task_scores": [
          -0.2,
          0.13,
          3.83,
          42.73,
          -3.69
        ],
        "reasoning_task_scores": [
          -2.93,
          -0.97,
          0.03,
          -0.03
        ]
      },
      "affiliation": "CAMEL-AI",
      "year": "2023",
      "size": "50k",
      "link": "https://huggingface.co/datasets/camel-ai/math"
    },
    {
      "id": 23,
      "name": "reasoning-0.01",
      "domain": "math",
      "general_avg": 38.26,
      "math_avg": 10.48,
      "code_avg": 25.05,
      "reasoning_avg": 30.07,
      "overall_avg": 25.97,
      "general_scores": [
        57.63,
        37.445,
        48.61,
        7.2,
        59.14,
        37.19,
        48.41,
        9.63714286,
        59.3,
        38.0825,
        49.5,
        6.98
      ],
      "math_scores": [
        14.1,
        11.3,
        6.8,
        19.29,
        0.0,
        13.12,
        11.62,
        7.8,
        19.38,
        0.0,
        14.25,
        12.12,
        7.4,
        20.01,
        0.0
      ],
      "code_scores": [
        2.44,
        72.76,
        8.6,
        6.47,
        38.69,
        4.27,
        71.6,
        6.45,
        7.52,
        31.22,
        6.71,
        71.6,
        8.24,
        4.8,
        34.39
      ],
      "reasoning_scores": [
        29.15,
        61.45,
        0.30858696,
        25.84,
        31.19,
        60.35,
        0.30521739,
        25.36,
        36.95,
        61.07,
        0.30413043,
        28.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.76
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.26
              },
              {
                "metric": "lcb_test_output",
                "score": 34.77
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.96
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.59
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -5.05,
        "math_avg": -39.99,
        "code_avg": -13.67,
        "reasoning_avg": -4.86,
        "overall_avg": -15.89,
        "general_task_scores": [
          -8.3,
          2.08,
          3.98,
          -17.97
        ],
        "math_task_scores": [
          -73.52,
          -53.2,
          -60.07,
          -6.48,
          -6.67
        ],
        "code_task_scores": [
          -71.14,
          0.39,
          -0.48,
          5.22,
          -2.33
        ],
        "reasoning_task_scores": [
          -4.17,
          -8.5,
          -0.08,
          -6.69
        ]
      },
      "affiliation": "SkunkworksAI",
      "year": "2025",
      "size": "29.9k",
      "link": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01"
    },
    {
      "id": 24,
      "name": "tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 43.67,
      "math_avg": 41.51,
      "code_avg": 41.66,
      "reasoning_avg": 31.99,
      "overall_avg": 39.71,
      "general_scores": [
        75.73,
        47.58,
        48.91,
        0.10928571,
        75.64,
        48.9925,
        48.55,
        0.09428571,
        76.02,
        49.7,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.45,
        6.67,
        69.75,
        29.6,
        67.8,
        27.35,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15
      ],
      "reasoning_scores": [
        31.86,
        68.7,
        0.35880435,
        24.56,
        29.83,
        68.11,
        0.36271739,
        25.84,
        36.61,
        70.3,
        0.37706522,
        26.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.36,
        "math_avg": -8.96,
        "code_avg": 2.95,
        "reasoning_avg": -2.94,
        "overall_avg": -2.15,
        "general_task_scores": [
          8.81,
          13.27,
          4.85,
          -25.5
        ],
        "math_task_scores": [
          -15.14,
          -36.67,
          0.73,
          0.73,
          5.55
        ],
        "code_task_scores": [
          -32.52,
          0.12,
          4.78,
          34.8,
          7.55
        ],
        "reasoning_task_scores": [
          -3.83,
          -0.42,
          -0.02,
          -7.49
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "150k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math"
    },
    {
      "id": 25,
      "name": "tulu-3-sft-personas-algebra",
      "domain": "math",
      "general_avg": 48.62,
      "math_avg": 43.07,
      "code_avg": 44.39,
      "reasoning_avg": 29.68,
      "overall_avg": 41.44,
      "general_scores": [
        74.88,
        49.64,
        49.5,
        19.2128571,
        75.1,
        48.8,
        49.16,
        19.1157143,
        75.39,
        48.3725,
        48.62,
        25.6778571
      ],
      "math_scores": [
        86.96,
        37.56,
        35.0,
        27.46,
        13.33,
        86.66,
        36.68,
        36.6,
        86.13,
        36.7,
        36.6,
        26.94,
        13.33
      ],
      "code_scores": [
        80.49,
        73.93,
        12.9,
        6.47,
        47.06,
        81.1,
        73.93,
        12.19,
        7.93,
        47.29,
        82.93,
        73.93,
        12.19,
        7.1,
        46.38
      ],
      "reasoning_scores": [
        33.22,
        67.6,
        0.37391304,
        15.92,
        35.59,
        69.85,
        0.37108696,
        16.8,
        31.53,
        68.1,
        0.37076087,
        16.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.12
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.94
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.09
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 21.34
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.58
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.2
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 81.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.43
              },
              {
                "metric": "lcb_code_execution",
                "score": 7.17
              },
              {
                "metric": "lcb_test_output",
                "score": 46.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.45
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.52
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 16.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 5.31,
        "math_avg": -7.39,
        "code_avg": 5.67,
        "reasoning_avg": -5.25,
        "overall_avg": -0.42,
        "general_task_scores": [
          8.13,
          13.45,
          4.23,
          -4.57
        ],
        "math_task_scores": [
          -0.76,
          -27.9,
          -31.33,
          1.16,
          6.66
        ],
        "code_task_scores": [
          5.9,
          2.33,
          4.19,
          6.13,
          9.81
        ],
        "reasoning_task_scores": [
          -3.15,
          -0.94,
          -0.02,
          -16.88
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "20k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra"
    },
    {
      "id": 26,
      "name": "tulu-3-sft-personas-math-grade",
      "domain": "math",
      "general_avg": 43.68,
      "math_avg": 41.51,
      "code_avg": 41.66,
      "reasoning_avg": 31.99,
      "overall_avg": 39.71,
      "general_scores": [
        75.73,
        47.6575,
        48.91,
        0.10928571,
        75.64,
        49.0225,
        48.55,
        0.09428571,
        76.02,
        49.775,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.49,
        6.67,
        69.75,
        29.6,
        67.8,
        27.39,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15
      ],
      "reasoning_scores": [
        31.86,
        68.7,
        0.35880435,
        24.56,
        25.84,
        29.83,
        68.11,
        0.36271739,
        26.96,
        36.61,
        70.3,
        0.37706522
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.8
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.04
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 0.37,
        "math_avg": -8.95,
        "code_avg": 2.95,
        "reasoning_avg": -2.94,
        "overall_avg": -2.14,
        "general_task_scores": [
          8.81,
          13.33,
          4.85,
          -25.5
        ],
        "math_task_scores": [
          -15.14,
          -36.67,
          0.73,
          0.76,
          5.55
        ],
        "code_task_scores": [
          -32.52,
          0.12,
          4.78,
          34.8,
          7.55
        ],
        "reasoning_task_scores": [
          -3.83,
          -0.42,
          -0.02,
          -7.49
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "50k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade"
    },
    {
      "id": 27,
      "name": "ScaleQuest-Math",
      "domain": "math",
      "general_avg": 42.36,
      "math_avg": 54.56,
      "code_avg": 36.0,
      "reasoning_avg": 25.15,
      "overall_avg": 39.52,
      "general_scores": [
        87.67,
        35.41,
        46.59,
        0.00785714,
        87.29,
        36.31,
        45.59,
        0.0,
        87.32,
        37.18,
        44.91,
        0.0
      ],
      "math_scores": [
        88.1,
        72.64,
        74.0,
        26.78,
        13.33,
        88.17,
        72.22,
        74.0,
        27.12,
        10.0,
        88.1,
        72.2,
        73.8,
        27.87,
        10.0
      ],
      "code_scores": [
        64.63,
        73.54,
        9.68,
        20.88,
        13.57,
        61.59,
        72.76,
        11.47,
        20.46,
        11.76,
        64.02,
        73.54,
        9.32,
        18.79,
        14.03
      ],
      "reasoning_scores": [
        33.9,
        63.97,
        0.22065217,
        1.44,
        37.29,
        64.9,
        0.2201087,
        0.96,
        32.88,
        64.72,
        0.21913043,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.16
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.04
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 64.53
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -0.96,
        "math_avg": 4.09,
        "code_avg": -2.72,
        "reasoning_avg": -9.79,
        "overall_avg": -2.34,
        "general_task_scores": [
          20.44,
          0.81,
          0.84,
          -25.91
        ],
        "math_task_scores": [
          0.78,
          7.47,
          6.53,
          1.22,
          4.44
        ],
        "code_task_scores": [
          -12.2,
          1.68,
          1.92,
          19.0,
          -23.98
        ],
        "reasoning_task_scores": [
          -1.91,
          -4.93,
          -0.17,
          -32.13
        ]
      },
      "affiliation": "Soochow Univ",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/dyyyyyyyy/ScaleQuest-Math"
    },
    {
      "id": 28,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 53.46,
      "math_avg": 41.78,
      "code_avg": 48.27,
      "reasoning_avg": 31.99,
      "overall_avg": 43.88,
      "general_scores": [
        70.76,
        52.305,
        56.6,
        36.5364286,
        69.01,
        53.4925,
        56.83,
        33.3778571,
        67.16,
        53.3425,
        57.03,
        35.135
      ],
      "math_scores": [
        81.05,
        50.6,
        52.2,
        20.48,
        6.67,
        79.98,
        49.42,
        50.0,
        18.61,
        13.33,
        77.63,
        49.1,
        50.0,
        17.62,
        10.0
      ],
      "code_scores": [
        77.44,
        71.98,
        8.96,
        44.05,
        44.8,
        73.17,
        70.82,
        6.45,
        42.59,
        43.67,
        75.61,
        71.6,
        3.94,
        44.68,
        44.34
      ],
      "reasoning_scores": [
        34.24,
        67.9,
        0.38032609,
        29.44,
        27.8,
        68.84,
        0.38195652,
        29.52,
        26.78,
        68.41,
        0.38130435,
        29.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.05
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.02
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.55
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.47
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 44.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.61
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.38
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.6
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.15,
        "math_avg": -8.69,
        "code_avg": 9.56,
        "reasoning_avg": -2.94,
        "overall_avg": 2.02,
        "general_task_scores": [
          1.99,
          17.56,
          11.96,
          9.11
        ],
        "math_task_scores": [
          -7.79,
          -15.17,
          -16.67,
          -7.14,
          3.33
        ],
        "code_task_scores": [
          -0.2,
          -0.13,
          -1.79,
          42.73,
          7.17
        ],
        "reasoning_task_scores": [
          -6.99,
          -1.08,
          -0.01,
          -3.68
        ]
      },
      "affiliation": "glaiveAI",
      "year": "2023",
      "size": "20k",
      "link": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k"
    },
    {
      "id": 29,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 57.83,
      "math_avg": 46.32,
      "code_avg": 45.9,
      "reasoning_avg": 34.08,
      "overall_avg": 46.03,
      "general_scores": [
        73.95,
        54.0475,
        57.61,
        45.75,
        73.28,
        55.2325,
        58.17,
        45.3114286,
        72.35,
        55.42,
        58.46,
        44.4035714
      ],
      "math_scores": [
        86.81,
        49.72,
        51.2,
        21.12,
        26.67,
        86.28,
        49.78,
        51.6,
        21.43,
        20.0,
        84.69,
        48.5,
        52.2,
        21.43,
        23.33
      ],
      "code_scores": [
        78.66,
        75.88,
        13.26,
        40.92,
        16.52,
        79.27,
        72.76,
        13.98,
        41.13,
        17.65,
        82.32,
        75.49,
        14.34,
        38.0,
        28.28
      ],
      "reasoning_scores": [
        33.56,
        67.17,
        0.44717391,
        35.6,
        33.22,
        67.49,
        0.43336957,
        35.76,
        32.54,
        68.17,
        0.43282609,
        34.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.02
              },
              {
                "metric": "lcb_test_output",
                "score": 20.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.61
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.52,
        "math_avg": -4.15,
        "code_avg": 7.18,
        "reasoning_avg": -0.86,
        "overall_avg": 4.17,
        "general_task_scores": [
          6.2,
          19.41,
          13.22,
          19.25
        ],
        "math_task_scores": [
          -1.41,
          -15.55,
          -15.73,
          -4.71,
          16.66
        ],
        "code_task_scores": [
          4.47,
          3.11,
          5.62,
          38.98,
          -16.28
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.85,
          0.05,
          1.87
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "436k",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2"
    },
    {
      "id": 30,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 55.87,
      "math_avg": 42.32,
      "code_avg": 49.43,
      "reasoning_avg": 32.65,
      "overall_avg": 45.07,
      "general_scores": [
        71.55,
        51.1425,
        57.7,
        42.3457143,
        72.22,
        50.4425,
        58.0,
        43.0657143,
        72.94,
        50.2575,
        58.04,
        42.7221429
      ],
      "math_scores": [
        81.96,
        49.72,
        52.6,
        20.57,
        10.0,
        79.76,
        49.92,
        50.8,
        20.37,
        6.67,
        82.11,
        48.64,
        51.4,
        20.23,
        10.0
      ],
      "code_scores": [
        78.66,
        74.32,
        13.62,
        39.04,
        43.67,
        79.27,
        70.04,
        14.7,
        38.41,
        44.34,
        80.49,
        70.82,
        13.98,
        38.2,
        41.86
      ],
      "reasoning_scores": [
        28.47,
        67.62,
        0.41380435,
        33.44,
        28.14,
        67.7,
        0.41195652,
        32.8,
        29.83,
        68.4,
        0.42804348,
        34.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 43.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.81
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.91
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.47
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.56,
        "math_avg": -8.15,
        "code_avg": 10.71,
        "reasoning_avg": -2.28,
        "overall_avg": 3.21,
        "general_task_scores": [
          5.25,
          15.12,
          13.05,
          16.8
        ],
        "math_task_scores": [
          -6.06,
          -15.45,
          -15.8,
          -5.65,
          2.22
        ],
        "code_task_scores": [
          3.86,
          0.13,
          5.86,
          37.51,
          6.19
        ],
        "reasoning_task_scores": [
          -7.79,
          -1.55,
          0.03,
          0.19
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "157k",
      "link": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction"
    },
    {
      "id": 31,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 55.97,
      "math_avg": 42.77,
      "code_avg": 48.81,
      "reasoning_avg": 33.63,
      "overall_avg": 45.29,
      "general_scores": [
        73.46,
        50.4625,
        58.39,
        40.425,
        72.79,
        51.2425,
        58.7,
        42.015,
        73.71,
        52.2175,
        58.47,
        39.75
      ],
      "math_scores": [
        85.97,
        48.64,
        51.8,
        21.03,
        6.67,
        85.75,
        50.6,
        52.6,
        20.05,
        3.33,
        85.22,
        49.72,
        53.0,
        20.53,
        6.67
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        37.58,
        42.76,
        78.66,
        71.6,
        12.19,
        36.95,
        43.21,
        78.05,
        73.15,
        11.47,
        38.41,
        43.44
      ],
      "reasoning_scores": [
        32.2,
        67.97,
        0.43880435,
        33.6,
        30.17,
        68.39,
        0.43891304,
        34.64,
        31.86,
        68.34,
        0.4401087,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.23
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.43
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.66,
        "math_avg": -7.69,
        "code_avg": 10.09,
        "reasoning_avg": -1.31,
        "overall_avg": 3.44,
        "general_task_scores": [
          6.33,
          15.82,
          13.66,
          14.82
        ],
        "math_task_scores": [
          -1.69,
          -15.23,
          -14.93,
          -5.5,
          -1.11
        ],
        "code_task_scores": [
          3.46,
          0.77,
          3.59,
          36.61,
          6.04
        ],
        "reasoning_task_scores": [
          -5.19,
          -1.23,
          0.05,
          1.15
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "111k",
      "link": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1"
    },
    {
      "id": 32,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 56.18,
      "math_avg": 43.35,
      "code_avg": 48.67,
      "reasoning_avg": 33.49,
      "overall_avg": 45.42,
      "general_scores": [
        72.15,
        52.31,
        59.02,
        40.0664286,
        73.45,
        50.7375,
        58.68,
        41.9592857,
        73.67,
        52.3975,
        58.3,
        41.36
      ],
      "math_scores": [
        84.69,
        50.9,
        53.4,
        20.71,
        10.0,
        85.14,
        49.32,
        50.6,
        19.87,
        6.67,
        85.22,
        49.98,
        52.6,
        21.14,
        10.0
      ],
      "code_scores": [
        78.05,
        73.54,
        11.47,
        38.41,
        45.48,
        79.88,
        71.21,
        12.19,
        35.49,
        43.21,
        78.66,
        72.37,
        10.75,
        36.53,
        42.76
      ],
      "reasoning_scores": [
        33.56,
        67.98,
        0.44086957,
        34.24,
        27.8,
        67.78,
        0.44271739,
        34.48,
        30.17,
        67.71,
        0.42641304,
        36.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.81
              },
              {
                "metric": "lcb_test_output",
                "score": 43.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.51
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.2
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 12.86,
        "math_avg": -7.12,
        "code_avg": 9.95,
        "reasoning_avg": -1.44,
        "overall_avg": 3.56,
        "general_task_scores": [
          6.1,
          16.33,
          13.81,
          15.22
        ],
        "math_task_scores": [
          -2.32,
          -14.81,
          -15.2,
          -5.47,
          2.22
        ],
        "code_task_scores": [
          3.25,
          0.77,
          3.23,
          35.77,
          6.72
        ],
        "reasoning_task_scores": [
          -6.09,
          -1.64,
          0.05,
          1.92
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "110k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K"
    },
    {
      "id": 33,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 57.03,
      "math_avg": 48.88,
      "code_avg": 48.19,
      "reasoning_avg": 33.52,
      "overall_avg": 46.91,
      "general_scores": [
        74.18,
        50.5175,
        58.27,
        44.5807143,
        75.17,
        49.6875,
        58.73,
        45.1207143
      ],
      "math_scores": [
        77.86,
        61.66,
        61.2,
        26.31,
        16.67,
        81.27,
        61.66,
        60.0,
        25.5,
        16.67
      ],
      "code_scores": [
        79.27,
        72.37,
        11.83,
        42.8,
        35.29,
        79.88,
        71.6,
        12.54,
        42.38,
        33.94
      ],
      "reasoning_scores": [
        35.25,
        69.52,
        0.41336957,
        29.12,
        34.24,
        70.48,
        0.40923913,
        28.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.57
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.59
              },
              {
                "metric": "lcb_test_output",
                "score": 34.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 70.0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.92
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.72,
        "math_avg": -1.59,
        "code_avg": 9.47,
        "reasoning_avg": -1.41,
        "overall_avg": 5.05,
        "general_task_scores": [
          7.69,
          14.61,
          13.64,
          18.94
        ],
        "math_task_scores": [
          -7.78,
          -3.22,
          -6.8,
          -0.14,
          10.0
        ],
        "code_task_scores": [
          3.96,
          0.38,
          3.94,
          41.55,
          -2.49
        ],
        "reasoning_task_scores": [
          -1.85,
          0.54,
          0.02,
          -4.36
        ]
      },
      "affiliation": "Intelligent Software Engineering (UIUC)",
      "year": "2023",
      "size": "75.2k",
      "link": "https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K"
    },
    {
      "id": 34,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 34.24,
      "code_avg": 45.39,
      "reasoning_avg": 32.58,
      "overall_avg": 40.92,
      "general_scores": [
        75.82,
        48.3275,
        55.52,
        27.7192857,
        76.13,
        48.3925,
        54.78,
        25.4685714,
        76.39,
        45.895,
        54.65,
        28.5435714
      ],
      "math_scores": [
        76.8,
        30.94,
        29.0,
        22.34,
        10.0,
        75.44,
        31.74,
        30.2,
        23.06,
        13.33,
        73.92,
        31.56,
        30.4,
        21.59,
        13.33
      ],
      "code_scores": [
        75.0,
        73.15,
        12.19,
        24.01,
        44.34,
        73.17,
        73.54,
        12.9,
        26.1,
        41.4,
        71.34,
        71.98,
        13.26,
        27.35,
        41.18
      ],
      "reasoning_scores": [
        33.22,
        68.02,
        0.32086957,
        28.96,
        30.17,
        67.36,
        0.32782609,
        30.16,
        35.93,
        67.51,
        0.32369565,
        28.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.82
              },
              {
                "metric": "lcb_test_output",
                "score": 42.31
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.11
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.63
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.25
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 8.16,
        "math_avg": -16.22,
        "code_avg": 6.68,
        "reasoning_avg": -2.35,
        "overall_avg": -0.94,
        "general_task_scores": [
          9.12,
          12.05,
          10.12,
          1.33
        ],
        "math_task_scores": [
          -11.95,
          -33.47,
          -37.53,
          -3.71,
          5.55
        ],
        "code_task_scores": [
          -2.44,
          1.29,
          4.54,
          24.78,
          5.21
        ],
        "reasoning_task_scores": [
          -3.49,
          -1.83,
          -0.07,
          -4.03
        ]
      },
      "affiliation": "Multimodal Art Projection (M-A-P)",
      "year": "2024",
      "size": "66.4k",
      "link": "https://huggingface.co/datasets/m-a-p/Code-Feedback"
    },
    {
      "id": 35,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 49.96,
      "math_avg": 36.48,
      "code_avg": 39.29,
      "reasoning_avg": 30.68,
      "overall_avg": 39.1,
      "general_scores": [
        71.86,
        39.445,
        44.32,
        42.75,
        72.08,
        39.645,
        45.15,
        44.2685714,
        72.69,
        39.16,
        44.39,
        43.7485714
      ],
      "math_scores": [
        68.08,
        40.54,
        36.6,
        25.09,
        6.67,
        69.9,
        42.76,
        39.6,
        25.43,
        10.0,
        66.26,
        39.7,
        34.6,
        25.25,
        16.67
      ],
      "code_scores": [
        69.51,
        68.87,
        9.32,
        40.71,
        0.45,
        70.73,
        66.93,
        11.83,
        46.76,
        3.17,
        69.51,
        66.54,
        8.24,
        48.43,
        8.37
      ],
      "reasoning_scores": [
        33.22,
        68.44,
        0.38967391,
        19.6,
        34.24,
        68.19,
        0.38804348,
        21.92,
        31.53,
        67.94,
        0.39413043,
        21.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 45.3
              },
              {
                "metric": "lcb_test_output",
                "score": 4.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.0
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.19
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.15
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 6.65,
        "math_avg": -13.99,
        "code_avg": 0.57,
        "reasoning_avg": -4.25,
        "overall_avg": -2.76,
        "general_task_scores": [
          5.22,
          3.93,
          -0.24,
          17.68
        ],
        "math_task_scores": [
          -19.26,
          -23.88,
          -30.47,
          -0.78,
          4.44
        ],
        "code_task_scores": [
          -5.69,
          -4.15,
          1.56,
          44.26,
          -33.1
        ],
        "reasoning_task_scores": [
          -3.6,
          -1.27,
          -0.0,
          -12.13
        ]
      },
      "affiliation": "independent",
      "year": "2024",
      "size": "55.1k",
      "link": "https://huggingface.co/datasets/Vezora/Open-Critic-GPT"
    },
    {
      "id": 36,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 53.39,
      "math_avg": 35.08,
      "code_avg": 42.42,
      "reasoning_avg": 30.13,
      "overall_avg": 40.26,
      "general_scores": [
        75.76,
        45.205,
        55.36,
        37.7707143,
        75.5,
        46.14,
        54.79,
        37.9335714,
        75.76,
        45.295,
        55.3,
        35.8214286
      ],
      "math_scores": [
        61.11,
        45.02,
        42.8,
        23.28,
        6.67,
        55.5,
        43.84,
        42.4,
        23.01,
        6.67,
        56.33,
        44.3,
        41.4,
        23.92,
        10.0
      ],
      "code_scores": [
        72.56,
        73.54,
        13.98,
        24.43,
        28.73,
        71.95,
        71.21,
        13.98,
        25.47,
        26.47,
        71.95,
        71.21,
        14.34,
        27.14,
        29.41
      ],
      "reasoning_scores": [
        31.19,
        68.64,
        0.38173913,
        19.6,
        31.86,
        67.61,
        0.38108696,
        18.56,
        34.24,
        68.38,
        0.37456522,
        20.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.68
              },
              {
                "metric": "lcb_test_output",
                "score": 28.2
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.43
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.21
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.49
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.07,
        "math_avg": -15.38,
        "code_avg": 3.71,
        "reasoning_avg": -4.8,
        "overall_avg": -1.6,
        "general_task_scores": [
          8.68,
          10.06,
          10.29,
          11.27
        ],
        "math_task_scores": [
          -29.69,
          -20.49,
          -25.2,
          -2.64,
          1.11
        ],
        "code_task_scores": [
          -3.46,
          0.39,
          5.86,
          24.64,
          -8.9
        ],
        "reasoning_task_scores": [
          -4.17,
          -1.25,
          -0.01,
          -13.79
        ]
      },
      "affiliation": "bigcode (Huggingface)",
      "year": "2024",
      "size": "50.7k",
      "link": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k"
    },
    {
      "id": 37,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.5,
      "math_avg": 34.35,
      "code_avg": 46.65,
      "reasoning_avg": 32.18,
      "overall_avg": 42.42,
      "general_scores": [
        74.85,
        51.775,
        58.8,
        42.7107143,
        74.91,
        50.195,
        58.67,
        41.5028571,
        73.57,
        51.02,
        57.95,
        42.0721429
      ],
      "math_scores": [
        58.68,
        46.02,
        41.6,
        20.53,
        3.33,
        54.06,
        44.66,
        42.4,
        21.5,
        3.33,
        62.77,
        47.18,
        44.2,
        21.61,
        3.33
      ],
      "code_scores": [
        76.83,
        71.6,
        13.26,
        37.16,
        43.21,
        78.05,
        71.6,
        12.19,
        27.97,
        42.76,
        81.71,
        69.26,
        11.47,
        18.58,
        44.12
      ],
      "reasoning_scores": [
        25.76,
        65.39,
        0.43847826,
        33.52,
        32.88,
        65.57,
        0.43271739,
        34.16,
        28.14,
        66.51,
        0.4475,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.9
              },
              {
                "metric": "lcb_test_output",
                "score": 43.36
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.93
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 65.82
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.52
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.19,
        "math_avg": -16.12,
        "code_avg": 7.93,
        "reasoning_avg": -2.76,
        "overall_avg": 0.56,
        "general_task_scores": [
          7.45,
          15.51,
          13.61,
          16.19
        ],
        "math_task_scores": [
          -28.84,
          -18.93,
          -24.67,
          -4.83,
          -3.34
        ],
        "code_task_scores": [
          3.25,
          -0.78,
          4.07,
          26.86,
          6.26
        ],
        "reasoning_task_scores": [
          -7.67,
          -3.64,
          0.05,
          0.24
        ]
      },
      "affiliation": "INF AI",
      "year": "2024",
      "size": "1M",
      "link": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage1"
    },
    {
      "id": 38,
      "name": "tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 54.71,
      "math_avg": 51.5,
      "code_avg": 40.42,
      "reasoning_avg": 32.58,
      "overall_avg": 44.8,
      "general_scores": [
        72.01,
        51.76,
        59.64,
        37.0535714,
        70.65,
        51.2125,
        58.86,
        37.1528571,
        70.48,
        51.0075,
        58.75,
        37.935
      ],
      "math_scores": [
        83.17,
        65.76,
        66.6,
        27.48,
        13.33,
        81.58,
        66.6,
        70.6,
        27.08,
        13.33,
        81.5,
        66.88,
        67.8,
        27.51,
        13.33
      ],
      "code_scores": [
        77.44,
        73.93,
        10.04,
        43.01,
        0.0,
        75.61,
        73.54,
        11.47,
        40.92,
        0.0,
        78.66,
        73.54,
        9.68,
        38.41,
        0.0
      ],
      "reasoning_scores": [
        28.47,
        69.63,
        0.42565217,
        28.64,
        30.85,
        69.84,
        0.41673913,
        29.76,
        30.85,
        69.71,
        0.42467391,
        31.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.78
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.06
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 69.73
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 30.11
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 11.4,
        "math_avg": 1.04,
        "code_avg": 1.7,
        "reasoning_avg": -2.35,
        "overall_avg": 2.94,
        "general_task_scores": [
          4.06,
          15.84,
          14.22,
          11.47
        ],
        "math_task_scores": [
          -5.26,
          1.53,
          0.93,
          1.32,
          6.66
        ],
        "code_task_scores": [
          1.63,
          2.07,
          2.16,
          39.74,
          -37.1
        ],
        "reasoning_task_scores": [
          -6.54,
          0.27,
          0.03,
          -3.17
        ]
      },
      "affiliation": "AllenAI",
      "year": "2024",
      "size": "35k",
      "link": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code"
    },
    {
      "id": 39,
      "name": "KodCode-V1",
      "domain": "code",
      "general_avg": 53.94,
      "math_avg": 48.18,
      "code_avg": 35.22,
      "reasoning_avg": 25.19,
      "overall_avg": 40.63,
      "general_scores": [
        66.21,
        56.4025,
        51.46,
        42.5707143,
        65.93,
        56.725,
        51.79,
        39.9364286,
        66.4,
        55.995,
        52.15,
        41.6585714
      ],
      "math_scores": [
        74.83,
        63.16,
        63.4,
        26.51,
        6.67,
        75.28,
        61.7,
        65.6,
        25.56,
        20.0,
        73.39,
        60.18,
        60.2,
        26.17,
        20.0
      ],
      "code_scores": [
        77.44,
        76.65,
        0.0,
        22.76,
        0.0,
        81.1,
        74.32,
        0.0,
        19.42,
        0.45,
        78.05,
        77.82,
        0.0,
        20.04,
        0.23
      ],
      "reasoning_scores": [
        32.54,
        55.51,
        0.19467391,
        7.92,
        33.22,
        63.21,
        0.1926087,
        10.48,
        32.2,
        58.36,
        0.19097826,
        8.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.18
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 56.37
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.39
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 76.26
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.74
              },
              {
                "metric": "lcb_test_output",
                "score": 0.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.65
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.03
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.19
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.88
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.62,
        "math_avg": -2.29,
        "code_avg": -3.5,
        "reasoning_avg": -9.74,
        "overall_avg": -1.23,
        "general_task_scores": [
          -0.81,
          20.88,
          6.94,
          15.48
        ],
        "math_task_scores": [
          -12.84,
          -3.2,
          -4.33,
          0.04,
          8.89
        ],
        "code_task_scores": [
          3.25,
          4.66,
          -8.24,
          19.7,
          -36.87
        ],
        "reasoning_task_scores": [
          -3.95,
          -10.43,
          -0.2,
          -24.4
        ]
      },
      "affiliation": "Microsoft",
      "year": "2025",
      "size": "444k",
      "link": "https://huggingface.co/datasets/KodCode/KodCode-V1"
    },
    {
      "id": 40,
      "name": "self-instruct-starcoder",
      "domain": "code",
      "general_avg": 34.86,
      "math_avg": 19.28,
      "code_avg": 26.66,
      "reasoning_avg": 29.38,
      "overall_avg": 27.55,
      "general_scores": [
        19.9,
        38.3025,
        51.93,
        26.8121429,
        26.92,
        38.275,
        51.86,
        25.6107143,
        23.18,
        39.3875,
        51.8,
        24.3114286
      ],
      "math_scores": [
        25.78,
        21.88,
        25.2,
        20.69,
        3.33,
        21.46,
        22.5,
        25.8,
        18.16,
        6.67,
        23.73,
        22.08,
        26.0,
        19.26,
        6.67
      ],
      "code_scores": [
        42.68,
        52.14,
        0.0,
        35.7,
        3.85,
        42.68,
        51.36,
        0.0,
        36.53,
        4.07,
        42.68,
        49.42,
        0.0,
        36.53,
        2.26
      ],
      "reasoning_scores": [
        34.24,
        60.38,
        0.31304348,
        22.32,
        30.85,
        60.26,
        0.3151087,
        23.04,
        37.63,
        60.7,
        0.31413043,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.25
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 60.45
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.53
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": -8.46,
        "math_avg": -31.19,
        "code_avg": -12.06,
        "reasoning_avg": -5.55,
        "overall_avg": -14.31,
        "general_task_scores": [
          -43.66,
          3.17,
          7.0,
          -0.33
        ],
        "math_task_scores": [
          -63.68,
          -42.73,
          -41.73,
          -6.67,
          -1.11
        ],
        "code_task_scores": [
          -32.93,
          -20.63,
          -8.24,
          35.21,
          -33.71
        ],
        "reasoning_task_scores": [
          -2.36,
          -9.01,
          -0.08,
          -10.75
        ]
      },
      "affiliation": "CodeParrot",
      "year": "2023",
      "size": "5k",
      "link": "https://huggingface.co/datasets/codeparrot/self-instruct-starcoder"
    },
    {
      "id": 41,
      "name": "Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.32,
      "math_avg": 44.84,
      "code_avg": 49.23,
      "reasoning_avg": 34.05,
      "overall_avg": 46.11,
      "general_scores": [
        74.26,
        50.47,
        57.39,
        41.7371429,
        74.82,
        51.1875,
        56.61,
        42.4564286,
        74.81,
        52.885,
        56.95,
        42.315
      ],
      "math_scores": [
        76.35,
        54.98,
        56.2,
        23.28,
        10.0,
        77.71,
        54.5,
        57.4,
        22.83,
        16.67,
        73.39,
        55.54,
        57.0,
        23.37,
        13.33
      ],
      "code_scores": [
        77.44,
        71.21,
        12.9,
        41.34,
        45.48,
        75.61,
        71.98,
        12.54,
        40.08,
        46.15,
        78.05,
        69.65,
        12.54,
        40.08,
        43.44
      ],
      "reasoning_scores": [
        33.9,
        68.0,
        0.41326087,
        33.52,
        67.95,
        0.41228261,
        33.44,
        31.53,
        67.95,
        0.41228261,
        33.44,
        38.31,
        67.84,
        0.39076087,
        33.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.63
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.17
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.03
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.5
              },
              {
                "metric": "lcb_test_output",
                "score": 45.02
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.58
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.94
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.4
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.01,
        "math_avg": -5.63,
        "code_avg": 10.51,
        "reasoning_avg": -0.89,
        "overall_avg": 4.25,
        "general_task_scores": [
          7.64,
          16.02,
          12.12,
          16.26
        ],
        "math_task_scores": [
          -11.52,
          -9.87,
          -10.53,
          -2.88,
          6.66
        ],
        "code_task_scores": [
          1.42,
          -0.65,
          4.42,
          39.46,
          7.92
        ],
        "reasoning_task_scores": [
          -2.02,
          -1.52,
          0.02,
          0.12
        ]
      },
      "affiliation": "Microsoft",
      "year": "2023",
      "size": "78.3k",
      "link": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1"
    },
    {
      "id": 42,
      "name": "code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.09,
      "math_avg": 40.41,
      "code_avg": 45.38,
      "reasoning_avg": 32.64,
      "overall_avg": 43.13,
      "general_scores": [
        65.44,
        52.59,
        55.78,
        42.89,
        63.63,
        53.985,
        55.37,
        42.6692857,
        64.56,
        53.6375,
        55.57,
        42.9792857
      ],
      "math_scores": [
        81.12,
        46.72,
        48.6,
        18.41,
        6.67,
        82.34,
        47.38,
        47.0,
        18.77,
        6.67,
        77.26,
        46.42,
        49.4,
        19.44,
        10.0
      ],
      "code_scores": [
        74.39,
        67.32,
        2.15,
        42.17,
        39.59,
        73.78,
        68.48,
        1.79,
        42.17,
        43.21,
        73.17,
        66.54,
        3.23,
        40.92,
        41.86
      ],
      "reasoning_scores": [
        36.61,
        67.06,
        0.38369565,
        30.32,
        30.51,
        67.12,
        0.3825,
        30.08,
        32.88,
        66.96,
        0.38565217,
        29.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.54
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 41.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.05
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.81
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 10.78,
        "math_avg": -10.05,
        "code_avg": 6.67,
        "reasoning_avg": -2.29,
        "overall_avg": 1.28,
        "general_task_scores": [
          -2.45,
          17.91,
          10.71,
          16.94
        ],
        "math_task_scores": [
          -7.1,
          -18.04,
          -19.07,
          -7.17,
          1.11
        ],
        "code_task_scores": [
          -1.83,
          -4.15,
          -5.85,
          40.71,
          4.45
        ],
        "reasoning_task_scores": [
          -3.27,
          -2.41,
          -0.01,
          -3.47
        ]
      },
      "affiliation": "TokenBender",
      "year": "2024",
      "size": "122k",
      "link": "https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca"
    },
    {
      "id": 43,
      "name": "python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 56.57,
      "math_avg": 46.13,
      "code_avg": 44.38,
      "reasoning_avg": 33.98,
      "overall_avg": 45.26,
      "general_scores": [
        75.1,
        50.66,
        58.01,
        42.9478571,
        74.71,
        50.6575,
        57.61,
        43.2478571,
        74.48,
        51.18,
        57.74,
        42.5057143
      ],
      "math_scores": [
        80.52,
        54.22,
        54.2,
        24.86,
        13.33,
        81.05,
        54.94,
        56.6,
        25.29,
        13.33,
        81.43,
        54.54,
        52.8,
        24.86,
        20.0
      ],
      "code_scores": [
        75.61,
        71.6,
        0.36,
        42.17,
        34.62,
        73.78,
        67.32,
        0.0,
        42.38,
        35.97,
        73.78,
        69.26,
        0.0,
        42.38,
        36.43
      ],
      "reasoning_scores": [
        35.59,
        67.93,
        0.38195652,
        32.08,
        32.88,
        68.66,
        0.38565217,
        32.32,
        35.93,
        68.61,
        0.38586957,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.12
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 35.67
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.8
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 68.4
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 32.32
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 13.26,
        "math_avg": -4.33,
        "code_avg": 5.66,
        "reasoning_avg": -0.96,
        "overall_avg": 3.41,
        "general_task_scores": [
          7.77,
          15.34,
          12.93,
          16.99
        ],
        "math_task_scores": [
          -6.34,
          -10.31,
          -12.87,
          -1.04,
          8.88
        ],
        "code_task_scores": [
          -1.22,
          -2.21,
          -8.12,
          41.27,
          -1.43
        ],
        "reasoning_task_scores": [
          -1.8,
          -1.06,
          -0.01,
          -0.96
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "18.6k",
      "link": "https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca"
    },
    {
      "id": 44,
      "name": "Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 57.46,
      "math_avg": 49.84,
      "code_avg": 44.32,
      "reasoning_avg": 35.29,
      "overall_avg": 46.73,
      "general_scores": [
        76.64,
        48.8025,
        57.65,
        46.33,
        76.22,
        50.9975,
        57.76,
        45.9364286,
        76.23,
        49.385,
        57.47,
        46.1164286
      ],
      "math_scores": [
        82.56,
        61.22,
        60.8,
        25.72,
        23.33,
        84.15,
        61.54,
        61.2,
        25.61,
        10.0,
        83.24,
        61.2,
        61.6,
        25.47,
        20.0
      ],
      "code_scores": [
        75.61,
        68.09,
        14.34,
        17.95,
        44.8,
        79.27,
        67.7,
        14.34,
        15.24,
        44.8,
        77.44,
        69.65,
        13.26,
        15.87,
        46.38
      ],
      "reasoning_scores": [
        35.93,
        67.14,
        0.3825,
        37.84,
        35.59,
        67.06,
        0.39032609,
        37.92,
        35.59,
        67.67,
        0.38032609,
        37.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.98
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.35
              },
              {
                "metric": "lcb_test_output",
                "score": 45.33
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.7
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 67.29
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 37.79
              }
            ]
          }
        ]
      },
      "improvement": {
        "general_avg": 14.15,
        "math_avg": -0.62,
        "code_avg": 5.6,
        "reasoning_avg": 0.36,
        "overall_avg": 4.87,
        "general_task_scores": [
          9.37,
          14.24,
          12.77,
          20.22
        ],
        "math_task_scores": [
          -4.02,
          -3.56,
          -6.2,
          -0.44,
          11.11
        ],
        "code_task_scores": [
          1.83,
          -3.12,
          5.74,
          15.31,
          8.23
        ],
        "reasoning_task_scores": [
          -0.9,
          -2.17,
          -0.01,
          4.51
        ]
      },
      "affiliation": "independent",
      "year": "2023",
      "size": "22.6k",
      "link": "https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT"
    }
  ]
}